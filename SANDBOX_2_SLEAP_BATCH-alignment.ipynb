{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3c57b50",
   "metadata": {},
   "source": [
    "!!! To use: make sure that SLEAP data has been analysed using the SANDBOX 1_1 notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a329aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "# this notebook SAVES halt aligned data and baselined data as CSV together with PLOTS, different compared to the previous SANDBOX_2_noSLEAP#\n",
    "#---------------------------------------------------------------------------------------------------#\n",
    "from pathlib import Path\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "import plotly.express as px\n",
    "import plotly.subplots as sp\n",
    "import math\n",
    "from pprint import pprint\n",
    "\n",
    "from matplotlib.collections import LineCollection\n",
    "import seaborn as sns\n",
    "import traceback\n",
    "import gc\n",
    "from typing import Optional, Tuple, List, Dict, Any\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy.stats import mode\n",
    "from scipy.integrate import cumulative_trapezoid\n",
    "from scipy.signal import correlate\n",
    "from scipy.signal import find_peaks\n",
    "import ipywidgets as widgets\n",
    "import ipympl\n",
    "%matplotlib widget\n",
    "\n",
    "import json\n",
    "%config Completer.use_jedi = False  # Fixes autocomplete issues\n",
    "%config InlineBackend.figure_format = 'retina'  # Improves plot resolution\n",
    "\n",
    "import gc # garbage collector for removing large variables from memory instantly \n",
    "import importlib #for force updating changed packages \n",
    "\n",
    "#import harp\n",
    "import harp_resources.process\n",
    "import harp_resources.utils\n",
    "from harp_resources import process, utils # Reassign to maintain direct references for force updating \n",
    "#from sleap import load_and_process as lp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9c5d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------\n",
    "# data paths setup\n",
    "#-------------------------------\n",
    "data_dirs = [  # Add your data directories here\n",
    "    # Path('~/RANCZLAB-NAS/data/ONIX/20250409_Cohort3_rotation/Vestibular_mismatch_day1').expanduser(),\n",
    "    Path('/home/ikharitonov/RANCZLAB-NAS/data/ONIX/20250923_Cohort6_rotation/EXP_1_fluoxetine_1').expanduser()\n",
    "    # Path('/Volumes/RanczLab2/20250409_Cohort3_rotation/Visual_mismatch_day4').expanduser()\n",
    "]\n",
    "# Collect raw data paths (excluding '_processedData' dirs)\n",
    "rawdata_paths = []\n",
    "for data_dir in data_dirs:\n",
    "    subdirs = [p for p in data_dir.iterdir() if p.is_dir() and not p.name.endswith('_processedData')]\n",
    "    rawdata_paths.extend(subdirs)  # Collect all subdirectories\n",
    "\n",
    "# Build processed data paths\n",
    "data_paths = [raw.parent / f\"{raw.name}_processedData/downsampled_data\" for raw in rawdata_paths]\n",
    "# Print data paths in a more readable format\n",
    "print(\"Processed Data Paths:\")\n",
    "pprint(data_paths)\n",
    "\n",
    "#-------------------------------\n",
    "# initial variables setup\n",
    "#-------------------------------\n",
    "time_window_start = -5  # s, FOR PLOTTING PURPOSES\n",
    "time_window_end = 10  # s, FOR PLOTTING PURPOSES\n",
    "baseline_window = (-1, 0)  # s, FOR baselining averages\n",
    "plot_width = 10\n",
    "\n",
    "event_name = \"Apply halt: 2s\"  # Apply halt: 2s, No halt, DrumWithReverseflow block started, DrumBase block started\n",
    "vestibular_mismatch = False\n",
    "common_resampled_rate = 1000  # in Hz\n",
    "plot_fig1 = False\n",
    "\n",
    "# for saccades\n",
    "framerate = 59.77  # Hz (in the future, should come from saved data)\n",
    "threshold = 65  # px/s FIXME make this adaptive\n",
    "refractory_period = pd.Timedelta(milliseconds=100)  # msec, using pd.Timedelta for datetime index\n",
    "plot_saccade_detection_QC = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caec3450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load downsampled data for each data path\n",
    "#-------------------------------\n",
    "loaded_data = {}  # Dictionary to store loaded data for each path\n",
    "\n",
    "for idx, data_path in enumerate(data_paths, start=1):\n",
    "    print(f\"\\nProcessing data path {idx}/{len(data_paths)}: {data_path}\")\n",
    "    try:\n",
    "        # Load all parquet files for this data path\n",
    "        photometry_tracking_encoder_data = pd.read_parquet(data_path / \"photometry_tracking_encoder_data.parquet\", engine=\"pyarrow\")\n",
    "        camera_photodiode_data = pd.read_parquet(data_path / \"camera_photodiode_data.parquet\", engine=\"pyarrow\")\n",
    "        experiment_events = pd.read_parquet(data_path / \"experiment_events.parquet\", engine=\"pyarrow\")\n",
    "        photometry_info = pd.read_parquet(data_path / \"photometry_info.parquet\", engine=\"pyarrow\")\n",
    "        session_settings = pd.read_parquet(data_path / \"session_settings.parquet\", engine=\"pyarrow\")\n",
    "        session_settings[\"metadata\"] = session_settings[\"metadata\"].apply(process.safe_from_json)\n",
    "        \n",
    "        print(f\"‚úÖ Successfully loaded all parquet files for {data_path.name}\")\n",
    "        \n",
    "        # Calculate time differences between event_name events\n",
    "        event_times = experiment_events[experiment_events[\"Event\"] == event_name].index\n",
    "        if len(event_times) > 1:\n",
    "            time_diffs = event_times.to_series().diff().dropna().dt.total_seconds()\n",
    "            # Print the 5 shortest time differences\n",
    "            # print(\"5 shortest time differences between events:\")\n",
    "            # print(time_diffs.nsmallest(5))\n",
    "            if (time_diffs < 10).any():\n",
    "                print(f\"‚ö†Ô∏è Warning: Some '{event_name}' events are less than 10 seconds apart. Consider applying a filter to events.\")\n",
    "        else:\n",
    "            print(f\"‚ÑπÔ∏è INFO: Found {len(event_times)} events with name '{event_name}' - not enough to calculate differences\")\n",
    "        \n",
    "        # Check experiment events and get mouse name\n",
    "        mouse_name = process.check_exp_events(experiment_events, photometry_info, verbose=True)\n",
    "#--------------------------------------------------------------------------------   \n",
    "        ##event detection\n",
    "        # def detect_events_with_peaks(signal, height, distance):\n",
    "        #     \"\"\"\n",
    "        #     Detect events in a signal using scipy's find_peaks.\n",
    "\n",
    "        #     Args:\n",
    "        #     signal (pd.Series): The signal to analyze.\n",
    "        #     height (float): Minimum height of peaks.\n",
    "        #     distance (int): Minimum number of samples between peaks.\n",
    "\n",
    "        #     Returns:\n",
    "        #     pd.DataFrame: DataFrame containing timestamps and amplitudes of detected peaks.\n",
    "        #     \"\"\"\n",
    "        #     peaks, properties = find_peaks(signal, height=height, distance=distance)\n",
    "        #     return pd.DataFrame({\n",
    "        #     \"Timestamp\": signal.index[peaks],\n",
    "        #     \"Amplitude\": signal.iloc[peaks].values\n",
    "        #     })\n",
    "\n",
    "        # # Define thresholds based on signal standard deviation\n",
    "        # z_470_height = photometry_tracking_encoder_data[\"z_470\"].mean() + 2 * photometry_tracking_encoder_data[\"z_470\"].std()\n",
    "        # z_560_height = photometry_tracking_encoder_data[\"z_560\"].mean() + 2 * photometry_tracking_encoder_data[\"z_560\"].std()\n",
    "        # min_distance = int(refractory_period.total_seconds() * common_resampled_rate)  # Convert refractory period to samples\n",
    "\n",
    "        # # Adjust min_distance to avoid detecting multiple events on the same peak\n",
    "        # min_distance = max(min_distance, int(0.5 * common_resampled_rate))  # Ensure at least 0.5 seconds between peaks\n",
    "\n",
    "        # # Detect events for z_470 and z_560 using peak detection\n",
    "        # z_470_events = detect_events_with_peaks(photometry_tracking_encoder_data[\"z_470\"], z_470_height, min_distance)\n",
    "        # z_560_events = detect_events_with_peaks(photometry_tracking_encoder_data[\"z_560\"], z_560_height, min_distance)\n",
    "#-------------------------------------------------------------------------------\n",
    "        # # Downsample the data to speed up plotting\n",
    "        # downsample_factor = 10  # Adjust this factor as needed\n",
    "        # photometry_tracking_encoder_data_downsampled = photometry_tracking_encoder_data.iloc[::downsample_factor]\n",
    "#-------------------------------------------------------------------------------\n",
    "        ##event detection plotting\n",
    "        # fig, axes = plt.subplots(2, 1, figsize=(10, 6), sharex=True)\n",
    "        # # Plot z_470 trace\n",
    "        # axes[0].plot(photometry_tracking_encoder_data.index, photometry_tracking_encoder_data[\"z_470\"], label=\"z_470\", color=\"blue\")\n",
    "        # axes[0].scatter(z_470_events[\"Timestamp\"], z_470_events[\"Amplitude\"], color=\"red\", label=\"Detected Events\", zorder=5)\n",
    "        # axes[0].set_title(\"z_470 Trace with Detected Events\")\n",
    "        # axes[0].set_ylabel(\"z_470\")\n",
    "        # axes[0].legend()\n",
    "        # # Plot z_560 trace\n",
    "        # axes[1].plot(photometry_tracking_encoder_data.index, photometry_tracking_encoder_data[\"z_560\"], label=\"z_560\", color=\"green\")\n",
    "        # axes[1].scatter(z_560_events[\"Timestamp\"], z_560_events[\"Amplitude\"], color=\"orange\", label=\"Detected Events\", zorder=5)\n",
    "        # axes[1].set_title(\"z_560 Trace with Detected Events\")\n",
    "        # axes[1].set_xlabel(\"Time\")\n",
    "        # axes[1].set_ylabel(\"z_560\")\n",
    "        # axes[1].legend()\n",
    "        # plt.tight_layout()\n",
    "        # plt.show()\n",
    "        # print(f\"Detected {len(z_470_events)} events in z_470 and {len(z_560_events)} events in z_560\")\n",
    "#-------------------------------------------------------------------------------\n",
    "        # Store all loaded data and detected events in the dictionary\n",
    "        loaded_data[data_path] = {\n",
    "            \"photometry_tracking_encoder_data\": photometry_tracking_encoder_data,\n",
    "            \"camera_photodiode_data\": camera_photodiode_data,\n",
    "            \"experiment_events\": experiment_events,\n",
    "            \"photometry_info\": photometry_info,\n",
    "            \"session_settings\": session_settings,\n",
    "            \"mouse_name\": mouse_name,\n",
    "            # \"z_470_events\": z_470_events,\n",
    "            # \"z_560_events\": z_560_events\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è ERROR processing data path {data_path}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\n‚úÖ Finished loading data for all {len(loaded_data)} successfully processed data paths\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638c9ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create DFs and plot figure for each data path\n",
    "#---------------------------------------------------\n",
    "# Dictionary to store analysis results for each data path\n",
    "data_path_variables = {}\n",
    "\n",
    "for idx, data_path in enumerate(data_paths, start=1):\n",
    "    print(f\"\\n--------- Processing analysis for data path {idx}/{len(data_paths)}: {data_path} ---------\")\n",
    "    \n",
    "    # Skip if data wasn't successfully loaded for this path\n",
    "    if data_path not in loaded_data:\n",
    "        print(f\"‚ö†Ô∏è Skipping analysis for {data_path} - data not loaded successfully\")\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        # Extract data from loaded_data dictionary\n",
    "        photometry_tracking_encoder_data = loaded_data[data_path][\"photometry_tracking_encoder_data\"]\n",
    "        camera_photodiode_data = loaded_data[data_path][\"camera_photodiode_data\"]\n",
    "        experiment_events = loaded_data[data_path][\"experiment_events\"]\n",
    "        mouse_name = loaded_data[data_path][\"mouse_name\"]\n",
    "        session_name = f\"{mouse_name}_{data_path.name}\"  # Assuming session_name is constructed this way\n",
    "        \n",
    "        # Create dataframe to analyze\n",
    "        df_to_analyze = photometry_tracking_encoder_data[\"Photodiode_int\"]  # Using downsampled values in common time grid\n",
    "        # df_to_analyze = camera_photodiode_data[\"Photodiode\"]  # Use async raw values if needed for troubleshooting\n",
    "        \n",
    "        # Determine halt times based on different conditions\n",
    "        if vestibular_mismatch or event_name == \"No halt\":  # Determine halt times based on experiment events\n",
    "            events_matching_name = experiment_events[experiment_events[\"Event\"] == event_name]\n",
    "            if events_matching_name.empty:\n",
    "                print(f\"‚ö†Ô∏è WARNING: No events found with name '{event_name}', skipping this data path\")\n",
    "                continue\n",
    "                \n",
    "            photodiode_halts = events_matching_name.index.tolist()\n",
    "            nearest_indices = photometry_tracking_encoder_data.index.get_indexer(photodiode_halts, method='nearest')\n",
    "            photodiode_halts = photometry_tracking_encoder_data.index[nearest_indices]  # Align to downsampled data time grid\n",
    "            print(f\"‚ÑπÔ∏è INFO: vestibular MM or 'No halt', no signal in the photodiode, using experiment events for MM times\")\n",
    "            photodiode_delay_min = photodiode_delay_avg = photodiode_delay_max = None\n",
    "        else:  # Determine exact halt times based on photodiode signal\n",
    "            try:\n",
    "                photodiode_halts, photodiode_delay_min, photodiode_delay_avg, photodiode_delay_max = process.analyze_photodiode(\n",
    "                    df_to_analyze, experiment_events, event_name, plot=True\n",
    "                )\n",
    "                print(f\"‚úÖ Successfully analyzed photodiode signal for {data_path.name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è ERROR analyzing photodiode signal: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        # Store analysis results\n",
    "        data_path_variables[data_path] = {\n",
    "            \"photodiode_halts\": photodiode_halts,\n",
    "            \"photodiode_delay_min\": photodiode_delay_min,\n",
    "            \"photodiode_delay_avg\": photodiode_delay_avg,\n",
    "            \"photodiode_delay_max\": photodiode_delay_max,\n",
    "            \"session_name\": session_name\n",
    "        }\n",
    "        \n",
    "        #Plot figure if requested\n",
    "        if plot_fig1:\n",
    "            try:\n",
    "                process.plot_figure_1(\n",
    "                    photometry_tracking_encoder_data, \n",
    "                    session_name, \n",
    "                    save_path, \n",
    "                    common_resampled_rate, \n",
    "                    photodiode_halts, \n",
    "                    save_figure=True, \n",
    "                    show_figure=True, \n",
    "                    downsample_factor=50\n",
    "                )\n",
    "                print(f\"‚úÖ Successfully created figure 1 for {data_path.name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è ERROR creating figure 1: {str(e)}\")\n",
    "        else:\n",
    "            print(f\"‚ÑπÔ∏è INFO: skipping figure 1 for {data_path.name}\")\n",
    "        \n",
    "        # Clean up to free memory\n",
    "        del df_to_analyze\n",
    "        gc.collect()\n",
    "        \n",
    "        print(f\"‚úÖ Completed analysis for data path: {data_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è ERROR during analysis of {data_path}: {str(e)}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Finished analyzing all {len(data_path_variables)} successfully processed data paths\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2acf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIXME BEFORE PROCEEDING WITH BATCH SACCADE DETECTION\n",
    "# NEED TO PROCESS SLEAP FOR ALL DATA IN THE FOLDER \n",
    "# 11.09.25: ADDED CODE TO CELL BELOW (ALL LINES THAT START WITH \"NEW\" - also  NEW PROCESS SESSION)\n",
    "# NEW CODE IS TO DO SACCADE DETECTION AND ALIGNMENT WITHIN THE PHOTOMETRY ANALYSER CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36d8084",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NEW separates RIGHT vs LEFT TURNS, creates heatmaps, and comprehensive (Mean +- sem) mismatch aligned data for each data path, processes saccades\n",
    "#----------------------------------------------------\n",
    "\"\"\"\n",
    "Refactored photometry analysis code for processing aligned behavioral data.\n",
    "Separates left vs right turns, creates heatmaps, and generates comprehensive plots.\n",
    "\"\"\"\n",
    "class PhotometryAnalyzer:\n",
    "    \"\"\"Class for analyzing photometry data with behavioral events.\"\"\"\n",
    "    \n",
    "    # Class constants\n",
    "    REQUIRED_COLUMNS = [\n",
    "        \"Time (s)\", \"Photodiode_int\", \"z_470\", \"z_560\", \n",
    "        \"dfF_470\", \"dfF_560\", \"Motor_Velocity\", \"Velocity_0X\", \"Velocity_0Y\"\n",
    "    ]\n",
    "    \n",
    "    FLUORESCENCE_CHANNELS = {\n",
    "        'z_470': {'color': 'cornflowerblue', 'label': 'z_470'},\n",
    "        'z_560': {'color': 'red', 'label': 'z_560'},\n",
    "        'dfF_470': {'color': 'blue', 'label': 'dfF_470'},\n",
    "        'dfF_560': {'color': 'orange', 'label': 'dfF_560'}\n",
    "    }\n",
    "    \n",
    "    def __init__(self, time_window: Tuple[float, float] = (time_window_start, time_window_end)):\n",
    "        \"\"\"\n",
    "        Initialize analyzer with time window parameters.\n",
    "        \n",
    "        Args:\n",
    "            time_window: Tuple of (start, end) times relative to event (seconds)\n",
    "        \"\"\"\n",
    "        self.time_window_start, self.time_window_end = time_window\n",
    "\n",
    "        if time_window is None:\n",
    "            time_window = (time_window_start, time_window_end)\n",
    "            self.time_window_start, self.time_window_end = time_window\n",
    "        \n",
    "    def process_aligned_data(self, df: pd.DataFrame, halt_time: pd.Timestamp) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Process a single halt event efficiently.\n",
    "        \n",
    "        Args:\n",
    "            df: Main dataframe with photometry and behavioral data\n",
    "            halt_time: Timestamp of the halt event\n",
    "            \n",
    "        Returns:\n",
    "            Windowed dataframe or None if no data in window\n",
    "        \"\"\"\n",
    "        window_start = halt_time + pd.Timedelta(seconds=self.time_window_start)\n",
    "        window_end = halt_time + pd.Timedelta(seconds=self.time_window_end)\n",
    "        mask = (df.index >= window_start) & (df.index <= window_end)\n",
    "        \n",
    "        if not mask.any():\n",
    "            return None\n",
    "        \n",
    "        window = df.loc[mask].copy()\n",
    "        window[\"Time (s)\"] = (window.index - halt_time).total_seconds()\n",
    "        window[\"Halt Time\"] = halt_time\n",
    "        return window\n",
    "    \n",
    "    def separate_turns(self, aligned_df: pd.DataFrame) -> Tuple[List[pd.Timestamp], List[pd.Timestamp]]:\n",
    "        \"\"\"\n",
    "        Separate halt events into left and right turns based on motor velocity.\n",
    "        \n",
    "        Args:\n",
    "            aligned_df: Aligned dataframe with all halt events\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (left_turn_halts, right_turn_halts)\n",
    "        \"\"\"\n",
    "        left_turn_halts = []\n",
    "        right_turn_halts = []\n",
    "        \n",
    "        for halt in aligned_df[\"Halt Time\"].unique():\n",
    "            # Look at motor velocity in pre-event window\n",
    "            subset = aligned_df[\n",
    "                (aligned_df[\"Halt Time\"] == halt) & \n",
    "                (aligned_df[\"Time (s)\"] >= -1) & \n",
    "                (aligned_df[\"Time (s)\"] < 0)\n",
    "            ]\n",
    "            \n",
    "            if subset.empty:\n",
    "                continue\n",
    "                \n",
    "            mean_velocity = subset[\"Motor_Velocity\"].mean()\n",
    "            \n",
    "            if mean_velocity < 0:  # Negative = left turn\n",
    "                left_turn_halts.append(halt)\n",
    "            elif mean_velocity > 0:  # Positive = right turn\n",
    "                right_turn_halts.append(halt)\n",
    "        \n",
    "        return left_turn_halts, right_turn_halts\n",
    "    \n",
    "    def save_turn_data(self, aligned_df: pd.DataFrame, left_turns: List, right_turns: List, \n",
    "                      session_name: str, event_name: str, output_dir: Path) -> None:\n",
    "        \"\"\"Save separated turn data to CSV files only if turns are detected.\"\"\"\n",
    "        \n",
    "        # Only save left turns if there are any\n",
    "        if left_turns:\n",
    "            left_df = aligned_df[aligned_df[\"Halt Time\"].isin(left_turns)]\n",
    "            left_file = output_dir / f\"{session_name}_{event_name}_left_turns.csv\"\n",
    "            left_df.to_csv(left_file, index=False, float_format='%.4f')\n",
    "            print(f\"Saved {len(left_turns)} left turns to {left_file}\")\n",
    "        else:\n",
    "            print(f\"No left turns detected - no CSV file saved\")\n",
    "        \n",
    "        # Only save right turns if there are any\n",
    "        if right_turns:\n",
    "            right_df = aligned_df[aligned_df[\"Halt Time\"].isin(right_turns)]\n",
    "            right_file = output_dir / f\"{session_name}_{event_name}_right_turns.csv\"\n",
    "            right_df.to_csv(right_file, index=False, float_format='%.4f')\n",
    "            print(f\"Saved {len(right_turns)} right turns to {right_file}\")\n",
    "        else:\n",
    "            print(f\"No right turns detected - no CSV file saved\")\n",
    "    \n",
    "    def create_heatmap(self, pivot_data: pd.DataFrame, session_name: str, event_name: str, \n",
    "                      channel: str, save_path: Path, figsize: Tuple[int, int] = (10, 6)) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create and save normalized heatmap.\n",
    "        \n",
    "        Args:\n",
    "            pivot_data: Pivoted data (events x time)\n",
    "            session_name: Name of session\n",
    "            event_name: Name of event type\n",
    "            channel: Channel name (e.g., 'z_470')\n",
    "            save_path: Path to save figure\n",
    "            figsize: Figure size tuple\n",
    "            \n",
    "        Returns:\n",
    "            Normalized data used for heatmap\n",
    "        \"\"\"\n",
    "        # Baseline normalization\n",
    "        baseline_cols = (pivot_data.columns >= -1) & (pivot_data.columns < 0)\n",
    "        if baseline_cols.any():\n",
    "            baseline_means = pivot_data.loc[:, baseline_cols].mean(axis=1)\n",
    "            normalized_data = pivot_data.subtract(baseline_means, axis=0)\n",
    "        else:\n",
    "            normalized_data = pivot_data\n",
    "        \n",
    "        # Create figure\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        \n",
    "        sns.heatmap(\n",
    "            normalized_data, \n",
    "            cmap=\"RdBu_r\", \n",
    "            center=0, \n",
    "            ax=ax,\n",
    "            cbar_kws={'label': f'Normalized {channel}'},\n",
    "            rasterized=True\n",
    "        )\n",
    "        \n",
    "        ax.set_title(f\"Heatmap ({channel}) - {session_name}\")\n",
    "        ax.set_xlabel(\"Time (s)\")\n",
    "        ax.set_ylabel(\"Event\")\n",
    "        \n",
    "        # Optimize y-axis ticks\n",
    "        n_events = len(normalized_data.index)\n",
    "        y_positions = range(0, n_events, max(1, n_events // 10))\n",
    "        y_labels = [str(i+1) for i in y_positions]\n",
    "        ax.set_yticks(y_positions)\n",
    "        ax.set_yticklabels(y_labels)\n",
    "        \n",
    "        # Add event line at time 0\n",
    "        if 0 in normalized_data.columns:\n",
    "            zero_idx = list(normalized_data.columns).index(0)\n",
    "            ax.axvline(zero_idx, linestyle='--', color='black', alpha=0.7)\n",
    "        \n",
    "        # Optimize x-axis ticks\n",
    "        time_cols = normalized_data.columns\n",
    "        tick_indices = [i for i, val in enumerate(time_cols) \n",
    "                       if isinstance(val, (int, float)) and val % 2 == 0]\n",
    "        tick_labels = [f\"{int(time_cols[i])}\" for i in tick_indices]\n",
    "        \n",
    "        ax.set_xticks(tick_indices)\n",
    "        ax.set_xticklabels(tick_labels, rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path, dpi=200, bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "        \n",
    "        return normalized_data\n",
    "    \n",
    "    def create_line_collections(self, df: pd.DataFrame, channels: List[str]) -> Dict[str, LineCollection]:\n",
    "        \"\"\"Create line collections for individual traces.\"\"\"\n",
    "        line_collections = {}\n",
    "        \n",
    "        for channel in channels:\n",
    "            lines = []\n",
    "            for halt in df[\"Halt Time\"].unique():\n",
    "                subset = df[df[\"Halt Time\"] == halt]\n",
    "                time_vals = subset[\"Time (s)\"].values\n",
    "                channel_vals = subset[channel].values\n",
    "                lines.append(list(zip(time_vals, channel_vals)))\n",
    "            \n",
    "            color = self.FLUORESCENCE_CHANNELS.get(channel, {}).get('color', 'gray')\n",
    "            line_collections[channel] = LineCollection(lines, colors=color, alpha=0.3, linewidths=1)\n",
    "        \n",
    "        return line_collections\n",
    "    \n",
    "    def add_mean_sem_plot(self, ax: plt.Axes, df: pd.DataFrame, channels: List[str], \n",
    "                         turn_type: str, line_style: str = '-') -> None:\n",
    "        \"\"\"Add mean ¬± SEM traces to axis.\"\"\"\n",
    "        grouped = df.groupby(\"Time (s)\")\n",
    "        time_index = grouped.mean().index.values\n",
    "        \n",
    "        for channel in channels:\n",
    "            color = self.FLUORESCENCE_CHANNELS[channel]['color']\n",
    "            label = f\"{turn_type} {channel}\"\n",
    "            \n",
    "            mean_vals = grouped.mean()[channel]\n",
    "            sem_vals = grouped.sem()[channel]\n",
    "            \n",
    "            ax.plot(time_index, mean_vals, color=color, linestyle=line_style, \n",
    "                   linewidth=2, label=label)\n",
    "            ax.fill_between(time_index, mean_vals - sem_vals, mean_vals + sem_vals,\n",
    "                           color=color, alpha=0.2)\n",
    "    \n",
    "    def create_summary_plot(self, aligned_df: pd.DataFrame, session_name: str, \n",
    "                          event_name: str, save_path: Path) -> None:\n",
    "        \"\"\"\n",
    "        Create comprehensive summary plots comparing left vs right turns.\n",
    "        \n",
    "        Args:\n",
    "            aligned_df: Aligned dataframe with all events\n",
    "            session_name: Session identifier\n",
    "            event_name: Event type name\n",
    "            save_path: Path to save the plot\n",
    "        \"\"\"\n",
    "        # Separate turns\n",
    "        left_turns, right_turns = self.separate_turns(aligned_df)\n",
    "        \n",
    "        if not left_turns and not right_turns:\n",
    "            print(\"No valid turns found for summary plot\")\n",
    "            return\n",
    "        \n",
    "        # Create DataFrames for each turn type\n",
    "        left_df = aligned_df[aligned_df[\"Halt Time\"].isin(left_turns)]\n",
    "        right_df = aligned_df[aligned_df[\"Halt Time\"].isin(right_turns)]\n",
    "        \n",
    "        # Create figure\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(21, 6), sharex=True)\n",
    "        z_channels = ['z_470', 'z_560']\n",
    "        \n",
    "        # Left plot - Left turn traces\n",
    "        if not left_df.empty:\n",
    "            ax1 = axes[0]\n",
    "            left_collections = self.create_line_collections(left_df, z_channels + ['Motor_Velocity'])\n",
    "            \n",
    "            # Add fluorescence traces\n",
    "            for channel in z_channels:\n",
    "                ax1.add_collection(left_collections[channel])\n",
    "            \n",
    "            # Add motor velocity on secondary axis\n",
    "            ax1_motor = ax1.twinx()\n",
    "            ax1_motor.add_collection(left_collections['Motor_Velocity'])\n",
    "            ax1_motor.set_ylabel(\"Motor Velocity\", color='slategray')\n",
    "            ax1_motor.tick_params(axis='y', labelcolor='slategray')\n",
    "            ax1_motor.autoscale()\n",
    "            \n",
    "            ax1.set_title(f'Left Turn Traces (n={len(left_turns)})')\n",
    "            ax1.set_ylabel(\"Fluorescence (z-score)\")\n",
    "            ax1.axvline(0, linestyle='--', color='black', alpha=0.7)\n",
    "            ax1.autoscale()\n",
    "        \n",
    "        # Right plot - Right turn traces  \n",
    "        if not right_df.empty:\n",
    "            ax2 = axes[1]\n",
    "            right_collections = self.create_line_collections(right_df, z_channels + ['Motor_Velocity'])\n",
    "            \n",
    "            # Add fluorescence traces\n",
    "            for channel in z_channels:\n",
    "                ax2.add_collection(right_collections[channel])\n",
    "            \n",
    "            # Add motor velocity on secondary axis\n",
    "            ax2_motor = ax2.twinx()\n",
    "            ax2_motor.add_collection(right_collections['Motor_Velocity'])\n",
    "            ax2_motor.set_ylabel(\"Motor Velocity\", color='slategray')\n",
    "            ax2_motor.tick_params(axis='y', labelcolor='slategray')\n",
    "            ax2_motor.autoscale()\n",
    "            \n",
    "            ax2.set_title(f'Right Turn Traces (n={len(right_turns)})')\n",
    "            ax2.set_ylabel(\"Fluorescence (z-score)\")\n",
    "            ax2.axvline(0, linestyle='--', color='black', alpha=0.7)\n",
    "            ax2.autoscale()\n",
    "        \n",
    "        # Comparison plot - Mean ¬± SEM\n",
    "        ax3 = axes[2]\n",
    "        \n",
    "        if not left_df.empty:\n",
    "            self.add_mean_sem_plot(ax3, left_df, z_channels, \"Left\", '--')\n",
    "        if not right_df.empty:\n",
    "            self.add_mean_sem_plot(ax3, right_df, z_channels, \"Right\", '-')\n",
    "        \n",
    "        # Add motor velocity comparison\n",
    "        ax3_motor = ax3.twinx()\n",
    "        motor_color = 'slategray'\n",
    "        \n",
    "        if not left_df.empty:\n",
    "            left_motor_grouped = left_df.groupby(\"Time (s)\")\n",
    "            time_idx = left_motor_grouped.mean().index.values\n",
    "            mean_motor = left_motor_grouped.mean()[\"Motor_Velocity\"]\n",
    "            sem_motor = left_motor_grouped.sem()[\"Motor_Velocity\"]\n",
    "            \n",
    "            ax3_motor.plot(time_idx, mean_motor, color=motor_color, linestyle='--', \n",
    "                          linewidth=1.5, label=\"Left Motor\")\n",
    "            ax3_motor.fill_between(time_idx, mean_motor - sem_motor, mean_motor + sem_motor,\n",
    "                                  color=motor_color, alpha=0.2)\n",
    "        \n",
    "        if not right_df.empty:\n",
    "            right_motor_grouped = right_df.groupby(\"Time (s)\")\n",
    "            time_idx = right_motor_grouped.mean().index.values\n",
    "            mean_motor = right_motor_grouped.mean()[\"Motor_Velocity\"]\n",
    "            sem_motor = right_motor_grouped.sem()[\"Motor_Velocity\"]\n",
    "            \n",
    "            ax3_motor.plot(time_idx, mean_motor, color=motor_color, linestyle='-',\n",
    "                          linewidth=1.5, label=\"Right Motor\")\n",
    "            ax3_motor.fill_between(time_idx, mean_motor - sem_motor, mean_motor + sem_motor,\n",
    "                                  color=motor_color, alpha=0.2)\n",
    "        \n",
    "        ax3_motor.set_ylabel(\"Motor Velocity\", color=motor_color)\n",
    "        ax3_motor.tick_params(axis='y', labelcolor=motor_color)\n",
    "        ax3_motor.axhline(0, linestyle='--', color='gray', alpha=0.5)\n",
    "        ax3_motor.legend(loc='upper right')\n",
    "        \n",
    "        ax3.axvline(0, linestyle='--', color='black', alpha=0.7)\n",
    "        ax3.set_xlabel(\"Time (s)\")\n",
    "        ax3.set_ylabel(\"Fluorescence (z-score)\")\n",
    "        ax3.set_title(\"Mean ¬± SEM Comparison\")\n",
    "        ax3.legend(loc='upper left')\n",
    "        \n",
    "        # Format all x-axes\n",
    "        for ax in axes:\n",
    "            ax.set_xlabel(\"Time (s)\")\n",
    "        \n",
    "        fig.suptitle(f\"{session_name} - {event_name}\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path, dpi=200, bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "        plt.ioff()\n",
    "    \n",
    "\n",
    "        #>> NEW MODIFIED process_session <<<\n",
    "    def process_session(self, data_path: Path, data: Dict[str, Any], \n",
    "                       variables: Dict[str, Any], event_name: str = \"halt\") -> None:\n",
    "        print(f\"\\n--------- Processing: {data_path} ---------\")\n",
    "\n",
    "        try:\n",
    "            df = data[\"photometry_tracking_encoder_data\"]\n",
    "            halts = variables[\"photodiode_halts\"]\n",
    "            session_name = variables.get(\"session_name\", f\"{data.get('mouse_name','unknown')}_{data_path.stem}\")\n",
    "\n",
    "            aligned_data = []\n",
    "            for halt_time in halts:\n",
    "                window_data = self.process_aligned_data(df, halt_time)\n",
    "                if window_data is not None:\n",
    "                    aligned_data.append(window_data)\n",
    "\n",
    "            if not aligned_data:\n",
    "                print(f\"No aligned data for {session_name}, skipping\")\n",
    "                return\n",
    "\n",
    "            aligned_df = pd.concat(aligned_data, ignore_index=True)\n",
    "\n",
    "            aligned_dir = data_path.parent / \"aligned_data\"\n",
    "            aligned_dir.mkdir(exist_ok=True)\n",
    "\n",
    "            # Save aligned data\n",
    "            aligned_file = aligned_dir / f\"{session_name}_{event_name}_aligned.csv\"\n",
    "            aligned_df.to_csv(aligned_file, index=False, float_format='%.4f')\n",
    "\n",
    "            # >>> NEW: Run saccade detection <<<\n",
    "            sac_df = self._smooth_and_velocity(aligned_df)\n",
    "            sac_df = self._detect_saccades(sac_df)\n",
    "\n",
    "            sac_pos = self._summarize_saccades(sac_df, 'sac_pos_id', 'pos')\n",
    "            sac_neg = self._summarize_saccades(sac_df, 'sac_neg_id', 'neg')\n",
    "            saccades = pd.concat([sac_pos, sac_neg], ignore_index=True)\n",
    "\n",
    "            filtered_saccades = self._apply_refractory(saccades)\n",
    "\n",
    "            sac_file = aligned_dir / f\"{session_name}_{event_name}_saccades.csv\"\n",
    "            filtered_saccades.to_csv(sac_file, index=False, float_format='%.4f')\n",
    "            print(f\"Saved saccades to {sac_file} ({len(filtered_saccades)} total)\")\n",
    "\n",
    "            # Continue with existing turn separation + plots\n",
    "            left_turns, right_turns = self.separate_turns(aligned_df)\n",
    "            self.save_turn_data(aligned_df, left_turns, right_turns, session_name, event_name, aligned_dir)\n",
    "\n",
    "            summary_path = data_path.parent / f\"{session_name}_{event_name}.pdf\"\n",
    "            self.create_summary_plot(aligned_df, session_name, event_name, summary_path)\n",
    "\n",
    "            if aligned_df[\"Halt Time\"].nunique() > 1:\n",
    "                self._create_all_heatmaps(aligned_df, session_name, event_name, data_path)\n",
    "\n",
    "            del aligned_data, aligned_df\n",
    "            gc.collect()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR processing {data_path}: {str(e)}\")\n",
    "            traceback.print_exc()\n",
    "            gc.collect()\n",
    "    \n",
    "    def _create_all_heatmaps(self, aligned_df: pd.DataFrame, session_name: str, \n",
    "                           event_name: str, data_path: Path) -> None:\n",
    "        \"\"\"Create all heatmaps for different channels.\"\"\"\n",
    "        heatmap_channels = ['z_470', 'z_560', 'dfF_470', 'dfF_560']\n",
    "        \n",
    "        for channel in heatmap_channels:\n",
    "            try:\n",
    "                # Create pivot table\n",
    "                pivot_data = aligned_df.pivot_table(\n",
    "                    index=\"Halt Time\", \n",
    "                    columns=\"Time (s)\", \n",
    "                    values=channel, \n",
    "                    aggfunc='first'\n",
    "                )\n",
    "                \n",
    "                # Create heatmap\n",
    "                heatmap_path = data_path.parent / f\"{session_name}_{event_name}_heatmap_{channel}.pdf\"\n",
    "                self.create_heatmap(pivot_data, session_name, event_name, channel, heatmap_path)\n",
    "                print(f\"Saved {channel} heatmap\")\n",
    "                \n",
    "                # Cleanup\n",
    "                del pivot_data\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error creating {channel} heatmap: {e}\")\n",
    "        \n",
    "        gc.collect()\n",
    "\n",
    "    # >>> NEW <<<\n",
    "    def _smooth_and_velocity(self, aligned_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Smooth eye X position and compute velocity.\"\"\"\n",
    "        df = aligned_df.copy()\n",
    "        df['X_smooth'] = (\n",
    "            df['Ellipse.Center.X_eye1']\n",
    "              .rolling(window=5, center=True)\n",
    "              .median()\n",
    "              .bfill()\n",
    "              .ffill()\n",
    "        )\n",
    "        df['dt'] = df['Time (s)'].diff()\n",
    "        df['vel_x'] = df['X_smooth'].diff() / df['dt']\n",
    "        return df\n",
    "\n",
    "    # >>> NEW <<<\n",
    "    def _detect_saccades(self, df: pd.DataFrame, k: float = 3) -> pd.DataFrame:\n",
    "        \"\"\"Detect saccades using velocity thresholding.\"\"\"\n",
    "        abs_vel = df['vel_x'].abs().dropna()\n",
    "        vel_thresh = abs_vel.mean() + k * abs_vel.std()\n",
    "\n",
    "        df['is_sac_pos'] = df['vel_x'] > vel_thresh\n",
    "        df['is_sac_neg'] = df['vel_x'] < -vel_thresh\n",
    "\n",
    "        df['sac_pos_id'] = (\n",
    "            (df['is_sac_pos'] & ~df['is_sac_pos'].shift(fill_value=False))\n",
    "            .cumsum()\n",
    "            .where(df['is_sac_pos'], 0)\n",
    "        )\n",
    "        df['sac_neg_id'] = (\n",
    "            (df['is_sac_neg'] & ~df['is_sac_neg'].shift(fill_value=False))\n",
    "            .cumsum()\n",
    "            .where(df['is_sac_neg'], 0)\n",
    "        )\n",
    "        return df\n",
    "\n",
    "    # >>> NEW <<<\n",
    "    def _summarize_saccades(self, df: pd.DataFrame, col_id: str, direction: str) -> pd.DataFrame:\n",
    "        return (\n",
    "            df[df[col_id] > 0]\n",
    "            .groupby(['Halt Time', col_id], as_index=False)\n",
    "            .agg(\n",
    "                start_time=('Time (s)', 'first'),\n",
    "                end_time=('Time (s)', 'last'),\n",
    "                peak_vel=('vel_x', lambda x: x.max() if direction == 'pos' else x.min()),\n",
    "                amplitude=('X_smooth', lambda x: (x.max()-x.min()) if direction == 'pos' else (x.min()-x.max()))\n",
    "            )\n",
    "            .assign(direction=direction)\n",
    "        )\n",
    "\n",
    "    # >>> NEW <<<\n",
    "    def _apply_refractory(self, saccades: pd.DataFrame, refractory: float = 0.100) -> pd.DataFrame:\n",
    "        def keep_with_refractory(group):\n",
    "            last_end = -float('inf')\n",
    "            kept = []\n",
    "            for _, row in group.sort_values('start_time').iterrows():\n",
    "                if row['start_time'] - last_end >= refractory:\n",
    "                    kept.append(row.to_dict())\n",
    "                    last_end = row['end_time']\n",
    "            return pd.DataFrame(kept)\n",
    "\n",
    "        return (\n",
    "            saccades\n",
    "            .sort_values(['Halt Time', 'start_time'])\n",
    "            .groupby('Halt Time', group_keys=False)\n",
    "            .apply(keep_with_refractory)\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "\n",
    "def main(data_paths: List[Path], loaded_data: Dict, data_path_variables: Dict, \n",
    "         event_name: str = \"halt\", time_window: Tuple[float, float] = (-5, 10)):\n",
    "    \"\"\"\n",
    "    Main processing function.\n",
    "    \n",
    "    Args:\n",
    "        data_paths: List of data directory paths\n",
    "        loaded_data: Dictionary of loaded data for each path\n",
    "        data_path_variables: Dictionary of analysis variables for each path\n",
    "        event_name: Name of event type (default: \"halt\")\n",
    "        time_window: Tuple of (start, end) times relative to event in seconds\n",
    "    \"\"\"\n",
    "    # Initialize analyzer\n",
    "    analyzer = PhotometryAnalyzer(time_window)\n",
    "    \n",
    "    # Process each data path\n",
    "    for idx, data_path in enumerate(data_paths, start=1):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Processing {idx}/{len(data_paths)}: {data_path.name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        if data_path not in data_path_variables:\n",
    "            print(f\"Skipping {data_path} - no analysis data found\")\n",
    "            continue\n",
    "        \n",
    "        if data_path not in loaded_data:\n",
    "            print(f\"Skipping {data_path} - no loaded data found\")\n",
    "            continue\n",
    "        \n",
    "        # Process this session\n",
    "        analyzer.process_session(\n",
    "            data_path, \n",
    "            loaded_data[data_path], \n",
    "            data_path_variables[data_path], \n",
    "            event_name\n",
    "        )\n",
    "    \n",
    "    print(\"\\n‚úÖ Finished processing all data paths.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de826ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_window = (time_window_start, time_window_end) \n",
    "main(data_paths, loaded_data, data_path_variables, \n",
    "     event_name=event_name, time_window=time_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda60d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASELINING\n",
    "#----------------------------------------------------\n",
    "def process_aligned_data_folders(data_dirs, baseline_window, event_name=event_name, plot_width=12, create_plots=True):\n",
    "    \"\"\"\n",
    "    Process all aligned_data folders and generate baseline plots.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data_dirs : list\n",
    "        List of Path objects pointing to your main data directories\n",
    "    baseline_window : tuple\n",
    "        Tuple of (start_time, end_time) for baseline window\n",
    "    event_name : str\n",
    "        Event name for file naming (default: \"halt\")\n",
    "    plot_width : int\n",
    "        Width of the plot in inches\n",
    "    create_plots : bool\n",
    "        Whether to create and save plots (default: True)\n",
    "    \"\"\"\n",
    "    \n",
    "    results = {\n",
    "        'processed': [],\n",
    "        'errors': [],\n",
    "        'total_folders': 0\n",
    "    }\n",
    "    \n",
    "    # Find all aligned_data folders\n",
    "    aligned_folders = []\n",
    "    for data_dir in data_dirs:\n",
    "        print(f\"Searching in: {data_dir}\")\n",
    "        # Find all aligned_data folders recursively\n",
    "        found_folders = list(data_dir.rglob(\"aligned_data\"))\n",
    "        aligned_folders.extend(found_folders)\n",
    "        print(f\"  Found {len(found_folders)} aligned_data folders\")\n",
    "    \n",
    "    results['total_folders'] = len(aligned_folders)\n",
    "    print(f\"\\nTotal aligned_data folders found: {len(aligned_folders)}\")\n",
    "    \n",
    "    for aligned_folder in aligned_folders:\n",
    "        try:\n",
    "            print(f\"\\nüìÅ Processing folder: {aligned_folder}\")\n",
    "            \n",
    "            # Find only the original aligned CSV files (exclude already processed baselined files and turn files)\n",
    "            all_csv_files = list(aligned_folder.glob(\"*.csv\"))\n",
    "            csv_files = [f for f in all_csv_files if not f.name.endswith('_baselined_data.csv') \n",
    "                        and not f.name.endswith('_left_turns.csv') \n",
    "                        and not f.name.endswith('_right_turns.csv')]\n",
    "            \n",
    "            if not csv_files:\n",
    "                print(f\"  ‚ö†Ô∏è  No original aligned CSV files found in {aligned_folder}\")\n",
    "                print(f\"  Available files: {[f.name for f in all_csv_files]}\")\n",
    "                results['errors'].append({\n",
    "                    'folder': str(aligned_folder),\n",
    "                    'error': 'No original aligned CSV files found',\n",
    "                    'status': 'skipped'\n",
    "                })\n",
    "                continue\n",
    "            print(f\"  Found {len(csv_files)} aligned CSV files to process\")\n",
    "            \n",
    "            for csv_file in csv_files:\n",
    "                try:\n",
    "                    # Check if the CSV file name matches the event name\n",
    "                    if event_name not in csv_file.name:\n",
    "                        print(f\"    ‚ö†Ô∏è Skipping {csv_file.name} as it does not match the event name '{event_name}'\")\n",
    "                        continue\n",
    "                    \n",
    "                    print(f\"    üìä Processing: {csv_file.name}\")\n",
    "                    \n",
    "                    # Load the data\n",
    "                    aligned_df = pd.read_csv(csv_file)\n",
    "                    \n",
    "                    # Create aligned DataFrames for left and right turns\n",
    "                    # Replace '_aligned' with '_left_turns' and '_right_turns'\n",
    "                    left_turns_csv = csv_file.with_name(csv_file.stem.replace('_aligned', '_left_turns') + \".csv\")\n",
    "                    right_turns_csv = csv_file.with_name(csv_file.stem.replace('_aligned', '_right_turns') + \".csv\")\n",
    "                    \n",
    "                    left_turns_df = None\n",
    "                    right_turns_df = None\n",
    "                    \n",
    "                    if left_turns_csv.exists():\n",
    "                        print(f\"    üìÇ Found left turns CSV: {left_turns_csv.name}\")\n",
    "                        left_turns_df = pd.read_csv(left_turns_csv)\n",
    "                    else:\n",
    "                        print(f\"    ‚ö†Ô∏è Left turns CSV not found: {left_turns_csv.name}\")\n",
    "                    \n",
    "                    if right_turns_csv.exists():\n",
    "                        print(f\"    üìÇ Found right turns CSV: {right_turns_csv.name}\")\n",
    "                        right_turns_df = pd.read_csv(right_turns_csv)\n",
    "                    else:\n",
    "                        print(f\"    ‚ö†Ô∏è Right turns CSV not found: {right_turns_csv.name}\")\n",
    "                    \n",
    "                    # Clean up the mouse name (remove extra suffixes)\n",
    "                    mouse_name = csv_file.stem.replace('_aligned', '').replace('_downsampled_data_Apply halt: 2s', '').split('_')[0]\n",
    "                    # Get session name from the folder structure\n",
    "                    session_name = aligned_folder.parent.name\n",
    "                    \n",
    "                    # Check if required columns exist\n",
    "                    required_columns = [\"Time (s)\", \"Halt Time\", \"z_470\", \"z_560\", \"Motor_Velocity\", \n",
    "                                      \"Velocity_0X\", \"Velocity_0Y\", \"Photodiode_int\"]\n",
    "                    missing_columns = [col for col in required_columns if col not in aligned_df.columns]\n",
    "                    \n",
    "                    if missing_columns:\n",
    "                        print(f\"    ‚ö†Ô∏è  Missing columns: {missing_columns}\")\n",
    "                        print(f\"    Available columns: {list(aligned_df.columns)}\")\n",
    "                        raise ValueError(f\"Missing required columns: {missing_columns}\")\n",
    "                    \n",
    "                    # Process the data and create plot\n",
    "                    fig = baseline_aligned_data_simple(\n",
    "                        aligned_df=aligned_df,\n",
    "                        left_turns_df=left_turns_df,\n",
    "                        right_turns_df=right_turns_df,\n",
    "                        baseline_window=baseline_window,\n",
    "                        mouse_name=mouse_name,\n",
    "                        session_name=session_name,\n",
    "                        event_name=event_name,\n",
    "                        output_folder=aligned_folder,\n",
    "                        csv_file=csv_file,\n",
    "                        plot_width=plot_width,\n",
    "                        create_plots=create_plots\n",
    "                    )\n",
    "                    \n",
    "                    results['processed'].append({\n",
    "                        'file': str(csv_file),\n",
    "                        'mouse_name': mouse_name,\n",
    "                        'session_name': session_name,\n",
    "                        'folder': str(aligned_folder),\n",
    "                        'status': 'success'\n",
    "                    })\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    error_info = {\n",
    "                        'file': str(csv_file),\n",
    "                        'error': str(e),\n",
    "                        'status': 'failed'\n",
    "                    }\n",
    "                    results['errors'].append(error_info)\n",
    "                    print(f\"    ‚ùå Error processing {csv_file.name}: {str(e)}\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            error_info = {\n",
    "                'folder': str(aligned_folder),\n",
    "                'error': str(e),\n",
    "                'status': 'failed'\n",
    "            }\n",
    "            results['errors'].append(error_info)\n",
    "            print(f\"‚ùå Error accessing {aligned_folder}: {str(e)}\")\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"PROCESSING SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Total aligned_data folders: {results['total_folders']}\")\n",
    "    print(f\"Successfully processed files: {len(results['processed'])}\")\n",
    "    print(f\"Errors encountered: {len(results['errors'])}\")\n",
    "    \n",
    "    if results['errors']:\n",
    "        print(f\"\\nErrors:\")\n",
    "        for error in results['errors']:\n",
    "            if 'file' in error:\n",
    "                print(f\"  - File {Path(error['file']).name}: {error['error']}\")\n",
    "            else:\n",
    "                print(f\"  - Folder {Path(error['folder']).name}: {error['error']}\")\n",
    "    \n",
    "    if results['processed']:\n",
    "        print(f\"\\nSuccessfully processed:\")\n",
    "        for proc in results['processed']:\n",
    "            print(f\"  - {proc['mouse_name']} in {Path(proc['folder']).parent.name}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def baseline_aligned_data_simple(aligned_df, left_turns_df, right_turns_df, baseline_window, mouse_name, session_name, event_name, output_folder, csv_file, plot_width=12, create_plots=True):\n",
    "    \"\"\"\n",
    "    Simple baseline correction and plotting function.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    aligned_df : pd.DataFrame\n",
    "        Main aligned data\n",
    "    left_turns_df : pd.DataFrame or None\n",
    "        Left turns data\n",
    "    right_turns_df : pd.DataFrame or None\n",
    "        Right turns data\n",
    "    baseline_window : tuple\n",
    "        Tuple of (start_time, end_time) for baseline window\n",
    "    mouse_name : str\n",
    "        Mouse name for file naming\n",
    "    session_name : str\n",
    "        Session name for file naming\n",
    "    event_name : str\n",
    "        Event name for file naming\n",
    "    output_folder : Path\n",
    "        Output folder path\n",
    "    csv_file : Path\n",
    "        Original CSV file path\n",
    "    plot_width : int\n",
    "        Width of the plot in inches\n",
    "    create_plots : bool\n",
    "        Whether to create and save plots (default: True)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"      üîÑ Performing baseline correction...\")\n",
    "\n",
    "    def baseline_dataframe(df, baseline_window, mouse_name, event_name, output_folder, suffix=\"\"):\n",
    "        \"\"\"Helper function to baseline a single dataframe\"\"\"\n",
    "        # Make a copy to avoid modifying the original data\n",
    "        df_copy = df.copy()\n",
    "        \n",
    "        # Calculate baseline values\n",
    "        baseline_df = df_copy[\n",
    "            (df_copy[\"Time (s)\"] >= baseline_window[0]) & \n",
    "            (df_copy[\"Time (s)\"] <= baseline_window[1])\n",
    "        ].groupby(\"Halt Time\").mean(numeric_only=True)\n",
    "        \n",
    "        # Create baseline-corrected columns\n",
    "        for signal_name in [\"z_470\", \"z_560\", \"Motor_Velocity\", \"Velocity_0X\", \"Velocity_0Y\"]:\n",
    "            if signal_name in df_copy.columns:\n",
    "                df_copy[f\"{signal_name}_Baseline\"] = df_copy[signal_name] - df_copy[\"Halt Time\"].map(baseline_df[signal_name])\n",
    "            else:\n",
    "                print(f\"      ‚ö†Ô∏è  Column {signal_name} not found in {suffix} data, skipping...\")\n",
    "        \n",
    "        # Define the baseline data file path\n",
    "        if suffix:\n",
    "            baseline_data_file = output_folder / f\"{mouse_name}_{event_name}_{suffix}_baselined_data.csv\"\n",
    "        else:\n",
    "            baseline_data_file = output_folder / f\"{mouse_name}_{event_name}_baselined_data.csv\"\n",
    "        \n",
    "        # Save the baseline-corrected data\n",
    "        df_copy.to_csv(baseline_data_file, index=False)\n",
    "        print(f\"      üíæ Saved {suffix} baseline data to: {baseline_data_file.name}\")\n",
    "        \n",
    "        return df_copy\n",
    "\n",
    "    # ---------------- Baseline Correction ----------------\n",
    "    # Process main aligned data\n",
    "    aligned_df_baselined = baseline_dataframe(aligned_df, baseline_window, mouse_name, event_name, output_folder)\n",
    "    \n",
    "    # Process left turns data if available\n",
    "    left_turns_df_baselined = None\n",
    "    if left_turns_df is not None:\n",
    "        print(f\"      üîÑ Processing left turns data...\")\n",
    "        left_turns_df_baselined = baseline_dataframe(left_turns_df, baseline_window, mouse_name, event_name, output_folder, \"left_turns\")\n",
    "    \n",
    "    # Process right turns data if available\n",
    "    right_turns_df_baselined = None\n",
    "    if right_turns_df is not None:\n",
    "        print(f\"      üîÑ Processing right turns data...\")\n",
    "        right_turns_df_baselined = baseline_dataframe(right_turns_df, baseline_window, mouse_name, event_name, output_folder, \"right_turns\")\n",
    "\n",
    "    # ---------------- Mean and SEM for plotting (using main aligned data) ----------------\n",
    "    # Select only numeric columns for aggregation\n",
    "    numeric_columns = aligned_df_baselined.select_dtypes(include=['number']).columns\n",
    "    mean_baseline_df = aligned_df_baselined.groupby(\"Time (s)\")[numeric_columns].mean()\n",
    "    sem_baseline_df = aligned_df_baselined.groupby(\"Time (s)\")[numeric_columns].sem()\n",
    "\n",
    "    def get_symmetric_ylim(mean_data, sem_data):\n",
    "        max_abs_value = max(\n",
    "            abs(mean_data).max() + sem_data.max(),\n",
    "            abs(mean_data).min() - sem_data.min()\n",
    "        )\n",
    "        return (-max_abs_value, max_abs_value)\n",
    "\n",
    "    if create_plots:\n",
    "        print(f\"      üìä Creating plot...\")\n",
    "\n",
    "        # ---------------- Plotting ----------------\n",
    "        fig, ax = plt.subplots(figsize=(plot_width, 6))\n",
    "\n",
    "        # Photodiode\n",
    "        ax.plot(mean_baseline_df.index, mean_baseline_df[\"Photodiode_int\"], color='grey', alpha=0.8, linewidth=2)\n",
    "        ax.fill_between(mean_baseline_df.index,\n",
    "                        mean_baseline_df[\"Photodiode_int\"] - sem_baseline_df[\"Photodiode_int\"],\n",
    "                        mean_baseline_df[\"Photodiode_int\"] + sem_baseline_df[\"Photodiode_int\"],\n",
    "                        color='grey', alpha=0.2)\n",
    "\n",
    "        ax.set_xlabel('Time (s) relative to halt')\n",
    "        ax.set_ylabel('Photodiode', color='grey')\n",
    "        ax.set_title(f'Baselined Signals - {mouse_name} ({session_name})')\n",
    "\n",
    "        # z_470 and z_560 (Fluorescence)\n",
    "        ax2 = ax.twinx()\n",
    "        ax2.plot(mean_baseline_df.index, mean_baseline_df[\"z_470_Baseline\"], color='green', alpha=0.8, linewidth=2, label='470nm')\n",
    "        ax2.fill_between(mean_baseline_df.index,\n",
    "                         mean_baseline_df[\"z_470_Baseline\"] - sem_baseline_df[\"z_470_Baseline\"],\n",
    "                         mean_baseline_df[\"z_470_Baseline\"] + sem_baseline_df[\"z_470_Baseline\"],\n",
    "                         color='green', alpha=0.2)\n",
    "        ax2.plot(mean_baseline_df.index, mean_baseline_df[\"z_560_Baseline\"], color='red', alpha=0.8, linewidth=2, label='560nm')\n",
    "        ax2.fill_between(mean_baseline_df.index,\n",
    "                         mean_baseline_df[\"z_560_Baseline\"] - sem_baseline_df[\"z_560_Baseline\"],\n",
    "                         mean_baseline_df[\"z_560_Baseline\"] + sem_baseline_df[\"z_560_Baseline\"],\n",
    "                         color='red', alpha=0.2)\n",
    "        ax2.set_ylabel('Fluorescence (z-score)', color='green')\n",
    "        ax2.set_ylim(get_symmetric_ylim(\n",
    "            pd.concat([mean_baseline_df[\"z_470_Baseline\"], mean_baseline_df[\"z_560_Baseline\"]]),\n",
    "            pd.concat([sem_baseline_df[\"z_470_Baseline\"], sem_baseline_df[\"z_560_Baseline\"]])\n",
    "        ))\n",
    "        ax2.yaxis.label.set_color('green')\n",
    "\n",
    "        # Motor velocity\n",
    "        ax3 = ax.twinx()\n",
    "        ax3.spines['right'].set_position(('outward', 50))\n",
    "        ax3.plot(mean_baseline_df.index, mean_baseline_df[\"Motor_Velocity_Baseline\"], color='#00008B', alpha=0.8, linewidth=2)\n",
    "        ax3.fill_between(mean_baseline_df.index,\n",
    "                         mean_baseline_df[\"Motor_Velocity_Baseline\"] - sem_baseline_df[\"Motor_Velocity_Baseline\"],\n",
    "                         mean_baseline_df[\"Motor_Velocity_Baseline\"] + sem_baseline_df[\"Motor_Velocity_Baseline\"],\n",
    "                         color='#00008B', alpha=0.2)\n",
    "        ax3.set_ylabel('Motor Velocity (deg/s¬≤)', color='#00008B')\n",
    "        ax3.set_ylim(get_symmetric_ylim(mean_baseline_df[\"Motor_Velocity_Baseline\"], sem_baseline_df[\"Motor_Velocity_Baseline\"]))\n",
    "        ax3.yaxis.label.set_color('#00008B')\n",
    "\n",
    "        # Running velocity (Velocity_0X)\n",
    "        ax4 = ax.twinx()\n",
    "        ax4.spines['right'].set_position(('outward', 100))\n",
    "        ax4.plot(mean_baseline_df.index, mean_baseline_df[\"Velocity_0X_Baseline\"] * 1000, color='orange', alpha=0.8, linewidth=2)\n",
    "        ax4.fill_between(mean_baseline_df.index,\n",
    "                         (mean_baseline_df[\"Velocity_0X_Baseline\"] - sem_baseline_df[\"Velocity_0X_Baseline\"]) * 1000,\n",
    "                         (mean_baseline_df[\"Velocity_0X_Baseline\"] + sem_baseline_df[\"Velocity_0X_Baseline\"]) * 1000,\n",
    "                         color='orange', alpha=0.2)\n",
    "        ax4.set_ylabel('Running velocity (mm/s¬≤)', color='orange')\n",
    "        ax4.set_ylim(get_symmetric_ylim(mean_baseline_df[\"Velocity_0X_Baseline\"] * 1000, sem_baseline_df[\"Velocity_0X_Baseline\"] * 1000))\n",
    "        ax4.yaxis.label.set_color('orange')\n",
    "\n",
    "        # Turning velocity (Velocity_0Y)\n",
    "        ax5 = ax.twinx()\n",
    "        ax5.spines['right'].set_position(('outward', 150))\n",
    "        ax5.plot(mean_baseline_df.index, mean_baseline_df[\"Velocity_0Y_Baseline\"], color='#4682B4', alpha=0.8, linewidth=2)\n",
    "        ax5.fill_between(mean_baseline_df.index,\n",
    "                         mean_baseline_df[\"Velocity_0Y_Baseline\"] - sem_baseline_df[\"Velocity_0Y_Baseline\"],\n",
    "                         mean_baseline_df[\"Velocity_0Y_Baseline\"] + sem_baseline_df[\"Velocity_0Y_Baseline\"],\n",
    "                         color='#4682B4', alpha=0.2)\n",
    "        ax5.set_ylabel('Turning velocity (deg/s¬≤)', color='#4682B4')\n",
    "        ax5.set_ylim(get_symmetric_ylim(mean_baseline_df[\"Velocity_0Y_Baseline\"], sem_baseline_df[\"Velocity_0Y_Baseline\"]))\n",
    "        ax5.yaxis.label.set_color('#4682B4')\n",
    "\n",
    "        # Add vertical line at event time (t=0)\n",
    "        ax.axvline(x=0, color='black', linestyle='--', alpha=0.5, linewidth=1)\n",
    "\n",
    "        fig.tight_layout()\n",
    "\n",
    "        figure_file = output_folder / f\"{session_name}_{event_name}_baselined.pdf\"\n",
    "\n",
    "        # Save the figure\n",
    "        fig.savefig(figure_file, format='pdf', bbox_inches='tight')\n",
    "        print(f\"      üíæ Saved plot to: {figure_file.name}\")\n",
    "        plt.close(fig)\n",
    "        return fig\n",
    "    else:\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c205cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Process all aligned_data folders\n",
    "    results = process_aligned_data_folders(\n",
    "        data_dirs=data_dirs,\n",
    "        baseline_window=baseline_window,\n",
    "        event_name=event_name,\n",
    "        plot_width=plot_width,\n",
    "        create_plots=True  # Ensure this is set to True to save plots\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüéâ Processing complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aeon2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
