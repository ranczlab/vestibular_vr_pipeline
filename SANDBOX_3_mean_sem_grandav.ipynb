{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b677f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------------------------------#\n",
    "from pathlib import Path\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from matplotlib import cm\n",
    "import plotly.io as pio\n",
    "import plotly.express as px\n",
    "import plotly.subplots as sp\n",
    "import math\n",
    "from pprint import pprint\n",
    "import pickle\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy.stats import mode\n",
    "from scipy.integrate import cumulative_trapezoid\n",
    "from scipy.signal import correlate\n",
    "%config Completer.use_jedi = False  # Fixes autocomplete issues\n",
    "%config InlineBackend.figure_format = 'retina'  # Improves plot resolution\n",
    "\n",
    "import gc # garbage collector for removing large variables from memory instantly \n",
    "import importlib #for force updating changed packages \n",
    "from scipy.stats import pearsonr, norm\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f05154b",
   "metadata": {},
   "source": [
    "to use: you need to change the csv file u want to process in cell 3, then you need to change which mice you want to process if you want to plot correlations, then below when using the csv file to plot the grand averages, make sure to select mice and data paths\n",
    "RUN THE FIRST 11 CELLS TO GENERATE PICKLE AND CSV FILE TO USE LATER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ca8b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#variables to DEFINE before running the script\n",
    "#----------------------------------------------------------------------------------------------------#\n",
    "selected_mice1 = ['B6J2717', 'B6J2718', 'B6J2719', 'B6J2721', 'B6J2722', 'B6J2723'] #for cohort 1 \n",
    "selected_mice2 = [\"B6J2780\", \"B6J2781\", \"B6J2783\", \"B6J2782\"] #for cohort 3\n",
    "cohort_identifier = \"Cohort3\" #for cohort 1 pickle saving\n",
    "# cohort_identifier = \"Cohort3\"\n",
    "\n",
    "save_grand_avg_with_sem = True  # Boolean to control whether to save grand averages with SEMs to a CSV file\n",
    "generate_new_plots = True  # Set to False if you don't want to generate new plots\n",
    "\n",
    "selected_columns = ['Velocity_0X_Baseline', 'Motor_Velocity_Baseline', 'z_470_Baseline','z_560_Baseline','Velocity_0X','Motor_Velocity','z_470','z_560']  # Add your selected columns here ', 'z_560', 'z_470', 'dfF_470', 'dfF_560'\n",
    "columns_to_plot =  ['Velocity_0X', 'Motor_Velocity', 'z_470','z_560']# Add more columns as needed , 'dfF_470', 'dfF_560', 'z_470', 'z_560'\n",
    "# selected_columns = ['Velocity_0X_Baseline','z_470_Baseline','z_560_Baseline','Motor_Velocity_Baseline']  # Add your selected columns here ', 'z_560', 'z_470', 'dfF_470', 'dfF_560'\n",
    "# columns_to_plot = ['Velocity_0X','z_470','z_560']  # Add more columns as needed , 'dfF_470', 'dfF_560', 'z_470', 'z_560'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed85884",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIXME this script should be more cohesive, eg only save 1 output file with all the data. NOW\n",
    "#it saves thre results as a pickle file in the repo dir, the grand average as a csv file in the experiment dir,\n",
    "#-------------------------------\n",
    "# data paths setup\n",
    "#-------------------------------\n",
    "data_dirs = [  # Add your data directories here\n",
    "    # Path('~/RANCZLAB-NAS/data/ONIX/20250409_Cohort3_rotation/Vestibular_mismatch_day1').expanduser(),\n",
    "    # Path('/Volumes/RanczLab2/20241125_Cohort1_rotation/Visual_mismatch_day4').expanduser(),\n",
    "    # Path('/Volumes/RanczLab2/20250409_Cohort3_rotation/Open_loop_day1').expanduser(),\n",
    "    # Path('/Volumes/RanczLab2/20241125_Cohort1_rotation/Visual_mismatch_day4').expanduser(),\n",
    "    Path('/Volumes/RanczLab2/20250409_Cohort3_rotation/Visual_mismatch_day4').expanduser(),\n",
    "    # Path('/Volumes/RanczLab2/20241125_Cohort1_rotation/Vestibular_mismatch_day1').expanduser(),\n",
    "]\n",
    "# Collect raw data paths (excluding '_processedData' dirs)\n",
    "rawdata_paths = []\n",
    "for data_dir in data_dirs:\n",
    "    subdirs = [p for p in data_dir.iterdir() if p.is_dir() and not p.name.endswith('_processedData')]\n",
    "    rawdata_paths.extend(subdirs)  # Collect all subdirectories\n",
    "\n",
    "# Build processed data paths\n",
    "data_paths = [raw.parent / f\"{raw.name}_processedData/aligned_data\" for raw in rawdata_paths]\n",
    "mouse_name = [raw.name.split('-')[0] for raw in rawdata_paths]\n",
    "#-------------------------------\n",
    "# load aligned_downsampled data for each data path\n",
    "#-------------------------------\n",
    "loaded_data = {}  # Dictionary to store loaded data for each path\n",
    "for idx, data_path in enumerate(data_paths, start=1):\n",
    "    print(f\"\\nProcessing data path {idx}/{len(data_paths)}: {data_path}\")\n",
    "    # event_name = '_No halt_right_turns_baselined_data.csv'  # or Apply halt_2s\n",
    "    # event_name = '_No halt_left_turns_baselined_data.csv'  # or Apply halt_2s\n",
    "    event_name = '_No halt_baselined_data.csv'  #or Apply halt_2s\n",
    "    csv_file_path = data_path / f\"{mouse_name[idx - 1]}{event_name}\"\n",
    "    if mouse_name[idx - 1] == 'baselined':\n",
    "        print(f\"⚠️ Skipping directory {data_path} as it does not contain a valid mouse name.\")\n",
    "        continue\n",
    "    try:\n",
    "        aligned_df = pd.read_csv(csv_file_path)\n",
    "        print(f\"✅ Successfully loaded data for {mouse_name[idx - 1]} from {csv_file_path}\")\n",
    "        loaded_data[data_path] = {\n",
    "            'mouse_name': mouse_name[idx - 1],\n",
    "            'data_path': data_path\n",
    "        }\n",
    "        # Add each column of aligned_df as a separate key in the dictionary\n",
    "        for column in aligned_df.columns:\n",
    "            loaded_data[data_path][column] = aligned_df[column].values\n",
    "        print(f\"Data loaded for {data_path.name}: {len(aligned_df)} rows, {len(aligned_df.columns)} columns\")\n",
    "        # Clean up memory\n",
    "        del aligned_df\n",
    "        gc.collect()  # Run garbage collection to free up memory\n",
    "    except FileNotFoundError:\n",
    "        print(f\"⚠️ File not found for {mouse_name[idx - 1]} at {csv_file_path}, skipping...\")\n",
    "        continue  # Skip to the next iteration if the file is not found\n",
    "#-------------------------------\n",
    "print(\"Processed Data Paths:\")\n",
    "pprint(data_paths)\n",
    "print(\"Mouse Name:\")\n",
    "pprint(mouse_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89dc0c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#computes mean and sem per mouse + stores grand averages across all mice\n",
    "\n",
    "def compute_mouse_means_and_grand_average(loaded_data, selected_columns, main_data_dir, selected_mice):\n",
    "    \"\"\"\n",
    "    Compute means per mouse and grand averages across selected mice for selected columns.\n",
    "    \n",
    "    Parameters:\n",
    "    loaded_data (dict): Dictionary with data paths as keys and mouse data as values\n",
    "    selected_columns (list): List of column names to analyze\n",
    "    main_data_dir (str/Path): Main directory to save results\n",
    "    selected_mice (list): List of mouse names to include in the grand average\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (mean_data_per_mouse, sem_data_per_mouse, grand_averages, grand_sems)\n",
    "    \"\"\"\n",
    "    \n",
    "    main_data_dir = Path(main_data_dir)\n",
    "    \n",
    "    print(f\"Processing selected columns: {selected_columns}\")\n",
    "    \n",
    "    # Step 1: Compute mean and SEM for each mouse\n",
    "    mean_data_per_mouse = {}\n",
    "    sem_data_per_mouse = {}\n",
    "    \n",
    "    for data_path, data in loaded_data.items():\n",
    "        mouse_name = data['mouse_name']\n",
    "        if mouse_name not in selected_mice:\n",
    "            print(f\"Skipping mouse {mouse_name} as it is not in the selected mice list.\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"Processing mouse: {mouse_name}\")\n",
    "        \n",
    "        # Create DataFrame from the loaded data\n",
    "        df = pd.DataFrame(data)\n",
    "        \n",
    "        # Check which selected columns are available\n",
    "        available_columns = [col for col in selected_columns if col in df.columns]\n",
    "        missing_columns = [col for col in selected_columns if col not in df.columns]\n",
    "        \n",
    "        if missing_columns:\n",
    "            print(f\"⚠️  Missing columns for {mouse_name}: {missing_columns}\")\n",
    "        \n",
    "        if 'Time (s)' not in df.columns:\n",
    "            print(f\"⚠️  'Time (s)' column not found for {mouse_name}, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        # Group by time and compute mean and SEM\n",
    "        grouped = df.groupby('Time (s)')\n",
    "        \n",
    "        # Only use numeric columns that are in our selected list\n",
    "        numeric_selected = []\n",
    "        for col in available_columns:\n",
    "            if col != 'Time (s)' and pd.api.types.is_numeric_dtype(df[col]):\n",
    "                numeric_selected.append(col)\n",
    "        \n",
    "        if len(numeric_selected) == 0:\n",
    "            print(f\"⚠️  No numeric columns found for {mouse_name}\")\n",
    "            continue\n",
    "        \n",
    "        mean_data_per_mouse[mouse_name] = grouped[numeric_selected].mean()\n",
    "        sem_data_per_mouse[mouse_name] = grouped[numeric_selected].sem()\n",
    "        \n",
    "        print(f\"✅ Processed {len(numeric_selected)} columns for {mouse_name}\")\n",
    "    \n",
    "    # Step 2: Compute grand averages across selected mice\n",
    "    print(f\"\\n📊 Computing grand averages across {len(mean_data_per_mouse)} selected mice...\")\n",
    "    \n",
    "    # Get all unique time points\n",
    "    all_time_points = set()\n",
    "    for mouse_data in mean_data_per_mouse.values():\n",
    "        all_time_points.update(mouse_data.index)\n",
    "    all_time_points = sorted(list(all_time_points))\n",
    "    \n",
    "    # Get all columns that were successfully processed\n",
    "    all_processed_columns = set()\n",
    "    for mouse_data in mean_data_per_mouse.values():\n",
    "        all_processed_columns.update(mouse_data.columns)\n",
    "    all_processed_columns = sorted(list(all_processed_columns))\n",
    "    \n",
    "    print(f\"Time points: {len(all_time_points)} from {min(all_time_points):.2f}s to {max(all_time_points):.2f}s\")\n",
    "    print(f\"Processed columns: {all_processed_columns}\")\n",
    "    \n",
    "    # Create grand average DataFrame\n",
    "    grand_averages = pd.DataFrame(index=all_time_points, columns=all_processed_columns)\n",
    "    grand_averages.index.name = 'Time (s)'\n",
    "    \n",
    "    grand_sems = pd.DataFrame(index=all_time_points, columns=all_processed_columns)\n",
    "    grand_sems.index.name = 'Time (s)'\n",
    "    \n",
    "    # Compute grand averages for each column and time point\n",
    "    for col in all_processed_columns:\n",
    "        for time_point in all_time_points:\n",
    "            # Collect data from all selected mice for this time point and column\n",
    "            mouse_values = []\n",
    "            for mouse_name, mouse_data in mean_data_per_mouse.items():\n",
    "                if time_point in mouse_data.index and col in mouse_data.columns:\n",
    "                    value = mouse_data.loc[time_point, col]\n",
    "                    if not pd.isna(value):\n",
    "                        mouse_values.append(value)\n",
    "            \n",
    "            if len(mouse_values) > 0:\n",
    "                grand_averages.loc[time_point, col] = np.mean(mouse_values)\n",
    "                if len(mouse_values) > 1:\n",
    "                    grand_sems.loc[time_point, col] = np.std(mouse_values) / np.sqrt(len(mouse_values))\n",
    "                else:\n",
    "                    grand_sems.loc[time_point, col] = 0\n",
    "    \n",
    "    return mean_data_per_mouse, sem_data_per_mouse, grand_averages, grand_sems\n",
    "\n",
    "def analyze_mice_data(loaded_data, selected_columns, main_data_dir):\n",
    "    \"\"\"\n",
    "    Complete analysis workflow: compute means, grand averages, save CSV, and create plots.\n",
    "    \n",
    "    Parameters:\n",
    "    loaded_data (dict): Your loaded_data dictionary\n",
    "    selected_columns (list): List of column names to analyze (including 'Time (s)')\n",
    "    main_data_dir (str/Path): Main directory to save results\n",
    "    \n",
    "    Returns:\n",
    "    dict: Complete results including individual and grand averages\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"MOUSE DATA ANALYSIS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Compute means and grand averages\n",
    "    mean_data_per_mouse, sem_data_per_mouse, grand_averages, grand_sems = compute_mouse_means_and_grand_average(\n",
    "        loaded_data, selected_columns, main_data_dir, selected_mice2\n",
    "    )\n",
    "\n",
    "    # Print summary\n",
    "    print(f\"\\n📊 ANALYSIS COMPLETE:\")\n",
    "    print(f\"   • Number of mice analyzed: {len(mean_data_per_mouse)}\")\n",
    "    print(f\"   • Mouse names: {list(mean_data_per_mouse.keys())}\")\n",
    "    print(f\"   • Columns processed: {list(grand_averages.columns)}\")\n",
    "    print(f\"   • Time range: {grand_averages.index.min():.2f}s to {grand_averages.index.max():.2f}s\")\n",
    "    print(f\"   • Files saved in: {main_data_dir}\")\n",
    "    \n",
    "    # Return all results\n",
    "    results = {\n",
    "        'mean_data_per_mouse': mean_data_per_mouse,\n",
    "        'sem_data_per_mouse': sem_data_per_mouse,\n",
    "        'grand_averages': grand_averages,\n",
    "        'grand_sems': grand_sems,\n",
    "    }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8c6e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STORE data in RESULTS DICT\n",
    "#DO WE NEED THE PICKLE FILE AT ALL? IF SO, SAVE INSTEAD OF GRAND AVERAGE CSV (SAVE THE GRAND AVERAGE IN THE SAME PICLKE FILE)\n",
    "\n",
    "#HERE you define which data dir you want the analysis to be performed on\n",
    "#---------------------------\n",
    "main_data_dir = data_dirs[0]  # Use the first directory from the list\n",
    "#here you run the analysis\n",
    "#----------------------------\n",
    "#to save the results to a pickle file\n",
    "def save_results(results, filename='results.pkl'):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(results, f)\n",
    "    print(f\"Results saved to {filename}\")\n",
    "#----------------------------\n",
    "# Function to load results from a pickle file\n",
    "def load_results(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "# Analyze mice data \n",
    "results = analyze_mice_data(loaded_data, selected_columns,main_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40b7c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save pickle - Generate a unique filename using the cohort identifier, directory name, and the original CSV file name\n",
    "# Extract the event name from the raw data file path\n",
    "vmm_event_name = rawdata_paths[0].parent.name  # Extract the parent directory name of the first raw data path\n",
    "pickle_filename = f\"{cohort_identifier}_{vmm_event_name}{event_name.replace('.csv', '')}_no_2782.pkl\"\n",
    "save_results(results, pickle_filename)  # Save results with the generated filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc37e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for PLOTTING!\n",
    "selected_mice = selected_mice1  # Define here which mice you want to plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea5ea4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOTTING CODE WITH DEBUGGING\n",
    "# FIXME: THIS IS NOT ITERATIVE, I HAVE TO MANUALLY SPECIFY WHICH MICE I WANT TO PLOT\n",
    "\n",
    "# Add debugging information\n",
    "print(f\"generate_new_plots value: {generate_new_plots}\")\n",
    "print(f\"selected_mice: {selected_mice}\")\n",
    "print(f\"Available mice in results: {list(results['mean_data_per_mouse'].keys())}\")\n",
    "print(f\"columns_to_plot: {columns_to_plot}\")\n",
    "\n",
    "# Check which mice are missing\n",
    "missing_mice = [mouse for mouse in selected_mice if mouse not in results['mean_data_per_mouse']]\n",
    "if missing_mice:\n",
    "    print(f\"WARNING: These mice are in selected_mice but not in results: {missing_mice}\")\n",
    "\n",
    "#PLOTTING MEAN PER MOUSE AND GRAND AVERAGE, STORING CSV WITH GRAND AVERAGES AND SEMs, STORING PLOTS\n",
    "#--------------------------------\n",
    "# PLOT properties\n",
    "plt.rcParams.update({\n",
    "    'font.size': 10,           # Set global font size\n",
    "    'font.family': 'sans-serif',  # Font family (e.g., 'serif', 'sans-serif', 'monospace')\n",
    "    'font.sans-serif': ['Arial'],  # Preferred font\n",
    "    'axes.titlesize': 10,      # Title font size\n",
    "    'axes.labelsize': 10,      # Axis label size\n",
    "    'legend.fontsize': 8,     # Legend text\n",
    "    'xtick.labelsize': 10,\n",
    "    'ytick.labelsize': 10\n",
    "})\n",
    "# Generate a color palette\n",
    "color_palette = plt.cm.Set2.colors  # Use the 'tab10' colormap from matplotlib\n",
    "# Define mouse colors using the color palette\n",
    "mouse_colors = {mouse: color_palette[i % len(color_palette)] for i, mouse in enumerate(selected_mice)}\n",
    "\n",
    "# Boolean to control whether to generate new plots\n",
    "if not generate_new_plots:\n",
    "    print(\"Skipping plot generation as per user configuration.\")\n",
    "else:\n",
    "    print(\"Starting plot generation...\")\n",
    "    #---------------------------\n",
    "    # plot data for each selected mouse and the grand average   \n",
    "    for column_to_plot in columns_to_plot:\n",
    "        print(f\"\\nProcessing column: {column_to_plot}\")\n",
    "        \n",
    "        # Create the plot\n",
    "        plt.figure(figsize=(8, 4))\n",
    "\n",
    "        # Plot mean and SEM for each selected mouse\n",
    "        mice_plotted = []\n",
    "        for mouse in selected_mice:\n",
    "            print(f\"Checking mouse: {mouse}\")\n",
    "            if mouse in results['mean_data_per_mouse']:\n",
    "                print(f\"  -> Found data for {mouse}, plotting...\")\n",
    "                mice_plotted.append(mouse)\n",
    "                \n",
    "                mean_data = results['mean_data_per_mouse'][mouse][column_to_plot]\n",
    "                sem_data = results['sem_data_per_mouse'][mouse][column_to_plot]\n",
    "                \n",
    "                # Ensure data is numeric and handle any conversion issues\n",
    "                mean_data = pd.to_numeric(mean_data, errors='coerce')\n",
    "                sem_data = pd.to_numeric(sem_data, errors='coerce')\n",
    "                time_points = pd.to_numeric(mean_data.index, errors='coerce')\n",
    "                \n",
    "                # Drop any NaN values that might have been created during conversion\n",
    "                valid_mask = ~(pd.isna(mean_data) | pd.isna(sem_data) | pd.isna(time_points))\n",
    "                mean_data_clean = mean_data[valid_mask]\n",
    "                sem_data_clean = sem_data[valid_mask]\n",
    "                time_points_clean = time_points[valid_mask]\n",
    "\n",
    "                # Plot mean with SEM as shaded area using the defined color palette\n",
    "                plt.plot(time_points_clean, mean_data_clean, label=f'{mouse} Mean', color=mouse_colors[mouse])\n",
    "                plt.fill_between(time_points_clean, mean_data_clean - sem_data_clean, \n",
    "                        mean_data_clean + sem_data_clean, color=mouse_colors[mouse], alpha=0.2)\n",
    "            else:\n",
    "                print(f\"  -> No data found for {mouse}\")\n",
    "\n",
    "        print(f\"Mice plotted in time series: {mice_plotted}\")\n",
    "\n",
    "        # Plot the grand average (OUTSIDE the mouse loop, INSIDE the column loop)\n",
    "        grand_mean = results['grand_averages'][column_to_plot]\n",
    "        grand_sem = results['grand_sems'][column_to_plot]\n",
    "            \n",
    "        # Ensure grand average data is numeric\n",
    "        grand_mean = pd.to_numeric(grand_mean, errors='coerce')\n",
    "        grand_sem = pd.to_numeric(grand_sem, errors='coerce')\n",
    "        time_points = pd.to_numeric(grand_mean.index, errors='coerce')\n",
    "            \n",
    "        # Drop any NaN values\n",
    "        valid_mask = ~(pd.isna(grand_mean) | pd.isna(grand_sem) | pd.isna(time_points))\n",
    "        grand_mean_clean = grand_mean[valid_mask]\n",
    "        grand_sem_clean = grand_sem[valid_mask]\n",
    "        time_points_clean = time_points[valid_mask]\n",
    "\n",
    "        plt.plot(time_points_clean, grand_mean_clean, label='Grand Average', color='black', linewidth=2)\n",
    "        plt.fill_between(time_points_clean, grand_mean_clean - grand_sem_clean, \n",
    "                        grand_mean_clean + grand_sem_clean, color='gray', alpha=0.3)\n",
    "        \n",
    "        # Add labels, legend, and title\n",
    "        plt.xlabel('Time (s)')\n",
    "        plt.ylabel(column_to_plot)\n",
    "        plt.title(f'Mean and SEM of {column_to_plot} Over Time')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Save the plot to the \"baselined\" subdirectory\n",
    "        try:\n",
    "            baselined_dir = main_data_dir / \"baselined\"\n",
    "            baselined_dir.mkdir(exist_ok=True)  # Create the directory if it doesn't exist\n",
    "            plot_filename = baselined_dir / f\"{column_to_plot}{event_name.replace('.csv', '')}.pdf\"\n",
    "            plt.savefig(plot_filename, dpi=150, bbox_inches='tight')\n",
    "            print(f\"Plot saved successfully to: {plot_filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR saving plot: {e}\")\n",
    "\n",
    "        # Show the plot\n",
    "        plt.show()\n",
    "        \n",
    "        #create a SCATTER PLOT\n",
    "        pre_time = (-2, 0)\n",
    "        post_time = (0, 2)\n",
    "\n",
    "        pre_values = []\n",
    "        post_values = []\n",
    "        mouse_labels = []\n",
    "\n",
    "        # Collect values\n",
    "        mice_in_scatter = []\n",
    "        for mouse in selected_mice:\n",
    "            if mouse in results['mean_data_per_mouse']:\n",
    "                mice_in_scatter.append(mouse)\n",
    "                mean_data = results['mean_data_per_mouse'][mouse][column_to_plot]\n",
    "                \n",
    "                pre_mean = mean_data.loc[(mean_data.index >= pre_time[0]) & (mean_data.index < pre_time[1])].mean()\n",
    "                post_mean = mean_data.loc[(mean_data.index >= post_time[0]) & (mean_data.index <= post_time[1])].mean()\n",
    "                \n",
    "                pre_values.append(pre_mean)\n",
    "                post_values.append(post_mean)\n",
    "                mouse_labels.append(mouse)\n",
    "        \n",
    "        print(f\"Mice plotted in scatter plot: {mice_in_scatter}\")\n",
    "\n",
    "        # Grand average\n",
    "        grand_mean = results['grand_averages'][column_to_plot]\n",
    "        pre_grand_mean = grand_mean.loc[(grand_mean.index >= pre_time[0]) & (grand_mean.index < pre_time[1])].mean()\n",
    "        post_grand_mean = grand_mean.loc[(grand_mean.index >= post_time[0]) & (grand_mean.index <= post_time[1])].mean()\n",
    "\n",
    "        grand_sem = results['grand_sems'][column_to_plot]\n",
    "        pre_grand_sem = grand_sem.loc[(grand_sem.index >= pre_time[0]) & (grand_sem.index < pre_time[1])].mean()\n",
    "        post_grand_sem = grand_sem.loc[(grand_sem.index >= post_time[0]) & (grand_sem.index <= post_time[1])].mean()\n",
    "\n",
    "        # Plotting\n",
    "        plt.figure(figsize=(3, 4)) \n",
    "\n",
    "        # Plot each mouse with connecting line\n",
    "        for i, mouse in enumerate(mouse_labels):\n",
    "            plt.plot([1, 2], [pre_values[i], post_values[i]], color=mouse_colors[mouse], marker='o', linewidth=1, label=mouse)\n",
    "\n",
    "        # Grand average as large black dots\n",
    "        plt.plot([1, 2], [pre_grand_mean, post_grand_mean], color='black', marker='o', markersize=8, linewidth=1, label='Grand Avg')\n",
    "\n",
    "        # Add error bars for grand average SEM\n",
    "        plt.errorbar([1, 2], [pre_grand_mean, post_grand_mean], yerr=[pre_grand_sem, post_grand_sem], fmt='o', color='black', capsize=5)\n",
    "\n",
    "        # Formatting\n",
    "        plt.xticks([1, 2], [pre_time, post_time])\n",
    "        plt.title(f'Mean {column_to_plot} Before and After Time 0')\n",
    "        plt.ylabel(column_to_plot)\n",
    "        plt.xlim(0.8, 2.2)  # tighter x-axis\n",
    "        plt.grid(True)\n",
    "\n",
    "        # Legend: one color per mouse\n",
    "        handles, labels = plt.gca().get_legend_handles_labels()\n",
    "        unique_labels = {}\n",
    "        for h, l in zip(handles, labels):\n",
    "            if l not in unique_labels:\n",
    "                unique_labels[l] = h\n",
    "        plt.legend(unique_labels.values(), unique_labels.keys(), loc='best', fontsize='small')\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Save the plot to the \"baselined\" subdirectory\n",
    "        try:\n",
    "            baselined_dir = main_data_dir / \"baselined\"\n",
    "            baselined_dir.mkdir(exist_ok=True)  # Create the directory if it doesn't exist\n",
    "            plot_filename = baselined_dir / f\"{column_to_plot}{event_name.replace('.csv', '')}_scatterplot.pdf\"\n",
    "            plt.savefig(plot_filename, dpi=150, bbox_inches='tight')\n",
    "            print(f\"Scatter plot saved successfully to: {plot_filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR saving scatter plot: {e}\")\n",
    "\n",
    "        plt.show()\n",
    "        # Clear the current figure to free up memory\n",
    "        plt.clf()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ab42b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIXME: IDEALLY WE SHOULD ONLY USE 1 FILE TO STORE RESULTS AND GRAND AVERAGES, NOW SPREAD ACROSS PICLKE AND CSV FILES\n",
    "#save csv with grand averages and SEMs\n",
    "if save_grand_avg_with_sem:\n",
    "    # Create a DataFrame combining grand averages and SEMs\n",
    "    grand_avg_with_sem = results['grand_averages'].copy()\n",
    "    for col in results['grand_sems'].columns:\n",
    "        grand_avg_with_sem[f'{col}_SEM'] = results['grand_sems'][col]\n",
    "\n",
    "    # Generate a filename that includes the selected mice\n",
    "    # csv_filename = main_data_dir / f\"(grand_averages_with_sem_{mice_str}{event_name.replace('.csv', '')}).csv\"\n",
    "    csv_filename = main_data_dir / f\"grand_averages_with_sem{event_name.replace('.csv', '')}_no_2782.csv\"\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    grand_avg_with_sem.to_csv(csv_filename)\n",
    "    print(f\"Grand averages with SEM saved to: {csv_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c84214",
   "metadata": {},
   "outputs": [],
   "source": [
    "#select for plotting correlations   \n",
    "selected_mice2 = [\"B6J2780\", \"B6J2781\", \"B6J2783\", \"B6J2782\"]\n",
    "# selected_mice1 = [\"B6J2780\", \"B6J2781\", \"B6J2783\", \"B6J2782\"]\n",
    "selected_mice1 = ['B6J2717', 'B6J2718', 'B6J2719', 'B6J2721', 'B6J2722', 'B6J2723'] #for cohort 1 'B6J2722, 'B6J2723\n",
    "# selected_mice2 = ['B6J2717', 'B6J2718', 'B6J2719', 'B6J2721', 'B6J2722', 'B6J2723'] #for cohort 1 'B6J2722, 'B6J2723\n",
    "\n",
    "def load_results(filename):\n",
    "    import pickle\n",
    "    with open(filename, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "# Load both result files\n",
    "results_cohort1 = load_results('Cohort1_Visual_mismatch_day4_Apply halt_2s_baselined_data.pkl')\n",
    "results_cohort2 = load_results('Cohort3_Visual_mismatch_day4_Apply halt_2s_baselined_data.pkl')\n",
    "\n",
    "time_window = (2,4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8446ce8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute correlations between Velocity_0X and z_470, z_560 for each cohort\n",
    "def fisher_z(r):\n",
    "    return 0.5 * np.log((1 + r) / (1 - r))\n",
    "\n",
    "def compare_correlations(r1, n1, r2, n2):\n",
    "    z1 = fisher_z(r1)\n",
    "    z2 = fisher_z(r2)\n",
    "    se = np.sqrt(1 / (n1 - 3) + 1 / (n2 - 3))\n",
    "    z = (z1 - z2) / se\n",
    "    p = 2 * (1 - norm.cdf(abs(z)))\n",
    "    return z, p\n",
    "\n",
    "# def extract_means(results, mice, time_window, columns=('Velocity_0X_Baseline', 'z_470_Baseline', 'z_560_Baseline')):\n",
    "def extract_means(results, mice, time_window, columns=columns_to_plot):\n",
    "    \"\"\"\n",
    "    Extract mean values for specified columns within a given time window for a list of mice.\n",
    "\n",
    "    Parameters:\n",
    "    results (dict): Dictionary containing data for each mouse.\n",
    "    mice (list): List of mouse names to extract data for.\n",
    "    time_wi\n",
    "    ndow (tuple): Time window (start, end) for extracting mean values.\n",
    "    columns (list): List of column names to extract.\n",
    "\n",
    "    Returns:\n",
    "    tuple: Mean values for Velocity_0X, z_470, z_560, and valid mouse names.\n",
    "    \"\"\"\n",
    "    v_means, z470_means, z560_means, valid_mice = [], [], [], []\n",
    "    t0, t1 = time_window\n",
    "\n",
    "    for mouse in mice:\n",
    "        if mouse not in results['mean_data_per_mouse']:\n",
    "            continue\n",
    "        df = results['mean_data_per_mouse'][mouse]\n",
    "        if not all(col in df.columns for col in columns):\n",
    "            continue\n",
    "\n",
    "        df_window = df.loc[(df.index >= t0) & (df.index <= t1)]\n",
    "        v = df_window[columns[0]].mean()\n",
    "        z470 = df_window[columns[1]].mean()\n",
    "        z560 = df_window[columns[2]].mean()\n",
    "\n",
    "        if not any(pd.isnull([v, z470, z560])):\n",
    "            v_means.append(v)\n",
    "            z470_means.append(z470)\n",
    "            z560_means.append(z560)\n",
    "            valid_mice.append(mouse)\n",
    "\n",
    "    return v_means, z470_means, z560_means, valid_mice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd96d92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTRACT MEANS FOR EACH COHORT\n",
    "\n",
    "time_window = time_window  # Define the time window\n",
    "v1, z470_1, z560_1, ids1 = extract_means(results_cohort1, selected_mice1, time_window)\n",
    "v2, z470_2, z560_2, ids2 = extract_means(results_cohort2, selected_mice2, time_window)\n",
    "\n",
    "# Move velocity and z_470 of B6J2782 from cohort2 to cohort1\n",
    "target_mouse = \"B6J2782\"  # Specify the mouse to move\n",
    "exclude_mice = [\"B6J2722\"]  # List of mice to exclude\n",
    "\n",
    "# Exclude mice from both cohorts\n",
    "ids1 = [mouse for mouse in ids1 if mouse not in exclude_mice]\n",
    "ids2 = [mouse for mouse in ids2 if mouse not in exclude_mice]\n",
    "\n",
    "if target_mouse in ids2:\n",
    "    idx = ids2.index(target_mouse)\n",
    "\n",
    "    # --- MOVE velocity and z_470 to Cohort 1 ---\n",
    "    v1.append(v2[idx])\n",
    "    z470_1.append(z470_2[idx])\n",
    "    # Add placeholder to z560_1 to keep list lengths equal\n",
    "    z560_1.append(np.nan)  \n",
    "    ids1.append(target_mouse + \" (from Cohort 2)\")\n",
    "\n",
    "    # --- KEEP velocity and z_560 in Cohort 2 ---\n",
    "    v2.append(v2[idx])          # duplicate velocity for z_560 correlation\n",
    "    z560_2.append(z560_2[idx])\n",
    "    z470_2.append(np.nan)       # add placeholder to z470_2 to keep lengths equal\n",
    "    ids2.append(target_mouse + \" (z_560)\")\n",
    "\n",
    "    # --- REMOVE original entry from Cohort 2 ---\n",
    "    del v2[idx]\n",
    "    del z470_2[idx]\n",
    "    del z560_2[idx]\n",
    "    del ids2[idx]\n",
    "\n",
    "    print(f\"✅ Moved velocity & z_470 of {target_mouse} to Cohort 1.\")\n",
    "    print(f\"✅ Retained velocity & z_560 of {target_mouse} in Cohort 2.\")\n",
    "else:\n",
    "    print(f\"⚠️ Mouse {target_mouse} not found in Cohort 2.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3034a08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute correlations for both cohorts\n",
    "#--------    \n",
    "# PLOT properties\n",
    "plt.rcParams.update({\n",
    "    'font.size': 10,\n",
    "    'font.family': 'sans-serif',\n",
    "    'font.sans-serif': ['Arial'],\n",
    "    'axes.titlesize': 10,\n",
    "    'axes.labelsize': 10,\n",
    "    'legend.fontsize': 8,\n",
    "    'xtick.labelsize': 10,\n",
    "    'ytick.labelsize': 10\n",
    "})\n",
    "\n",
    "def move_mouse_data_fixed(v1, z470_1, z560_1, ids1, v2, z470_2, z560_2, ids2, \n",
    "                         target_mouse=target_mouse, exclude_mice=exclude_mice):\n",
    "    \"\"\"\n",
    "    Fixed version that maintains array consistency\n",
    "    \"\"\"\n",
    "    # Convert to lists if not already (for easier manipulation)\n",
    "    v1, z470_1, z560_1, ids1 = list(v1), list(z470_1), list(z560_1), list(ids1)\n",
    "    v2, z470_2, z560_2, ids2 = list(v2), list(z470_2), list(z560_2), list(ids2)\n",
    "    \n",
    "    # Exclude mice from both cohorts\n",
    "    for mouse in exclude_mice:\n",
    "        # Remove from cohort 1\n",
    "        while mouse in ids1:\n",
    "            idx = ids1.index(mouse)\n",
    "            del v1[idx], z470_1[idx], z560_1[idx], ids1[idx]\n",
    "        \n",
    "        # Remove from cohort 2\n",
    "        while mouse in ids2:\n",
    "            idx = ids2.index(mouse)\n",
    "            del v2[idx], z470_2[idx], z560_2[idx], ids2[idx]\n",
    "    \n",
    "    # Move target mouse data\n",
    "    if target_mouse in ids2:\n",
    "        idx = ids2.index(target_mouse)\n",
    "        \n",
    "        # Store the data to move\n",
    "        target_v = v2[idx]\n",
    "        target_z470 = z470_2[idx]\n",
    "        target_z560 = z560_2[idx]\n",
    "        \n",
    "        # Remove original entry from Cohort 2\n",
    "        del v2[idx], z470_2[idx], z560_2[idx], ids2[idx]\n",
    "        \n",
    "        # Add to Cohort 1 (velocity and z_470, NaN for z_560)\n",
    "        v1.append(target_v)\n",
    "        z470_1.append(target_z470)\n",
    "        z560_1.append(np.nan)\n",
    "        ids1.append(target_mouse + \" (from Cohort 2)\")\n",
    "        \n",
    "        # Keep z_560 correlation in Cohort 2 (velocity and z_560, NaN for z_470)\n",
    "        v2.append(target_v)\n",
    "        z470_2.append(np.nan)\n",
    "        z560_2.append(target_z560)\n",
    "        ids2.append(target_mouse + \" (z_560)\")\n",
    "        \n",
    "        print(f\"✅ Moved velocity & z_470 of {target_mouse} to Cohort 1.\")\n",
    "        print(f\"✅ Retained velocity & z_560 of {target_mouse} in Cohort 2.\")\n",
    "    else:\n",
    "        print(f\"⚠️ Mouse {target_mouse} not found in Cohort 2.\")\n",
    "    \n",
    "    return v1, z470_1, z560_1, ids1, v2, z470_2, z560_2, ids2\n",
    "\n",
    "def analyze_correlations_from_data_fixed(\n",
    "    v1, z470_1, z560_1, ids1,\n",
    "    v2, z470_2, z560_2, ids2,\n",
    "    time_window=None,\n",
    "    plot=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Fixed correlation analysis function\n",
    "    \"\"\"\n",
    "    def fisher_z(r):\n",
    "        return 0.5 * np.log((1 + r) / (1 - r))\n",
    "\n",
    "    def compare_correlations(r1, n1, r2, n2):\n",
    "        z1 = fisher_z(r1)\n",
    "        z2 = fisher_z(r2)\n",
    "        se = np.sqrt(1 / (n1 - 3) + 1 / (n2 - 3))\n",
    "        z = (z1 - z2) / se\n",
    "        p = 2 * (1 - norm.cdf(abs(z)))\n",
    "        return z, p\n",
    "\n",
    "    def filter_valid_pairs(x, y, labels):\n",
    "        x = np.array(x)\n",
    "        y = np.array(y)\n",
    "        labels = np.array(labels)\n",
    "        \n",
    "        # Ensure all arrays have the same length\n",
    "        min_len = min(len(x), len(y), len(labels))\n",
    "        x = x[:min_len]\n",
    "        y = y[:min_len]\n",
    "        labels = labels[:min_len]\n",
    "        \n",
    "        # Filter out NaN values\n",
    "        mask = ~np.isnan(x) & ~np.isnan(y)\n",
    "        return x[mask], y[mask], labels[mask]\n",
    "\n",
    "    if time_window is None:\n",
    "        time_window = (0, 2)  # Default time window if not specified\n",
    "\n",
    "    # Filter valid pairs for each correlation\n",
    "    v1_470, z470_1_filt, ids1_470 = filter_valid_pairs(v1, z470_1, ids1)\n",
    "    v1_560, z560_1_filt, ids1_560 = filter_valid_pairs(v1, z560_1, ids1)\n",
    "    v2_470, z470_2_filt, ids2_470 = filter_valid_pairs(v2, z470_2, ids2)\n",
    "    v2_560, z560_2_filt, ids2_560 = filter_valid_pairs(v2, z560_2, ids2)\n",
    "\n",
    "    # Calculate correlations\n",
    "    results = {}\n",
    "    if len(v1_470) > 1:\n",
    "        corr1_470, p1_470 = pearsonr(v1_470, z470_1_filt)\n",
    "        results['corr1_470'] = (corr1_470, p1_470, len(v1_470))\n",
    "    else:\n",
    "        results['corr1_470'] = (np.nan, np.nan, len(v1_470))\n",
    "        \n",
    "    if len(v1_560) > 1:\n",
    "        corr1_560, p1_560 = pearsonr(v1_560, z560_1_filt)\n",
    "        results['corr1_560'] = (corr1_560, p1_560, len(v1_560))\n",
    "    else:\n",
    "        results['corr1_560'] = (np.nan, np.nan, len(v1_560))\n",
    "        \n",
    "    if len(v2_470) > 1:\n",
    "        corr2_470, p2_470 = pearsonr(v2_470, z470_2_filt)\n",
    "        results['corr2_470'] = (corr2_470, p2_470, len(v2_470))\n",
    "    else:\n",
    "        results['corr2_470'] = (np.nan, np.nan, len(v2_470))\n",
    "        \n",
    "    if len(v2_560) > 1:\n",
    "        corr2_560, p2_560 = pearsonr(v2_560, z560_2_filt)\n",
    "        results['corr2_560'] = (corr2_560, p2_560, len(v2_560))\n",
    "    else:\n",
    "        results['corr2_560'] = (np.nan, np.nan, len(v2_560))\n",
    "\n",
    "    # Print results\n",
    "    print(\"\\n📊 Correlations:\")\n",
    "    print(f\"Cohort 1: Velocity ~ z_470: r = {results['corr1_470'][0]:.3f}, p = {results['corr1_470'][1]:.3f} (n={results['corr1_470'][2]})\")\n",
    "    print(f\"Cohort 1: Velocity ~ z_560: r = {results['corr1_560'][0]:.3f}, p = {results['corr1_560'][1]:.3f} (n={results['corr1_560'][2]})\")\n",
    "    print(f\"Cohort 2: Velocity ~ z_470: r = {results['corr2_470'][0]:.3f}, p = {results['corr2_470'][1]:.3f} (n={results['corr2_470'][2]})\")\n",
    "    print(f\"Cohort 2: Velocity ~ z_560: r = {results['corr2_560'][0]:.3f}, p = {results['corr2_560'][1]:.3f} (n={results['corr2_560'][2]})\")\n",
    "\n",
    "    # Compare correlations\n",
    "    if results['corr1_470'][2] > 3 and results['corr2_470'][2] > 3:\n",
    "        z_470, p_470 = compare_correlations(results['corr1_470'][0], results['corr1_470'][2], \n",
    "                                          results['corr2_470'][0], results['corr2_470'][2])\n",
    "        print(f\"\\n🔍 Comparison of correlations for z_470:\")\n",
    "        print(f\"z = {z_470:.3f}, p = {p_470:.3f}\")\n",
    "    else:\n",
    "        print(\"⚠️ Not enough data to compare z_470 correlations.\")\n",
    "\n",
    "    if results['corr1_560'][2] > 3 and results['corr2_560'][2] > 3:\n",
    "        z_560, p_560 = compare_correlations(results['corr1_560'][0], results['corr1_560'][2], \n",
    "                                          results['corr2_560'][0], results['corr2_560'][2])\n",
    "        print(f\"\\n🔍 Comparison of correlations for z_560:\")\n",
    "        print(f\"z = {z_560:.3f}, p = {p_560:.3f}\")\n",
    "    else:\n",
    "        print(\"⚠️ Not enough data to compare z_560 correlations.\")\n",
    "\n",
    "    # Create plots\n",
    "    if plot:\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "        t_str = f\"{time_window[0]} to {time_window[1]}s\"\n",
    "\n",
    "        # Plot z_470 correlations\n",
    "        if len(v1_470) > 0:\n",
    "            axs[0].scatter(v1_470, z470_1_filt, color='green', label=f'Cohort 1 (n={len(v1_470)})', alpha=0.7)\n",
    "            for i, mouse in enumerate(ids1_470):\n",
    "                axs[0].text(v1_470[i], z470_1_filt[i], mouse, fontsize=6, color='green', alpha=0.6)\n",
    "            if len(v1_470) > 1:\n",
    "                m1, b1 = np.polyfit(v1_470, z470_1_filt, 1)\n",
    "                x_range = np.linspace(min(v1_470), max(v1_470), 100)\n",
    "                axs[0].plot(x_range, m1 * x_range + b1, color='green', linestyle='--', alpha=0.8)\n",
    "        \n",
    "        if len(v2_470) > 0:\n",
    "            axs[0].scatter(v2_470, z470_2_filt, color='orange', label=f'Cohort 2 (n={len(v2_470)})', alpha=0.7)\n",
    "            for i, mouse in enumerate(ids2_470):\n",
    "                axs[0].text(v2_470[i], z470_2_filt[i], mouse, fontsize=6, color='orange', alpha=0.6)\n",
    "            if len(v2_470) > 1:\n",
    "                m2, b2 = np.polyfit(v2_470, z470_2_filt, 1)\n",
    "                x_range = np.linspace(min(v2_470), max(v2_470), 100)\n",
    "                axs[0].plot(x_range, m2 * x_range + b2, color='orange', linestyle='--', alpha=0.8)\n",
    "        \n",
    "        axs[0].set_title(f'Velocity vs z_470\\n({t_str})')\n",
    "        axs[0].set_xlabel('Mean Velocity_0X (m/s)')\n",
    "        axs[0].set_ylabel('Mean z-score (470nm)')\n",
    "        axs[0].legend()\n",
    "        axs[0].grid(True, alpha=0.3)\n",
    "\n",
    "        # Plot z_560 correlations\n",
    "        if len(v1_560) > 0:\n",
    "            axs[1].scatter(v1_560, z560_1_filt, color='red', label=f'Cohort 1 (n={len(v1_560)})', alpha=0.7)\n",
    "            for i, mouse in enumerate(ids1_560):\n",
    "                axs[1].text(v1_560[i], z560_1_filt[i], mouse, fontsize=6, color='red', alpha=0.6)\n",
    "            if len(v1_560) > 1:\n",
    "                m1, b1 = np.polyfit(v1_560, z560_1_filt, 1)\n",
    "                x_range = np.linspace(min(v1_560), max(v1_560), 100)\n",
    "                axs[1].plot(x_range, m1 * x_range + b1, color='red', linestyle='--', alpha=0.8)\n",
    "        \n",
    "        if len(v2_560) > 0:\n",
    "            axs[1].scatter(v2_560, z560_2_filt, color='darkred', label=f'Cohort 2 (n={len(v2_560)})', alpha=0.7)\n",
    "            for i, mouse in enumerate(ids2_560):\n",
    "                axs[1].text(v2_560[i], z560_2_filt[i], mouse, fontsize=6, color='darkred', alpha=0.6)\n",
    "            if len(v2_560) > 1:\n",
    "                m2, b2 = np.polyfit(v2_560, z560_2_filt, 1)\n",
    "                x_range = np.linspace(min(v2_560), max(v2_560), 100)\n",
    "                axs[1].plot(x_range, m2 * x_range + b2, color='darkred', linestyle='--', alpha=0.8)\n",
    "        \n",
    "        axs[1].set_title(f'Velocity vs z_560\\n({t_str})')\n",
    "        axs[1].set_xlabel('Mean Velocity_0X (m/s)')\n",
    "        axs[1].set_ylabel('Mean z-score (560nm)')\n",
    "        axs[1].legend()\n",
    "        axs[1].grid(True, alpha=0.3)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84ef2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = analyze_correlations_from_data_fixed(\n",
    "    v1, z470_1, z560_1, ids1,\n",
    "    v2, z470_2, z560_2, ids2,\n",
    "    time_window=(0, 2),  # Specify your time window\n",
    "    plot=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fe0919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRAND AVERAGE PLOT WHATEVER YOU WANT but make sure to change the legends accordingly\n",
    "#-------------  \n",
    "#COHORT3 paths\n",
    "# vmm3_cohort3_right_nohalt = '/Volumes/RanczLab2/20250409_Cohort3_rotation/Visual_mismatch_day3/grand_averages_with_sem_No halt_right_turns_baselined_data.csv'\n",
    "# vmm3_cohort3_left_nohalt = '/Volumes/RanczLab2/20250409_Cohort3_rotation/Visual_mismatch_day3/grand_averages_with_sem_No halt_left_turns_baselined_data.csv'\n",
    "# vmm3_cohort3_left = '/Volumes/RanczLab2/20250409_Cohort3_rotation/Visual_mismatch_day3/grand_averages_with_sem_Apply halt_2s_left_turns_baselined_data.csv'\n",
    "# vmm3_cohort3_right = '/Volumes/RanczLab2/20250409_Cohort3_rotation/Visual_mismatch_day3/grand_averages_with_sem_Apply halt_2s_right_turns_baselined_data.csv'\n",
    "# vmm4_cohort3_right_nohalt = '/Volumes/RanczLab2/20250409_Cohort3_rotation/Visual_mismatch_day4/grand_averages_with_sem_No halt_right_turns_baselined_data.csv'\n",
    "# vmm4_cohort3_left_nohalt = '/Volumes/RanczLab2/20250409_Cohort3_rotation/Visual_mismatch_day4/grand_averages_with_sem_No halt_left_turns_baselined_data.csv'\n",
    "# vmm4_cohort3_left = '/Volumes/RanczLab2/20250409_Cohort3_rotation/Visual_mismatch_day4/grand_averages_with_sem_Apply halt_2s_left_turns_baselined_data.csv'\n",
    "# vmm4_cohort3_right = '/Volumes/RanczLab2/20250409_Cohort3_rotation/Visual_mismatch_day4/grand_averages_with_sem_Apply halt_2s_right_turns_baselined_data.csv'\n",
    "# vmm3_cohort3= '/Volumes/RanczLab2/20250409_Cohort3_rotation/Visual_mismatch_day3/grand_averages_with_sem_B6J2780_B6J2781_B6J2783_B6J2782.csv'\n",
    "# vmm3_cohort3_nohalt = '/Volumes/RanczLab2/20250409_Cohort3_rotation/Visual_mismatch_day3/grand_averages_with_sem_No halt_baselined_data.csv'\n",
    "vmm4_cohort3 = '/Volumes/RanczLab2/20250409_Cohort3_rotation/Visual_mismatch_day4/grand_averages_with_sem_Apply halt_2s_baselined_data_no_2782.csv'\n",
    "# vmm4_cohort3_nohalt = '/Volumes/RanczLab2/20250409_Cohort3_rotation/Visual_mismatch_day4/grand_averages_with_sem_No halt_baselined_data.csv'\n",
    "\n",
    "# #COHORT1 paths\n",
    "# vmm3_cohort1_right_nohalt = '/Volumes/RanczLab2/20241125_Cohort1_rotation/Visual_mismatch_day3/grand_averages_with_sem_No halt_right_turns_baselined_data.csv'\n",
    "# vmm3_cohort1_left_nohalt = '/Volumes/RanczLab2/20241125_Cohort1_rotation/Visual_mismatch_day3/grand_averages_with_sem_No halt_left_turns_baselined_data.csv'\n",
    "# vmm3_cohort1_left = '/Volumes/RanczLab2/20241125_Cohort1_rotation/Visual_mismatch_day3/grand_averages_with_sem_Apply halt_2s_left_turns_baselined_data.csv'\n",
    "# vmm3_cohort1_right = '/Volumes/RanczLab2/20241125_Cohort1_rotation/Visual_mismatch_day3/grand_averages_with_sem_Apply halt_2s_right_turns_baselined_data.csv'\n",
    "# vmm4_cohort1_right_nohalt = '/Volumes/RanczLab2/20241125_Cohort1_rotation/Visual_mismatch_day4/grand_averages_with_sem_No halt_right_turns_baselined_data.csv'\n",
    "# vmm4_cohort1_left_nohalt = '/Volumes/RanczLab2/20241125_Cohort1_rotation/Visual_mismatch_day4/grand_averages_with_sem_No halt_left_turns_baselined_data.csv'\n",
    "# vmm4_cohort1_left = '/Volumes/RanczLab2/20241125_Cohort1_rotation/Visual_mismatch_day4/grand_averages_with_sem_Apply halt_2s_left_turns_baselined_data.csv'\n",
    "# vmm4_cohort1_right = '/Volumes/RanczLab2/20241125_Cohort1_rotation/Visual_mismatch_day4/grand_averages_with_sem_Apply halt_2s_right_turns_baselined_data.csv'\n",
    "# vmm3_cohort1 = '/Volumes/RanczLab2/20241125_Cohort1_rotation/Visual_mismatch_day3/grand_averages_with_sem_B6J2717_B6J2718_B6J2719_B6J2721_B6J2722_B6J2723.csv'\n",
    "# vmm3_cohort1_nohalt = '/Volumes/RanczLab2/20241125_Cohort1_rotation/Visual_mismatch_day3/grand_averages_with_sem_No halt_baselined_data.csv'\n",
    "vmm4_cohort1= '/Volumes/RanczLab2/20241125_Cohort1_rotation/Visual_mismatch_day4/grand_averages_with_sem_Apply halt_2s_baselined_data.csv'\n",
    "# vmm4_cohort1_nohalt= '/Volumes/RanczLab2/20241125_Cohort1_rotation/Visual_mismatch_day4/grand_averages_with_sem_No halt_baselined_data.csv'\n",
    "\n",
    "# Load the data\n",
    "df_cohort1 = pd.read_csv(vmm4_cohort1)\n",
    "df_cohort3 = pd.read_csv(vmm4_cohort3)\n",
    "\n",
    "# PLOT properties\n",
    "plt.rcParams.update({\n",
    "    'font.size': 10,           # Set global font size\n",
    "    'font.family': 'sans-serif',  # Font family (e.g., 'serif', 'sans-serif', 'monospace')\n",
    "    'font.sans-serif': ['Arial'],  # Preferred font\n",
    "    'axes.titlesize': 10,      # Title font size\n",
    "    'axes.labelsize': 10,      # Axis label size\n",
    "    'legend.fontsize': 8,     # Legend text\n",
    "    'xtick.labelsize': 10,\n",
    "    'ytick.labelsize': 10\n",
    "})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))  # Create figure and primary axis\n",
    "ax2 = ax.twinx()  # Create secondary y-axis\n",
    "ax.minorticks_on()  # Enable minor ticks\n",
    "ax.grid(which='both', linestyle='--', linewidth=0.5, alpha=0.7)  # Add grid for both major and minor ticks\n",
    "\n",
    "# Plot df1v\n",
    "ax.plot(df_cohort1['Time (s)'], df_cohort1['z_470'], label='GRAB-5HT3.0(z-score)', color='green', alpha=1)\n",
    "ax.fill_between(df_cohort1['Time (s)'],\n",
    "                df_cohort1['z_470'] - df_cohort1['z_470_SEM'],\n",
    "                df_cohort1['z_470'] + df_cohort1['z_470_SEM'],\n",
    "                color='green', alpha=0.1)\n",
    "\n",
    "# Plot df3\n",
    "ax.plot(df_cohort3['Time (s)'], df_cohort3['z_470'], label='mut-GRAB-5HT3.0 (z-score)', color='orange', alpha=1, linestyle='--')\n",
    "ax.fill_between(df_cohort3['Time (s)'],\n",
    "                df_cohort3['z_470'] - df_cohort3['z_470_SEM'],\n",
    "                df_cohort3['z_470'] + df_cohort3['z_470_SEM'],\n",
    "                color='orange', alpha=0.1)\n",
    "\n",
    "# Create a secondary y-axis for velocity\n",
    "ax2.plot(df_cohort1['Time (s)'], df_cohort1['Velocity_0X'], label='MM (running Velocity)', color='slategray')\n",
    "ax2.fill_between(df_cohort1['Time (s)'],\n",
    "                 df_cohort1['Velocity_0X'] - df_cohort1['Velocity_0X_SEM'],\n",
    "                 df_cohort1['Velocity_0X'] + df_cohort1['Velocity_0X_SEM'],\n",
    "                 color='slategray', alpha=0.2)\n",
    "\n",
    "ax2.plot(df_cohort3['Time (s)'], df_cohort3['Velocity_0X'], label='MM (running Velocity)', color='slategray',linestyle='--')\n",
    "ax2.fill_between(df_cohort3['Time (s)'],\n",
    "                 df_cohort3['Velocity_0X'] - df_cohort3['Velocity_0X_SEM'],\n",
    "                 df_cohort3['Velocity_0X'] + df_cohort3['Velocity_0X_SEM'],\n",
    "                 color='slategray', alpha=0.2)\n",
    "\n",
    "# Customize the axes\n",
    "ax.set_ylabel('z-score', fontname='Arial', fontsize=10, color='black')\n",
    "ax.tick_params(axis='y', labelcolor='black')\n",
    "ax2.set_ylabel('Running speed (m/s)', fontname='Arial', fontsize=10, color='slategray')\n",
    "ax2.tick_params(axis='y', labelcolor='slategray')\n",
    "\n",
    "# Add a gray shadowed area between seconds 0 and 2\n",
    "ax.axvspan(0, 2, color='gray', alpha=0.2, label='Visual mismatch (0-2s)')\n",
    "\n",
    "# Plot customization\n",
    "ax.set_title('Grand Averages with SEM', fontname='Arial', fontsize=12)\n",
    "ax.set_xlabel('Time (s)', fontname='Arial', fontsize=10)\n",
    "\n",
    "# Combine legends from both axes and adjust position\n",
    "lines, labels = ax.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax.legend(lines + lines2, labels + labels2, loc='upper right', prop={'family': 'Arial', 'size': 10})\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2880d5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aeon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
