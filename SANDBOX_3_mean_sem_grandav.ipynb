{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "52b677f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------------------------------#\n",
    "# IMPORTS\n",
    "#---------------------------------------------------------------------------------------------------#\n",
    "from pathlib import Path\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from matplotlib import cm\n",
    "import plotly.io as pio\n",
    "import plotly.express as px\n",
    "import plotly.subplots as sp\n",
    "import math\n",
    "import re\n",
    "from pprint import pprint\n",
    "import pickle\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy.stats import mode, pearsonr, norm, ttest_rel\n",
    "from scipy.integrate import cumulative_trapezoid\n",
    "from scipy.signal import correlate\n",
    "import gc  # garbage collector for removing large variables from memory instantly \n",
    "import importlib  # for force updating changed packages \n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from typing import Dict, Optional, Tuple, Iterable\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Interactive widgets for dropdowns\n",
    "try:\n",
    "    import ipywidgets as widgets\n",
    "    from IPython.display import display, clear_output\n",
    "    WIDGETS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è ipywidgets not available. Install with: pip install ipywidgets\")\n",
    "    WIDGETS_AVAILABLE = False\n",
    "\n",
    "%config Completer.use_jedi = False  # Fixes autocomplete issues\n",
    "%config InlineBackend.figure_format = 'retina'  # Improves plot resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f05154b",
   "metadata": {},
   "source": [
    "# Aligned Data Analysis Notebook: for analysis of .csv files in the 'aligned data' folders produced in Sandbox_2 \n",
    "\n",
    "This notebook computes per-mouse mean/SEM traces, grand averages for selected columns in the \"_baselined_data.csv\" (user defines which csv to process in the configuration cell), and post-alignment signal feature metrics (peak amplitude, onset time at 20% of peak, half-width decay) for z_560_Baseline and z_470_Baseline. \n",
    "In addition, the analysis always reports the across-mouse mean ¬± SEM of `z_560_Baseline` between 0‚Äì5 seconds post-alignment.\n",
    "for interactive grand averages plotting, you can just run imports, define paths, and directly skip to the cell called \"INTERACTIVE GRAND AVERAGES PLOTTING\". It wil load previously generated files.\n",
    "**Usage:**\n",
    "1. Configure settings in the Configuration section (cohort, mice, event, analysis window).\n",
    "2. Select animals to process.\n",
    "3. Tune the post-alignment feature window via `POST_ALIGNMENT_WINDOW_DURATION` if needed.\n",
    "4. Choose whether to save grand average CSVs and signal-metric outputs.\n",
    "5. Run the analysis cells.\n",
    "\n",
    "**Output:**\n",
    "- Grand averages CSV: per-timepoint means and SEM across animals.\n",
    "- Signal metric CSVs: mean and SEM of \"SIGNAL_METRIC_FEATURES\" saved under `aligned_data_feature_summary/`.\n",
    "- z_560 window stat: group mean ¬± SEM for `z_560_Baseline` in the 0‚Äì5 s post window (printed alongside other metrics).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "15ca8b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cohort: Cohort3\n",
      "‚úÖ Available mice: ['B6J2780', 'B6J2781', 'B6J2783', 'B6J2782']\n",
      "‚úÖ Selected mice: ['B6J2780', 'B6J2781', 'B6J2783', 'B6J2782']\n"
     ]
    }
   ],
   "source": [
    "# CONFIGURATION SECTION\n",
    "#---------------------------------------------------------------------------------------------------#\n",
    "# Configure all settings here before running the analysis\n",
    "# EVENT NAME to decide which files to process in the aligned_data folder\n",
    "event_name = '_Apply halt_2s_baselined_data.csv'  # Options: '_Apply halt_2s_baselined_data.csv','_Apply halt_2s_right_turns_baselined_data', '_Apply halt_2s_left_turns_baselined_data', '_No halt_right_turns_baselined_data.csv', '_No halt_left_turns_baselined_data.csv'\n",
    "# Cohort selection\n",
    "COHORT_OPTIONS = {\n",
    "    \"Cohort1\": {\n",
    "        \"mice\": ['B6J2717', 'B6J2718', 'B6J2719', 'B6J2721', 'B6J2722', 'B6J2723'],\n",
    "        \"identifier\": \"Cohort1\"\n",
    "    },\n",
    "    \"Cohort3\": {\n",
    "        \"mice\": [\"B6J2780\", \"B6J2781\", \"B6J2783\", \"B6J2782\"],\n",
    "        \"identifier\": \"Cohort3\"\n",
    "    }\n",
    "}\n",
    "# Select cohort\n",
    "cohort_identifier = \"Cohort3\"  # Options: \"Cohort1\" or \"Cohort31\n",
    "# Select which animals to process (subset of the cohort's available mice)\n",
    "# Leave empty list [] to process all mice in the cohort\n",
    "selected_mice = []  # Example: ['B6J2717', 'B6J2718'] or [] for all\n",
    "# Data columns to analyze\n",
    "selected_columns = [\n",
    "    'Velocity_0X_Baseline', 'Motor_Velocity_Baseline', \n",
    "    'z_470_Baseline', 'z_560_Baseline'\n",
    "]\n",
    "# Data directories (add your paths here)\n",
    "DATA_DIRS = [\n",
    "    # Path('/home/ikharitonov/RANCZLAB-NAS/data/ONIX/20241125_Cohort1_rotation/Visual_mismatch_day4').expanduser(),\n",
    "    # Path('/Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day3').expanduser(),\n",
    "    # Path('/Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4').expanduser(),\n",
    "    # Path('/Volumes/RanczLab2/Cohort3_rotation/Visual_mismatch_day3').expanduser(),\n",
    "    Path('/Volumes/RanczLab2/Cohort3_rotation/Visual_mismatch_day4').expanduser(),\n",
    "    # # Add more directories as needed\n",
    "]\n",
    "SAVE_CSV = False      # Save grand averages with SEM as CSV file\n",
    "SAVE_SIGNAL_METRICS = True\n",
    "SIGNAL_METRICS_OUTPUT_SUBDIR = DATA_DIRS[0]\n",
    "\n",
    "# Signal feature metric settings (incorporated from SANDBOX 4 workflow)\n",
    "SIGNAL_METRIC_COLUMNS = [\n",
    "    'z_470_Baseline',\n",
    "    'z_560_Baseline',\n",
    "]\n",
    "SIGNAL_METRIC_FEATURES = (\n",
    "    'peak',\n",
    "    'onset_time',\n",
    "    'decay_tau',\n",
    ")\n",
    "# Configure the post-alignment analysis window (seconds relative to alignment time).\n",
    "# Adjust POST_ALIGNMENT_WINDOW_DURATION to control the length of the comparison interval.\n",
    "POST_ALIGNMENT_WINDOW_START = 0.0\n",
    "POST_ALIGNMENT_WINDOW_DURATION = 1.0\n",
    "POST_ALIGNMENT_WINDOW = (\n",
    "    POST_ALIGNMENT_WINDOW_START,\n",
    "    POST_ALIGNMENT_WINDOW_START + POST_ALIGNMENT_WINDOW_DURATION,\n",
    ")\n",
    "#---------------------------------------------------------------------------------------------------#\n",
    "# Auto-configure based on cohort selection\n",
    "#---------------------------------------------------------------------------------------------------#\n",
    "if cohort_identifier in COHORT_OPTIONS:\n",
    "    cohort_info = COHORT_OPTIONS[cohort_identifier]\n",
    "    available_mice = cohort_info[\"mice\"]\n",
    "    if not selected_mice:  # If empty, use all mice\n",
    "        selected_mice = available_mice\n",
    "    else:  # Filter to only include valid mice for the selected cohort\n",
    "        filtered_mice = [m for m in selected_mice if m in available_mice]\n",
    "        ignored_mice = [m for m in selected_mice if m not in available_mice]\n",
    "        selected_mice = filtered_mice\n",
    "        if ignored_mice:\n",
    "            print(f\"‚è≠Ô∏è Ignoring mice not in cohort {cohort_identifier}: {ignored_mice}\")\n",
    "    print(f\"‚úÖ Cohort: {cohort_identifier}\")\n",
    "    print(f\"‚úÖ Available mice: {available_mice}\")\n",
    "    print(f\"‚úÖ Selected mice: {selected_mice}\")\n",
    "else:\n",
    "    raise ValueError(f\"Invalid cohort_identifier: {cohort_identifier}. Must be one of {list(COHORT_OPTIONS.keys())}\")\n",
    "\n",
    "# Save options\n",
    "# SAVE_PICKLE = False  # Save results as pickle file (deprecated - use SAVE_ANIMAL_CSV instead)\n",
    "#FIXME THIS IS REGENERATING SOMETHING BUT IT DOES NOT MAKE SENSE! SAVE_ANIMAL_CSV = False  # Save averaged mismatch aligned data for each animal as CSV\n",
    "#FIXME WE WILL NOT USE THESE PLOTS FOR NOW!!! GENERATE_PLOTS = True  # Generate plots\n",
    "# # # Columns to plot\n",
    "# # columns_to_plot = [\n",
    "# #     'Velocity_0X_Baseline', 'Motor_Velocity_Baseline', \n",
    "# #     'z_470_Baseline', 'z_560_Baseline'\n",
    "# ]\n",
    "# Pre/post comparison plotting options\n",
    "# FIXME WE WILL NOT USE THESE PLOTS FOR NOW!!! PLOT_PREPOST_FROM_RESULTS = True  # Generate pre/post plots from freshly computed results\n",
    "# FIXME NEED TO VERIFY WHAT IS GOING ON LOAD_EXISTING_PREPOST_CSV = False  # Load a previously created cohort_aligned_data_analysis.csv\n",
    "# FIXME VERIFY WHAT IS GOING ONEXISTING_PREPOST_CSV_PATH = Path('/Users/nora/Desktop/for_poster/cohort_3/cohort_aligned_data_analysis.csv').expanduser()\n",
    "# PREPOST_SAVE_DIR = None  # Optional custom directory to save pre/post plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3ed85884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded B6J2780: /Volumes/RanczLab2/Cohort3_rotation/Visual_mismatch_day4/B6J2780-2025-04-28T13-10-18_processedData/aligned_data/B6J2780_Apply halt_2s_baselined_data.csv\n",
      "‚úÖ Loaded B6J2781: /Volumes/RanczLab2/Cohort3_rotation/Visual_mismatch_day4/B6J2781-2025-04-28T13-45-40_processedData/aligned_data/B6J2781_Apply halt_2s_baselined_data.csv\n",
      "‚úÖ Loaded B6J2782: /Volumes/RanczLab2/Cohort3_rotation/Visual_mismatch_day4/B6J2782-2025-04-28T14-22-03_processedData/aligned_data/B6J2782_Apply halt_2s_baselined_data.csv\n",
      "‚úÖ Loaded B6J2783: /Volumes/RanczLab2/Cohort3_rotation/Visual_mismatch_day4/B6J2783-2025-04-28T14-57-30_processedData/aligned_data/B6J2783_Apply halt_2s_baselined_data.csv\n"
     ]
    }
   ],
   "source": [
    "# DATA LOADING\n",
    "#---------------------------------------------------------------------------------------------------#\n",
    "def load_aligned_data(data_dirs, event_name, selected_mice, allowed_mice=None):\n",
    "    \"\"\"\n",
    "    Load aligned data from CSV files for selected mice.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data_dirs : list\n",
    "        List of data directory paths\n",
    "    event_name : str\n",
    "        Event name suffix for CSV files\n",
    "    selected_mice : list\n",
    "        List of mouse names explicitly requested (may be empty)\n",
    "    allowed_mice : list\n",
    "        Full set of cohort-allowed mice (used when selected_mice is empty)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    loaded_data : dict\n",
    "        Dictionary with data paths as keys and mouse data as values\n",
    "    \"\"\"\n",
    "    if selected_mice:\n",
    "        selected_mice_set = set(selected_mice)\n",
    "    elif allowed_mice:\n",
    "        selected_mice_set = set(allowed_mice)\n",
    "    else:\n",
    "        selected_mice_set = None\n",
    "\n",
    "    # Collect raw data paths (excluding '_processedData' dirs) and keep only selected/cohort mice when provided\n",
    "    rawdata_paths = []\n",
    "    for data_dir in data_dirs:\n",
    "        path_obj = Path(data_dir)\n",
    "        if not path_obj.exists():\n",
    "            continue\n",
    "        for subdir in path_obj.iterdir():\n",
    "            if not subdir.is_dir() or subdir.name.endswith('_processedData'):\n",
    "                continue\n",
    "            mouse_id = subdir.name.split('-')[0]\n",
    "            if selected_mice_set and mouse_id not in selected_mice_set:\n",
    "                continue\n",
    "            rawdata_paths.append(subdir)\n",
    "    \n",
    "    if not rawdata_paths:\n",
    "        if selected_mice_set:\n",
    "            print(\"‚ö†Ô∏è No matching data directories found for the selected cohort/mice.\")\n",
    "        return {}\n",
    "    \n",
    "    # Build processed data paths\n",
    "    data_paths = [raw.parent / f\"{raw.name}_processedData/aligned_data\" for raw in rawdata_paths]\n",
    "    mouse_names = [raw.name.split('-')[0] for raw in rawdata_paths]\n",
    "    mouse_name_set = set(mouse_names)\n",
    "\n",
    "    if selected_mice_set:\n",
    "        filtered_selection = [m for m in selected_mice_set if m in mouse_name_set]\n",
    "        ignored_selection = [m for m in selected_mice_set if m not in mouse_name_set]\n",
    "        if ignored_selection:\n",
    "            print(f\"‚è≠Ô∏è Ignoring selected mice not present in data directories: {sorted(ignored_selection)}\")\n",
    "        target_mice = set(filtered_selection)\n",
    "    else:\n",
    "        target_mice = mouse_name_set\n",
    "    \n",
    "    # Load data\n",
    "    loaded_data = {}\n",
    "    for idx, data_path in enumerate(data_paths, start=1):\n",
    "        mouse_name = mouse_names[idx - 1]\n",
    "        \n",
    "        if mouse_name not in target_mice:\n",
    "            continue\n",
    "    \n",
    "        if mouse_name == 'baselined':\n",
    "            continue\n",
    "    \n",
    "        csv_file_path = data_path / f\"{mouse_name}{event_name}\"\n",
    "    \n",
    "        try:\n",
    "            aligned_df = pd.read_csv(csv_file_path)\n",
    "            print(f\"‚úÖ Loaded {mouse_name}: {csv_file_path}\")\n",
    "            \n",
    "            loaded_data[data_path] = {\n",
    "                'mouse_name': mouse_name,\n",
    "                'data_path': data_path\n",
    "            }\n",
    "            \n",
    "            # Add each column as a separate key\n",
    "            for column in aligned_df.columns:\n",
    "                loaded_data[data_path][column] = aligned_df[column].values\n",
    "            \n",
    "            del aligned_df\n",
    "            gc.collect()\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            continue\n",
    "    \n",
    "    return loaded_data\n",
    "\n",
    "# Load the data\n",
    "loaded_data = load_aligned_data(DATA_DIRS, event_name, selected_mice, available_mice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f8516c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OUTPUT DIRECTORY HELPERS\n",
    "#---------------------------------------------------------------------------------------------------#\n",
    "\n",
    "def determine_main_data_dir(loaded_data, data_dirs, cohort_identifier):\n",
    "    \"\"\"Select the base data directory corresponding to the loaded cohort data.\"\"\"\n",
    "    data_dir_paths = [Path(p).expanduser().resolve() for p in data_dirs]\n",
    "\n",
    "    loaded_base_dirs = []\n",
    "    for data_path in loaded_data.keys():\n",
    "        base_dir = Path(data_path).resolve().parent.parent\n",
    "        if base_dir not in loaded_base_dirs:\n",
    "            loaded_base_dirs.append(base_dir)\n",
    "\n",
    "    if not loaded_base_dirs:\n",
    "        for candidate in data_dir_paths:\n",
    "            if candidate.exists():\n",
    "                return candidate\n",
    "        return Path.cwd()\n",
    "\n",
    "    cohort_id = (cohort_identifier or \"\").lower()\n",
    "    if cohort_id:\n",
    "        for candidate in data_dir_paths:\n",
    "            candidate_resolved = Path(candidate).resolve()\n",
    "            if candidate_resolved in loaded_base_dirs and cohort_id in str(candidate_resolved).lower():\n",
    "                return candidate_resolved\n",
    "\n",
    "    for candidate in data_dir_paths:\n",
    "        candidate_resolved = Path(candidate).resolve()\n",
    "        if candidate_resolved in loaded_base_dirs:\n",
    "            return candidate_resolved\n",
    "\n",
    "    return loaded_base_dirs[0]\n",
    "\n",
    "def extract_experiment_day(base_dir: Path) -> str:\n",
    "    \"\"\"Derive experiment day label from the selected base directory.\"\"\"\n",
    "    if not base_dir or not isinstance(base_dir, Path):\n",
    "        return \"unknown\"\n",
    "    return base_dir.name or \"unknown\"\n",
    "\n",
    "\n",
    "def sanitize_for_filename(value: Optional[str], fallback: str = \"unknown\") -> str:\n",
    "    \"\"\"Convert a string into a filesystem-friendly fragment.\"\"\"\n",
    "    if value is None:\n",
    "        return fallback\n",
    "    cleaned = re.sub(r\"[^A-Za-z0-9_-]+\", \"_\", str(value)).strip(\"_\")\n",
    "    return cleaned or fallback\n",
    "\n",
    "\n",
    "def build_grand_average_filename(cohort_id: str, experiment_day: str, event_name: str) -> str:\n",
    "    \"\"\"Compose the output filename for grand average CSV exports.\"\"\"\n",
    "    cohort_part = sanitize_for_filename(cohort_id, \"cohort\")\n",
    "    day_part = sanitize_for_filename(experiment_day, \"day\")\n",
    "    event_label = sanitize_for_filename(event_name.replace(\".csv\", \"\"), \"event\")\n",
    "    return f\"{cohort_part}_{day_part}_grand_averages_with_sem_{event_label}.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89dc0c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANALYSIS FUNCTIONS\n",
    "#---------------------------------------------------------------------------------------------------#\n",
    "\n",
    "def compute_mouse_means_and_grand_average(loaded_data, selected_columns, main_data_dir, selected_mice):\n",
    "    \"\"\"\n",
    "    Compute means per mouse and grand averages across selected mice for selected columns.\n",
    "    \n",
    "    Parameters:\n",
    "    loaded_data (dict): Dictionary with data paths as keys and mouse data as values\n",
    "    selected_columns (list): List of column names to analyze\n",
    "    main_data_dir (str/Path): Main directory to save results\n",
    "    selected_mice (list): List of mouse names to include in the grand average\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (mean_data_per_mouse, sem_data_per_mouse, grand_averages, grand_sems, mouse_to_data_path)\n",
    "        mouse_to_data_path: Dictionary mapping mouse names to their data_path (aligned_data folder)\n",
    "    \"\"\"\n",
    "    \n",
    "    main_data_dir = Path(main_data_dir)\n",
    "    \n",
    "    loaded_mouse_names = [data['mouse_name'] for data in loaded_data.values()]\n",
    "    loaded_mouse_set = set(loaded_mouse_names)\n",
    "\n",
    "    if selected_mice:\n",
    "        filtered_selection = [m for m in selected_mice if m in loaded_mouse_set]\n",
    "        ignored_selection = [m for m in selected_mice if m not in loaded_mouse_set]\n",
    "        if ignored_selection:\n",
    "            print(f\"‚è≠Ô∏è Ignoring selected mice without loaded data: {ignored_selection}\")\n",
    "        selected_mice = filtered_selection\n",
    "    else:\n",
    "        selected_mice = loaded_mouse_names\n",
    "\n",
    "    if not selected_mice:\n",
    "        print(\"‚ö†Ô∏è No mice available for grand average computation.\")\n",
    "        return {}, {}, pd.DataFrame(), pd.DataFrame(), {}\n",
    "    \n",
    "    print(f\"Processing selected columns: {selected_columns}\")\n",
    "    \n",
    "    # Step 1: Compute mean and SEM for each mouse\n",
    "    mean_data_per_mouse = {}\n",
    "    sem_data_per_mouse = {}\n",
    "    mouse_to_data_path = {}  # Track data_path for each mouse\n",
    "    \n",
    "    for data_path, data in loaded_data.items():\n",
    "        mouse_name = data['mouse_name']\n",
    "        if mouse_name not in selected_mice:\n",
    "            continue\n",
    "        \n",
    "        print(f\"Processing mouse: {mouse_name}\")\n",
    "        \n",
    "        # Track data_path for this mouse\n",
    "        mouse_to_data_path[mouse_name] = data_path\n",
    "        \n",
    "        # Create DataFrame from the loaded data\n",
    "        df = pd.DataFrame(data)\n",
    "        \n",
    "        # Check which selected columns are available\n",
    "        available_columns = [col for col in selected_columns if col in df.columns]\n",
    "        missing_columns = [col for col in selected_columns if col not in df.columns]\n",
    "        \n",
    "        if missing_columns:\n",
    "            print(f\"‚ö†Ô∏è  Missing columns for {mouse_name}: {missing_columns}\")\n",
    "        \n",
    "        if 'Time (s)' not in df.columns:\n",
    "            print(f\"‚ö†Ô∏è  'Time (s)' column not found for {mouse_name}, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        # Group by time and compute mean and SEM\n",
    "        grouped = df.groupby('Time (s)')\n",
    "        \n",
    "        # Only use numeric columns that are in our selected list\n",
    "        numeric_selected = []\n",
    "        for col in available_columns:\n",
    "            if col != 'Time (s)' and pd.api.types.is_numeric_dtype(df[col]):\n",
    "                numeric_selected.append(col)\n",
    "        \n",
    "        if len(numeric_selected) == 0:\n",
    "            print(f\"‚ö†Ô∏è  No numeric columns found for {mouse_name}\")\n",
    "            continue\n",
    "        \n",
    "        mean_data_per_mouse[mouse_name] = grouped[numeric_selected].mean()\n",
    "        sem_data_per_mouse[mouse_name] = grouped[numeric_selected].sem()\n",
    "        \n",
    "        print(f\"‚úÖ Processed {len(numeric_selected)} columns for {mouse_name}\")\n",
    "    \n",
    "    # Step 2: Compute grand averages across selected mice\n",
    "    print(f\"\\nüìä Computing grand averages across {len(mean_data_per_mouse)} selected mice...\")\n",
    "    \n",
    "    # Get all unique time points\n",
    "    all_time_points = set()\n",
    "    for mouse_data in mean_data_per_mouse.values():\n",
    "        all_time_points.update(mouse_data.index)\n",
    "    all_time_points = sorted(list(all_time_points))\n",
    "    \n",
    "    # Get all columns that were successfully processed\n",
    "    all_processed_columns = set()\n",
    "    for mouse_data in mean_data_per_mouse.values():\n",
    "        all_processed_columns.update(mouse_data.columns)\n",
    "    all_processed_columns = sorted(list(all_processed_columns))\n",
    "    \n",
    "    print(f\"Time points: {len(all_time_points)} from {min(all_time_points):.2f}s to {max(all_time_points):.2f}s\")\n",
    "    print(f\"Processed columns: {all_processed_columns}\")\n",
    "    \n",
    "    # Create grand average DataFrame\n",
    "    grand_averages = pd.DataFrame(index=all_time_points, columns=all_processed_columns)\n",
    "    grand_averages.index.name = 'Time (s)'\n",
    "    \n",
    "    grand_sems = pd.DataFrame(index=all_time_points, columns=all_processed_columns)\n",
    "    grand_sems.index.name = 'Time (s)'\n",
    "    \n",
    "    # Compute grand averages for each column and time point\n",
    "    for col in all_processed_columns:\n",
    "        for time_point in all_time_points:\n",
    "            # Collect data from all selected mice for this time point and column\n",
    "            mouse_values = []\n",
    "            for mouse_name, mouse_data in mean_data_per_mouse.items():\n",
    "                if time_point in mouse_data.index and col in mouse_data.columns:\n",
    "                    value = mouse_data.loc[time_point, col]\n",
    "                    if not pd.isna(value):\n",
    "                        mouse_values.append(value)\n",
    "            \n",
    "            if len(mouse_values) > 0:\n",
    "                grand_averages.loc[time_point, col] = np.mean(mouse_values)\n",
    "                if len(mouse_values) > 1:\n",
    "                    grand_sems.loc[time_point, col] = np.std(mouse_values) / np.sqrt(len(mouse_values))\n",
    "                else:\n",
    "                    grand_sems.loc[time_point, col] = 0\n",
    "    \n",
    "    return mean_data_per_mouse, sem_data_per_mouse, grand_averages, grand_sems, mouse_to_data_path\n",
    "\n",
    "def analyze_mice_data(loaded_data, selected_columns, main_data_dir):\n",
    "    \"\"\"\n",
    "    Complete analysis workflow: compute means, grand averages, save CSV, and create plots.\n",
    "    \n",
    "    Parameters:\n",
    "    loaded_data (dict): Your loaded_data dictionary\n",
    "    selected_columns (list): List of column names to analyze (including 'Time (s)')\n",
    "    main_data_dir (str/Path): Main directory to save results\n",
    "    \n",
    "    Returns:\n",
    "    dict: Complete results including individual and grand averages\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"MOUSE DATA ANALYSIS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Use the selected_mice from the configuration (defined in Cell 2)\n",
    "    # Compute means and grand averages\n",
    "    mean_data_per_mouse, sem_data_per_mouse, grand_averages, grand_sems, mouse_to_data_path = compute_mouse_means_and_grand_average(\n",
    "        loaded_data, selected_columns, main_data_dir, selected_mice\n",
    "    )\n",
    "\n",
    "    # Print summary\n",
    "    print(f\"\\nüìä ANALYSIS COMPLETE:\")\n",
    "    print(f\"   ‚Ä¢ Number of mice analyzed: {len(mean_data_per_mouse)}\")\n",
    "    print(f\"   ‚Ä¢ Mouse names: {list(mean_data_per_mouse.keys())}\")\n",
    "    print(f\"   ‚Ä¢ Columns processed: {list(grand_averages.columns)}\")\n",
    "    print(f\"   ‚Ä¢ Time range: {grand_averages.index.min():.2f}s to {grand_averages.index.max():.2f}s\")\n",
    "    print(f\"   ‚Ä¢ Files saved in: {main_data_dir}\")\n",
    "    \n",
    "    # Return all results\n",
    "    results = {\n",
    "        'mean_data_per_mouse': mean_data_per_mouse,\n",
    "        'sem_data_per_mouse': sem_data_per_mouse,\n",
    "        'grand_averages': grand_averages,\n",
    "        'grand_sems': grand_sems,\n",
    "        'mouse_to_data_path': mouse_to_data_path,\n",
    "    }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8c6e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME why this?# RUN ANALYSIS\n",
    "# #---------------------------------------------------------------------------------------------------#\n",
    "\n",
    "# def save_animal_csv_files(mean_data_per_mouse, sem_data_per_mouse, mouse_to_data_path, event_name, experiment_day):\n",
    "#     \"\"\"\n",
    "#     Save averaged mismatch aligned data for each animal as individual CSV files.\n",
    "#     Files are saved in each animal's aligned_data folder (same location as SANDBOX 2 output).\n",
    "    \n",
    "#     Parameters:\n",
    "#     -----------\n",
    "#     mean_data_per_mouse : dict\n",
    "#         Dictionary with mouse names as keys and DataFrames with mean data as values\n",
    "#     sem_data_per_mouse : dict\n",
    "#         Dictionary with mouse names as keys and DataFrames with SEM data as values\n",
    "#     mouse_to_data_path : dict\n",
    "#         Dictionary mapping mouse names to their data_path (aligned_data folder)\n",
    "#     event_name : str\n",
    "#         Event name for the analysis (used in filename)\n",
    "#     experiment_day : str\n",
    "#         Experiment day identifier (e.g., \"Visual_mismatch_day3\")\n",
    "#     \"\"\"\n",
    "#     print(f\"\\n{'='*60}\")\n",
    "#     print(f\"SAVING ANIMAL CSV FILES\")\n",
    "#     print(f\"{'='*60}\")\n",
    "    \n",
    "#     for mouse_name, mean_df in mean_data_per_mouse.items():\n",
    "#         if mouse_name not in sem_data_per_mouse:\n",
    "#             print(f\"‚ö†Ô∏è  No SEM data for {mouse_name}, skipping...\")\n",
    "#             continue\n",
    "        \n",
    "#         if mouse_name not in mouse_to_data_path:\n",
    "#             print(f\"‚ö†Ô∏è  No data_path found for {mouse_name}, skipping...\")\n",
    "#             continue\n",
    "        \n",
    "#         # Get the aligned_data folder for this mouse (same as SANDBOX 2)\n",
    "#         animal_aligned_data_dir = mouse_to_data_path[mouse_name]\n",
    "        \n",
    "#         # Ensure the directory exists\n",
    "#         animal_aligned_data_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "#         sem_df = sem_data_per_mouse[mouse_name]\n",
    "        \n",
    "#         # Create a combined DataFrame with mean and SEM\n",
    "#         combined_df = mean_df.copy()\n",
    "#         combined_df.index.name = 'Time (s)'\n",
    "#         combined_df = combined_df.reset_index()\n",
    "        \n",
    "#         # Add SEM columns\n",
    "#         for col in sem_df.columns:\n",
    "#             if col in combined_df.columns:\n",
    "#                 combined_df[f'{col}_SEM'] = sem_df[col].values\n",
    "        \n",
    "#         # Generate filename - use naming convention that distinguishes from SANDBOX 2\n",
    "#         # SANDBOX 2 uses: {mouse_name}_{event_name}_baselined_data.csv\n",
    "#         # We use: {mouse_name}_{event_name}_mean_sem_averaged.csv\n",
    "#         # Remove .csv extension and _baselined_data suffix if present\n",
    "#         event_name_clean = event_name.replace('.csv', '')\n",
    "#         if event_name_clean.endswith('_baselined_data'):\n",
    "#             event_name_clean = event_name_clean[:-len('_baselined_data')]\n",
    "#         csv_filename = animal_aligned_data_dir / f\"{mouse_name}{event_name_clean}_mean_sem_averaged.csv\"\n",
    "        \n",
    "#         # Save to CSV\n",
    "#         combined_df.to_csv(csv_filename, index=False)\n",
    "#         print(f\"‚úÖ Saved: {csv_filename} ({len(combined_df)} rows, {len(combined_df.columns)} columns)\")\n",
    "    \n",
    "#     print(f\"\\n‚úÖ Saved {len(mean_data_per_mouse)} animal CSV files to their respective aligned_data folders\")\n",
    "\n",
    "\n",
    "# def save_cohort_csv_file(mean_data_per_mouse, sem_data_per_mouse, selected_columns, main_data_dir, \n",
    "#                          event_name, experiment_day, cohort_identifier):\n",
    "#     \"\"\"\n",
    "#     Save cohort-level CSV file with pre/post averages and SEMs for each animal (similar to cohort behavioral analysis format).\n",
    "#     This creates a single CSV file that can be merged across cohorts.\n",
    "    \n",
    "#     Parameters:\n",
    "#     -----------\n",
    "#     mean_data_per_mouse : dict\n",
    "#         Dictionary with mouse names as keys and DataFrames with mean data as values\n",
    "#     sem_data_per_mouse : dict\n",
    "#         Dictionary with mouse names as keys and DataFrames with SEM data as values\n",
    "#     selected_columns : list\n",
    "#         List of column names to analyze (excluding 'Time (s)')\n",
    "#     main_data_dir : Path\n",
    "#         Main data directory (experiment day level)\n",
    "#     event_name : str\n",
    "#         Event name for the analysis\n",
    "#     experiment_day : str\n",
    "#         Experiment day identifier (e.g., \"Visual_mismatch_day3\")\n",
    "#     cohort_identifier : str\n",
    "#         Cohort identifier (e.g., \"Cohort1\", \"Cohort3\")\n",
    "#     \"\"\"\n",
    "#     # Navigate to cohort level (2 levels above _processedData if needed)\n",
    "#     # The main_data_dir should already be at the experiment day level\n",
    "#     # We need to go up to the cohort level\n",
    "#     cohort_dir = main_data_dir.parent if ('_processedData' in str(main_data_dir) or \n",
    "#                                           'Visual_mismatch' in str(main_data_dir) or \n",
    "#                                           'Vestibular_mismatch' in str(main_data_dir)) else main_data_dir\n",
    "    \n",
    "#     # Create cohort CSV path (similar to cohort_behavioral_analysis.csv)\n",
    "#     cohort_csv_path = cohort_dir / \"cohort_aligned_data_analysis.csv\"\n",
    "    \n",
    "#     print(f\"\\n{'='*60}\")\n",
    "#     print(f\"SAVING COHORT-LEVEL CSV\")\n",
    "#     print(f\"{'='*60}\")\n",
    "#     print(f\"Cohort CSV path: {cohort_csv_path}\")\n",
    "    \n",
    "#     # Define pre and post time windows\n",
    "#     pre_time = (-2, 0)\n",
    "#     post_time = (0, 2)\n",
    "    \n",
    "#     # Collect data for each mouse\n",
    "#     cohort_data = []\n",
    "    \n",
    "#     for mouse_name, mean_df in mean_data_per_mouse.items():\n",
    "#         if mouse_name not in sem_data_per_mouse:\n",
    "#             print(f\"‚ö†Ô∏è  No SEM data for {mouse_name}, skipping...\")\n",
    "#             continue\n",
    "        \n",
    "#         sem_df = sem_data_per_mouse[mouse_name]\n",
    "        \n",
    "#         # Create row data with Animal_ID and Experiment_Day as first columns\n",
    "#         row_data = {\n",
    "#             'Animal_ID': mouse_name,\n",
    "#             'Experiment_Day': experiment_day\n",
    "#         }\n",
    "        \n",
    "#         # For each selected column, calculate pre and post averages and SEMs\n",
    "#         for col in selected_columns:\n",
    "#             if col == 'Time (s)':\n",
    "#                 continue\n",
    "            \n",
    "#             if col not in mean_df.columns or col not in sem_df.columns:\n",
    "#                 print(f\"‚ö†Ô∏è  Column {col} not found for {mouse_name}, skipping...\")\n",
    "#                 continue\n",
    "            \n",
    "#             # Get pre-time window data\n",
    "#             pre_mask = (mean_df.index >= pre_time[0]) & (mean_df.index < pre_time[1])\n",
    "#             if pre_mask.any():\n",
    "#                 pre_mean = mean_df.loc[pre_mask, col].mean()\n",
    "#                 pre_sem = sem_df.loc[pre_mask, col].mean()  # Average SEM across pre window\n",
    "#             else:\n",
    "#                 pre_mean = np.nan\n",
    "#                 pre_sem = np.nan\n",
    "            \n",
    "#             # Get post-time window data\n",
    "#             post_mask = (mean_df.index >= post_time[0]) & (mean_df.index <= post_time[1])\n",
    "#             if post_mask.any():\n",
    "#                 post_mean = mean_df.loc[post_mask, col].mean()\n",
    "#                 post_sem = sem_df.loc[post_mask, col].mean()  # Average SEM across post window\n",
    "#             else:\n",
    "#                 post_mean = np.nan\n",
    "#                 post_sem = np.nan\n",
    "            \n",
    "#             # Convert velocity to cm/s if column is a velocity column\n",
    "#             if 'Velocity' in col or 'velocity' in col.lower():\n",
    "#                 if not pd.isna(pre_mean):\n",
    "#                     pre_mean = pre_mean * 100  # Convert m/s to cm/s\n",
    "#                 if not pd.isna(pre_sem):\n",
    "#                     pre_sem = pre_sem * 100\n",
    "#                 if not pd.isna(post_mean):\n",
    "#                     post_mean = post_mean * 100\n",
    "#                 if not pd.isna(post_sem):\n",
    "#                     post_sem = post_sem * 100\n",
    "            \n",
    "#             # Add columns for this metric: pre_mean, pre_sem, post_mean, post_sem\n",
    "#             row_data[f'{col}_pre_mean'] = pre_mean\n",
    "#             row_data[f'{col}_pre_sem'] = pre_sem\n",
    "#             row_data[f'{col}_post_mean'] = post_mean\n",
    "#             row_data[f'{col}_post_sem'] = post_sem\n",
    "        \n",
    "#         cohort_data.append(row_data)\n",
    "    \n",
    "#     # Convert to DataFrame\n",
    "#     cohort_df = pd.DataFrame(cohort_data)\n",
    "    \n",
    "#     # Ensure Animal_ID and Experiment_Day are first columns\n",
    "#     if len(cohort_df) > 0:\n",
    "#         cols = ['Animal_ID', 'Experiment_Day'] + [col for col in cohort_df.columns \n",
    "#                                                    if col not in ['Animal_ID', 'Experiment_Day']]\n",
    "#         cohort_df = cohort_df[cols]\n",
    "    \n",
    "#     # Check if file exists and append or create new (similar to behavioral analyzer)\n",
    "#     if cohort_csv_path.exists():\n",
    "#         print(f\"üìÑ Existing cohort CSV found, loading...\")\n",
    "#         existing_df = pd.read_csv(cohort_csv_path)\n",
    "        \n",
    "#         # Check if columns match\n",
    "#         existing_cols = set(existing_df.columns)\n",
    "#         new_cols = set(cohort_df.columns)\n",
    "        \n",
    "#         if existing_cols != new_cols:\n",
    "#             print(f\"‚ö†Ô∏è  Column mismatch detected:\")\n",
    "#             print(f\"   Columns in existing file: {sorted(existing_cols)}\")\n",
    "#             print(f\"   Columns in new data: {sorted(new_cols)}\")\n",
    "            \n",
    "#             # Add missing columns to existing data\n",
    "#             for col in new_cols - existing_cols:\n",
    "#                 existing_df[col] = np.nan\n",
    "#                 print(f\"   ‚ûï Added missing column: {col}\")\n",
    "            \n",
    "#             # Add missing columns to new data\n",
    "#             for col in existing_cols - new_cols:\n",
    "#                 cohort_df[col] = np.nan\n",
    "#                 print(f\"   ‚ûï Added missing column to new data: {col}\")\n",
    "        \n",
    "#         # Remove rows with matching Animal_ID and Experiment_Day (to avoid duplicates)\n",
    "#         for _, row in cohort_df.iterrows():\n",
    "#             animal_id = row['Animal_ID']\n",
    "#             exp_day = row['Experiment_Day']\n",
    "            \n",
    "#             mask = (existing_df['Animal_ID'] == animal_id) & (existing_df['Experiment_Day'] == exp_day)\n",
    "#             if mask.any():\n",
    "#                 print(f\"   üîÑ Overwriting existing data for {animal_id} - {exp_day}\")\n",
    "#                 existing_df = existing_df[~mask]\n",
    "        \n",
    "#         # Append new data\n",
    "#         combined_df = pd.concat([existing_df, cohort_df], ignore_index=True)\n",
    "        \n",
    "#         # Sort by Animal_ID and Experiment_Day\n",
    "#         combined_df = combined_df.sort_values(['Animal_ID', 'Experiment_Day']).reset_index(drop=True)\n",
    "        \n",
    "#         print(f\"   ‚úÖ Appended {len(cohort_df)} rows to existing data\")\n",
    "#         print(f\"   üìä Total rows in cohort CSV: {len(combined_df)}\")\n",
    "        \n",
    "#     else:\n",
    "#         print(f\"üìÑ Creating new cohort CSV file...\")\n",
    "#         combined_df = cohort_df\n",
    "#         print(f\"   ‚úÖ Created new file with {len(combined_df)} rows\")\n",
    "    \n",
    "#     # Save to CSV\n",
    "#     combined_df.to_csv(cohort_csv_path, index=False)\n",
    "#     print(f\"‚úÖ Saved cohort CSV: {cohort_csv_path}\")\n",
    "#     print(f\"   Columns: {list(combined_df.columns)}\")\n",
    "#     if len(combined_df) > 0:\n",
    "#         print(f\"   Animals: {combined_df['Animal_ID'].unique().tolist()}\")\n",
    "#     print(f\"{'='*60}\\n\")\n",
    "\n",
    "\n",
    "# def save_results(results, filename='results.pkl'):\n",
    "#     \"\"\"Save results to a pickle file (deprecated - use CSV functions instead).\"\"\"\n",
    "#     with open(filename, 'wb') as f:\n",
    "#         pickle.dump(results, f)\n",
    "#     print(f\"‚úÖ Results saved to {filename}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40b7c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # FIXME why this? SAVE RESULTS\n",
    "# #---------------------------------------------------------------------------------------------------#\n",
    "\n",
    "# # Extract experiment day from main_data_dir path\n",
    "# # The experiment day is typically the directory name (e.g., \"Visual_mismatch_day3\")\n",
    "# experiment_day = main_data_dir.name\n",
    "\n",
    "# if results and SAVE_ANIMAL_CSV:\n",
    "#     # Save individual animal CSV files\n",
    "#     save_animal_csv_files(\n",
    "#         results['mean_data_per_mouse'],\n",
    "#         results['sem_data_per_mouse'],\n",
    "#         results['mouse_to_data_path'],\n",
    "#         event_name,\n",
    "#         experiment_day\n",
    "#     )\n",
    "    \n",
    "#     # Save cohort-level CSV file with pre/post averages and SEMs\n",
    "#     save_cohort_csv_file(\n",
    "#         results['mean_data_per_mouse'],\n",
    "#         results['sem_data_per_mouse'],\n",
    "#         selected_columns,\n",
    "#         main_data_dir,\n",
    "#         event_name,\n",
    "#         experiment_day,\n",
    "#         cohort_identifier\n",
    "#     )\n",
    "# else:\n",
    "#     print(\"‚è≠Ô∏è  Skipping animal CSV save (SAVE_ANIMAL_CSV=False or no results)\")\n",
    "\n",
    "# # Legacy pickle save (deprecated)\n",
    "# if results and SAVE_PICKLE:\n",
    "#     # Generate unique filename\n",
    "#     vmm_event_name = main_data_dir.name\n",
    "#     pickle_filename = f\"{cohort_identifier}_{vmm_event_name}{event_name.replace('.csv', '')}.pkl\"\n",
    "#     save_results(results, pickle_filename)\n",
    "#     print(\"‚ö†Ô∏è  Note: Pickle saving is deprecated. Use SAVE_ANIMAL_CSV instead.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a258f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME not needed # PLOTTING FUNCTIONS\n",
    "# #---------------------------------------------------------------------------------------------------#\n",
    "\n",
    "# def plot_time_series_and_scatter(results, columns_to_plot, selected_mice, main_data_dir, event_name):\n",
    "#     \"\"\"\n",
    "#     Plot time series and pre/post comparison plots for each column.\n",
    "#     Includes paired t-test and converts velocity to cm/s.\n",
    "#     Uses gnuplot2 color palette for consistent mouse colors.\n",
    "#     \"\"\"\n",
    "#     # Plot properties\n",
    "#     plt.rcParams.update({\n",
    "#         'font.size': 10,\n",
    "#         'font.family': 'sans-serif',\n",
    "#         'font.sans-serif': ['DejaVu Sans'],\n",
    "#         'axes.titlesize': 10,\n",
    "#         'axes.labelsize': 10,\n",
    "#         'legend.fontsize': 8,\n",
    "#         'xtick.labelsize': 10,\n",
    "#         'ytick.labelsize': 10\n",
    "#     })\n",
    "    \n",
    "#     # Generate color palette using gnuplot2 (same as cohort running analysis)\n",
    "#     # Get ALL unique mice from results (not just selected) to ensure consistent color assignment\n",
    "#     # This matches the approach in cohort running analysis which uses all mice from the dataframe\n",
    "#     all_mice_in_results = sorted(results.get('mean_data_per_mouse', {}).keys())\n",
    "#     if not all_mice_in_results:\n",
    "#         # Fallback: use selected_mice if no results yet\n",
    "#         all_mice_in_results = sorted(selected_mice)\n",
    "#     n_colors = len(all_mice_in_results) if all_mice_in_results else len(selected_mice)\n",
    "#     # Use gnuplot2 color palette (similar to gnuplot's default palette)\n",
    "#     # Avoid the white end (value 1.0) by using 0 to 0.95 instead of 0 to 1\n",
    "#     if n_colors > 0:\n",
    "#         colors = plt.cm.gnuplot2(np.linspace(0, 0.95, n_colors))\n",
    "#         mouse_colors = {mouse: colors[i] for i, mouse in enumerate(all_mice_in_results)}\n",
    "#     else:\n",
    "#         mouse_colors = {}\n",
    "#     # Add any missing mice from selected_mice with a default color\n",
    "#     for mouse in selected_mice:\n",
    "#         if mouse not in mouse_colors:\n",
    "#             # Use a default color if mouse not in results\n",
    "#             mouse_colors[mouse] = 'gray'\n",
    "    \n",
    "#     # Helper function to check if column is velocity and convert to cm/s\n",
    "#     def convert_velocity_if_needed(column_name, value):\n",
    "#         \"\"\"Convert velocity from m/s to cm/s if column is a velocity column.\n",
    "#         Handles both scalars and pandas Series/arrays.\n",
    "#         \"\"\"\n",
    "#         if 'Velocity' in column_name or 'velocity' in column_name.lower():\n",
    "#             if isinstance(value, (pd.Series, np.ndarray)):\n",
    "#                 return value * 100  # Convert m/s to cm/s for arrays/Series\n",
    "#             else:\n",
    "#                 return value * 100  # Convert m/s to cm/s for scalars\n",
    "#         return value\n",
    "    \n",
    "#     # Helper function to get ylabel with correct units\n",
    "#     def get_ylabel(column_name):\n",
    "#         \"\"\"Get ylabel with correct units.\"\"\"\n",
    "#         if 'Velocity' in column_name or 'velocity' in column_name.lower():\n",
    "#             return column_name.replace('_', ' ') + ' (cm/s)'\n",
    "#         return column_name.replace('_', ' ')\n",
    "    \n",
    "#     for column_to_plot in columns_to_plot:\n",
    "#         if column_to_plot not in results['grand_averages'].columns:\n",
    "#             print(f\"‚ö†Ô∏è Column {column_to_plot} not found in results, skipping...\")\n",
    "#             continue\n",
    "        \n",
    "#         print(f\"\\nüìä Plotting: {column_to_plot}\")\n",
    "        \n",
    "#         # Time series plot\n",
    "#         plt.figure(figsize=(8, 4))\n",
    "#         mice_plotted = []\n",
    "        \n",
    "#         for mouse in selected_mice:\n",
    "#             if mouse in results['mean_data_per_mouse']:\n",
    "#                 mice_plotted.append(mouse)\n",
    "#                 mean_data = results['mean_data_per_mouse'][mouse][column_to_plot]\n",
    "#                 sem_data = results['sem_data_per_mouse'][mouse][column_to_plot]\n",
    "                \n",
    "#                 mean_data = pd.to_numeric(mean_data, errors='coerce')\n",
    "#                 sem_data = pd.to_numeric(sem_data, errors='coerce')\n",
    "#                 time_points = pd.to_numeric(mean_data.index, errors='coerce')\n",
    "                \n",
    "#                 valid_mask = ~(pd.isna(mean_data) | pd.isna(sem_data) | pd.isna(time_points))\n",
    "#                 mean_data_clean = mean_data[valid_mask]\n",
    "#                 sem_data_clean = sem_data[valid_mask]\n",
    "#                 time_points_clean = time_points[valid_mask]\n",
    "                \n",
    "#                 # Convert velocity to cm/s\n",
    "#                 mean_data_clean = convert_velocity_if_needed(column_to_plot, mean_data_clean)\n",
    "#                 sem_data_clean = convert_velocity_if_needed(column_to_plot, sem_data_clean)\n",
    "                \n",
    "#                 # Get color for this mouse, with fallback\n",
    "#                 mouse_color = mouse_colors.get(mouse, 'gray')\n",
    "#                 plt.plot(time_points_clean, mean_data_clean, label=f'{mouse} Mean', color=mouse_color)\n",
    "#                 plt.fill_between(time_points_clean, mean_data_clean - sem_data_clean, \n",
    "#                                 mean_data_clean + sem_data_clean, color=mouse_color, alpha=0.2)\n",
    "        \n",
    "#         # Grand average\n",
    "#         grand_mean = results['grand_averages'][column_to_plot]\n",
    "#         grand_sem = results['grand_sems'][column_to_plot]\n",
    "#         grand_mean = pd.to_numeric(grand_mean, errors='coerce')\n",
    "#         grand_sem = pd.to_numeric(grand_sem, errors='coerce')\n",
    "#         time_points = pd.to_numeric(grand_mean.index, errors='coerce')\n",
    "        \n",
    "#         valid_mask = ~(pd.isna(grand_mean) | pd.isna(grand_sem) | pd.isna(time_points))\n",
    "#         grand_mean_clean = grand_mean[valid_mask]\n",
    "#         grand_sem_clean = grand_sem[valid_mask]\n",
    "#         time_points_clean = time_points[valid_mask]\n",
    "        \n",
    "#         # Convert velocity to cm/s\n",
    "#         grand_mean_clean = convert_velocity_if_needed(column_to_plot, grand_mean_clean)\n",
    "#         grand_sem_clean = convert_velocity_if_needed(column_to_plot, grand_sem_clean)\n",
    "        \n",
    "#         plt.plot(time_points_clean, grand_mean_clean, label='Grand Average', color='black', linewidth=2)\n",
    "#         plt.fill_between(time_points_clean, grand_mean_clean - grand_sem_clean, \n",
    "#                         grand_mean_clean + grand_sem_clean, color='gray', alpha=0.3)\n",
    "        \n",
    "#         plt.xlabel('Time (s)')\n",
    "#         plt.ylabel(get_ylabel(column_to_plot))\n",
    "#         plt.title(f'Mean and SEM of {column_to_plot} Over Time')\n",
    "#         plt.legend()\n",
    "#         plt.grid(True)\n",
    "#         plt.tight_layout()\n",
    "        \n",
    "#         # Save plot\n",
    "#         try:\n",
    "#             baselined_dir = main_data_dir / \"baselined\"\n",
    "#             baselined_dir.mkdir(exist_ok=True)\n",
    "#             plot_filename = baselined_dir / f\"{column_to_plot}{event_name.replace('.csv', '')}.pdf\"\n",
    "#             plt.savefig(plot_filename, dpi=150, bbox_inches='tight')\n",
    "#             print(f\"   ‚úÖ Saved: {plot_filename}\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"   ‚ö†Ô∏è Error saving plot: {e}\")\n",
    "        \n",
    "#         plt.show()\n",
    "        \n",
    "#         # Pre/post comparison plot with paired t-test\n",
    "#         pre_time = (-2, 0)\n",
    "#         post_time = (0, 2)\n",
    "        \n",
    "#         pre_values, post_values, mouse_labels = [], [], []\n",
    "        \n",
    "#         for mouse in selected_mice:\n",
    "#             if mouse in results['mean_data_per_mouse']:\n",
    "#                 mean_data = results['mean_data_per_mouse'][mouse][column_to_plot]\n",
    "#                 # Get pre and post means\n",
    "#                 pre_mask = (mean_data.index >= pre_time[0]) & (mean_data.index < pre_time[1])\n",
    "#                 post_mask = (mean_data.index >= post_time[0]) & (mean_data.index <= post_time[1])\n",
    "                \n",
    "#                 if pre_mask.any() and post_mask.any():\n",
    "#                     pre_mean = mean_data.loc[pre_mask].mean()\n",
    "#                     post_mean = mean_data.loc[post_mask].mean()\n",
    "                    \n",
    "#                     # Convert velocity to cm/s\n",
    "#                     pre_mean = convert_velocity_if_needed(column_to_plot, pre_mean)\n",
    "#                     post_mean = convert_velocity_if_needed(column_to_plot, post_mean)\n",
    "                    \n",
    "#                     # Only add if both values are valid\n",
    "#                     if not (pd.isna(pre_mean) or pd.isna(post_mean)):\n",
    "#                         pre_values.append(pre_mean)\n",
    "#                         post_values.append(post_mean)\n",
    "#                         mouse_labels.append(mouse)\n",
    "        \n",
    "#         # Calculate grand averages\n",
    "#         grand_mean = results['grand_averages'][column_to_plot]\n",
    "#         grand_sem = results['grand_sems'][column_to_plot]\n",
    "        \n",
    "#         # Get pre and post grand means\n",
    "#         pre_mask = (grand_mean.index >= pre_time[0]) & (grand_mean.index < pre_time[1])\n",
    "#         post_mask = (grand_mean.index >= post_time[0]) & (grand_mean.index <= post_time[1])\n",
    "        \n",
    "#         if not (pre_mask.any() and post_mask.any()):\n",
    "#             print(f\"   ‚ö†Ô∏è No data in pre/post time windows for grand average, skipping pre/post plot\")\n",
    "#             continue\n",
    "        \n",
    "#         pre_grand_mean = grand_mean.loc[pre_mask].mean()\n",
    "#         post_grand_mean = grand_mean.loc[post_mask].mean()\n",
    "#         pre_grand_sem = grand_sem.loc[pre_mask].mean()\n",
    "#         post_grand_sem = grand_sem.loc[post_mask].mean()\n",
    "        \n",
    "#         # Check for NaN values\n",
    "#         if pd.isna(pre_grand_mean) or pd.isna(post_grand_mean):\n",
    "#             print(f\"   ‚ö†Ô∏è Grand average contains NaN values, skipping pre/post plot\")\n",
    "#             continue\n",
    "        \n",
    "#         # Convert velocity to cm/s\n",
    "#         pre_grand_mean = convert_velocity_if_needed(column_to_plot, pre_grand_mean)\n",
    "#         post_grand_mean = convert_velocity_if_needed(column_to_plot, post_grand_mean)\n",
    "#         pre_grand_sem = convert_velocity_if_needed(column_to_plot, pre_grand_sem)\n",
    "#         post_grand_sem = convert_velocity_if_needed(column_to_plot, post_grand_sem)\n",
    "        \n",
    "#         # Check if we have data to plot\n",
    "#         if len(pre_values) == 0 or len(post_values) == 0:\n",
    "#             print(f\"   ‚ö†Ô∏è No valid pre/post data found for {column_to_plot}, skipping pre/post plot\")\n",
    "#             continue\n",
    "        \n",
    "#         # Perform paired t-test\n",
    "#         if len(pre_values) > 1 and len(post_values) > 1:\n",
    "#             pre_array = np.array(pre_values)\n",
    "#             post_array = np.array(post_values)\n",
    "#             # Remove any NaN values for t-test\n",
    "#             valid_mask = ~(np.isnan(pre_array) | np.isnan(post_array))\n",
    "#             if np.sum(valid_mask) > 1:\n",
    "#                 t_stat, p_value = ttest_rel(pre_array[valid_mask], post_array[valid_mask])\n",
    "#                 print(f\"   üìä Paired t-test (pre vs post): t={t_stat:.4f}, p={p_value:.4f}, n={np.sum(valid_mask)}\")\n",
    "#             else:\n",
    "#                 t_stat, p_value = np.nan, np.nan\n",
    "#                 print(f\"   ‚ö†Ô∏è Not enough valid data for paired t-test\")\n",
    "#         else:\n",
    "#             t_stat, p_value = np.nan, np.nan\n",
    "#             print(f\"   ‚ö†Ô∏è Not enough data for paired t-test (n={len(pre_values)})\")\n",
    "        \n",
    "#         # Create pre/post comparison plot\n",
    "#         fig, ax = plt.subplots(figsize=(6, 5))\n",
    "        \n",
    "#         # Plot individual mouse lines\n",
    "#         for i, mouse in enumerate(mouse_labels):\n",
    "#             # Get color for this mouse, with fallback\n",
    "#             mouse_color = mouse_colors.get(mouse, 'gray')\n",
    "#             ax.plot([0, 1], [pre_values[i], post_values[i]], color=mouse_color, \n",
    "#                     marker='o', linewidth=1.5, markersize=6, alpha=0.7, label=mouse)\n",
    "        \n",
    "#         # Plot grand average with error bars\n",
    "#         ax.errorbar([0, 1], [pre_grand_mean, post_grand_mean], \n",
    "#                     yerr=[pre_grand_sem, post_grand_sem], fmt='o-', color='black', \n",
    "#                     linewidth=2.5, markersize=10, capsize=8, capthick=2, \n",
    "#                     label='Grand Average ¬± SEM', zorder=10)\n",
    "        \n",
    "#         ax.set_xticks([0, 1])\n",
    "#         ax.set_xticklabels([f'Pre\\n({pre_time[0]} to {pre_time[1]}s)', \n",
    "#                            f'Post\\n({post_time[0]} to {post_time[1]}s)'], fontsize=11)\n",
    "#         ax.set_ylabel(get_ylabel(column_to_plot), fontsize=12, fontweight='bold')\n",
    "#         ax.set_title(f'{column_to_plot.replace(\"_\", \" \")}: Pre vs Post Comparison', \n",
    "#                     fontsize=12, fontweight='bold')\n",
    "#         ax.grid(True, alpha=0.3)\n",
    "        \n",
    "#         # Set y-axis limits for velocity if needed\n",
    "#         if 'Velocity' in column_to_plot or 'velocity' in column_to_plot.lower():\n",
    "#             # Collect all values, filtering out NaN\n",
    "#             all_values = [v for v in pre_values + post_values if not np.isnan(v)]\n",
    "#             all_values.extend([pre_grand_mean - pre_grand_sem, pre_grand_mean + pre_grand_sem,\n",
    "#                               post_grand_mean - post_grand_sem, post_grand_mean + post_grand_sem])\n",
    "#             all_values = [v for v in all_values if not np.isnan(v)]\n",
    "#             if all_values:\n",
    "#                 y_min = min(all_values) * 0.9\n",
    "#                 y_max = max(all_values) * 1.1\n",
    "#                 ax.set_ylim(y_min, y_max)\n",
    "        \n",
    "#         # Add t-test result to plot (after ylim is set)\n",
    "#         if not np.isnan(p_value):\n",
    "#             significance = '***' if p_value < 0.001 else '**' if p_value < 0.01 else '*' if p_value < 0.05 else 'ns'\n",
    "#             y_lim = ax.get_ylim()\n",
    "#             y_pos = y_lim[0] + (y_lim[1] - y_lim[0]) * 0.95\n",
    "#             ax.text(0.5, y_pos, f'p = {p_value:.4f} {significance}', \n",
    "#                    ha='center', va='top', fontsize=11, fontweight='bold',\n",
    "#                    bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "        \n",
    "#         # Legend\n",
    "#         handles, labels = ax.get_legend_handles_labels()\n",
    "#         unique_labels = {}\n",
    "#         for h, l in zip(handles, labels):\n",
    "#             if l not in unique_labels:\n",
    "#                 unique_labels[l] = h\n",
    "#         ax.legend(unique_labels.values(), unique_labels.keys(), loc='best', fontsize=8, ncol=2)\n",
    "        \n",
    "#         plt.tight_layout()\n",
    "        \n",
    "#         try:\n",
    "#             plot_filename = baselined_dir / f\"{column_to_plot}{event_name.replace('.csv', '')}_prepost_comparison.pdf\"\n",
    "#             plt.savefig(plot_filename, dpi=150, bbox_inches='tight')\n",
    "#             print(f\"   ‚úÖ Saved pre/post comparison: {plot_filename}\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"   ‚ö†Ô∏è Error saving pre/post comparison plot: {e}\")\n",
    "        \n",
    "#         plt.show()\n",
    "#         plt.clf()\n",
    "#         plt.close()\n",
    "\n",
    "\n",
    "# #---------------------------------------------------------------------------------------------------#\n",
    "# # PRE/POST COMPARISON PLOTTING UTILITIES\n",
    "# #---------------------------------------------------------------------------------------------------#\n",
    "\n",
    "# def _compute_sem(values: np.ndarray) -> float:\n",
    "#     \"\"\"Compute the standard error of the mean for an array, ignoring NaNs.\"\"\"\n",
    "#     arr = np.asarray(values, dtype=float)\n",
    "#     arr = arr[np.isfinite(arr)]\n",
    "#     if arr.size <= 1:\n",
    "#         return 0.0 if arr.size == 1 else np.nan\n",
    "#     return np.nanstd(arr, ddof=1) / np.sqrt(arr.size)\n",
    "\n",
    "\n",
    "# def _convert_cm_to_inches(cm: float) -> float:\n",
    "#     \"\"\"Convert centimeters to inches for matplotlib figsize.\"\"\"\n",
    "#     return cm / 2.54\n",
    "\n",
    "\n",
    "# def _build_mouse_color_map(mouse_labels):\n",
    "#     \"\"\"Assign consistent colors to each mouse using the gnuplot2 palette.\"\"\"\n",
    "#     unique_mice = sorted(set(mouse_labels))\n",
    "#     if not unique_mice:\n",
    "#         return {}\n",
    "#     colors = plt.cm.gnuplot2(np.linspace(0, 0.95, len(unique_mice)))\n",
    "#     return {mouse: colors[idx] for idx, mouse in enumerate(unique_mice)}\n",
    "\n",
    "\n",
    "# def _format_ylabel(column_name: str) -> str:\n",
    "#     \"\"\"Return a nicely formatted y-label, including units for relevant columns.\"\"\"\n",
    "#     if 'Motor' in column_name and ('Velocity' in column_name or 'velocity' in column_name.lower()):\n",
    "#         return column_name.replace('_', ' ') + ' (deg/s)'\n",
    "#     if 'Velocity' in column_name or 'velocity' in column_name.lower():\n",
    "#         return column_name.replace('_', ' ') + ' (cm/s)'\n",
    "#     return column_name.replace('_', ' ')\n",
    "\n",
    "\n",
    "# def _convert_velocity_values(column_name: str, values):\n",
    "#     \"\"\"Convert velocity data from m/s to cm/s when necessary.\"\"\"\n",
    "#     if ('Velocity' in column_name or 'velocity' in column_name.lower()) and 'Motor' not in column_name:\n",
    "#         return np.asarray(values, dtype=float) * 100.0\n",
    "#     return np.asarray(values, dtype=float)\n",
    "\n",
    "\n",
    "# def _render_prepost_plot(\n",
    "#     column_to_plot: str,\n",
    "#     pre_values: np.ndarray,\n",
    "#     post_values: np.ndarray,\n",
    "#     mouse_labels,\n",
    "#     mouse_colors,\n",
    "#     p_value: float,\n",
    "#     save_path: Optional[Path] = None,\n",
    "# ) -> None:\n",
    "#     \"\"\"Render a pre/post plot with individual mice and grand average.\"\"\"\n",
    "#     pre_values = np.asarray(pre_values, dtype=float)\n",
    "#     post_values = np.asarray(post_values, dtype=float)\n",
    "\n",
    "#     # Compute summary statistics\n",
    "#     pre_mean = np.nanmean(pre_values)\n",
    "#     post_mean = np.nanmean(post_values)\n",
    "#     pre_sem = _compute_sem(pre_values)\n",
    "#     post_sem = _compute_sem(post_values)\n",
    "\n",
    "#     # Update plotting style\n",
    "#     plt.rcParams.update({\n",
    "#         'font.size': 15,\n",
    "#         'font.family': 'sans-serif',\n",
    "#         'font.sans-serif': ['Arial'],\n",
    "#         'axes.titlesize': 15,\n",
    "#         'axes.labelsize': 15,\n",
    "#         'legend.fontsize': 12,\n",
    "#         'xtick.labelsize': 15,\n",
    "#         'ytick.labelsize': 15\n",
    "#     })\n",
    "\n",
    "#     fig, ax = plt.subplots(\n",
    "#         figsize=(\n",
    "#             _convert_cm_to_inches(7.0),\n",
    "#             _convert_cm_to_inches(7.0),\n",
    "#         )\n",
    "#     )\n",
    "\n",
    "#     # Plot individual mice\n",
    "#     for label, pre_val, post_val in zip(mouse_labels, pre_values, post_values):\n",
    "#         color = mouse_colors.get(label, 'gray')\n",
    "#         ax.plot(\n",
    "#             [0, 1],\n",
    "#             [pre_val, post_val],\n",
    "#             color=color,\n",
    "#             marker='o',\n",
    "#             linewidth=1.5,\n",
    "#             markersize=6,\n",
    "#             alpha=0.7,\n",
    "#             zorder=1,\n",
    "#         )\n",
    "\n",
    "#     # Add SEM shading along the grand mean line\n",
    "#     x_band = np.linspace(0, 1, 100)\n",
    "#     upper_band = np.interp(x_band, [0, 1], [pre_mean + pre_sem, post_mean + post_sem])\n",
    "#     lower_band = np.interp(x_band, [0, 1], [pre_mean - pre_sem, post_mean - post_sem])\n",
    "#     ax.fill_between(\n",
    "#         x_band,\n",
    "#         lower_band,\n",
    "#         upper_band,\n",
    "#         color='gray',\n",
    "#         alpha=0.3,\n",
    "#         zorder=2,\n",
    "#     )\n",
    "\n",
    "#     # Plot grand average line\n",
    "#     ax.plot(\n",
    "#         [0, 1],\n",
    "#         [pre_mean, post_mean],\n",
    "#         color='black',\n",
    "#         marker='o',\n",
    "#         linewidth=2.5,\n",
    "#         markersize=10,\n",
    "#         alpha=1,\n",
    "#         zorder=3,\n",
    "#         label='Grand Average',\n",
    "#     )\n",
    "\n",
    "#     ax.set_xticks([0, 1])\n",
    "#     ax.set_xticklabels(['Pre', 'Post'], fontsize=15, fontfamily='Arial')\n",
    "#     ax.set_xlim(-0.4, 1.4)\n",
    "\n",
    "#     ax.set_ylabel(_format_ylabel(column_to_plot), fontsize=15, fontfamily='Arial')\n",
    "#     ax.grid(False)\n",
    "\n",
    "#     # Determine y-limits with padding\n",
    "#     finite_values = np.concatenate([pre_values, post_values])\n",
    "#     finite_values = finite_values[np.isfinite(finite_values)]\n",
    "#     if finite_values.size:\n",
    "#         data_min = finite_values.min()\n",
    "#         data_max = finite_values.max()\n",
    "#         span = data_max - data_min\n",
    "#         if span == 0:\n",
    "#             span = max(abs(data_max), 1.0) * 0.1\n",
    "#         lower = data_min - 0.1 * span\n",
    "#         upper = data_max + 0.25 * span\n",
    "#         ax.set_ylim(lower, upper)\n",
    "#     else:\n",
    "#         lower, upper = -1, 1.2\n",
    "#         span = upper - lower\n",
    "#         ax.set_ylim(lower, upper)\n",
    "\n",
    "#     # Annotate significance\n",
    "#     if not np.isnan(p_value):\n",
    "#         significance = '***' if p_value < 0.001 else '**' if p_value < 0.01 else '*' if p_value < 0.05 else 'ns'\n",
    "#         y_lim = ax.get_ylim()\n",
    "#         span = y_lim[1] - y_lim[0]\n",
    "#         y_pos = y_lim[1] - 0.01 * span\n",
    "#         ax.text(\n",
    "#             0.5,\n",
    "#             y_pos,\n",
    "#             f'p = {p_value:.3f} {significance}',\n",
    "#             ha='center',\n",
    "#             va='top',\n",
    "#             fontsize=12,\n",
    "#             fontfamily='Arial',\n",
    "#         )\n",
    "\n",
    "#     plt.tight_layout()\n",
    "\n",
    "#     if save_path:\n",
    "#         save_path = Path(save_path)\n",
    "#         save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "#         try:\n",
    "#             fig.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "#             print(f\"   ‚úÖ Saved: {save_path}\")\n",
    "#         except Exception as exc:\n",
    "#             print(f\"   ‚ö†Ô∏è Error saving plot: {exc}\")\n",
    "\n",
    "#     plt.show()\n",
    "#     plt.close(fig)\n",
    "\n",
    "\n",
    "# def plot_prepost_from_results(\n",
    "#     results: dict,\n",
    "#     columns_to_plot,\n",
    "#     selected_mice,\n",
    "#     pre_time=(-2, 0),\n",
    "#     post_time=(0, 2),\n",
    "#     save_dir: Optional[Path] = None,\n",
    "#     main_data_dir: Optional[Path] = None,\n",
    "# ) -> None:\n",
    "#     \"\"\"Generate pre/post comparison plots using freshly computed results.\"\"\"\n",
    "#     if not results:\n",
    "#         print(\"‚ö†Ô∏è No results available for pre/post plotting.\")\n",
    "#         return\n",
    "\n",
    "#     target_dir: Optional[Path]\n",
    "#     if save_dir is not None:\n",
    "#         target_dir = Path(save_dir)\n",
    "#     elif main_data_dir is not None:\n",
    "#         target_dir = Path(main_data_dir) / \"baselined\"\n",
    "#     else:\n",
    "#         target_dir = None\n",
    "\n",
    "#     if target_dir is not None:\n",
    "#         target_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#     mouse_colors = _build_mouse_color_map(selected_mice or results.get('mean_data_per_mouse', {}).keys())\n",
    "\n",
    "#     for column_to_plot in columns_to_plot:\n",
    "#         if column_to_plot not in results['grand_averages'].columns:\n",
    "#             print(f\"‚ö†Ô∏è Column {column_to_plot} not found in results, skipping...\")\n",
    "#             continue\n",
    "\n",
    "#         pre_values = []\n",
    "#         post_values = []\n",
    "#         mouse_labels = []\n",
    "\n",
    "#         for mouse in selected_mice:\n",
    "#             if mouse not in results['mean_data_per_mouse']:\n",
    "#                 continue\n",
    "\n",
    "#             mean_data = results['mean_data_per_mouse'][mouse][column_to_plot]\n",
    "#             pre_mask = (mean_data.index >= pre_time[0]) & (mean_data.index < pre_time[1])\n",
    "#             post_mask = (mean_data.index >= post_time[0]) & (mean_data.index <= post_time[1])\n",
    "\n",
    "#             if not (pre_mask.any() and post_mask.any()):\n",
    "#                 continue\n",
    "\n",
    "#             pre_mean = mean_data.loc[pre_mask].mean()\n",
    "#             post_mean = mean_data.loc[post_mask].mean()\n",
    "\n",
    "#             pre_mean = _convert_velocity_values(column_to_plot, [pre_mean])[0]\n",
    "#             post_mean = _convert_velocity_values(column_to_plot, [post_mean])[0]\n",
    "\n",
    "#             if pd.isna(pre_mean) or pd.isna(post_mean):\n",
    "#                 continue\n",
    "\n",
    "#             pre_values.append(pre_mean)\n",
    "#             post_values.append(post_mean)\n",
    "#             mouse_labels.append(mouse)\n",
    "\n",
    "#         if not pre_values:\n",
    "#             print(f\"   ‚ö†Ô∏è No valid data found for {column_to_plot}, skipping...\")\n",
    "#             continue\n",
    "\n",
    "#         pre_array = np.asarray(pre_values, dtype=float)\n",
    "#         post_array = np.asarray(post_values, dtype=float)\n",
    "\n",
    "#         if pre_array.size > 1:\n",
    "#             t_stat, p_value = ttest_rel(pre_array, post_array)\n",
    "#             print(f\"   üìä Paired t-test ({column_to_plot}): t={t_stat:.4f}, p={p_value:.4f}, n={pre_array.size}\")\n",
    "#         else:\n",
    "#             p_value = np.nan\n",
    "#             print(f\"   ‚ö†Ô∏è Not enough data for paired t-test ({column_to_plot}); n={pre_array.size}\")\n",
    "\n",
    "#         save_path = None\n",
    "#         if target_dir is not None:\n",
    "#             safe_name = column_to_plot.replace('/', '_')\n",
    "#             save_path = target_dir / f\"{safe_name}_prepost_comparison.pdf\"\n",
    "\n",
    "#         _render_prepost_plot(\n",
    "#             column_to_plot=column_to_plot,\n",
    "#             pre_values=pre_array,\n",
    "#             post_values=post_array,\n",
    "#             mouse_labels=mouse_labels,\n",
    "#             mouse_colors=mouse_colors,\n",
    "#             p_value=p_value,\n",
    "#             save_path=save_path,\n",
    "#         )\n",
    "\n",
    "\n",
    "# def plot_prepost_from_cohort_csv(\n",
    "#     cohort_csv_path: Path,\n",
    "#     columns_to_plot,\n",
    "#     selected_mice=None,\n",
    "#     pre_time=(-2, 0),\n",
    "#     post_time=(0, 2),\n",
    "#     save_dir: Optional[Path] = None,\n",
    "# ) -> None:\n",
    "#     \"\"\"Generate pre/post comparison plots from an existing cohort_aligned_data_analysis.csv.\"\"\"\n",
    "#     cohort_csv_path = Path(cohort_csv_path)\n",
    "#     if not cohort_csv_path.exists():\n",
    "#         print(f\"‚ö†Ô∏è Cohort CSV not found: {cohort_csv_path}\")\n",
    "#         return\n",
    "\n",
    "#     df = pd.read_csv(cohort_csv_path)\n",
    "#     if df.empty:\n",
    "#         print(f\"‚ö†Ô∏è Cohort CSV is empty: {cohort_csv_path}\")\n",
    "#         return\n",
    "\n",
    "#     if selected_mice:\n",
    "#         df = df[df['Animal_ID'].astype(str).isin(selected_mice)]\n",
    "#         if df.empty:\n",
    "#             print(f\"‚ö†Ô∏è No matching animals found in cohort CSV for {selected_mice}\")\n",
    "#             return\n",
    "\n",
    "#     if save_dir is not None:\n",
    "#         target_dir = Path(save_dir)\n",
    "#     else:\n",
    "#         target_dir = cohort_csv_path.parent / \"prepost_plots\"\n",
    "#     target_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#     mouse_labels_all = df['Animal_ID'].astype(str).tolist()\n",
    "#     mouse_colors = _build_mouse_color_map(mouse_labels_all)\n",
    "\n",
    "#     for column_to_plot in columns_to_plot:\n",
    "#         pre_col = f\"{column_to_plot}_pre_mean\"\n",
    "#         post_col = f\"{column_to_plot}_post_mean\"\n",
    "\n",
    "#         if pre_col not in df.columns or post_col not in df.columns:\n",
    "#             print(f\"‚ö†Ô∏è Required columns not found for {column_to_plot}, skipping...\")\n",
    "#             continue\n",
    "\n",
    "#         pre_values = df[pre_col].astype(float).to_numpy()\n",
    "#         post_values = df[post_col].astype(float).to_numpy()\n",
    "#         mouse_labels = df['Animal_ID'].astype(str).tolist()\n",
    "\n",
    "#         valid_mask = ~(pd.isna(pre_values) | pd.isna(post_values))\n",
    "#         if valid_mask.sum() == 0:\n",
    "#             print(f\"   ‚ö†Ô∏è No valid data found for {column_to_plot} in cohort CSV, skipping...\")\n",
    "#             continue\n",
    "\n",
    "#         pre_values = _convert_velocity_values(column_to_plot, pre_values[valid_mask])\n",
    "#         post_values = _convert_velocity_values(column_to_plot, post_values[valid_mask])\n",
    "#         filtered_labels = [mouse_labels[idx] for idx, keep in enumerate(valid_mask) if keep]\n",
    "\n",
    "#         if pre_values.size > 1:\n",
    "#             t_stat, p_value = ttest_rel(pre_values, post_values)\n",
    "#             print(f\"   üìä Cohort CSV paired t-test ({column_to_plot}): t={t_stat:.4f}, p={p_value:.4f}, n={pre_values.size}\")\n",
    "#         else:\n",
    "#             p_value = np.nan\n",
    "#             print(f\"   ‚ö†Ô∏è Not enough data in cohort CSV for paired t-test ({column_to_plot}); n={pre_values.size}\")\n",
    "\n",
    "#         safe_name = column_to_plot.replace('/', '_')\n",
    "#         save_path = target_dir / f\"{safe_name}_prepost_from_csv.pdf\"\n",
    "\n",
    "#         # Build color map specific to filtered labels\n",
    "#         filtered_colors = {label: mouse_colors.get(label, 'gray') for label in filtered_labels}\n",
    "\n",
    "#         _render_prepost_plot(\n",
    "#             column_to_plot=column_to_plot,\n",
    "#             pre_values=pre_values,\n",
    "#             post_values=post_values,\n",
    "#             mouse_labels=filtered_labels,\n",
    "#             mouse_colors=filtered_colors,\n",
    "#             p_value=p_value,\n",
    "#             save_path=save_path,\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0259d621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME not needed# GENERATE PLOTS\n",
    "# #---------------------------------------------------------------------------------------------------#\n",
    "\n",
    "# if results and GENERATE_PLOTS:\n",
    "#     print(f\"üìä Generating plots for {len(columns_to_plot)} columns...\")\n",
    "#     print(f\"üìä Selected mice: {selected_mice}\")\n",
    "#     print(f\"üìä Available mice in results: {list(results['mean_data_per_mouse'].keys())}\")\n",
    "    \n",
    "#     missing_mice = [mouse for mouse in selected_mice if mouse not in results['mean_data_per_mouse']]\n",
    "#     if missing_mice:\n",
    "#         print(f\"‚ö†Ô∏è WARNING: These mice are in selected_mice but not in results: {missing_mice}\")\n",
    "    \n",
    "#     plot_time_series_and_scatter(results, columns_to_plot, selected_mice, main_data_dir, event_name)\n",
    "# else:\n",
    "#     print(\"‚è≠Ô∏è  Skipping plot generation (GENERATE_PLOTS=False or no results)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0480bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME not needed # PRE/POST COMPARISON PLOTS\n",
    "# #---------------------------------------------------------------------------------------------------#\n",
    "\n",
    "# prepost_output_dir = Path(PREPOST_SAVE_DIR).expanduser() if PREPOST_SAVE_DIR else None\n",
    "\n",
    "# if PLOT_PREPOST_FROM_RESULTS and results:\n",
    "#     print(f\"\\n{'='*60}\")\n",
    "#     print(\"PRE/POST COMPARISON FROM CURRENT RESULTS\")\n",
    "#     print(f\"{'='*60}\")\n",
    "#     plot_prepost_from_results(\n",
    "#         results=results,\n",
    "#         columns_to_plot=columns_to_plot,\n",
    "#         selected_mice=selected_mice,\n",
    "#         pre_time=(-2, 0),\n",
    "#         post_time=(0, 2),\n",
    "#         save_dir=prepost_output_dir,\n",
    "#         main_data_dir=main_data_dir,\n",
    "#     )\n",
    "# else:\n",
    "#     print(\"‚è≠Ô∏è  Skipping pre/post plots from results (flag disabled or no results)\")\n",
    "\n",
    "# if LOAD_EXISTING_PREPOST_CSV and EXISTING_PREPOST_CSV_PATH:\n",
    "#     print(f\"\\n{'='*60}\")\n",
    "#     print(\"PRE/POST COMPARISON FROM EXISTING COHORT CSV\")\n",
    "#     print(f\"{'='*60}\")\n",
    "#     try:\n",
    "#         plot_prepost_from_cohort_csv(\n",
    "#             cohort_csv_path=EXISTING_PREPOST_CSV_PATH,\n",
    "#             columns_to_plot=columns_to_plot,\n",
    "#             selected_mice=selected_mice if selected_mice else None,\n",
    "#             pre_time=(-2, 0),\n",
    "#             post_time=(0, 2),\n",
    "#             save_dir=prepost_output_dir,\n",
    "#         )\n",
    "#     except Exception as exc:\n",
    "#         print(f\"‚ö†Ô∏è Error plotting pre/post from existing cohort CSV: {exc}\")\n",
    "# else:\n",
    "#     print(\"‚è≠Ô∏è  Skipping pre/post plots from existing CSV (flag disabled or path not set)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f45ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE GRAND AVERAGES CSV\n",
    "# Determine main data directory (first existing entry in DATA_DIRS, otherwise current working dir)\n",
    "_existing_data_dirs = [Path(p).expanduser() for p in DATA_DIRS if Path(p).expanduser().exists()]\n",
    "if 'loaded_data' in locals():\n",
    "    main_data_dir = determine_main_data_dir(loaded_data, DATA_DIRS, cohort_identifier)\n",
    "else:\n",
    "    main_data_dir = _existing_data_dirs[0] if _existing_data_dirs else Path.cwd()\n",
    "main_data_dir = Path(main_data_dir).resolve()\n",
    "experiment_day = extract_experiment_day(main_data_dir)\n",
    "\n",
    "# Run analysis and keep results available for later cells/plots\n",
    "if loaded_data:\n",
    "    results = analyze_mice_data(loaded_data, selected_columns, main_data_dir)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No data loaded. Please check your configuration and data paths.\")\n",
    "    results = None\n",
    "if results and SAVE_CSV:\n",
    "    # Create a DataFrame combining grand averages and SEMs\n",
    "    grand_avg_with_sem = results['grand_averages'].copy()\n",
    "    for col in results['grand_sems'].columns:\n",
    "        grand_avg_with_sem[f'{col}_SEM'] = results['grand_sems'][col]\n",
    "\n",
    "    # Generate filename with cohort and experiment day context\n",
    "    csv_filename = main_data_dir / build_grand_average_filename(\n",
    "        cohort_identifier,\n",
    "        experiment_day,\n",
    "        event_name,\n",
    "    )\n",
    "\n",
    "    # Save the DataFrame to a CSV file (retain Time (s) as index)\n",
    "    grand_avg_with_sem.to_csv(csv_filename)\n",
    "    print(f\"‚úÖ Grand averages with SEM saved to: {csv_filename}\")\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è  Skipping CSV save (SAVE_CSV=False or no results)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2c2394",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# INTERACTIVE GRAND AVERAGE PLOTTING\n",
    "#---------------------------------------------------------------------------------------------------#\n",
    "\n",
    "def find_csv_files(base_paths):\n",
    "    \"\"\"Find all CSV files containing 'grand_averages' in the given paths.\"\"\"\n",
    "    csv_files = set()\n",
    "    for base_path in base_paths:\n",
    "        path = Path(base_path)\n",
    "        if path.exists():\n",
    "            for csv_file in path.rglob('*.csv'):\n",
    "                name = csv_file.name\n",
    "                if name.startswith('._'):\n",
    "                    continue\n",
    "                if 'grand_averages' in name:\n",
    "                    csv_files.add(str(csv_file))\n",
    "    return sorted(csv_files)\n",
    "\n",
    "def get_available_columns(df):\n",
    "    \"\"\"Get available data columns (excluding Time and SEM columns).\"\"\"\n",
    "    # Filter out Time column and SEM columns\n",
    "    data_cols = [col for col in df.columns \n",
    "                 if col != 'Time (s)' and not col.endswith('_SEM')]\n",
    "    return sorted(data_cols)\n",
    "\n",
    "def get_column_label(column_name):\n",
    "    \"\"\"Generate a readable label for a column.\"\"\"\n",
    "    # Map common column names to readable labels\n",
    "    label_map = {\n",
    "        'z_470': 'GRAB-5HT3.0 (z-score)',\n",
    "        'z_470_Baseline': 'GRAB-5HT3.0 (z-score)',\n",
    "        'z_560': 'RGeco1a (z-score)',\n",
    "        'z_560_Baseline': 'RGeco1a (z-score)',\n",
    "        'Velocity_0X': 'Running speed',\n",
    "        'Velocity_0X_Baseline': 'Running speed',\n",
    "        'Motor_Velocity': 'Motor Velocity',\n",
    "        'Motor_Velocity_Baseline': 'Motor Velocity',\n",
    "    }\n",
    "    return label_map.get(column_name, column_name)\n",
    "\n",
    "def get_axis_label(column_name):\n",
    "    \"\"\"Generate axis label based on column name.\"\"\"\n",
    "    if 'z_' in column_name or 'z-score' in column_name.lower():\n",
    "        return 'z-score'\n",
    "    elif 'Velocity' in column_name or 'velocity' in column_name.lower():\n",
    "        return 'Running speed (m/s)'\n",
    "    elif 'Motor' in column_name:\n",
    "        return 'Motor velocity (m/s)'\n",
    "    else:\n",
    "        return column_name\n",
    "\n",
    "def should_use_right_axis(column_name):\n",
    "    \"\"\"Determine if a column should be plotted on the right axis.\"\"\"\n",
    "    # Velocity and Motor columns go on right axis\n",
    "    return 'Velocity' in column_name or 'Motor' in column_name\n",
    "\n",
    "\n",
    "def infer_series_metadata(file_path: str) -> dict:\n",
    "    \"\"\"Extract cohort and experiment day heuristically from a CSV path.\"\"\"\n",
    "    path = Path(file_path)\n",
    "    parts = [p for p in path.parts]\n",
    "\n",
    "    cohort = next((p for p in parts if 'Cohort' in p), 'Unknown')\n",
    "    experiment_day = next((p for p in parts if 'Visual_mismatch' in p or 'Vestibular_mismatch' in p), 'UnknownDay')\n",
    "\n",
    "    # Normalise formatting\n",
    "    cohort_clean = cohort.replace('_', ' ')\n",
    "    day_clean = experiment_day.replace('_', ' ')\n",
    "    return {'cohort': cohort_clean, 'experiment_day': day_clean}\n",
    "\n",
    "\n",
    "def select_series_color(cohort: str) -> str:\n",
    "    \"\"\"Choose a colour based on cohort name.\"\"\"\n",
    "    cohort_lower = cohort.lower()\n",
    "    if 'cohort3' in cohort_lower:\n",
    "        return '#8B0000'  # dark red\n",
    "    if 'cohort1' in cohort_lower:\n",
    "        return '#FF0000'  # red\n",
    "    return 'slategray'\n",
    "\n",
    "\n",
    "def plot_grand_averages_multi_series(series_list):\n",
    "    \"\"\"Plot multiple series from selected grand-average CSV files on a shared figure.\"\"\"\n",
    "    plt.rcParams.update({\n",
    "        'font.size': 9,\n",
    "        'font.family': 'sans-serif',\n",
    "        'font.sans-serif': ['Arial'],\n",
    "        'axes.titlesize': 9,\n",
    "        'axes.labelsize': 9,\n",
    "        'legend.fontsize': 7,\n",
    "        'xtick.labelsize': 8,\n",
    "        'ytick.labelsize': 8\n",
    "    })\n",
    "\n",
    "    if not series_list:\n",
    "        print(\"‚ö†Ô∏è No series selected for plotting.\")\n",
    "        return\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax2 = None\n",
    "\n",
    "    left_axis_series = []\n",
    "    right_axis_series = []\n",
    "\n",
    "    for series in series_list:\n",
    "        file_path = series.get('file')\n",
    "        column = series.get('column')\n",
    "        label = series.get('label')\n",
    "\n",
    "        if not file_path or not column:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            abs_file_path = Path(file_path).resolve()\n",
    "            df = pd.read_csv(abs_file_path)\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        if column not in df.columns:\n",
    "            continue\n",
    "\n",
    "        metadata = infer_series_metadata(file_path)\n",
    "        colour = series.get('color') or select_series_color(metadata['cohort'])\n",
    "\n",
    "        label_parts = [get_column_label(column)]\n",
    "        if metadata['cohort'] != 'Unknown':\n",
    "            label_parts.append(metadata['cohort'])\n",
    "        if metadata['experiment_day'] != 'UnknownDay':\n",
    "            label_parts.append(metadata['experiment_day'])\n",
    "        plot_label = label if label else ' | '.join(label_parts)\n",
    "\n",
    "        use_right = should_use_right_axis(column)\n",
    "\n",
    "        if use_right:\n",
    "            if ax2 is None:\n",
    "                ax2 = ax.twinx()\n",
    "            ax_handle = ax2\n",
    "            collection = right_axis_series\n",
    "        else:\n",
    "            ax_handle = ax\n",
    "            collection = left_axis_series\n",
    "\n",
    "        ax_handle.plot(\n",
    "            df['Time (s)'],\n",
    "            df[column],\n",
    "            label=plot_label,\n",
    "            color=colour,\n",
    "            linestyle=series.get('linestyle', '-'),\n",
    "            alpha=1,\n",
    "        )\n",
    "\n",
    "        sem_column = f'{column}_SEM'\n",
    "        if sem_column in df.columns:\n",
    "            ax_handle.fill_between(\n",
    "                df['Time (s)'],\n",
    "                df[column] - df[sem_column],\n",
    "                df[column] + df[sem_column],\n",
    "                color=colour,\n",
    "                alpha=0.1 if not use_right else 0.2,\n",
    "            )\n",
    "\n",
    "        collection.append(series)\n",
    "\n",
    "    if left_axis_series:\n",
    "        first_col = left_axis_series[0]['column']\n",
    "        ax.set_ylabel(get_axis_label(first_col), fontname='Arial', fontsize=9, color='black')\n",
    "        ax.tick_params(axis='y', labelcolor='black')\n",
    "\n",
    "    if right_axis_series and ax2:\n",
    "        first_col = right_axis_series[0]['column']\n",
    "        ax2.set_ylabel(get_axis_label(first_col), fontname='Arial', fontsize=9, color='slategray')\n",
    "        ax2.tick_params(axis='y', labelcolor='slategray')\n",
    "\n",
    "    ax.axvspan(0, 2, color='gray', alpha=0.2, label='Visual mismatch (0-2s)')\n",
    "    ax.set_title('Grand Averages Comparison', fontname='Arial', fontsize=10)\n",
    "    ax.set_xlabel('Time (s)', fontname='Arial', fontsize=9)\n",
    "\n",
    "    lines, labels = ax.get_legend_handles_labels()\n",
    "    if ax2:\n",
    "        lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "        ax.legend(lines + lines2, labels + labels2, loc='upper right',\n",
    "                  prop={'family': 'Arial', 'size': 8})\n",
    "    else:\n",
    "        ax.legend(lines, labels, loc='upper right', prop={'family': 'Arial', 'size': 8})\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    return fig\n",
    "\n",
    "def plot_grand_averages_single(df, label, y1_col=None, y2_col=None):\n",
    "    \"\"\"Plot grand averages from a single CSV file with selected columns.\"\"\"\n",
    "    plt.rcParams.update({\n",
    "        'font.size': 10,\n",
    "        'font.family': 'sans-serif',\n",
    "        'font.sans-serif': ['DejaVu Sans'],\n",
    "        'axes.titlesize': 10,\n",
    "        'axes.labelsize': 10,\n",
    "        'legend.fontsize': 8,\n",
    "        'xtick.labelsize': 10,\n",
    "        'ytick.labelsize': 10\n",
    "    })\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax2 = None\n",
    "    \n",
    "    # Plot y1 column on left axis\n",
    "    if y1_col and y1_col in df.columns:\n",
    "        ax.plot(df['Time (s)'], df[y1_col], label=get_column_label(y1_col), \n",
    "                color='green', alpha=1)\n",
    "        if f'{y1_col}_SEM' in df.columns:\n",
    "            ax.fill_between(df['Time (s)'],\n",
    "                           df[y1_col] - df[f'{y1_col}_SEM'],\n",
    "                           df[y1_col] + df[f'{y1_col}_SEM'],\n",
    "                           color='green', alpha=0.1)\n",
    "        ax.set_ylabel(get_axis_label(y1_col), fontname='DejaVu Sans', fontsize=10, color='black')\n",
    "        ax.tick_params(axis='y', labelcolor='black')\n",
    "    \n",
    "    # Plot y2 column on right axis (if different from y1)\n",
    "    if y2_col and y2_col in df.columns and y2_col != y1_col:\n",
    "        ax2 = ax.twinx()\n",
    "        ax2.plot(df['Time (s)'], df[y2_col], label=get_column_label(y2_col), \n",
    "                color='slategray', alpha=1)\n",
    "        if f'{y2_col}_SEM' in df.columns:\n",
    "            ax2.fill_between(df['Time (s)'],\n",
    "                            df[y2_col] - df[f'{y2_col}_SEM'],\n",
    "                            df[y2_col] + df[f'{y2_col}_SEM'],\n",
    "                            color='slategray', alpha=0.2)\n",
    "        ax2.set_ylabel(get_axis_label(y2_col), fontname='DejaVu Sans', fontsize=10, color='slategray')\n",
    "        ax2.tick_params(axis='y', labelcolor='slategray')\n",
    "    \n",
    "    ax.axvspan(0, 2, color='gray', alpha=0.2, label='Visual mismatch (0-2s)')\n",
    "    ax.set_title(f'Grand Averages: {label}', fontname='DejaVu Sans', fontsize=12)\n",
    "    ax.set_xlabel('Time (s)', fontname='DejaVu Sans', fontsize=10)\n",
    "    \n",
    "    # Combine legends\n",
    "    lines, labels = ax.get_legend_handles_labels()\n",
    "    if ax2:\n",
    "        lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "        ax.legend(lines + lines2, labels + labels2, loc='upper right', \n",
    "                 prop={'family': 'DejaVu Sans', 'size': 10})\n",
    "    else:\n",
    "        ax.legend(lines, labels, loc='upper right', prop={'family': 'DejaVu Sans', 'size': 10})\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_grand_averages_comparison(df1, df2, label1, label2, y1_col=None, y2_col=None):\n",
    "    \"\"\"Plot grand averages comparing two CSV files with selected columns.\"\"\"\n",
    "    plt.rcParams.update({\n",
    "        'font.size': 10,\n",
    "        'font.family': 'sans-serif',\n",
    "        'font.sans-serif': ['DejaVu Sans'],\n",
    "        'axes.titlesize': 10,\n",
    "        'axes.labelsize': 10,\n",
    "        'legend.fontsize': 8,\n",
    "        'xtick.labelsize': 10,\n",
    "        'ytick.labelsize': 10\n",
    "    })\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax2 = None\n",
    "    \n",
    "    # Plot y1 column on left axis\n",
    "    if y1_col:\n",
    "        if y1_col in df1.columns:\n",
    "            ax.plot(df1['Time (s)'], df1[y1_col], label=f\"{get_column_label(y1_col)} ({label1})\", \n",
    "                    color='green', alpha=1)\n",
    "            if f'{y1_col}_SEM' in df1.columns:\n",
    "                ax.fill_between(df1['Time (s)'],\n",
    "                               df1[y1_col] - df1[f'{y1_col}_SEM'],\n",
    "                               df1[y1_col] + df1[f'{y1_col}_SEM'],\n",
    "                               color='green', alpha=0.1)\n",
    "        \n",
    "        if y1_col in df2.columns:\n",
    "            ax.plot(df2['Time (s)'], df2[y1_col], label=f\"{get_column_label(y1_col)} ({label2})\", \n",
    "                    color='orange', alpha=1, linestyle='--')\n",
    "            if f'{y1_col}_SEM' in df2.columns:\n",
    "                ax.fill_between(df2['Time (s)'],\n",
    "                               df2[y1_col] - df2[f'{y1_col}_SEM'],\n",
    "                               df2[y1_col] + df2[f'{y1_col}_SEM'],\n",
    "                               color='orange', alpha=0.1)\n",
    "        \n",
    "        ax.set_ylabel(get_axis_label(y1_col), fontname='DejaVu Sans', fontsize=10, color='black')\n",
    "        ax.tick_params(axis='y', labelcolor='black')\n",
    "    \n",
    "    # Plot y2 column on right axis (if different from y1)\n",
    "    if y2_col and y2_col != y1_col:\n",
    "        ax2 = ax.twinx()\n",
    "        \n",
    "        if y2_col in df1.columns:\n",
    "            ax2.plot(df1['Time (s)'], df1[y2_col], label=f\"{get_column_label(y2_col)} ({label1})\", \n",
    "                    color='slategray', alpha=1)\n",
    "            if f'{y2_col}_SEM' in df1.columns:\n",
    "                ax2.fill_between(df1['Time (s)'],\n",
    "                                df1[y2_col] - df1[f'{y2_col}_SEM'],\n",
    "                                df1[y2_col] + df1[f'{y2_col}_SEM'],\n",
    "                                color='slategray', alpha=0.2)\n",
    "        \n",
    "        if y2_col in df2.columns:\n",
    "            ax2.plot(df2['Time (s)'], df2[y2_col], label=f\"{get_column_label(y2_col)} ({label2})\", \n",
    "                    color='slategray', alpha=1, linestyle='--')\n",
    "            if f'{y2_col}_SEM' in df2.columns:\n",
    "                ax2.fill_between(df2['Time (s)'],\n",
    "                                df2[y2_col] - df2[f'{y2_col}_SEM'],\n",
    "                                df2[y2_col] + df2[f'{y2_col}_SEM'],\n",
    "                                color='slategray', alpha=0.2)\n",
    "        \n",
    "        ax2.set_ylabel(get_axis_label(y2_col), fontname='DejaVu Sans', fontsize=10, color='slategray')\n",
    "        ax2.tick_params(axis='y', labelcolor='slategray')\n",
    "    \n",
    "    ax.axvspan(0, 2, color='gray', alpha=0.2, label='Visual mismatch (0-2s)')\n",
    "    ax.set_title('Grand Averages with SEM', fontname='DejaVu Sans', fontsize=12)\n",
    "    ax.set_xlabel('Time (s)', fontname='DejaVu Sans', fontsize=10)\n",
    "    \n",
    "    # Combine legends\n",
    "    lines, labels = ax.get_legend_handles_labels()\n",
    "    if ax2:\n",
    "        lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "        ax.legend(lines + lines2, labels + labels2, loc='upper right', \n",
    "                 prop={'family': 'DejaVu Sans', 'size': 10})\n",
    "    else:\n",
    "        ax.legend(lines, labels, loc='upper right', prop={'family': 'DejaVu Sans', 'size': 10})\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_grand_averages_interactive():\n",
    "    \"\"\"Create interactive plot with dropdowns for multiple CSV file and column selection.\"\"\"\n",
    "    \n",
    "    # Find available CSV files using the same paths as DATA_DIRS\n",
    "    base_paths = DATA_DIRS if DATA_DIRS else []\n",
    "    \n",
    "    csv_files = find_csv_files(base_paths)\n",
    "    \n",
    "    if not csv_files:\n",
    "        return\n",
    "    \n",
    "    if WIDGETS_AVAILABLE:\n",
    "        # Store series widgets\n",
    "        series_widgets = []\n",
    "        series_container = widgets.VBox([])\n",
    "        \n",
    "        # Store current figure for saving\n",
    "        current_fig = [None]  # Use list to allow modification in nested functions\n",
    "        \n",
    "        # Color options for different series\n",
    "        color_options = ['green', 'blue', 'red', 'orange', 'purple', 'brown', 'pink', 'gray', 'olive', 'cyan']\n",
    "        linestyle_options = ['-', '--', '-.', ':']\n",
    "        \n",
    "        def create_series_row(series_id):\n",
    "            \"\"\"Create a row of widgets for selecting file and column.\"\"\"\n",
    "            file_dropdown = widgets.Dropdown(\n",
    "                options=['None'] + csv_files,\n",
    "                value='None',\n",
    "                description=f'File {series_id}:',\n",
    "                style={'description_width': 'initial'},\n",
    "                layout=widgets.Layout(width='400px')\n",
    "            )\n",
    "            \n",
    "            col_dropdown = widgets.Dropdown(\n",
    "                options=[''],\n",
    "                value='',\n",
    "                description='Column:',\n",
    "                style={'description_width': 'initial'},\n",
    "                layout=widgets.Layout(width='250px')\n",
    "            )\n",
    "            \n",
    "            remove_btn = widgets.Button(\n",
    "                description='Remove',\n",
    "                button_style='danger',\n",
    "                layout=widgets.Layout(width='80px', height='30px')\n",
    "            )\n",
    "            \n",
    "            def update_columns(change):\n",
    "                \"\"\"Update column dropdown when file changes.\"\"\"\n",
    "                if change['new'] and change['new'] != 'None':\n",
    "                    try:\n",
    "                        # Clear the current column value first\n",
    "                        col_dropdown.value = ''\n",
    "                        \n",
    "                        # Read the new file\n",
    "                        df = pd.read_csv(change['new'])\n",
    "                        available_cols = get_available_columns(df)\n",
    "                        \n",
    "                        # Update options and set to first available column\n",
    "                        col_dropdown.options = available_cols\n",
    "                        if available_cols:\n",
    "                            col_dropdown.value = available_cols[0]\n",
    "                        else:\n",
    "                            col_dropdown.value = ''\n",
    "                    except Exception:\n",
    "                        col_dropdown.options = ['']\n",
    "                        col_dropdown.value = ''\n",
    "                else:\n",
    "                    col_dropdown.options = ['']\n",
    "                    col_dropdown.value = ''\n",
    "            \n",
    "            def remove_series(b):\n",
    "                \"\"\"Remove this series row.\"\"\"\n",
    "                for i, (f, c, r, _) in enumerate(series_widgets):\n",
    "                    if f == file_dropdown:\n",
    "                        series_widgets.pop(i)\n",
    "                        update_series_display()\n",
    "                        break\n",
    "            \n",
    "            file_dropdown.observe(update_columns, names='value')\n",
    "            remove_btn.on_click(remove_series)\n",
    "            \n",
    "            return file_dropdown, col_dropdown, remove_btn, widgets.HBox([file_dropdown, col_dropdown, remove_btn])\n",
    "        \n",
    "        def update_series_display():\n",
    "            \"\"\"Update the display of all series rows.\"\"\"\n",
    "            children = [row[3] for row in series_widgets]\n",
    "            series_container.children = children\n",
    "        \n",
    "        def add_series(b):\n",
    "            \"\"\"Add a new series row.\"\"\"\n",
    "            series_id = len(series_widgets) + 1\n",
    "            widgets_row = create_series_row(series_id)\n",
    "            series_widgets.append(widgets_row)\n",
    "            update_series_display()\n",
    "        \n",
    "        def plot_all_series(b):\n",
    "            \"\"\"Plot all selected series.\"\"\"\n",
    "            with output:\n",
    "                clear_output(wait=True)\n",
    "                series_list = []\n",
    "                seen = set()\n",
    "\n",
    "                for file_dropdown, col_dropdown, _, _ in series_widgets:\n",
    "                    file_path = file_dropdown.value\n",
    "                    column = col_dropdown.value\n",
    "\n",
    "                    key = (file_path, column)\n",
    "                    if (\n",
    "                        file_path\n",
    "                        and file_path != 'None'\n",
    "                        and column\n",
    "                        and column != ''\n",
    "                        and key not in seen\n",
    "                    ):\n",
    "                        seen.add(key)\n",
    "                        series_list.append({\n",
    "                            'file': file_path,\n",
    "                            'column': column,\n",
    "                            'linestyle': linestyle_options[(len(series_list) // len(color_options)) % len(linestyle_options)]\n",
    "                        })\n",
    "\n",
    "                if not series_list:\n",
    "                    return\n",
    "\n",
    "                try:\n",
    "                    fig = plot_grand_averages_multi_series(series_list)\n",
    "                    current_fig[0] = fig  # Store figure for saving\n",
    "                    if fig is not None:\n",
    "                        display(fig)\n",
    "                        plt.close(fig)\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "        def save_current_plot(b):\n",
    "            \"\"\"Save the current plot to a file.\"\"\"\n",
    "            if current_fig[0] is None:\n",
    "                return\n",
    "\n",
    "            # Generate filename based on selected series\n",
    "            series_names = []\n",
    "            first_file_path = None\n",
    "\n",
    "            for file_dropdown, col_dropdown, _, _ in series_widgets:\n",
    "                file_path = file_dropdown.value\n",
    "                column = col_dropdown.value\n",
    "                if file_path and file_path != 'None' and column and column != '':\n",
    "                    if first_file_path is None:\n",
    "                        first_file_path = file_path\n",
    "                    file_label = Path(file_path).parent.name\n",
    "                    series_names.append(f\"{file_label}_{column}\")\n",
    "\n",
    "            if series_names:\n",
    "                filename_base = \"_vs_\".join(series_names[:3])  # Limit filename length\n",
    "                if len(series_names) > 3:\n",
    "                    filename_base += f\"_and_{len(series_names)-3}_more\"\n",
    "            else:\n",
    "                filename_base = \"grand_averages_plot\"\n",
    "\n",
    "            # Get save directory from the first CSV file being plotted\n",
    "            if first_file_path:\n",
    "                save_dir = Path(first_file_path).parent\n",
    "            else:\n",
    "                save_dir = Path(DATA_DIRS[0]) if DATA_DIRS else Path.cwd()\n",
    "\n",
    "            # Save as PDF and PNG\n",
    "            pdf_path = save_dir / f\"{filename_base}.pdf\"\n",
    "            png_path = save_dir / f\"{filename_base}.png\"\n",
    "\n",
    "            try:\n",
    "                current_fig[0].savefig(pdf_path, dpi=300, bbox_inches='tight')\n",
    "                current_fig[0].savefig(png_path, dpi=300, bbox_inches='tight')\n",
    "            except Exception:\n",
    "                pass\n",
    "        \n",
    "        # Create initial series row\n",
    "        add_series(None)\n",
    "        \n",
    "        # Buttons\n",
    "        add_btn = widgets.Button(\n",
    "            description='+ Add Series',\n",
    "            button_style='info',\n",
    "            layout=widgets.Layout(width='150px')\n",
    "        )\n",
    "        add_btn.on_click(add_series)\n",
    "        \n",
    "        plot_button = widgets.Button(\n",
    "            description='Plot All Series',\n",
    "            button_style='success',\n",
    "            layout=widgets.Layout(width='200px')\n",
    "        )\n",
    "\n",
    "        def plot_and_show(_):\n",
    "            plot_all_series(_)\n",
    "\n",
    "        plot_button.on_click(plot_and_show)\n",
    "        \n",
    "        save_button = widgets.Button(\n",
    "            description='Save Plot',\n",
    "            button_style='warning',\n",
    "            layout=widgets.Layout(width='150px')\n",
    "        )\n",
    "        save_button.on_click(save_current_plot)\n",
    "        \n",
    "        output = widgets.Output()\n",
    "        \n",
    "        display(widgets.VBox([\n",
    "            widgets.HTML(\"<h3>Select multiple files and columns to plot:</h3>\"),\n",
    "            widgets.HTML(\"<p><i>Add multiple series to compare columns from different files on the same graph.</i></p>\"),\n",
    "            series_container,\n",
    "            widgets.HBox([add_btn, plot_button, save_button]),\n",
    "            output\n",
    "        ]))\n",
    "    else:\n",
    "        return\n",
    "\n",
    "# Run interactive plotting\n",
    "plot_grand_averages_interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b03811",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# FIXME not sure if it works, not used now # CORRELATION ANALYSIS\n",
    "# #---------------------------------------------------------------------------------------------------#\n",
    "# # This section performs correlation analysis between Velocity and z-scores (z_470, z_560)\n",
    "# # Requires loading previously saved results from pickle files for multiple cohorts\n",
    "\n",
    "# #---------------------------------------------------------------------------------------------------#\n",
    "# # Helper Functions\n",
    "# #---------------------------------------------------------------------------------------------------#\n",
    "\n",
    "# def load_results(filename):\n",
    "#     \"\"\"Load results from a pickle file.\"\"\"\n",
    "#     with open(filename, 'rb') as f:\n",
    "#         return pickle.load(f)\n",
    "\n",
    "# def fisher_z(r):\n",
    "#     \"\"\"Convert correlation coefficient to Fisher's z.\"\"\"\n",
    "#     return 0.5 * np.log((1 + r) / (1 - r))\n",
    "\n",
    "# def compare_correlations(r1, n1, r2, n2):\n",
    "#     \"\"\"Compare two correlation coefficients using Fisher's z transformation.\"\"\"\n",
    "#     z1 = fisher_z(r1)\n",
    "#     z2 = fisher_z(r2)\n",
    "#     se = np.sqrt(1 / (n1 - 3) + 1 / (n2 - 3))\n",
    "#     z = (z1 - z2) / se\n",
    "#     p = 2 * (1 - norm.cdf(abs(z)))\n",
    "#     return z, p\n",
    "\n",
    "# def extract_means(results, mice, time_window, columns=columns_to_plot):\n",
    "#     \"\"\"\n",
    "#     Extract mean values for specified columns within a given time window for a list of mice.\n",
    "\n",
    "#     Parameters:\n",
    "#     results (dict): Dictionary containing data for each mouse.\n",
    "#     mice (list): List of mouse names to extract data for.\n",
    "#     time_window (tuple): Time window (start, end) for extracting mean values.\n",
    "#     columns (list): List of column names to extract.\n",
    "\n",
    "#     Returns:\n",
    "#     tuple: Mean values for Velocity_0X, z_470, z_560, and valid mouse names.\n",
    "#     \"\"\"\n",
    "#     v_means, z470_means, z560_means, valid_mice = [], [], [], []\n",
    "#     t0, t1 = time_window\n",
    "\n",
    "#     for mouse in mice:\n",
    "#         if mouse not in results['mean_data_per_mouse']:\n",
    "#             continue\n",
    "#         df = results['mean_data_per_mouse'][mouse]\n",
    "#         if not all(col in df.columns for col in columns):\n",
    "#             continue\n",
    "\n",
    "#         df_window = df.loc[(df.index >= t0) & (df.index <= t1)]\n",
    "#         v = df_window[columns[0]].mean()\n",
    "#         z470 = df_window[columns[1]].mean()\n",
    "#         z560 = df_window[columns[2]].mean()\n",
    "\n",
    "#         if not any(pd.isnull([v, z470, z560])):\n",
    "#             v_means.append(v)\n",
    "#             z470_means.append(z470)\n",
    "#             z560_means.append(z560)\n",
    "#             valid_mice.append(mouse)\n",
    "\n",
    "#     return v_means, z470_means, z560_means, valid_mice\n",
    "\n",
    "# def move_mouse_data_fixed(v1, z470_1, z560_1, ids1, v2, z470_2, z560_2, ids2, \n",
    "#                          target_mouse, exclude_mice):\n",
    "#     \"\"\"\n",
    "#     Fixed version that maintains array consistency when moving mouse data between cohorts.\n",
    "#     \"\"\"\n",
    "#     # Convert to lists if not already (for easier manipulation)\n",
    "#     v1, z470_1, z560_1, ids1 = list(v1), list(z470_1), list(z560_1), list(ids1)\n",
    "#     v2, z470_2, z560_2, ids2 = list(v2), list(z470_2), list(z560_2), list(ids2)\n",
    "    \n",
    "#     # Exclude mice from both cohorts\n",
    "#     for mouse in exclude_mice:\n",
    "#         # Remove from cohort 1\n",
    "#         while mouse in ids1:\n",
    "#             idx = ids1.index(mouse)\n",
    "#             del v1[idx], z470_1[idx], z560_1[idx], ids1[idx]\n",
    "        \n",
    "#         # Remove from cohort 2\n",
    "#         while mouse in ids2:\n",
    "#             idx = ids2.index(mouse)\n",
    "#             del v2[idx], z470_2[idx], z560_2[idx], ids2[idx]\n",
    "    \n",
    "#     # Move target mouse data\n",
    "#     if target_mouse in ids2:\n",
    "#         idx = ids2.index(target_mouse)\n",
    "        \n",
    "#         # Store the data to move\n",
    "#         target_v = v2[idx]\n",
    "#         target_z470 = z470_2[idx]\n",
    "#         target_z560 = z560_2[idx]\n",
    "        \n",
    "#         # Remove original entry from Cohort 2\n",
    "#         del v2[idx], z470_2[idx], z560_2[idx], ids2[idx]\n",
    "        \n",
    "#         # Add to Cohort 1 (velocity and z_470, NaN for z_560)\n",
    "#         v1.append(target_v)\n",
    "#         z470_1.append(target_z470)\n",
    "#         z560_1.append(np.nan)\n",
    "#         ids1.append(target_mouse + \" (from Cohort 2)\")\n",
    "        \n",
    "#         # Keep z_560 correlation in Cohort 2 (velocity and z_560, NaN for z_470)\n",
    "#         v2.append(target_v)\n",
    "#         z470_2.append(np.nan)\n",
    "#         z560_2.append(target_z560)\n",
    "#         ids2.append(target_mouse + \" (z_560)\")\n",
    "        \n",
    "#         print(f\"‚úÖ Moved velocity & z_470 of {target_mouse} to Cohort 1.\")\n",
    "#         print(f\"‚úÖ Retained velocity & z_560 of {target_mouse} in Cohort 2.\")\n",
    "#     else:\n",
    "#         print(f\"‚ö†Ô∏è Mouse {target_mouse} not found in Cohort 2.\")\n",
    "    \n",
    "#     return v1, z470_1, z560_1, ids1, v2, z470_2, z560_2, ids2\n",
    "\n",
    "# def analyze_correlations_from_data_fixed(\n",
    "#     v1, z470_1, z560_1, ids1,\n",
    "#     v2, z470_2, z560_2, ids2,\n",
    "#     time_window=None,\n",
    "#     plot=True\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Fixed correlation analysis function that computes correlations and compares them.\n",
    "#     \"\"\"\n",
    "#     def filter_valid_pairs(x, y, labels):\n",
    "#         x = np.array(x)\n",
    "#         y = np.array(y)\n",
    "#         labels = np.array(labels)\n",
    "        \n",
    "#         # Ensure all arrays have the same length\n",
    "#         min_len = min(len(x), len(y), len(labels))\n",
    "#         x = x[:min_len]\n",
    "#         y = y[:min_len]\n",
    "#         labels = labels[:min_len]\n",
    "        \n",
    "#         # Filter out NaN values\n",
    "#         mask = ~np.isnan(x) & ~np.isnan(y)\n",
    "#         return x[mask], y[mask], labels[mask]\n",
    "\n",
    "#     if time_window is None:\n",
    "#         time_window = (0, 2)  # Default time window if not specified\n",
    "\n",
    "#     # Filter valid pairs for each correlation\n",
    "#     v1_470, z470_1_filt, ids1_470 = filter_valid_pairs(v1, z470_1, ids1)\n",
    "#     v1_560, z560_1_filt, ids1_560 = filter_valid_pairs(v1, z560_1, ids1)\n",
    "#     v2_470, z470_2_filt, ids2_470 = filter_valid_pairs(v2, z470_2, ids2)\n",
    "#     v2_560, z560_2_filt, ids2_560 = filter_valid_pairs(v2, z560_2, ids2)\n",
    "\n",
    "#     # Calculate correlations\n",
    "#     results = {}\n",
    "#     if len(v1_470) > 1:\n",
    "#         corr1_470, p1_470 = pearsonr(v1_470, z470_1_filt)\n",
    "#         results['corr1_470'] = (corr1_470, p1_470, len(v1_470))\n",
    "#     else:\n",
    "#         results['corr1_470'] = (np.nan, np.nan, len(v1_470))\n",
    "        \n",
    "#     if len(v1_560) > 1:\n",
    "#         corr1_560, p1_560 = pearsonr(v1_560, z560_1_filt)\n",
    "#         results['corr1_560'] = (corr1_560, p1_560, len(v1_560))\n",
    "#     else:\n",
    "#         results['corr1_560'] = (np.nan, np.nan, len(v1_560))\n",
    "        \n",
    "#     if len(v2_470) > 1:\n",
    "#         corr2_470, p2_470 = pearsonr(v2_470, z470_2_filt)\n",
    "#         results['corr2_470'] = (corr2_470, p2_470, len(v2_470))\n",
    "#     else:\n",
    "#         results['corr2_470'] = (np.nan, np.nan, len(v2_470))\n",
    "        \n",
    "#     if len(v2_560) > 1:\n",
    "#         corr2_560, p2_560 = pearsonr(v2_560, z560_2_filt)\n",
    "#         results['corr2_560'] = (corr2_560, p2_560, len(v2_560))\n",
    "#     else:\n",
    "#         results['corr2_560'] = (np.nan, np.nan, len(v2_560))\n",
    "\n",
    "#     # Print results\n",
    "#     print(\"\\nüìä Correlations:\")\n",
    "#     print(f\"Cohort 1: Velocity ~ z_470: r = {results['corr1_470'][0]:.3f}, p = {results['corr1_470'][1]:.3f} (n={results['corr1_470'][2]})\")\n",
    "#     print(f\"Cohort 1: Velocity ~ z_560: r = {results['corr1_560'][0]:.3f}, p = {results['corr1_560'][1]:.3f} (n={results['corr1_560'][2]})\")\n",
    "#     print(f\"Cohort 2: Velocity ~ z_470: r = {results['corr2_470'][0]:.3f}, p = {results['corr2_470'][1]:.3f} (n={results['corr2_470'][2]})\")\n",
    "#     print(f\"Cohort 2: Velocity ~ z_560: r = {results['corr2_560'][0]:.3f}, p = {results['corr2_560'][1]:.3f} (n={results['corr2_560'][2]})\")\n",
    "\n",
    "#     # Compare correlations\n",
    "#     if results['corr1_470'][2] > 3 and results['corr2_470'][2] > 3:\n",
    "#         z_470, p_470 = compare_correlations(results['corr1_470'][0], results['corr1_470'][2], \n",
    "#                                           results['corr2_470'][0], results['corr2_470'][2])\n",
    "#         print(f\"\\nüîç Comparison of correlations for z_470:\")\n",
    "#         print(f\"z = {z_470:.3f}, p = {p_470:.3f}\")\n",
    "#     else:\n",
    "#         print(\"‚ö†Ô∏è Not enough data to compare z_470 correlations.\")\n",
    "\n",
    "#     if results['corr1_560'][2] > 3 and results['corr2_560'][2] > 3:\n",
    "#         z_560, p_560 = compare_correlations(results['corr1_560'][0], results['corr1_560'][2], \n",
    "#                                           results['corr2_560'][0], results['corr2_560'][2])\n",
    "#         print(f\"\\nüîç Comparison of correlations for z_560:\")\n",
    "#         print(f\"z = {z_560:.3f}, p = {p_560:.3f}\")\n",
    "#     else:\n",
    "#         print(\"‚ö†Ô∏è Not enough data to compare z_560 correlations.\")\n",
    "\n",
    "#     # Create plots\n",
    "#     if plot:\n",
    "#         # Plot properties\n",
    "#         plt.rcParams.update({\n",
    "#             'font.size': 10,\n",
    "#             'font.family': 'sans-serif',\n",
    "#             'font.sans-serif': ['DejaVu Sans'],\n",
    "#             'axes.titlesize': 10,\n",
    "#             'axes.labelsize': 10,\n",
    "#             'legend.fontsize': 8,\n",
    "#             'xtick.labelsize': 10,\n",
    "#             'ytick.labelsize': 10\n",
    "#         })\n",
    "        \n",
    "#         fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "#         t_str = f\"{time_window[0]} to {time_window[1]}s\"\n",
    "\n",
    "#         # Plot z_470 correlations\n",
    "#         if len(v1_470) > 0:\n",
    "#             axs[0].scatter(v1_470, z470_1_filt, color='green', label=f'Cohort 1 (n={len(v1_470)})', alpha=0.7)\n",
    "#             for i, mouse in enumerate(ids1_470):\n",
    "#                 axs[0].text(v1_470[i], z470_1_filt[i], mouse, fontsize=6, color='green', alpha=0.6)\n",
    "#             if len(v1_470) > 1:\n",
    "#                 m1, b1 = np.polyfit(v1_470, z470_1_filt, 1)\n",
    "#                 x_range = np.linspace(min(v1_470), max(v1_470), 100)\n",
    "#                 axs[0].plot(x_range, m1 * x_range + b1, color='green', linestyle='--', alpha=0.8)\n",
    "        \n",
    "#         if len(v2_470) > 0:\n",
    "#             axs[0].scatter(v2_470, z470_2_filt, color='orange', label=f'Cohort 2 (n={len(v2_470)})', alpha=0.7)\n",
    "#             for i, mouse in enumerate(ids2_470):\n",
    "#                 axs[0].text(v2_470[i], z470_2_filt[i], mouse, fontsize=6, color='orange', alpha=0.6)\n",
    "#             if len(v2_470) > 1:\n",
    "#                 m2, b2 = np.polyfit(v2_470, z470_2_filt, 1)\n",
    "#                 x_range = np.linspace(min(v2_470), max(v2_470), 100)\n",
    "#                 axs[0].plot(x_range, m2 * x_range + b2, color='orange', linestyle='--', alpha=0.8)\n",
    "        \n",
    "#         axs[0].set_title(f'Velocity vs z_470\\n({t_str})')\n",
    "#         axs[0].set_xlabel('Mean Velocity_0X (m/s)')\n",
    "#         axs[0].set_ylabel('Mean z-score (470nm)')\n",
    "#         axs[0].legend()\n",
    "#         axs[0].grid(True, alpha=0.3)\n",
    "\n",
    "#         # Plot z_560 correlations\n",
    "#         if len(v1_560) > 0:\n",
    "#             axs[1].scatter(v1_560, z560_1_filt, color='red', label=f'Cohort 1 (n={len(v1_560)})', alpha=0.7)\n",
    "#             for i, mouse in enumerate(ids1_560):\n",
    "#                 axs[1].text(v1_560[i], z560_1_filt[i], mouse, fontsize=6, color='red', alpha=0.6)\n",
    "#             if len(v1_560) > 1:\n",
    "#                 m1, b1 = np.polyfit(v1_560, z560_1_filt, 1)\n",
    "#                 x_range = np.linspace(min(v1_560), max(v1_560), 100)\n",
    "#                 axs[1].plot(x_range, m1 * x_range + b1, color='red', linestyle='--', alpha=0.8)\n",
    "        \n",
    "#         if len(v2_560) > 0:\n",
    "#             axs[1].scatter(v2_560, z560_2_filt, color='darkred', label=f'Cohort 2 (n={len(v2_560)})', alpha=0.7)\n",
    "#             for i, mouse in enumerate(ids2_560):\n",
    "#                 axs[1].text(v2_560[i], z560_2_filt[i], mouse, fontsize=6, color='darkred', alpha=0.6)\n",
    "#             if len(v2_560) > 1:\n",
    "#                 m2, b2 = np.polyfit(v2_560, z560_2_filt, 1)\n",
    "#                 x_range = np.linspace(min(v2_560), max(v2_560), 100)\n",
    "#                 axs[1].plot(x_range, m2 * x_range + b2, color='darkred', linestyle='--', alpha=0.8)\n",
    "        \n",
    "#         axs[1].set_title(f'Velocity vs z_560\\n({t_str})')\n",
    "#         axs[1].set_xlabel('Mean Velocity_0X (m/s)')\n",
    "#         axs[1].set_ylabel('Mean z-score (560nm)')\n",
    "#         axs[1].legend()\n",
    "#         axs[1].grid(True, alpha=0.3)\n",
    "\n",
    "#         plt.tight_layout()\n",
    "#         plt.show()\n",
    "    \n",
    "#     return results\n",
    "\n",
    "# #---------------------------------------------------------------------------------------------------#\n",
    "# # Load Data and Run Analysis\n",
    "# #---------------------------------------------------------------------------------------------------#\n",
    "# # Uncomment and modify paths as needed for correlation analysis\n",
    "# # Example:\n",
    "# # results_cohort1 = load_results('/path/to/Cohort1_results.pkl')\n",
    "# # results_cohort2 = load_results('/path/to/Cohort2_results.pkl')\n",
    "# # selected_mice1 = ['B6J2717', 'B6J2718', 'B6J2719', 'B6J2721', 'B6J2722']\n",
    "# # selected_mice2 = ['B6J2780', 'B6J2781', 'B6J2783', 'B6J2782']\n",
    "# # time_window = (0, 2)\n",
    "\n",
    "# # Extract means for each cohort\n",
    "# # v1, z470_1, z560_1, ids1 = extract_means(results_cohort1, selected_mice1, time_window)\n",
    "# # v2, z470_2, z560_2, ids2 = extract_means(results_cohort2, selected_mice2, time_window)\n",
    "\n",
    "# # Optional: Move mouse data between cohorts\n",
    "# # target_mouse = \"B6J2782\"  # Specify the mouse to move\n",
    "# # exclude_mice = [\"B6J2722\"]  # List of mice to exclude\n",
    "# # v1, z470_1, z560_1, ids1, v2, z470_2, z560_2, ids2 = move_mouse_data_fixed(\n",
    "# #     v1, z470_1, z560_1, ids1, v2, z470_2, z560_2, ids2, target_mouse, exclude_mice\n",
    "# # )\n",
    "\n",
    "# # Run correlation analysis\n",
    "# # correlation_results = analyze_correlations_from_data_fixed(\n",
    "# #     v1, z470_1, z560_1, ids1,\n",
    "# #     v2, z470_2, z560_2, ids2,\n",
    "# #     time_window=time_window,\n",
    "# #     plot=True\n",
    "# # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4c19df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIGNAL METRIC ANALYSIS UTILITIES\n",
    "#---------------------------------------------------------------------------------------------------#\n",
    "\n",
    "def _estimate_decay_tau(times: np.ndarray, values: np.ndarray, peak_index: int) -> float:\n",
    "    \"\"\"Fit an exponential decay to estimate tau after the peak.\"\"\"\n",
    "    if times.size == 0 or values.size == 0:\n",
    "        return float(np.nan)\n",
    "\n",
    "    peak_value = values[peak_index]\n",
    "    if np.isnan(peak_value) or peak_value == 0.0:\n",
    "        return float(np.nan)\n",
    "\n",
    "    post_times = times[peak_index:]\n",
    "    post_values = values[peak_index:]\n",
    "    if post_times.size < 3:\n",
    "        return float(np.nan)\n",
    "\n",
    "    rel_times = post_times - post_times[0]\n",
    "    normalized = post_values / peak_value\n",
    "\n",
    "    mask = (\n",
    "        np.isfinite(rel_times)\n",
    "        & np.isfinite(normalized)\n",
    "        & (rel_times >= 0.0)\n",
    "        & (normalized > 0.0)\n",
    "    )\n",
    "    mask[0] = False  # exclude the peak itself to avoid log(1)=0 dominating\n",
    "\n",
    "    if mask.sum() < 2:\n",
    "        return float(np.nan)\n",
    "\n",
    "    try:\n",
    "        slope, intercept = np.polyfit(rel_times[mask], np.log(normalized[mask]), 1)\n",
    "    except np.linalg.LinAlgError:\n",
    "        return float(np.nan)\n",
    "\n",
    "    if slope >= 0:\n",
    "        return float(np.nan)\n",
    "\n",
    "    return float(-1.0 / slope)\n",
    "\n",
    "\n",
    "def _prepare_mouse_signal_frame(mouse_df: pd.DataFrame, signal_column: str) -> pd.DataFrame:\n",
    "    \"\"\"Return tidy DataFrame with time/value pairs for a given signal.\"\"\"\n",
    "    if signal_column not in mouse_df.columns:\n",
    "        return pd.DataFrame(columns=[\"time\", \"value\"])\n",
    "\n",
    "    times = pd.to_numeric(mouse_df.index, errors=\"coerce\")\n",
    "    values = pd.to_numeric(mouse_df[signal_column], errors=\"coerce\")\n",
    "    df_signal = pd.DataFrame({\"time\": times, \"value\": values}).dropna()\n",
    "\n",
    "    if df_signal.empty:\n",
    "        return df_signal\n",
    "\n",
    "    return df_signal.sort_values(\"time\")\n",
    "\n",
    "\n",
    "def _compute_signal_metrics_for_mouse(\n",
    "    mouse_df: pd.DataFrame,\n",
    "    signal_column: str,\n",
    "    window: Tuple[float, float],\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Compute peak, onset time (20% of peak), and half-width decay for a mouse signal.\"\"\"\n",
    "    df_signal = _prepare_mouse_signal_frame(mouse_df, signal_column)\n",
    "\n",
    "    if df_signal.empty:\n",
    "        return {\"peak\": np.nan, \"onset_time\": np.nan, \"half_width_decay\": np.nan}\n",
    "\n",
    "    window_mask = (df_signal[\"time\"] >= window[0]) & (df_signal[\"time\"] <= window[1])\n",
    "    window_df = df_signal.loc[window_mask]\n",
    "\n",
    "    if window_df.empty:\n",
    "        return {\"peak\": np.nan, \"onset_time\": np.nan, \"half_width_decay\": np.nan}\n",
    "\n",
    "    times = window_df[\"time\"].to_numpy(dtype=float)\n",
    "    values = window_df[\"value\"].to_numpy(dtype=float)\n",
    "\n",
    "    if values.size == 0:\n",
    "        return {\"peak\": np.nan, \"onset_time\": np.nan, \"half_width_decay\": np.nan}\n",
    "\n",
    "    peak_index = int(np.nanargmax(values))\n",
    "    peak_value = values[peak_index]\n",
    "\n",
    "    if np.isnan(peak_value) or peak_value == 0.0:\n",
    "        return {\"peak\": float(peak_value), \"onset_time\": np.nan, \"decay_tau\": np.nan}\n",
    "\n",
    "    decay_tau = _estimate_decay_tau(times, values, peak_index)\n",
    "\n",
    "    onset_time = np.nan\n",
    "    onset_threshold = peak_value * 0.2\n",
    "    comparator = (lambda val: val >= onset_threshold) if peak_value > 0 else (lambda val: val <= onset_threshold)\n",
    "\n",
    "    for idx in range(0, peak_index + 1):\n",
    "        current_value = values[idx]\n",
    "        if np.isnan(current_value):\n",
    "            continue\n",
    "        if comparator(current_value):\n",
    "            if idx == 0:\n",
    "                onset_time = times[idx]\n",
    "            else:\n",
    "                prev_time = times[idx - 1]\n",
    "                prev_value = values[idx - 1]\n",
    "                if np.isnan(prev_value) or prev_value == current_value:\n",
    "                    onset_time = times[idx]\n",
    "                else:\n",
    "                    fraction = (onset_threshold - prev_value) / (current_value - prev_value)\n",
    "                    onset_time = prev_time + fraction * (times[idx] - prev_time)\n",
    "            break\n",
    "\n",
    "    return {\n",
    "        \"peak\": float(peak_value),\n",
    "        \"onset_time\": float(onset_time),\n",
    "        \"decay_tau\": float(decay_tau),\n",
    "    }\n",
    "\n",
    "\n",
    "def sem(values) -> float:\n",
    "    \"\"\"Compute standard error of the mean, ignoring NaNs.\"\"\"\n",
    "    arr = np.asarray(values, dtype=float)\n",
    "    arr = arr[np.isfinite(arr)]\n",
    "    n = arr.size\n",
    "    if n <= 1:\n",
    "        return float(np.nan)\n",
    "    return float(arr.std(ddof=1) / np.sqrt(n))\n",
    "\n",
    "def compute_signal_metrics_from_results(\n",
    "    mean_data_per_mouse: Dict[str, pd.DataFrame],\n",
    "    signal_columns: list[str],\n",
    "    window: Tuple[float, float],\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Compute per-mouse and summary signal metrics directly from loaded results.\"\"\"\n",
    "    records = []\n",
    "\n",
    "    for mouse_id, mouse_df in mean_data_per_mouse.items():\n",
    "        if mouse_df is None or mouse_df.empty:\n",
    "            continue\n",
    "\n",
    "        for signal in signal_columns:\n",
    "            metrics = _compute_signal_metrics_for_mouse(mouse_df, signal, window)\n",
    "            records.append({\"mouse\": mouse_id, \"signal\": signal, **metrics})\n",
    "\n",
    "    metrics_df = pd.DataFrame(records)\n",
    "    if metrics_df.empty:\n",
    "        return metrics_df, pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "    summary_records = []\n",
    "    for signal_name, group in metrics_df.groupby(\"signal\"):\n",
    "        for metric_name in SIGNAL_METRIC_FEATURES:\n",
    "            values = pd.to_numeric(group[metric_name], errors=\"coerce\").dropna()\n",
    "            summary_records.append(\n",
    "                {\n",
    "                    \"signal\": signal_name,\n",
    "                    \"metric\": metric_name,\n",
    "                    \"n\": int(values.size),\n",
    "                    \"mean\": float(values.mean()) if values.size else np.nan,\n",
    "                    \"sem\": sem(values) if values.size else np.nan,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    summary_df = pd.DataFrame(summary_records)\n",
    "    if not summary_df.empty:\n",
    "        summary_df = summary_df.sort_values([\"signal\", \"metric\"]).reset_index(drop=True)\n",
    "        summary_pivot = summary_df.pivot(index=\"metric\", columns=\"signal\", values=[\"mean\", \"sem\"])\n",
    "    else:\n",
    "        summary_pivot = pd.DataFrame()\n",
    "\n",
    "    return metrics_df, summary_df, summary_pivot\n",
    "\n",
    "def save_signal_metrics_to_csv(\n",
    "    metrics: pd.DataFrame,\n",
    "    summary: pd.DataFrame,\n",
    "    output_dir: Path,\n",
    "    file_prefix: str,\n",
    "    cohort: Optional[str] = None,\n",
    "    experiment_day: Optional[str] = None,\n",
    ") -> Path:\n",
    "    \"\"\"Save per-mouse signal metrics to disk (summary is not written).\"\"\"\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    metrics_to_save = metrics.copy()\n",
    "    if cohort:\n",
    "        metrics_to_save.insert(0, \"Cohort\", cohort)\n",
    "    if experiment_day:\n",
    "        insert_idx = 1 if cohort else 0\n",
    "        metrics_to_save.insert(insert_idx, \"Experiment_Day\", experiment_day)\n",
    "\n",
    "    per_mouse_path = output_dir / f\"{file_prefix}_calcium_analysis_per_mouse_metrics.csv\"\n",
    "    metrics_to_save.to_csv(per_mouse_path, index=False)\n",
    "\n",
    "    return per_mouse_path\n",
    "\n",
    "\n",
    "def _make_signal_metrics_file_prefix(event_name: str) -> str:\n",
    "    \"\"\"Create a filesystem-friendly prefix from the configured event name.\"\"\"\n",
    "    if not event_name:\n",
    "        return \"signal_metrics\"\n",
    "    label = event_name.replace(\".csv\", \"\")\n",
    "    label = label.strip(\"_\")\n",
    "    label = label.replace(\" \", \"_\")\n",
    "    return label or \"signal_metrics\"\n",
    "\n",
    "def compute_signal_window_mean_per_mouse(\n",
    "    mean_data_per_mouse: Dict[str, pd.DataFrame],\n",
    "    signal_column: str,\n",
    "    window_start: float,\n",
    "    window_end: float,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Compute per-mouse mean of a signal within a specified time window.\"\"\"\n",
    "    records = []\n",
    "    for mouse_id, mouse_df in mean_data_per_mouse.items():\n",
    "        if mouse_df is None or signal_column not in mouse_df.columns:\n",
    "            continue\n",
    "\n",
    "        times = pd.to_numeric(mouse_df.index, errors=\"coerce\")\n",
    "        if np.all(np.isnan(times)):\n",
    "            continue\n",
    "\n",
    "        mask = (times >= window_start) & (times <= window_end)\n",
    "        if not mask.any():\n",
    "            continue\n",
    "\n",
    "        window_values = pd.to_numeric(mouse_df.loc[mask, signal_column], errors=\"coerce\").dropna()\n",
    "        if window_values.empty:\n",
    "            continue\n",
    "\n",
    "        records.append(\n",
    "            {\n",
    "                \"mouse\": mouse_id,\n",
    "                \"mean\": float(window_values.mean()),\n",
    "                \"n_samples\": int(window_values.size),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "\n",
    "def extract_mouse_dataframes_from_loaded(\n",
    "    loaded_data: Dict[object, Dict[str, object]],\n",
    "    signal_columns: Optional[Iterable[str]] = None,\n",
    "    time_column: str = \"Time (s)\",\n",
    ") -> Tuple[Dict[str, pd.DataFrame], Dict[str, Path]]:\n",
    "    \"\"\"Convert the raw loaded data into per-mouse DataFrames and track their origins.\"\"\"\n",
    "    mouse_frames: Dict[str, list[pd.DataFrame]] = {}\n",
    "    mouse_sources: Dict[str, Path] = {}\n",
    "\n",
    "    for entry in loaded_data.values():\n",
    "        mouse_name = entry.get(\"mouse_name\")\n",
    "        if not mouse_name:\n",
    "            continue\n",
    "\n",
    "        columns = [key for key in entry.keys() if key not in {\"mouse_name\", \"data_path\"}]\n",
    "        if time_column in entry:\n",
    "            columns = [time_column] + [col for col in columns if col != time_column]\n",
    "        if signal_columns is not None:\n",
    "            allowed = set(signal_columns)\n",
    "            columns = [col for col in columns if col == time_column or col in allowed]\n",
    "\n",
    "        data = {col: entry[col] for col in columns if col in entry}\n",
    "        if not data:\n",
    "            continue\n",
    "\n",
    "        df = pd.DataFrame(data)\n",
    "        if time_column in df.columns:\n",
    "            df = df.set_index(time_column)\n",
    "            df.index.name = time_column\n",
    "        df = df.apply(pd.to_numeric, errors=\"coerce\").dropna(how=\"all\")\n",
    "        if df.empty:\n",
    "            continue\n",
    "\n",
    "        df = df.sort_index()\n",
    "        mouse_frames.setdefault(mouse_name, []).append(df)\n",
    "\n",
    "        data_path = entry.get(\"data_path\")\n",
    "        if data_path is not None and mouse_name not in mouse_sources:\n",
    "            mouse_sources[mouse_name] = Path(data_path)\n",
    "\n",
    "    consolidated: Dict[str, pd.DataFrame] = {}\n",
    "    for mouse, frames in mouse_frames.items():\n",
    "        if len(frames) == 1:\n",
    "            consolidated[mouse] = frames[0]\n",
    "        else:\n",
    "            combined = pd.concat(frames)\n",
    "            combined = combined.groupby(combined.index).mean().sort_index()\n",
    "            consolidated[mouse] = combined\n",
    "\n",
    "    return consolidated, mouse_sources\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b27290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SIGNAL METRIC ANALYSIS (window = [0.00, 1.00] s)\n",
      "============================================================\n",
      "‚úÖ Computed signal metrics for 4 mice across 2 signals.\n",
      "z_560_Baseline mean fluorescence (2-8s post): -0.003891 ¬± 0.059481 (SEM, n=4)\n",
      "Saved per-mouse metrics to: /Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day3/Apply_halt_2s_baselined_data_calcium_analysis_per_mouse_metrics.csv\n",
      "\n",
      "Signal metric summary (mean/sem):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">mean</th>\n",
       "      <th colspan=\"2\" halign=\"left\">sem</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>signal</th>\n",
       "      <th>z_470_Baseline</th>\n",
       "      <th>z_560_Baseline</th>\n",
       "      <th>z_470_Baseline</th>\n",
       "      <th>z_560_Baseline</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>metric</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>decay_tau</th>\n",
       "      <td>1.040656</td>\n",
       "      <td>1.169526</td>\n",
       "      <td>0.239934</td>\n",
       "      <td>0.360561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_fluorescence_2_to_8s</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.003891</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.059481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>onset_time</th>\n",
       "      <td>0.013750</td>\n",
       "      <td>0.045000</td>\n",
       "      <td>0.013750</td>\n",
       "      <td>0.045000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>peak</th>\n",
       "      <td>2.151572</td>\n",
       "      <td>2.830489</td>\n",
       "      <td>0.388677</td>\n",
       "      <td>0.332076</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    mean                           sem  \\\n",
       "signal                    z_470_Baseline z_560_Baseline z_470_Baseline   \n",
       "metric                                                                   \n",
       "decay_tau                       1.040656       1.169526       0.239934   \n",
       "mean_fluorescence_2_to_8s            NaN      -0.003891            NaN   \n",
       "onset_time                      0.013750       0.045000       0.013750   \n",
       "peak                            2.151572       2.830489       0.388677   \n",
       "\n",
       "                                          \n",
       "signal                    z_560_Baseline  \n",
       "metric                                    \n",
       "decay_tau                       0.360561  \n",
       "mean_fluorescence_2_to_8s       0.059481  \n",
       "onset_time                      0.045000  \n",
       "peak                            0.332076  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# RUN SIGNAL METRIC ANALYSIS\n",
    "#---------------------------------------------------------------------------------------------------#\n",
    "\n",
    "signal_metrics_df = pd.DataFrame()\n",
    "signal_metrics_summary_df = pd.DataFrame()\n",
    "signal_metrics_summary_pivot = pd.DataFrame()\n",
    "\n",
    "mouse_signal_frames: Dict[str, pd.DataFrame] = {}\n",
    "mouse_source_dirs: Dict[str, Path] = {}\n",
    "used_loaded_data = False\n",
    "used_results_data = False\n",
    "\n",
    "if 'loaded_data' in locals() and loaded_data:\n",
    "    mouse_signal_frames, mouse_source_dirs = extract_mouse_dataframes_from_loaded(\n",
    "        loaded_data,\n",
    "        signal_columns=SIGNAL_METRIC_COLUMNS,\n",
    "    )\n",
    "    used_loaded_data = bool(mouse_signal_frames)\n",
    "\n",
    "results_available = locals().get(\"results\")\n",
    "if not mouse_signal_frames and results_available:\n",
    "    fallback_frames = results_available.get(\"mean_data_per_mouse\", {})\n",
    "    if fallback_frames:\n",
    "        mouse_signal_frames = fallback_frames\n",
    "        used_results_data = True\n",
    "        source_map = results_available.get(\"mouse_to_data_path\", {}) if isinstance(results_available, dict) else {}\n",
    "        mouse_source_dirs = {mouse: Path(path) for mouse, path in source_map.items()}\n",
    "\n",
    "if not mouse_signal_frames:\n",
    "    print(\"‚è≠Ô∏è  Skipping signal metric analysis (no loaded data available).\")\n",
    "else:\n",
    "    analysis_window = POST_ALIGNMENT_WINDOW\n",
    "    window_str = f\"[{analysis_window[0]:.2f}, {analysis_window[1]:.2f}] s\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"SIGNAL METRIC ANALYSIS (window = {window_str})\")\n",
    "    print(f\"{'='*60}\")\n",
    "    if used_results_data and not used_loaded_data:\n",
    "        print(\"‚ÑπÔ∏è Falling back to averaged results because no loaded data were available.\")\n",
    "\n",
    "    (\n",
    "        signal_metrics_df,\n",
    "        signal_metrics_summary_df,\n",
    "        signal_metrics_summary_pivot,\n",
    "    ) = compute_signal_metrics_from_results(\n",
    "        mouse_signal_frames,\n",
    "        SIGNAL_METRIC_COLUMNS,\n",
    "        analysis_window,\n",
    "    )\n",
    "\n",
    "    window_start = POST_ALIGNMENT_WINDOW_START + 2\n",
    "    window_end = POST_ALIGNMENT_WINDOW_START + 6.0\n",
    "    z560_window_stats = compute_signal_window_mean_per_mouse(\n",
    "        mouse_signal_frames,\n",
    "        \"z_560_Baseline\",\n",
    "        window_start,\n",
    "        window_end,\n",
    "    )\n",
    "\n",
    "    feature_output_dir: Optional[Path] = None\n",
    "    file_prefix: Optional[str] = None\n",
    "\n",
    "    if signal_metrics_df.empty:\n",
    "        print(\"‚ö†Ô∏è No signal metrics were computed. Check signal names and analysis window.\")\n",
    "    else:\n",
    "        n_mice = signal_metrics_df[\"mouse\"].nunique()\n",
    "        n_signals = signal_metrics_df[\"signal\"].nunique()\n",
    "        print(f\"‚úÖ Computed signal metrics for {n_mice} mice across {n_signals} signals.\")\n",
    "\n",
    "        file_prefix = _make_signal_metrics_file_prefix(event_name)\n",
    "\n",
    "        experiment_dirs: set[Path] = set()\n",
    "        for source_path in mouse_source_dirs.values():\n",
    "            if source_path is None:\n",
    "                continue\n",
    "            source_path = Path(source_path)\n",
    "            if source_path.name == \"aligned_data\":\n",
    "                experiment_dirs.add(source_path.parent.parent)\n",
    "            elif source_path.name.endswith(\"_processedData\"):\n",
    "                experiment_dirs.add(source_path.parent)\n",
    "            else:\n",
    "                experiment_dirs.add(source_path)\n",
    "\n",
    "        if experiment_dirs:\n",
    "            if len(experiment_dirs) > 1:\n",
    "                print(\"‚ö†Ô∏è Signal metrics span multiple experiment folders; saving to the first one detected.\")\n",
    "            feature_output_dir = next(iter(experiment_dirs))\n",
    "        else:\n",
    "            feature_output_dir = main_data_dir\n",
    "        experiment_day_label = extract_experiment_day(feature_output_dir)\n",
    "\n",
    "        if not z560_window_stats.empty:\n",
    "            mean_map = z560_window_stats.set_index(\"mouse\")[\"mean\"]\n",
    "            mask = signal_metrics_df[\"signal\"] == \"z_560_Baseline\"\n",
    "            signal_metrics_df.loc[mask, \"mean_fluorescence_2_to_8s\"] = signal_metrics_df.loc[mask, \"mouse\"].map(mean_map)\n",
    "\n",
    "    if not z560_window_stats.empty:\n",
    "        z560_summary = pd.DataFrame(\n",
    "            [\n",
    "                {\n",
    "                    \"signal\": \"z_560_Baseline\",\n",
    "                    \"metric\": \"mean_fluorescence_2_to_8s\",\n",
    "                    \"n\": int(z560_window_stats[\"mean\"].count()),\n",
    "                    \"mean\": float(z560_window_stats[\"mean\"].mean()),\n",
    "                    \"sem\": sem(z560_window_stats[\"mean\"]),\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "        signal_metrics_summary_df = pd.concat([signal_metrics_summary_df, z560_summary], ignore_index=True)\n",
    "        print(\n",
    "            \"z_560_Baseline mean fluorescence (2-8s post): \"\n",
    "            f\"{z560_summary.at[0, 'mean']:.6f} ¬± {z560_summary.at[0, 'sem']:.6f} \"\n",
    "            f\"(SEM, n={int(z560_summary.at[0, 'n'])})\"\n",
    "        )\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No z_560_Baseline data between 0 and 5 seconds post alignment.\")\n",
    "\n",
    "    if not signal_metrics_summary_df.empty:\n",
    "        signal_metrics_summary_df = signal_metrics_summary_df.sort_values([\"signal\", \"metric\"]).reset_index(drop=True)\n",
    "        signal_metrics_summary_pivot = signal_metrics_summary_df.pivot(index=\"metric\", columns=\"signal\", values=[\"mean\", \"sem\"])\n",
    "\n",
    "    if (\n",
    "        SAVE_SIGNAL_METRICS\n",
    "        and feature_output_dir is not None\n",
    "        and file_prefix is not None\n",
    "        and not signal_metrics_df.empty\n",
    "    ):\n",
    "        per_mouse_path = save_signal_metrics_to_csv(\n",
    "            signal_metrics_df,\n",
    "            signal_metrics_summary_df,\n",
    "            feature_output_dir,\n",
    "            file_prefix,\n",
    "            cohort_identifier,\n",
    "            experiment_day_label,\n",
    "        )\n",
    "        print(f\"Saved per-mouse metrics to: {per_mouse_path}\")\n",
    "\n",
    "    if not signal_metrics_summary_pivot.empty:\n",
    "        print(\"\\nSignal metric summary (mean/sem):\")\n",
    "        display(signal_metrics_summary_pivot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccff276d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "aeon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
