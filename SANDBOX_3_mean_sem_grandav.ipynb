{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b677f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------------------------------#\n",
    "from pathlib import Path\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from matplotlib import cm\n",
    "import plotly.io as pio\n",
    "import plotly.express as px\n",
    "import plotly.subplots as sp\n",
    "import math\n",
    "from pprint import pprint\n",
    "import pickle\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy.stats import mode\n",
    "from scipy.integrate import cumulative_trapezoid\n",
    "from scipy.signal import correlate\n",
    "%config Completer.use_jedi = False  # Fixes autocomplete issues\n",
    "%config InlineBackend.figure_format = 'retina'  # Improves plot resolution\n",
    "\n",
    "import gc # garbage collector for removing large variables from memory instantly \n",
    "import importlib #for force updating changed packages \n",
    "from scipy.stats import pearsonr, norm\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed85884",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#FIXME THIS NEEDS TO BECOME ITERATIVE: NOW, I HAVE TO SPECIFY FOR WHICH RESULTS I WANT THE PLOT TO BE COMPUTED IN \n",
    "#CELLS BLOW THE FIRST. HIGHLY CONFUSING. \n",
    "#-------------------------------\n",
    "# data paths setup\n",
    "#-------------------------------\n",
    "data_dirs = [  # Add your data directories here\n",
    "    # Path('~/RANCZLAB-NAS/data/ONIX/20250409_Cohort3_rotation/Vestibular_mismatch_day1').expanduser(),\n",
    "    # Path('/Volumes/RanczLab2/20241125_Cohort1_rotation/Visual_mismatch_day4').expanduser(),\n",
    "    Path('/Volumes/RanczLab2/20241125_Cohort1_rotation/Visual_mismatch_day4').expanduser(),\n",
    "    # Path('/Volumes/RanczLab2/20241125_Cohort1_rotation/Visual_mismatch_day3').expanduser(),\n",
    "    # Path('/Volumes/RanczLab2/20250409_Cohort3_rotation/Visual_mismatch_day4').expanduser(),\n",
    "]\n",
    "# Collect raw data paths (excluding '_processedData' dirs)\n",
    "rawdata_paths = []\n",
    "for data_dir in data_dirs:\n",
    "    subdirs = [p for p in data_dir.iterdir() if p.is_dir() and not p.name.endswith('_processedData')]\n",
    "    rawdata_paths.extend(subdirs)  # Collect all subdirectories\n",
    "\n",
    "# Build processed data paths\n",
    "data_paths = [raw.parent / f\"{raw.name}_processedData/aligned_data\" for raw in rawdata_paths]\n",
    "mouse_name = [raw.name.split('-')[0] for raw in rawdata_paths]\n",
    "selected_mice2 = [\"B6J2780\", \"B6J2781\", \"B6J2783\", \"B6J2782\"] #for cohort 3\n",
    "selected_mice1 = ['B6J2717', 'B6J2718', 'B6J2719', 'B6J2721', 'B6J2722'] #for cohort 1 'B6J2722, 'B6J2723\n",
    "#-------------------------------\n",
    "# load aligned_downsampled data for each data path\n",
    "#-------------------------------\n",
    "loaded_data = {}  # Dictionary to store loaded data for each path\n",
    "for idx, data_path in enumerate(data_paths, start=1):\n",
    "    print(f\"\\nProcessing data path {idx}/{len(data_paths)}: {data_path}\")\n",
    "    csv_file_path = data_path / f\"{mouse_name[idx - 1]}_No halt_baselined_data.csv\"\n",
    "    # csv_file_path = data_path / f\"{mouse_name[idx - 1]}_DrumWithReverseflow block started_baselined_data.csv\"\n",
    "    # Skip if no mouse name is found in the directory\n",
    "    if mouse_name[idx - 1] == 'baselined':\n",
    "        print(f\"‚ö†Ô∏è Skipping directory {data_path} as it does not contain a valid mouse name.\")\n",
    "        continue\n",
    "\n",
    "    aligned_df = pd.read_csv(csv_file_path)\n",
    "    print(f\"‚úÖ Successfully loaded all data files for {data_path.name}\")\n",
    "    loaded_data[data_path] = {\n",
    "        'mouse_name': mouse_name[idx - 1],\n",
    "        'data_path': data_path\n",
    "    }\n",
    "    # Add each column of aligned_df as a separate key in the dictionary\n",
    "    for column in aligned_df.columns:\n",
    "        loaded_data[data_path][column] = aligned_df[column].values\n",
    "    print(f\"Data loaded for {data_path.name}: {len(aligned_df)} rows, {len(aligned_df.columns)} columns\")\n",
    "    # Clean up memory\n",
    "    del aligned_df\n",
    "    gc.collect()  # Run garbage collection to free up memory\n",
    "#-------------------------------\n",
    "#variables\n",
    "save_grand_avg_with_sem = True  # Boolean to control whether to save grand averages with SEMs to a CSV file\n",
    "generate_new_plots = True  # Set to False if you don't want to generate new plots\n",
    "# selected_columns = ['Velocity_0X_Baseline','z_470_Baseline','z_560_Baseline']  # Add your selected columns here ', 'z_560', 'z_470', 'dfF_470', 'dfF_560'\n",
    "# columns_to_plot = ['Velocity_0X_Baseline','z_470_Baseline','z_560_Baseline']  # Add more columns as needed , 'dfF_470', 'dfF_560', 'z_470', 'z_560'\n",
    "selected_columns = ['Velocity_0X','z_470','z_560']  # Add your selected columns here ', 'z_560', 'z_470', 'dfF_470', 'dfF_560'\n",
    "columns_to_plot = ['Velocity_0X','z_470','z_560']  # Add more columns as needed , 'dfF_470', 'dfF_560', 'z_470', 'z_560'\n",
    "\n",
    "# Print data paths in a more readable format\n",
    "print(\"Processed Data Paths:\")\n",
    "pprint(data_paths)\n",
    "print(\"Mouse Name:\")\n",
    "pprint(mouse_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89dc0c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#computes mean and sem per mouse + stores grand averages across all mice\n",
    "\n",
    "def compute_mouse_means_and_grand_average(loaded_data, selected_columns, main_data_dir):\n",
    "    \"\"\"\n",
    "    Compute means per mouse and grand averages across mice for selected columns.\n",
    "    \n",
    "    Parameters:\n",
    "    loaded_data (dict): Dictionary with data paths as keys and mouse data as values\n",
    "    selected_columns (list): List of column names to analyze\n",
    "    main_data_dir (str/Path): Main directory to save results\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (mean_data_per_mouse, sem_data_per_mouse, grand_averages)\n",
    "    \"\"\"\n",
    "    \n",
    "    main_data_dir = Path(main_data_dir)\n",
    "    \n",
    "    print(f\"Processing selected columns: {selected_columns}\")\n",
    "    \n",
    "    # Step 1: Compute mean and SEM for each mouse\n",
    "    mean_data_per_mouse = {}\n",
    "    sem_data_per_mouse = {}\n",
    "    \n",
    "    for data_path, data in loaded_data.items():\n",
    "        mouse_name = data['mouse_name']\n",
    "        print(f\"Processing mouse: {mouse_name}\")\n",
    "        \n",
    "        # Create DataFrame from the loaded data\n",
    "        df = pd.DataFrame(data)\n",
    "        \n",
    "        # Check which selected columns are available\n",
    "        available_columns = [col for col in selected_columns if col in df.columns]\n",
    "        missing_columns = [col for col in selected_columns if col not in df.columns]\n",
    "        \n",
    "        if missing_columns:\n",
    "            print(f\"‚ö†Ô∏è  Missing columns for {mouse_name}: {missing_columns}\")\n",
    "        \n",
    "        if 'Time (s)' not in df.columns:\n",
    "            print(f\"‚ö†Ô∏è  'Time (s)' column not found for {mouse_name}, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        # Group by time and compute mean and SEM\n",
    "        grouped = df.groupby('Time (s)')\n",
    "        \n",
    "        # Only use numeric columns that are in our selected list\n",
    "        numeric_selected = []\n",
    "        for col in available_columns:\n",
    "            if col != 'Time (s)' and pd.api.types.is_numeric_dtype(df[col]):\n",
    "                numeric_selected.append(col)\n",
    "        \n",
    "        if len(numeric_selected) == 0:\n",
    "            print(f\"‚ö†Ô∏è  No numeric columns found for {mouse_name}\")\n",
    "            continue\n",
    "        \n",
    "        mean_data_per_mouse[mouse_name] = grouped[numeric_selected].mean()\n",
    "        sem_data_per_mouse[mouse_name] = grouped[numeric_selected].sem()\n",
    "        \n",
    "        print(f\"‚úÖ Processed {len(numeric_selected)} columns for {mouse_name}\")\n",
    "    \n",
    "    # Step 2: Compute grand averages across mice\n",
    "    print(f\"\\nüìä Computing grand averages across {len(mean_data_per_mouse)} mice...\")\n",
    "    \n",
    "    # Get all unique time points\n",
    "    all_time_points = set()\n",
    "    for mouse_data in mean_data_per_mouse.values():\n",
    "        all_time_points.update(mouse_data.index)\n",
    "    all_time_points = sorted(list(all_time_points))\n",
    "    \n",
    "    # Get all columns that were successfully processed\n",
    "    all_processed_columns = set()\n",
    "    for mouse_data in mean_data_per_mouse.values():\n",
    "        all_processed_columns.update(mouse_data.columns)\n",
    "    all_processed_columns = sorted(list(all_processed_columns))\n",
    "    \n",
    "    print(f\"Time points: {len(all_time_points)} from {min(all_time_points):.2f}s to {max(all_time_points):.2f}s\")\n",
    "    print(f\"Processed columns: {all_processed_columns}\")\n",
    "    \n",
    "    # Create grand average DataFrame\n",
    "    grand_averages = pd.DataFrame(index=all_time_points, columns=all_processed_columns)\n",
    "    grand_averages.index.name = 'Time (s)'\n",
    "    \n",
    "    grand_sems = pd.DataFrame(index=all_time_points, columns=all_processed_columns)\n",
    "    grand_sems.index.name = 'Time (s)'\n",
    "    \n",
    "    # Compute grand averages for each column and time point\n",
    "    for col in all_processed_columns:\n",
    "        for time_point in all_time_points:\n",
    "            # Collect data from all mice for this time point and column\n",
    "            mouse_values = []\n",
    "            for mouse_name, mouse_data in mean_data_per_mouse.items():\n",
    "                if time_point in mouse_data.index and col in mouse_data.columns:\n",
    "                    value = mouse_data.loc[time_point, col]\n",
    "                    if not pd.isna(value):\n",
    "                        mouse_values.append(value)\n",
    "            \n",
    "            if len(mouse_values) > 0:\n",
    "                grand_averages.loc[time_point, col] = np.mean(mouse_values)\n",
    "                if len(mouse_values) > 1:\n",
    "                    grand_sems.loc[time_point, col] = np.std(mouse_values) / np.sqrt(len(mouse_values))\n",
    "                else:\n",
    "                    grand_sems.loc[time_point, col] = 0\n",
    "    \n",
    "    # # Step 3: Save grand averages to CSV\n",
    "    # csv_filename = main_data_dir / f'grand_averages_across_mice_{main_data_dir.name}.csv'\n",
    "    # # Combine mean and SEM into one CSV for convenience\n",
    "    # combined_df = grand_averages.copy()\n",
    "    # for col in all_processed_columns:\n",
    "    #     combined_df[f'{col}_SEM'] = grand_sems[col]\n",
    "    \n",
    "    # combined_df.to_csv(csv_filename)\n",
    "    # print(f\"‚úÖ Grand averages saved to: {csv_filename}\")\n",
    "    \n",
    "    return mean_data_per_mouse, sem_data_per_mouse, grand_averages, grand_sems\n",
    "\n",
    "def analyze_mice_data(loaded_data, selected_columns, main_data_dir):\n",
    "    \"\"\"\n",
    "    Complete analysis workflow: compute means, grand averages, save CSV, and create plots.\n",
    "    \n",
    "    Parameters:\n",
    "    loaded_data (dict): Your loaded_data dictionary\n",
    "    selected_columns (list): List of column names to analyze (including 'Time (s)')\n",
    "    main_data_dir (str/Path): Main directory to save results\n",
    "    \n",
    "    Returns:\n",
    "    dict: Complete results including individual and grand averages\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"MOUSE DATA ANALYSIS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Compute means and grand averages\n",
    "    mean_data_per_mouse, sem_data_per_mouse, grand_averages, grand_sems = compute_mouse_means_and_grand_average(\n",
    "        loaded_data, selected_columns, main_data_dir\n",
    "    )\n",
    "    \n",
    "\n",
    "    # Print summary\n",
    "    print(f\"\\nüìä ANALYSIS COMPLETE:\")\n",
    "    print(f\"   ‚Ä¢ Number of mice analyzed: {len(mean_data_per_mouse)}\")\n",
    "    print(f\"   ‚Ä¢ Mouse names: {list(mean_data_per_mouse.keys())}\")\n",
    "    print(f\"   ‚Ä¢ Columns processed: {list(grand_averages.columns)}\")\n",
    "    print(f\"   ‚Ä¢ Time range: {grand_averages.index.min():.2f}s to {grand_averages.index.max():.2f}s\")\n",
    "    print(f\"   ‚Ä¢ Files saved in: {main_data_dir}\")\n",
    "    \n",
    "    # Return all results\n",
    "    results = {\n",
    "        'mean_data_per_mouse': mean_data_per_mouse,\n",
    "        'sem_data_per_mouse': sem_data_per_mouse,\n",
    "        'grand_averages': grand_averages,\n",
    "        'grand_sems': grand_sems,\n",
    "    }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8c6e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIXME: THIS IS A TEMPORARY FIX TO LOAD THE RESULTS FROM A PICKLE FILE\n",
    "#DO WE NEED THE PICKLE FILE AT ALL? IF SO, SAVE INSTEAD OF GRAND AVERAGE CSV (SAVE THE GRAND AVERAGE IN THE SAME PICLKE FILE)\n",
    "\n",
    "\n",
    "#HERE you define which data dir you want the analysis to be performed on\n",
    "#---------------------------\n",
    "main_data_dir = data_dirs[0]  # Use the first directory from the list\n",
    "#here you run the analysis\n",
    "#----------------------------\n",
    "#to save the results to a pickle file\n",
    "def save_results(results, filename='results.pkl'):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(results, f)\n",
    "    print(f\"Results saved to {filename}\")\n",
    "#----------------------------\n",
    "results_cohort1_vmm4_nohalt = analyze_mice_data(loaded_data, selected_columns, main_data_dir)\n",
    "# Save results to a pickle file\n",
    "save_results(results_cohort1_vmm4_nohalt, 'results_cohort1_vmm4_nohalt.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2856e05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIXME: THIS IS NOT ITERATIVE, I HAVE TO MANUALLY SPECIFY WHICH RESULTS I WANT TO PLOT\n",
    "\n",
    "#PLOTTING MEAN PER MOUSE AND GRAND AVERAGE, STORING CSV WITH GRAND AVERAGES AND SEMs, STORING PLOTS\n",
    "#--------------------------------\n",
    "selected_mice = selected_mice1 #defined above, do not remove from here \n",
    "# Define the column to plot the grand average of\n",
    "#--------------------------------\n",
    "# PLOT properties\n",
    "plt.rcParams.update({\n",
    "    'font.size': 10,           # Set global font size\n",
    "    'font.family': 'sans-serif',  # Font family (e.g., 'serif', 'sans-serif', 'monospace')\n",
    "    'font.sans-serif': ['Arial'],  # Preferred font\n",
    "    'axes.titlesize': 10,      # Title font size\n",
    "    'axes.labelsize': 10,      # Axis label size\n",
    "    'legend.fontsize': 8,     # Legend text\n",
    "    'xtick.labelsize': 10,\n",
    "    'ytick.labelsize': 10\n",
    "})\n",
    "# Generate a color palette\n",
    "color_palette = plt.cm.Set2.colors  # Use the 'tab10' colormap from matplotlib\n",
    "# Define mouse colors using the color palette\n",
    "mouse_colors = {mouse: color_palette[i % len(color_palette)] for i, mouse in enumerate(selected_mice)}\n",
    "\n",
    "# Boolean to control whether to generate new plots\n",
    "if not generate_new_plots:\n",
    "    print(\"Skipping plot generation as per user configuration.\")\n",
    "    # Move on to the next code bit\n",
    "    pass\n",
    "#---------------------------\n",
    "# plot data for each selected mouse and the grand average   \n",
    "for column_to_plot in columns_to_plot:\n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(8, 4))\n",
    "\n",
    "    # Plot mean and SEM for each selected mouse\n",
    "    for mouse in selected_mice:\n",
    "        if mouse in results_cohort1_vmm4_nohalt['mean_data_per_mouse']:\n",
    "            mean_data = results_cohort1_vmm4_nohalt['mean_data_per_mouse'][mouse][column_to_plot]\n",
    "            sem_data = results_cohort1_vmm4_nohalt['sem_data_per_mouse'][mouse][column_to_plot]\n",
    "            \n",
    "            # Ensure data is numeric and handle any conversion issues\n",
    "            mean_data = pd.to_numeric(mean_data, errors='coerce')\n",
    "            sem_data = pd.to_numeric(sem_data, errors='coerce')\n",
    "            time_points = pd.to_numeric(mean_data.index, errors='coerce')\n",
    "            \n",
    "            # Drop any NaN values that might have been created during conversion\n",
    "            valid_mask = ~(pd.isna(mean_data) | pd.isna(sem_data) | pd.isna(time_points))\n",
    "            mean_data_clean = mean_data[valid_mask]\n",
    "            sem_data_clean = sem_data[valid_mask]\n",
    "            time_points_clean = time_points[valid_mask]\n",
    "\n",
    "            # Plot mean with SEM as shaded area using the defined color palette\n",
    "            plt.plot(time_points_clean, mean_data_clean, label=f'{mouse} Mean', color=mouse_colors[mouse])\n",
    "            plt.fill_between(time_points_clean, mean_data_clean - sem_data_clean, \n",
    "                             mean_data_clean + sem_data_clean, color=mouse_colors[mouse], alpha=0.2)\n",
    "\n",
    "    # Plot the grand average\n",
    "    grand_mean = results_cohort1_vmm4_nohalt['grand_averages'][column_to_plot]\n",
    "    grand_sem = results_cohort1_vmm4_nohalt['grand_sems'][column_to_plot]\n",
    "    \n",
    "    # Ensure grand average data is numeric\n",
    "    grand_mean = pd.to_numeric(grand_mean, errors='coerce')\n",
    "    grand_sem = pd.to_numeric(grand_sem, errors='coerce')\n",
    "    time_points = pd.to_numeric(grand_mean.index, errors='coerce')\n",
    "    \n",
    "    # Drop any NaN values\n",
    "    valid_mask = ~(pd.isna(grand_mean) | pd.isna(grand_sem) | pd.isna(time_points))\n",
    "    grand_mean_clean = grand_mean[valid_mask]\n",
    "    grand_sem_clean = grand_sem[valid_mask]\n",
    "    time_points_clean = time_points[valid_mask]\n",
    "\n",
    "    plt.plot(time_points_clean, grand_mean_clean, label='Grand Average', color='black', linewidth=2)\n",
    "    plt.fill_between(time_points_clean, grand_mean_clean - grand_sem_clean, \n",
    "                    grand_mean_clean + grand_sem_clean, color='gray', alpha=0.3)\n",
    "    \n",
    "    # Add labels, legend, and title\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel(column_to_plot)\n",
    "    plt.title(f'Mean and SEM of {column_to_plot} Over Time')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the plot to the \"baselined\" subdirectory\n",
    "    baselined_dir = main_data_dir / \"baselined\"\n",
    "    baselined_dir.mkdir(exist_ok=True)  # Create the directory if it doesn't exist\n",
    "    plot_filename = baselined_dir / f\"{column_to_plot}_plot_nohalt.pdf\"\n",
    "    plt.savefig(plot_filename, dpi=150, bbox_inches='tight')\n",
    "    print(f\"Plot saved to: {plot_filename}\")\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "#create a SCATTER PLOT\n",
    "    pre_time = (-2, 0)\n",
    "    post_time = (0, 2)\n",
    "\n",
    "    pre_values = []\n",
    "    post_values = []\n",
    "    mouse_labels = []\n",
    "\n",
    "    # Collect values\n",
    "    for mouse in selected_mice:\n",
    "        if mouse in results_cohort1_vmm4_nohalt['mean_data_per_mouse']:\n",
    "            mean_data = results_cohort1_vmm4_nohalt['mean_data_per_mouse'][mouse][column_to_plot]\n",
    "            \n",
    "            pre_mean = mean_data.loc[(mean_data.index >= pre_time[0]) & (mean_data.index < pre_time[1])].mean()\n",
    "            post_mean = mean_data.loc[(mean_data.index >= post_time[0]) & (mean_data.index <= post_time[1])].mean()\n",
    "            \n",
    "            pre_values.append(pre_mean)\n",
    "            post_values.append(post_mean)\n",
    "            mouse_labels.append(mouse)\n",
    "\n",
    "    # Grand average\n",
    "    grand_mean = results_cohort1_vmm4_nohalt['grand_averages'][column_to_plot]\n",
    "    pre_grand_mean = grand_mean.loc[(grand_mean.index >= pre_time[0]) & (grand_mean.index < pre_time[1])].mean()\n",
    "    post_grand_mean = grand_mean.loc[(grand_mean.index >= post_time[0]) & (grand_mean.index <= post_time[1])].mean()\n",
    "\n",
    "    grand_sem = results_cohort1_vmm4_nohalt['grand_sems'][column_to_plot]\n",
    "    pre_grand_sem = grand_sem.loc[(grand_sem.index >= pre_time[0]) & (grand_sem.index < pre_time[1])].mean()\n",
    "    post_grand_sem = grand_sem.loc[(grand_sem.index >= post_time[0]) & (grand_sem.index <= post_time[1])].mean()\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(3, 4)) \n",
    "\n",
    "    # Plot each mouse with connecting line\n",
    "    for i, mouse in enumerate(mouse_labels):\n",
    "        plt.plot([1, 2], [pre_values[i], post_values[i]], color=mouse_colors[mouse], marker='o', linewidth=1, label=mouse)\n",
    "\n",
    "    # Grand average as large black dots\n",
    "    plt.plot([1, 2], [pre_grand_mean, post_grand_mean], color='black', marker='o', markersize=8, linewidth=1, label='Grand Avg')\n",
    "\n",
    "    # Add error bars for grand average SEM\n",
    "    plt.errorbar([1, 2], [pre_grand_mean, post_grand_mean], yerr=[pre_grand_sem, post_grand_sem], fmt='o', color='black', capsize=5)\n",
    "\n",
    "    # Formatting\n",
    "    plt.xticks([1, 2], [pre_time, post_time])\n",
    "    plt.title(f'Mean {column_to_plot} Before and After Time 0')\n",
    "    plt.ylabel(column_to_plot)\n",
    "    plt.xlim(0.8, 2.2)  # tighter x-axis\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Legend: one color per mouse\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    unique_labels = {}\n",
    "    for h, l in zip(handles, labels):\n",
    "        if l not in unique_labels:\n",
    "            unique_labels[l] = h\n",
    "    plt.legend(unique_labels.values(), unique_labels.keys(), loc='best', fontsize='small')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the plot to the \"baselined\" subdirectory\n",
    "    baselined_dir = main_data_dir / \"baselined\"\n",
    "    baselined_dir.mkdir(exist_ok=True)  # Create the directory if it doesn't exist\n",
    "    plot_filename = baselined_dir / f\"{column_to_plot}_scatterplot_nohalt.pdf\"\n",
    "    plt.savefig(plot_filename, dpi=150, bbox_inches='tight')\n",
    "    print(f\"Plot saved to: {plot_filename}\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ab42b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIXME: IDEALLY WE SHOULD ONLY USE 1 FILE TO STORE RESULTS AND GRAND AVERAGES, NOW SPREAD ACROSS PICLKE AND CSV FILES\n",
    "\n",
    "#save csv with grand averages and SEMs\n",
    "\n",
    "if save_grand_avg_with_sem:\n",
    "    # Create a DataFrame combining grand averages and SEMs\n",
    "    grand_avg_with_sem = results_cohort3_vmm3['grand_averages'].copy()\n",
    "    for col in results_cohort3_vmm3['grand_sems'].columns:\n",
    "        grand_avg_with_sem[f'{col}_SEM'] = results_cohort3_vmm3['grand_sems'][col]\n",
    "\n",
    "    # Generate a filename that includes the selected mice\n",
    "    mice_str = \"_\".join(selected_mice)\n",
    "    csv_filename = main_data_dir / f'grand_averages_with_sem_{mice_str}.csv'\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    grand_avg_with_sem.to_csv(csv_filename)\n",
    "    print(f\"Grand averages with SEM saved to: {csv_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c84214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the load_results function is defined like this:\n",
    "def load_results(filename):\n",
    "    import pickle\n",
    "    with open(filename, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "# Load both result files\n",
    "results_cohort1 = load_results('/Users/nora/Documents/GitHub/vestibular_vr_pipeline/results_cohort1_vmm4_nohalt.pkl')\n",
    "results_cohort2 = load_results('/Users/nora/Documents/GitHub/vestibular_vr_pipeline/results_cohort3_vmm4_nohalt.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8446ce8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIXME: IN EXTRACT MEANS, I HAVE TO MANUALLY SPECIFY THE COLUMNS TO EXTRACT, THIS IS NOT IDEAL\n",
    "\n",
    "#compute correlations between Velocity_0X and z_470, z_560 for each cohort\n",
    "from scipy.stats import pearsonr, norm\n",
    "\n",
    "def fisher_z(r):\n",
    "    return 0.5 * np.log((1 + r) / (1 - r))\n",
    "\n",
    "def compare_correlations(r1, n1, r2, n2):\n",
    "    z1 = fisher_z(r1)\n",
    "    z2 = fisher_z(r2)\n",
    "    se = np.sqrt(1 / (n1 - 3) + 1 / (n2 - 3))\n",
    "    z = (z1 - z2) / se\n",
    "    p = 2 * (1 - norm.cdf(abs(z)))\n",
    "    return z, p\n",
    "\n",
    "# def extract_means(results, mice, time_window, columns=('Velocity_0X_Baseline', 'z_470_Baseline', 'z_560_Baseline')):\n",
    "def extract_means(results, mice, time_window, columns=('Velocity_0X', 'z_470', 'z_560')):\n",
    "\n",
    "    v_means, z470_means, z560_means, valid_mice = [], [], [], []\n",
    "    t0, t1 = time_window\n",
    "\n",
    "    for mouse in mice:\n",
    "        if mouse not in results['mean_data_per_mouse']:\n",
    "            continue\n",
    "        df = results['mean_data_per_mouse'][mouse]\n",
    "        if not all(col in df.columns for col in columns):\n",
    "            continue\n",
    "\n",
    "        df_window = df.loc[(df.index >= t0) & (df.index <= t1)]\n",
    "        v = df_window[columns[0]].mean()\n",
    "        z470 = df_window[columns[1]].mean()\n",
    "        z560 = df_window[columns[2]].mean()\n",
    "\n",
    "        if not any(pd.isnull([v, z470, z560])):\n",
    "            v_means.append(v)\n",
    "            z470_means.append(z470)\n",
    "            z560_means.append(z560)\n",
    "            valid_mice.append(mouse)\n",
    "\n",
    "    return v_means, z470_means, z560_means, valid_mice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b22e918",
   "metadata": {},
   "outputs": [],
   "source": [
    "#analyse correlations between Velocity_0X and z_470, z_560 for each cohort\n",
    "def analyze_correlations(\n",
    "    cohort1, mice1,\n",
    "    cohort2, mice2,\n",
    "    time_window=(0, 2),\n",
    "    plot=True\n",
    "):\n",
    "    # Extract means\n",
    "    v1, z470_1, z560_1, ids1 = extract_means(cohort1, mice1, time_window)\n",
    "    v2, z470_2, z560_2, ids2 = extract_means(cohort2, mice2, time_window)\n",
    "\n",
    "    # Compute correlations\n",
    "    corr1_470, p1_470 = pearsonr(v1, z470_1)\n",
    "    corr1_560, p1_560 = pearsonr(v1, z560_1)\n",
    "    corr2_470, p2_470 = pearsonr(v2, z470_2)\n",
    "    corr2_560, p2_560 = pearsonr(v2, z560_2)\n",
    "\n",
    "    print(\"\\nüìä Correlations:\")\n",
    "    print(f\"Cohort 1: Velocity ~ z_470: r = {corr1_470:.3f}, p = {p1_470:.3f}\")\n",
    "    print(f\"Cohort 1: Velocity ~ z_560: r = {corr1_560:.3f}, p = {p1_560:.3f}\")\n",
    "    print(f\"Cohort 2: Velocity ~ z_470: r = {corr2_470:.3f}, p = {p2_470:.3f}\")\n",
    "    print(f\"Cohort 2: Velocity ~ z_560: r = {corr2_560:.3f}, p = {p2_560:.3f}\")\n",
    "\n",
    "    # Compare correlation coefficients\n",
    "    if len(v1) > 3 and len(v2) > 3:\n",
    "        z_470, p_470 = compare_correlations(corr1_470, len(v1), corr2_470, len(v2))\n",
    "        z_560, p_560 = compare_correlations(corr1_560, len(v1), corr2_560, len(v2))\n",
    "\n",
    "        print(\"\\nüîç Comparison of correlations:\")\n",
    "        print(f\"z_470: z = {z_470:.3f}, p = {p_470:.3f}\")\n",
    "        print(f\"z_560: z = {z_560:.3f}, p = {p_560:.3f}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Not enough data (need >3 samples per group) to compare correlation coefficients.\")\n",
    "\n",
    "    # Optional: Plot\n",
    "    if plot:\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(8, 4))\n",
    "        \n",
    "        # Plot Velocity vs z_470\n",
    "        axs[0].scatter(v1, z470_1, color='green', label='Cohort 1')\n",
    "        axs[0].scatter(v2, z470_2, color='orange', label='Cohort 2')\n",
    "        axs[0].set_title('Velocity vs z_470')\n",
    "        axs[0].legend()\n",
    "        \n",
    "        # Add mouse names next to the dots for z_470\n",
    "        for i, mouse in enumerate(ids1):\n",
    "            axs[0].text(v1[i], z470_1[i], mouse, fontsize=8, color='green', alpha=0.7)\n",
    "        for i, mouse in enumerate(ids2):\n",
    "            axs[0].text(v2[i], z470_2[i], mouse, fontsize=8, color='orange', alpha=0.7)\n",
    "\n",
    "        # Add regression lines for z_470\n",
    "        if len(v1) > 1:\n",
    "            m1, b1 = np.polyfit(v1, z470_1, 1)\n",
    "            axs[0].plot(v1, m1 * np.array(v1) + b1, color='green', linestyle='--', label='Cohort 1 Fit')\n",
    "        if len(v2) > 1:\n",
    "            m2, b2 = np.polyfit(v2, z470_2, 1)\n",
    "            axs[0].plot(v2, m2 * np.array(v2) + b2, color='orange', linestyle='--', label='Cohort 2 Fit')\n",
    "\n",
    "        # Plot Velocity vs z_560\n",
    "        axs[1].scatter(v1, z560_1, color='darkred', label='Cohort 1')\n",
    "        axs[1].scatter(v2, z560_2, color='red', label='Cohort 2')\n",
    "        axs[1].set_title('Velocity vs z_560')\n",
    "        axs[1].legend()\n",
    "        \n",
    "        # Add mouse names next to the dots for z_560 with some offset\n",
    "        for i, mouse in enumerate(ids1):\n",
    "            axs[1].text(v1[i], z560_1[i], mouse, fontsize=8, color='darkred', alpha=0.7)\n",
    "        for i, mouse in enumerate(ids2):\n",
    "            axs[1].text(v2[i], z560_2[i], mouse, fontsize=8, color='red', alpha=0.7)\n",
    "\n",
    "        # Add regression lines for z_560\n",
    "        if len(v1) > 1:\n",
    "            m1, b1 = np.polyfit(v1, z560_1, 1)\n",
    "            axs[1].plot(v1, m1 * np.array(v1) + b1, color='darkred', linestyle='--', label='Cohort 1 Fit')\n",
    "        if len(v2) > 1:\n",
    "            m2, b2 = np.polyfit(v2, z560_2, 1)\n",
    "            axs[1].plot(v2, m2 * np.array(v2) + b2, color='red', linestyle='--', label='Cohort 2 Fit')\n",
    "\n",
    "        for ax in axs:\n",
    "            ax.set_xlabel('Mean Velocity_0X (m/s)')\n",
    "            ax.set_ylabel('Mean z-score')\n",
    "            ax.grid(True)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1fd044",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_correlations(\n",
    "    cohort1=results_cohort1,\n",
    "    mice1=selected_mice1,\n",
    "    cohort2=results_cohort2,\n",
    "    mice2=selected_mice2,\n",
    "    time_window=(0, 2),\n",
    "    plot=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fe0919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to your CSV files for the session 'Visual_mismatch_day3'\n",
    "# Replace these with the actual paths to your files\n",
    "vmm3_cohort1 = '/Volumes/RanczLab2/20241125_Cohort1_rotation/Visual_mismatch_day3/grand_averages_with_sem_B6J2717_B6J2718_B6J2719_B6J2720_B6J2721_B6J2722.csv'\n",
    "vmm3_cohort3 = '/Volumes/RanczLab2/20250409_Cohort3_rotation/Visual_mismatch_day3/grand_averages_with_sem_B6J2780_B6J2781_B6J2783_B6J2782.csv'\n",
    "# vmm4_cohort1 = '/Volumes/RanczLab2/20241125_Cohort1_rotation/Visual_mismatch_day4/grand_averages_with_sem_B6J2717_B6J2718_B6J2719_B6J2720_B6J2721_B6J2722.csv'\n",
    "# vmm4_cohort3 = '/Volumes/RanczLab2/20250409_Cohort3_rotation/Visual_mismatch_day4/grand_averages_with_sem_B6J2780_B6J2781_B6J2783.csv'\n",
    "# ol1_cohort3 = '/Volumes/RanczLab2/20250409_Cohort3_rotation/Open_loop_day1/grand_averages_with_sem_B6J2780_B6J2781_B6J2783.csv'\n",
    "\n",
    "# Load the data\n",
    "df_cohort1 = pd.read_csv(vmm3_cohort1)\n",
    "df_cohort3 = pd.read_csv(vmm3_cohort3)\n",
    "\n",
    "# Plot the grand average and SEM for z_470_Baseline\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.minorticks_on()  # Enable minor ticks\n",
    "plt.grid(which='both', linestyle='--', linewidth=0.5, alpha=0.7)  # Add grid for both major and minor ticks\n",
    "\n",
    "# # # Plot Cohort1\n",
    "# plt.plot(df_cohort1['Time (s)'], df_cohort1['z_470_Baseline'], label='GRAB5HT3.0', color='green')\n",
    "# plt.fill_between(df_cohort1['Time (s)'],\n",
    "#                  df_cohort1['z_470_Baseline'] - df_cohort1['z_470_Baseline_SEM'],\n",
    "#                  df_cohort1['z_470_Baseline'] + df_cohort1['z_470_Baseline_SEM'],\n",
    "#                  color='green', alpha=0.2)\n",
    "\n",
    "# # Plot Cohort3\n",
    "# plt.plot(df_cohort3['Time (s)'], df_cohort3['z_470_Baseline'], label='mut-GRAB5HT3.0', color='orange')\n",
    "# plt.fill_between(df_cohort3['Time (s)'],\n",
    "#                  df_cohort3['z_470_Baseline'] - df_cohort3['z_470_Baseline_SEM'],\n",
    "#                  df_cohort3['z_470_Baseline'] + df_cohort3['z_470_Baseline_SEM'],\n",
    "#                  color='orange', alpha=0.2)\n",
    "# # Plot Cohort1\n",
    "plt.plot(df_cohort1['Time (s)'], df_cohort1['dfF_470_Baseline'], label='GRAB5HT3.0', color='green')\n",
    "plt.fill_between(df_cohort1['Time (s)'],\n",
    "                 df_cohort1['dfF_470_Baseline'] - df_cohort1['dfF_470_Baseline_SEM'],\n",
    "                 df_cohort1['dfF_470_Baseline'] + df_cohort1['dfF_470_Baseline_SEM'],\n",
    "                 color='green', alpha=0.2)\n",
    "\n",
    "# Plot Cohort3\n",
    "plt.plot(df_cohort3['Time (s)'], df_cohort3['dfF_470_Baseline'], label='mut-GRAB5HT3.0', color='orange')\n",
    "plt.fill_between(df_cohort3['Time (s)'],\n",
    "                 df_cohort3['dfF_470_Baseline'] - df_cohort3['dfF_470_Baseline_SEM'],\n",
    "                 df_cohort3['dfF_470_Baseline'] + df_cohort3['dfF_470_Baseline_SEM'],\n",
    "                 color='orange', alpha=0.2)\n",
    "\n",
    "# Add a gray shadowed area between seconds 0 and 2\n",
    "plt.axvspan(0, 2, color='gray', alpha=0.2, label='visual_mismatch')\n",
    "\n",
    "# Plot customization\n",
    "# plt.title('OLday1', fontname='Arial', fontsize=10)\n",
    "plt.xlabel('Time (s)', fontname='Arial', fontsize=10)\n",
    "plt.ylabel('dfF', fontname='Arial', fontsize=10)\n",
    "plt.legend(prop={'family': 'Arial', 'size': 10})\n",
    "plt.grid(True)\n",
    "\n",
    "# Save the plot to the GitHub directory\n",
    "github_dir = Path.home() / \"GitHub\" / \"plots\"\n",
    "github_dir.mkdir(parents=True, exist_ok=True)\n",
    "plot_filename_github = github_dir / \"mmday3dff470.pdf\"\n",
    "plt.savefig(plot_filename_github, format='pdf', dpi=300, bbox_inches='tight')\n",
    "print(f\"Plot saved to: {plot_filename_github}\")\n",
    "plt.tight_layout()\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff79219",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aeon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
