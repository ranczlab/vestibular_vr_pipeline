{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbf27cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "# this notebook SAVES halt aligned data and baselined data as CSV together with PLOTS, different compared to the previous SANDBOX_2_noSLEAP#\n",
    "#---------------------------------------------------------------------------------------------------#\n",
    "from pathlib import Path\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.collections import LineCollection\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "import plotly.express as px\n",
    "import plotly.subplots as sp\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "import math\n",
    "from pprint import pprint\n",
    "\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.signal import correlate\n",
    "from scipy.stats import mode\n",
    "from scipy.integrate import cumulative_trapezoid\n",
    "from scipy.signal import correlate\n",
    "import seaborn as sns\n",
    "\n",
    "from typing import Tuple, Dict, Any, List, Optional\n",
    "import warnings\n",
    "\n",
    "import traceback\n",
    "import gc\n",
    "import json\n",
    "import re\n",
    "\n",
    "%config Completer.use_jedi = False  # Fixes autocomplete issues\n",
    "%config InlineBackend.figure_format = 'retina'  # Improves plot resolution\n",
    "\n",
    "import gc # garbage collector for removing large variables from memory instantly \n",
    "import importlib #for force updating changed packages \n",
    "\n",
    "#import harp\n",
    "import harp_resources.process\n",
    "import harp_resources.utils\n",
    "from harp_resources import process, utils # Reassign to maintain direct references for force updating \n",
    "#from sleap import load_and_process as lp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2541726",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "352d54fe",
   "metadata": {},
   "source": [
    "## Notebook Overview\n",
    "This sandbox notebook loads downsampled ONIX behavioral datasets, slices them by experiment intervals, and runs the full `BehavioralAnalyzer` workflow. The process:\n",
    "\n",
    "- Configure input directories, event labels, baseline windows, and plotting flags.\n",
    "- Load parquet files for photometry, tracking, encoder, and event metadata for each session.\n",
    "- Instantiate `BehavioralAnalyzer` objects to compute running, turning, and platform metrics, saving per-animal figures/CSVs plus a cohort summary.\n",
    "\n",
    "### Key Parameters\n",
    "- `data_dirs`, `cohort_data_dir`: select which sessions are processed.\n",
    "- `event_name`, `vestibular_mismatch`: govern halt detection and slicing logic.\n",
    "- `time_window_start/end`, `baseline_window`, `common_resampled_rate`, `plot_fig1`: plotting and preprocessing controls.\n",
    "- `run_behavioral_analysis` arguments expose `encoder_column`, `min_bout_duration_s`, `running_percentile`, `turning_percentile`, and `turning_velocity_column` before launching the batch run.\n",
    "\n",
    "### Behavioral Analyzer Options\n",
    "When constructing `BehavioralAnalyzer`, you can adjust:\n",
    "- `threshold_plot_bins`: histogram resolution for threshold estimation.\n",
    "- `min_bout_duration_s`: minimum bout length for running/turning detection.\n",
    "- `running_percentile`: percentile-based threshold for running bouts.\n",
    "- `turning_percentile`: optional percentile for turning (falls back to median when `None`).\n",
    "- `turning_velocity_column`: choose the turning signal (`Motor_Velocity`, `Velocity_0Y`, etc.).\n",
    "\n",
    "Each call to `analyze_turning` also lets you override the column via the `column` argument, enabling quick comparisons between motor encoder- and flow-based turning metrics. BUT! THE TOTAL DISTANCE is ONLY computed from Position_0Y, never from encoder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf593d7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# data paths setup\n",
    "#-------------------------------\n",
    "data_dirs = [  # Add your data directories here\n",
    "    # Path('~/RANCZLAB-NAS/data/ONIX/20250409_Cohort3_rotation/Vestibular_mismatch_day1').expanduser(),\n",
    "    # Path('/home/ikharitonov/RANCZLAB-NAS/data/ONIX/20250923_Cohort6_rotation/EXP_1_fluoxetine_1').expanduser()\n",
    "    # Path('/home/ikharitonov/RANCZLAB-NAS/data/ONIX/20250409_Cohort3_rotation/Visual_mismatch_day4').expanduser()\n",
    "    Path('/Volumes/RanczLab/20241125_Cohort1_rotation/Visual_mismatch_day3').expanduser()\n",
    "]\n",
    "# FIXME can we do this straight from the data_dirs path (i.e. walk one directory back?)\n",
    "# cohort_data_dir = Path('/Volumes/RanczLab2/Cohort1_rotation/').expanduser() # for Nora\n",
    "# cohort_data_dir = Path('/home/ikharitonov/RANCZLAB-NAS/data/ONIX/20250409_Cohort3_rotation').expanduser() \n",
    "cohort_data_dir = Path('/Volumes/RanczLab/20241125_Cohort1_rotation').expanduser() \n",
    "\n",
    "# Collect raw data paths (excluding '_processedData' dirs)\n",
    "# Only include directories that match the expected pattern (have a timestamp: YYYY-MM-DDTHH-MM-SS)\n",
    "rawdata_paths = []\n",
    "timestamp_pattern = re.compile(r'\\d{4}-\\d{2}-\\d{2}T\\d{2}-\\d{2}-\\d{2}')  # Matches YYYY-MM-DDTHH-MM-SS\n",
    "for data_dir in data_dirs:\n",
    "    subdirs = [\n",
    "        p for p in data_dir.iterdir() \n",
    "        if p.is_dir() \n",
    "        and not p.name.endswith('_processedData')\n",
    "        and timestamp_pattern.search(p.name)  # Only include directories with timestamp pattern\n",
    "    ]\n",
    "    rawdata_paths.extend(subdirs)  # Collect all subdirectories\n",
    "pprint(rawdata_paths)\n",
    "\n",
    "# Build processed data paths - only include paths that actually exist\n",
    "data_paths = []\n",
    "for raw in rawdata_paths:\n",
    "    processed_path = raw.parent / f\"{raw.name}_processedData/downsampled_data\"\n",
    "    if processed_path.exists():\n",
    "        data_paths.append(processed_path)\n",
    "    else:\n",
    "        print(f\"⚠️ Skipping {raw.name}: processed data path does not exist: {processed_path}\")\n",
    "\n",
    "# Print data paths in a more readable format\n",
    "print(\"Processed Data Paths (existing only):\")\n",
    "pprint(data_paths)\n",
    "\n",
    "#-------------------------------\n",
    "# initial variables setup\n",
    "#-------------------------------\n",
    "time_window_start = -6  # s, FOR PLOTTING PURPOSES\n",
    "time_window_end = 10  # s, FOR PLOTTING PURPOSES\n",
    "baseline_window = (-6, -4)  # s, FOR baselining averages\n",
    "plot_width = 14\n",
    "saccade_bin_size_s = 0.5  # s, bin size for saccade density analysis (e.g., 0.1 = 100ms, 0.5 = 500ms)\n",
    "\n",
    "event_name = \"Apply halt: 2s\"  # Apply halt: 2s, No halt, DrumWithReverseflow block started, DrumBase block started\n",
    "vestibular_mismatch = False\n",
    "common_resampled_rate = 1000  # in Hz\n",
    "plot_fig1 = False # save_path not defined, commented out FIg 1 creation\n",
    "\n",
    "# # for saccades\n",
    "# framerate = 59.77  # Hz (in the future, should come from saved data)\n",
    "# threshold = 65  # px/s FIXME make this adaptive\n",
    "# refractory_period = pd.Timedelta(milliseconds=100)  # msec, using pd.Timedelta for datetime index\n",
    "# plot_saccade_detection_QC = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882844bc",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# load downsampled data for each data path\n",
    "#-------------------------------\n",
    "loaded_data = {}  # Dictionary to store loaded data for each path\n",
    "\n",
    "for idx, data_path in enumerate(data_paths, start=1):\n",
    "    print(f\"\\nProcessing data path {idx}/{len(data_paths)}: {data_path}\")\n",
    "    try:\n",
    "        # Load all parquet files for this data path\n",
    "        photometry_tracking_encoder_data = pd.read_parquet(data_path / \"photometry_tracking_encoder_data.parquet\", engine=\"pyarrow\")\n",
    "        camera_photodiode_data = pd.read_parquet(data_path / \"camera_photodiode_data.parquet\", engine=\"pyarrow\")\n",
    "        experiment_events = pd.read_parquet(data_path / \"experiment_events.parquet\", engine=\"pyarrow\")\n",
    "        photometry_info = pd.read_parquet(data_path / \"photometry_info.parquet\", engine=\"pyarrow\")\n",
    "        session_settings = pd.read_parquet(data_path / \"session_settings.parquet\", engine=\"pyarrow\")\n",
    "        session_settings[\"metadata\"] = session_settings[\"metadata\"].apply(process.safe_from_json)\n",
    "        video_dataframes: Dict[str, pd.DataFrame] = {}\n",
    "        video_join_frames: List[pd.DataFrame] = []\n",
    "        video_saccade_summaries: Dict[str, pd.DataFrame] = {}\n",
    "\n",
    "        for video_key, eye_suffix in ((\"VideoData1\", \"eye1\"), (\"VideoData2\", \"eye2\")):\n",
    "            video_path = data_path / f\"{video_key}_resampled.parquet\"\n",
    "            if not video_path.exists():\n",
    "                print(f\"ℹ️ No {video_key} resampled parquet found at {video_path}\")\n",
    "                continue\n",
    "\n",
    "            video_df = pd.read_parquet(video_path, engine=\"pyarrow\")\n",
    "            if \"Seconds\" in video_df.columns:\n",
    "                video_df = video_df.drop(columns=[\"Seconds\"])\n",
    "\n",
    "            rename_map = {\n",
    "                col: f\"{col}_{eye_suffix}\"\n",
    "                for col in video_df.columns\n",
    "                if not col.endswith(f\"_{eye_suffix}\")\n",
    "            }\n",
    "            if rename_map:\n",
    "                video_df = video_df.rename(columns=rename_map)\n",
    "\n",
    "            # ------------------------------\n",
    "            # Alignment diagnostics & guards\n",
    "            # ------------------------------\n",
    "            if not isinstance(video_df.index, pd.DatetimeIndex):\n",
    "                raise TypeError(f\"{video_key} index must be a DatetimeIndex. Found {type(video_df.index)}.\")\n",
    "\n",
    "            photo_index = photometry_tracking_encoder_data.index\n",
    "            if not isinstance(photo_index, pd.DatetimeIndex):\n",
    "                raise TypeError(\"photometry_tracking_encoder_data index must be a DatetimeIndex.\")\n",
    "\n",
    "            video_index = video_df.index.sort_values()\n",
    "            photo_index_sorted = photo_index.sort_values()\n",
    "\n",
    "            video_start, video_end = video_index[0], video_index[-1]\n",
    "            photo_start, photo_end = photo_index_sorted[0], photo_index_sorted[-1]\n",
    "\n",
    "            # Check and report time window differences\n",
    "            start_diff = (video_start - photo_start).total_seconds()\n",
    "            end_diff = (video_end - photo_end).total_seconds()\n",
    "            if start_diff != 0 or end_diff != 0:\n",
    "                start_msg = f\"starts {abs(start_diff):.3f}s {'before' if start_diff < 0 else 'after'}\"\n",
    "                end_msg = f\"ends {abs(end_diff):.3f}s {'before' if end_diff < 0 else 'after'}\"\n",
    "                print(f\"ℹ️ {video_key}: {start_msg}, {end_msg} photometry window - not a problem, it is trimmed and NaN-padded\")\n",
    "\n",
    "            # Check for timestamp misalignment and reindex if needed\n",
    "            in_range = video_index[(video_index >= photo_start) & (video_index <= photo_end)]\n",
    "            matched = in_range.intersection(photo_index_sorted)\n",
    "            unmatched = in_range.difference(photo_index_sorted)\n",
    "            \n",
    "            if len(unmatched) > 0:\n",
    "                # Calculate average time difference for reporting\n",
    "                unmatched_series = pd.Series(unmatched)\n",
    "                nearest_indices = photo_index_sorted.get_indexer(unmatched_series, method='nearest')\n",
    "                nearest_photo_times = photo_index_sorted[nearest_indices]\n",
    "                \n",
    "                # Calculate time differences (video - photo) in seconds\n",
    "                time_diffs_series = pd.Series(unmatched_series.values) - pd.Series(nearest_photo_times.values)\n",
    "                time_diffs = time_diffs_series.dt.total_seconds().values\n",
    "                mean_diff_ms = np.mean(time_diffs) * 1000  # Convert to milliseconds\n",
    "                \n",
    "                # Reindex video dataframe to photometry index using nearest neighbor matching\n",
    "                # Use get_indexer to find nearest video indices for each photometry timestamp\n",
    "                video_indices_for_photo = video_df.index.get_indexer(photo_index_sorted, method='nearest')\n",
    "                \n",
    "                # Create aligned dataframe by mapping video data to photometry timestamps\n",
    "                video_df_aligned = pd.DataFrame(\n",
    "                    index=photo_index_sorted,\n",
    "                    columns=video_df.columns\n",
    "                )\n",
    "                for col in video_df.columns:\n",
    "                    video_df_aligned[col] = video_df[col].iloc[video_indices_for_photo].values\n",
    "                \n",
    "                # Check for duplicate mappings (multiple photometry timestamps mapping to same video timestamp)\n",
    "                unique_mappings = len(np.unique(video_indices_for_photo))\n",
    "                if unique_mappings < len(photo_index_sorted):\n",
    "                    # This shouldn't happen with small offsets, but handle it if it does\n",
    "                    video_df_aligned = video_df_aligned.groupby(video_df_aligned.index).mean()\n",
    "                \n",
    "                # Report the alignment\n",
    "                print(f\"⚠️ {video_key}: Timestamp misalignment corrected (avg difference: {mean_diff_ms:.3f} ms)\")\n",
    "                \n",
    "                # Replace original video_df with aligned version\n",
    "                video_df = video_df_aligned\n",
    "            else:\n",
    "                print(f\"✅ {video_key}: Timestamps perfectly aligned with photometry data.\")\n",
    "\n",
    "            video_dataframes[video_key] = video_df\n",
    "            video_join_frames.append(video_df)\n",
    "            print(f\"✅ Loaded {video_key} resampled data from {video_path.name}\")\n",
    "\n",
    "        for video_df in video_join_frames:\n",
    "            photometry_tracking_encoder_data = photometry_tracking_encoder_data.join(video_df, how=\"left\")\n",
    "        \n",
    "        # DIAGNOSTIC: Log eye tracking columns after join\n",
    "        eye_tracking_cols = [col for col in photometry_tracking_encoder_data.columns \n",
    "                            if 'Pupil.' in col or 'Ellipse.Center' in col]\n",
    "        if eye_tracking_cols:\n",
    "            print(f\"✅ Joined eye tracking columns: {eye_tracking_cols}\")\n",
    "        else:\n",
    "            print(f\"ℹ️ No eye tracking columns joined (video data may not contain them)\")\n",
    "        \n",
    "        # Load saccade summary CSV files if they exist\n",
    "        for video_key in (\"VideoData1\", \"VideoData2\"):\n",
    "            saccade_summary_path = data_path / f\"{video_key}_saccade_summary.csv\"\n",
    "            if saccade_summary_path.exists():\n",
    "                try:\n",
    "                    saccade_summary_df = pd.read_csv(saccade_summary_path)\n",
    "                    # Convert aeon_time column to datetime if present\n",
    "                    if \"aeon_time\" in saccade_summary_df.columns:\n",
    "                        saccade_summary_df[\"aeon_time\"] = pd.to_datetime(saccade_summary_df[\"aeon_time\"])\n",
    "                        # Set aeon_time as index for reindexing\n",
    "                        saccade_summary_indexed = saccade_summary_df.set_index(\"aeon_time\")\n",
    "                        \n",
    "                        # Reindex to photometry timestamps using nearest neighbor matching\n",
    "                        photo_index_sorted = photometry_tracking_encoder_data.index.sort_values()\n",
    "                        saccade_indices_for_photo = saccade_summary_indexed.index.get_indexer(photo_index_sorted, method='nearest')\n",
    "                        \n",
    "                        # Calculate time differences for reporting (only for actual saccade events)\n",
    "                        # For each saccade timestamp, find its nearest photometry timestamp\n",
    "                        saccade_times = saccade_summary_indexed.index.values\n",
    "                        photo_indices_for_saccades = photo_index_sorted.get_indexer(saccade_times, method='nearest')\n",
    "                        nearest_photo_times = photo_index_sorted[photo_indices_for_saccades]\n",
    "                        \n",
    "                        # Calculate differences (saccade - photo) in seconds\n",
    "                        time_diffs_series = pd.Series(saccade_times) - pd.Series(nearest_photo_times.values)\n",
    "                        time_diffs = time_diffs_series.dt.total_seconds().values\n",
    "                        mean_diff_ms = np.mean(time_diffs) * 1000\n",
    "                        \n",
    "                        # Create reindexed dataframe\n",
    "                        saccade_summary_aligned = pd.DataFrame(\n",
    "                            index=photo_index_sorted,\n",
    "                            columns=saccade_summary_indexed.columns\n",
    "                        )\n",
    "                        for col in saccade_summary_indexed.columns:\n",
    "                            saccade_summary_aligned[col] = saccade_summary_indexed[col].iloc[saccade_indices_for_photo].values\n",
    "                        \n",
    "                        # Report alignment\n",
    "                        print(f\"⚠️ {video_key} saccade summary: Timestamps reindexed to photometry (avg difference: {mean_diff_ms:.3f} ms)\")\n",
    "                        \n",
    "                        video_saccade_summaries[video_key] = saccade_summary_aligned\n",
    "                    else:\n",
    "                        # No aeon_time column, store as-is\n",
    "                        video_saccade_summaries[video_key] = saccade_summary_df\n",
    "                        print(f\"✅ Loaded {video_key} saccade summary ({len(saccade_summary_df)} saccades)\")\n",
    "                except Exception as e:\n",
    "                    print(f\"⚠️ Error loading {video_key} saccade summary: {str(e)}\")\n",
    "            else:\n",
    "                print(f\"ℹ️ No {video_key} saccade summary found at {saccade_summary_path}\")\n",
    "        \n",
    "        # Create boolean saccade columns from the ORIGINAL saccade times (not reindexed)\n",
    "        # This marks each timestamp where a saccade occurred\n",
    "        print(f\"\\nCreating boolean saccade event columns...\")\n",
    "        for video_key, eye_suffix in ((\"VideoData1\", \"eye1\"), (\"VideoData2\", \"eye2\")):\n",
    "            saccade_col_name = f\"saccade_event_{eye_suffix}\"\n",
    "            # Initialize with False (no saccade at this timestamp)\n",
    "            photometry_tracking_encoder_data[saccade_col_name] = False\n",
    "            \n",
    "            if video_key in video_saccade_summaries and not video_saccade_summaries[video_key].empty:\n",
    "                saccade_df = video_saccade_summaries[video_key]\n",
    "                \n",
    "                # IMPORTANT: Use ORIGINAL saccade times from CSV, not reindexed times\n",
    "                # The reindexed data has one row per photometry timestamp (wrong!)\n",
    "                # We need the original saccade event times\n",
    "                if 'aeon_time' in saccade_df.columns:\n",
    "                    # Get original saccade times\n",
    "                    original_saccade_times = pd.to_datetime(saccade_df['aeon_time'].dropna().unique())\n",
    "                elif isinstance(saccade_df.index, pd.DatetimeIndex):\n",
    "                    # If index has unique saccade times, use those\n",
    "                    # But first check if it was artificially expanded by reindexing\n",
    "                    if len(saccade_df) == len(photometry_tracking_encoder_data):\n",
    "                        # This was reindexed to match photometry - we need original data\n",
    "                        print(f\"⚠️ {video_key}: Saccade data was reindexed, need original CSV\")\n",
    "                        # Re-read the original CSV to get true saccade times\n",
    "                        saccade_summary_path = data_path / f\"{video_key}_saccade_summary.csv\"\n",
    "                        if saccade_summary_path.exists():\n",
    "                            original_df = pd.read_csv(saccade_summary_path)\n",
    "                            if 'aeon_time' in original_df.columns:\n",
    "                                original_saccade_times = pd.to_datetime(original_df['aeon_time'].dropna().unique())\n",
    "                            else:\n",
    "                                print(f\"⚠️ {video_key}: No aeon_time in CSV, skipping\")\n",
    "                                continue\n",
    "                        else:\n",
    "                            print(f\"⚠️ {video_key}: Cannot re-read CSV, skipping\")\n",
    "                            continue\n",
    "                    else:\n",
    "                        original_saccade_times = saccade_df.index.unique()\n",
    "                else:\n",
    "                    print(f\"⚠️ {video_key}: Cannot extract saccade timestamps, skipping\")\n",
    "                    continue\n",
    "                \n",
    "                # Find photometry timestamps closest to each saccade time\n",
    "                photo_index = photometry_tracking_encoder_data.index\n",
    "                closest_indices = photo_index.get_indexer(original_saccade_times, method='nearest')\n",
    "                closest_times = photo_index[closest_indices]\n",
    "                \n",
    "                # Remove duplicates (multiple saccades might map to same photometry timestamp)\n",
    "                closest_times_unique = pd.DatetimeIndex(closest_times).unique()\n",
    "                \n",
    "                # Mark these timestamps as saccade events\n",
    "                photometry_tracking_encoder_data.loc[closest_times_unique, saccade_col_name] = True\n",
    "                \n",
    "                n_saccades = len(closest_times_unique)\n",
    "                n_original_saccades = len(original_saccade_times)\n",
    "                total_duration_s = (photometry_tracking_encoder_data.index[-1] - \n",
    "                                   photometry_tracking_encoder_data.index[0]).total_seconds()\n",
    "                saccade_rate_hz = n_saccades / total_duration_s if total_duration_s > 0 else 0\n",
    "                \n",
    "                print(f\"  ✅ {video_key}: Marked {n_saccades} saccade events from {n_original_saccades} original saccades (rate: {saccade_rate_hz:.2f} Hz)\")\n",
    "            else:\n",
    "                print(f\"  ℹ️ {video_key}: No saccade data available\")\n",
    "        \n",
    "        print(f\"✅ Successfully loaded all parquet files for {data_path.name}\")\n",
    "        \n",
    "        # Calculate time differences between event_name events\n",
    "        event_times = experiment_events[experiment_events[\"Event\"] == event_name].index\n",
    "        if len(event_times) > 1:\n",
    "            time_diffs = event_times.to_series().diff().dropna().dt.total_seconds()\n",
    "            # Print the 5 shortest time differences\n",
    "            # print(\"5 shortest time differences between events:\")\n",
    "            # print(time_diffs.nsmallest(5))\n",
    "            if (time_diffs < 10).any():\n",
    "                print(f\"⚠️ Warning: Some '{event_name}' events are less than 10 seconds apart. Consider applying a filter to events.\")\n",
    "        else:\n",
    "            print(f\"ℹ️ INFO: Found {len(event_times)} events with name '{event_name}' - not enough to calculate differences\")\n",
    "        \n",
    "        # Check experiment events and get mouse name\n",
    "        mouse_name = process.check_exp_events(experiment_events, photometry_info, verbose=True)\n",
    "        \n",
    "        # Store all loaded data in the dictionary\n",
    "        loaded_data[data_path] = {\n",
    "            \"photometry_tracking_encoder_data\": photometry_tracking_encoder_data,\n",
    "            \"camera_photodiode_data\": camera_photodiode_data,\n",
    "            \"experiment_events\": experiment_events,\n",
    "            \"photometry_info\": photometry_info,\n",
    "            \"session_settings\": session_settings,\n",
    "            \"video_data\": video_dataframes,\n",
    "            \"video_saccade_summaries\": video_saccade_summaries,\n",
    "            \"mouse_name\": mouse_name\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ ERROR processing data path {data_path}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\n✅ Finished loading data for all {len(loaded_data)} successfully processed data paths\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1d97e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIAGNOSTIC: Plot saccade density across entire session timeseries\n",
    "#-------------------------------\n",
    "# This plots the saccade boolean column to verify saccades are properly marked\n",
    "\n",
    "for idx, (data_path, data_dict) in enumerate(loaded_data.items(), start=1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"SACCADE DENSITY DIAGNOSTIC {idx}/{len(loaded_data)}\")\n",
    "    print(f\"Session: {data_dict['mouse_name']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    df = data_dict[\"photometry_tracking_encoder_data\"]\n",
    "    \n",
    "    # Check if saccade columns exist\n",
    "    saccade_cols = [col for col in df.columns if 'saccade_event_' in col]\n",
    "    if not saccade_cols:\n",
    "        print(f\"⚠️ No saccade event columns found in this session\")\n",
    "        continue\n",
    "    \n",
    "    # Calculate rolling saccade density (probability in time windows)\n",
    "    window_size_s = saccade_bin_size_s  # Use same bin size as analysis\n",
    "    sampling_rate = 1000  # Hz (from common_resampled_rate)\n",
    "    window_samples = int(window_size_s * sampling_rate)\n",
    "    \n",
    "    fig, axes = plt.subplots(len(saccade_cols), 1, figsize=(14, 4*len(saccade_cols)), sharex=True)\n",
    "    if len(saccade_cols) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for ax, col in zip(axes, saccade_cols):\n",
    "        # Calculate rolling saccade probability (proportion of samples with saccades in window)\n",
    "        saccade_bool = df[col].astype(float)  # Convert True/False to 1/0\n",
    "        rolling_prob = saccade_bool.rolling(window=window_samples, center=True).mean()\n",
    "        \n",
    "        # Calculate rolling saccade rate (Hz)\n",
    "        rolling_count = saccade_bool.rolling(window=window_samples, center=True).sum()\n",
    "        rolling_rate = rolling_count / window_size_s\n",
    "        \n",
    "        # Convert index to seconds for plotting\n",
    "        time_seconds = (df.index - df.index[0]).total_seconds()\n",
    "        \n",
    "        # Plot probability\n",
    "        ax_prob = ax\n",
    "        ax_prob.plot(time_seconds, rolling_prob, color='blue', linewidth=1, label=f'{col} (probability)')\n",
    "        ax_prob.set_ylabel(f'Saccade Probability\\n({window_size_s*1000:.0f}ms bins)', color='blue')\n",
    "        ax_prob.tick_params(axis='y', labelcolor='blue')\n",
    "        ax_prob.set_ylim(0, 1)\n",
    "        ax_prob.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot rate on secondary axis\n",
    "        ax_rate = ax.twinx()\n",
    "        ax_rate.plot(time_seconds, rolling_rate, color='red', linewidth=1, alpha=0.7, label=f'{col} (rate Hz)')\n",
    "        ax_rate.set_ylabel('Saccade Rate (Hz)', color='red')\n",
    "        ax_rate.tick_params(axis='y', labelcolor='red')\n",
    "        ax_rate.set_ylim(0, max(rolling_rate.max() * 1.2, 50))  # Set max to at least 50 Hz\n",
    "        \n",
    "        # Statistics\n",
    "        total_saccades = int(saccade_bool.sum())\n",
    "        mean_prob = rolling_prob.mean()\n",
    "        mean_rate = rolling_rate.mean()\n",
    "        \n",
    "        ax.set_title(f'{col}: {total_saccades} saccades | Mean prob: {mean_prob:.3f} | Mean rate: {mean_rate:.2f} Hz')\n",
    "        ax.legend(loc='upper left')\n",
    "        ax_rate.legend(loc='upper right')\n",
    "    \n",
    "    axes[-1].set_xlabel('Time (seconds)')\n",
    "    fig.suptitle(f\"Saccade Density Diagnostic - {data_dict['mouse_name']}\", fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save diagnostic plot\n",
    "    diagnostic_path = data_path.parent / f\"{data_dict['mouse_name']}_saccade_density_diagnostic.pdf\"\n",
    "    plt.savefig(diagnostic_path, dpi=150, bbox_inches='tight')\n",
    "    print(f\"✅ Saved diagnostic plot: {diagnostic_path}\")\n",
    "    plt.close(fig)\n",
    "    \n",
    "    # Print summary statistics\n",
    "    for col in saccade_cols:\n",
    "        saccade_bool = df[col].astype(float)\n",
    "        total_saccades = int(saccade_bool.sum())\n",
    "        total_duration_s = (df.index[-1] - df.index[0]).total_seconds()\n",
    "        overall_rate = total_saccades / total_duration_s if total_duration_s > 0 else 0\n",
    "        print(f\"\\n{col} summary:\")\n",
    "        print(f\"  Total saccades: {total_saccades}\")\n",
    "        print(f\"  Session duration: {total_duration_s:.1f} seconds ({total_duration_s/60:.1f} minutes)\")\n",
    "        print(f\"  Overall saccade rate: {overall_rate:.2f} Hz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70d1a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BEHAVIORAL ANALYSIS CLASS\n",
    "#--------------------------------\n",
    "class BehavioralAnalyzer:\n",
    "    \"\"\"\n",
    "    Analyzer for running, turning, and platform velocity behaviors.\n",
    "    Implements adaptive thresholding using dual Gaussian fitting.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_path: Path, loaded_data: Dict[str, Any],\n",
    "                 threshold_plot_bins: int = 50,\n",
    "                 min_bout_duration_s: float = 0.2,\n",
    "                 running_percentile: float = 10,\n",
    "                 turning_percentile: Optional[float] = None,\n",
    "                 turning_velocity_column: str = \"Motor_Velocity\"):\n",
    "        \"\"\"\n",
    "        Initialize analyzer with loaded data.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        data_path : Path\n",
    "            Path to the data directory\n",
    "        loaded_data : Dict\n",
    "            Dictionary containing loaded dataframes\n",
    "        threshold_plot_bins : int\n",
    "            Number of bins for threshold plots\n",
    "        min_bout_duration_s : float\n",
    "            Minimum duration for a bout to be considered valid (seconds).\n",
    "            This is converted to samples internally based on the data sampling rate.\n",
    "        running_percentile : float\n",
    "            Percentile to use for running threshold calculation (default: 10)\n",
    "        turning_percentile : Optional[float]\n",
    "            Percentile to use on values below median for turning threshold (default: 25).\n",
    "            If None, uses 25th percentile.\n",
    "        turning_velocity_column : str\n",
    "            Column name to use for turning analysis (e.g., \"Motor_Velocity\" or \"Velocity_0Y\").\n",
    "        \"\"\"\n",
    "        self.data_path = data_path\n",
    "        self.mouse_name = loaded_data.get(\"mouse_name\", \"Unknown\")\n",
    "        self.photometry_tracking_encoder_data = loaded_data[\"photometry_tracking_encoder_data\"]\n",
    "        self.experiment_events = loaded_data[\"experiment_events\"]\n",
    "        \n",
    "        # Results storage\n",
    "        self.results = {}\n",
    "        self.figures = {}\n",
    "        \n",
    "        # Thresholding configuration\n",
    "        self.threshold_plot_bins = threshold_plot_bins\n",
    "        self.min_bout_duration_s = min_bout_duration_s\n",
    "        self.running_percentile = running_percentile\n",
    "        self.turning_percentile = turning_percentile\n",
    "        \n",
    "        # Turning configuration\n",
    "        self.turning_velocity_column = turning_velocity_column\n",
    "        self.turning_source_label = self._format_turning_source_label(turning_velocity_column)\n",
    "        if turning_velocity_column not in self.photometry_tracking_encoder_data.columns:\n",
    "            print(f\"⚠️ Warning: Column '{turning_velocity_column}' not found in data. Available columns: {self.photometry_tracking_encoder_data.columns.tolist()}\")\n",
    "        \n",
    "    def _format_turning_source_label(self, column_name: str) -> str:\n",
    "        if column_name == \"Motor_Velocity\":\n",
    "            return \"motor\"\n",
    "        return column_name.lower().replace(\" \", \"_\")\n",
    "        \n",
    "    def _is_visual_mismatch_experiment(self) -> bool:\n",
    "        \"\"\"\n",
    "        Check if this is a visual mismatch experiment based on the data path.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        bool\n",
    "            True if visual mismatch experiment, False otherwise\n",
    "        \"\"\"\n",
    "        path_str = str(self.data_path).lower()\n",
    "        return 'visual_mismatch' in path_str or 'visual mismatch' in path_str\n",
    "    \n",
    "    def slice_data_until_block_timer_elapsed(self, interval: str = \"first_10min\"):\n",
    "        \"\"\"\n",
    "        Slice data based on experiment type and interval specification.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        interval : str\n",
    "            Interval specification:\n",
    "            - \"first_10min\": First 10 minutes (600 seconds) - default for non-visual mismatch\n",
    "            - \"first_block\": From session start to first \"Block timer elapsed\" - for visual mismatch first block\n",
    "            - \"last_block\": From first \"Block timer elapsed\" to last \"Block timer elapsed\" - for visual mismatch second block (independent duration)\n",
    "        \"\"\"\n",
    "        session_start_time = self.photometry_tracking_encoder_data.index[0]\n",
    "        \n",
    "        if interval == \"first_10min\":\n",
    "            # Default: first 10 minutes\n",
    "            end_time = session_start_time + pd.Timedelta(seconds=600)\n",
    "            self.sliced_data = self.photometry_tracking_encoder_data[\n",
    "                (self.photometry_tracking_encoder_data.index >= session_start_time) & \n",
    "                (self.photometry_tracking_encoder_data.index <= end_time)\n",
    "            ].copy()\n",
    "            \n",
    "            if len(self.sliced_data) == 0:\n",
    "                print(\"⚠️ Warning: No data found in first 10 minutes. Using all data.\")\n",
    "                self.sliced_data = self.photometry_tracking_encoder_data.copy()\n",
    "            else:\n",
    "                duration_seconds = (self.sliced_data.index[-1] - self.sliced_data.index[0]).total_seconds()\n",
    "                print(f\"✅ Data sliced to first 10 minutes: {duration_seconds:.1f} seconds\")\n",
    "        \n",
    "        elif interval in [\"first_block\", \"last_block\"]:\n",
    "            # For visual mismatch: use block timer elapsed events\n",
    "            block_elapsed_events = self.experiment_events[\n",
    "                self.experiment_events[\"Event\"] == \"Block timer elapsed\"\n",
    "            ]\n",
    "            \n",
    "            if len(block_elapsed_events) == 0:\n",
    "                print(f\"⚠️ Warning: No 'Block timer elapsed' event found for {interval}. Using all data.\")\n",
    "                self.sliced_data = self.photometry_tracking_encoder_data.copy()\n",
    "                return\n",
    "            \n",
    "            if interval == \"first_block\":\n",
    "                # First block: from session start to first \"Block timer elapsed\"\n",
    "                start_time = session_start_time\n",
    "                end_time = block_elapsed_events.index[0]  # First occurrence\n",
    "                print(f\"✅ Data sliced for first block: {start_time} → {end_time}\")\n",
    "            else:  # last_block\n",
    "                # Last block: from first \"Block timer elapsed\" to last \"Block timer elapsed\"\n",
    "                # This measures Block 2 independently (not cumulative from session start)\n",
    "                if len(block_elapsed_events) < 2:\n",
    "                    print(f\"⚠️ Warning: Only one 'Block timer elapsed' event found. Cannot create independent last_block. Using all data after first block.\")\n",
    "                    start_time = block_elapsed_events.index[0]\n",
    "                    end_time = self.photometry_tracking_encoder_data.index[-1]\n",
    "                else:\n",
    "                    start_time = block_elapsed_events.index[0]  # First \"Block timer elapsed\" (end of Block 1)\n",
    "                    end_time = block_elapsed_events.index[-1]  # Last \"Block timer elapsed\" (end of Block 2)\n",
    "                print(f\"✅ Data sliced for last block: {start_time} → {end_time}\")\n",
    "            \n",
    "            # Slice the data\n",
    "            self.sliced_data = self.photometry_tracking_encoder_data[\n",
    "                (self.photometry_tracking_encoder_data.index >= start_time) & \n",
    "                (self.photometry_tracking_encoder_data.index <= end_time)\n",
    "            ].copy()\n",
    "            \n",
    "            duration_seconds = (self.sliced_data.index[-1] - self.sliced_data.index[0]).total_seconds()\n",
    "            print(f\"   Duration: {duration_seconds:.1f} seconds ({duration_seconds/60:.1f} minutes)\")\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown interval specification: {interval}\")\n",
    "\n",
    "    \n",
    "    def _save_figure(self, fig: plt.Figure, save_path: Path, description: str = \"plot\") -> None:\n",
    "        \"\"\"\n",
    "        Save a matplotlib figure consistently.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        fig : plt.Figure\n",
    "            Figure to save\n",
    "        save_path : Path\n",
    "            Path to save the figure\n",
    "        description : str\n",
    "            Description for logging\n",
    "        \"\"\"\n",
    "        plt.tight_layout()\n",
    "        fig.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "        print(f\"✅ Saved {description}: {save_path}\")\n",
    "    \n",
    "    def _plot_thresholded_timeseries(self, \n",
    "                                     time_seconds: np.ndarray,\n",
    "                                     values: np.ndarray,\n",
    "                                     threshold: float,\n",
    "                                     title: str,\n",
    "                                     ylabel: str,\n",
    "                                     save_path: Path,\n",
    "                                     mask_above: Optional[np.ndarray] = None) -> None:\n",
    "        fig, ax = plt.subplots(figsize=(12, 4))\n",
    "        ax.plot(time_seconds, values, color='gray', linewidth=0.8, alpha=0.7, label='Velocity')\n",
    "        if mask_above is None:\n",
    "            mask_above = values > threshold\n",
    "        # Overlay segments above threshold\n",
    "        ax.plot(time_seconds[mask_above], values[mask_above], color='orange', linewidth=1.2, label='Above threshold')\n",
    "        ax.axhline(threshold, color='green', linestyle='--', linewidth=1.2, label=f'Threshold = {threshold:.2f}')\n",
    "        ax.set_title(title, fontsize=12, fontweight='bold')\n",
    "        ax.set_xlabel('Time (s)', fontsize=11)\n",
    "        ax.set_ylabel(ylabel, fontsize=11)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.legend(fontsize=10)\n",
    "        self._save_figure(fig, save_path, \"thresholded timeseries plot\")\n",
    "\n",
    "    def _save_histogram(self,\n",
    "                        values: np.ndarray,\n",
    "                        bins: int,\n",
    "                        threshold: Optional[float],\n",
    "                        title: str,\n",
    "                        xlabel: str,\n",
    "                        save_path: Path) -> None:\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        ax.hist(values, bins=bins, color='gray', alpha=0.7, edgecolor='white')\n",
    "        if threshold is not None:\n",
    "            ax.axvline(threshold, color='green', linestyle='--', linewidth=1.5, label=f'Threshold = {threshold:.2f}')\n",
    "            ax.legend(fontsize=10)\n",
    "        ax.set_title(title, fontsize=12, fontweight='bold')\n",
    "        ax.set_xlabel(xlabel, fontsize=11)\n",
    "        ax.set_ylabel('Count', fontsize=11)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        self._save_figure(fig, save_path, \"histogram\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def _mad(data: np.ndarray) -> float:\n",
    "        median = np.median(data)\n",
    "        return np.median(np.abs(data - median))\n",
    "    \n",
    "    def find_threshold_robust(self,\n",
    "                              velocities: np.ndarray,\n",
    "                              positive_only: bool = True,\n",
    "                              plot: bool = True,\n",
    "                              title: str = \"Velocity Distribution (robust)\",\n",
    "                              plot_bins: int = 50,\n",
    "                              percentile: float = 10) -> Tuple[float, Optional[plt.Figure]]:\n",
    "        \"\"\"\n",
    "        Simple threshold for separating stationary vs running.\n",
    "        Excludes values below 0.01 and uses the specified percentile as the threshold.\n",
    "        \"\"\"\n",
    "        if positive_only:\n",
    "            valid_velocities = velocities[(velocities > 0) & np.isfinite(velocities)]\n",
    "        else:\n",
    "            valid_velocities = velocities[np.isfinite(velocities)]\n",
    "            valid_velocities = np.abs(valid_velocities)\n",
    "        \n",
    "        # Exclude values below 0.01 as requested\n",
    "        valid_velocities = valid_velocities[valid_velocities >= 0.01]\n",
    "        \n",
    "        if len(valid_velocities) < 100:\n",
    "            print(f\"⚠️ Warning: Not enough valid data points ({len(valid_velocities)}) after filtering\")\n",
    "            return 0.0, None\n",
    "        \n",
    "        # Calculate threshold as specified percentile of the distribution\n",
    "        threshold = np.percentile(valid_velocities, percentile)\n",
    "\n",
    "        # Create plot if requested\n",
    "        fig = None\n",
    "        if plot:\n",
    "            fig, ax = plt.subplots(figsize=(10, 6))\n",
    "            \n",
    "            # Plot histogram\n",
    "            ax.hist(valid_velocities, bins=plot_bins, density=True, alpha=0.6, \n",
    "                   color='gray', edgecolor='white', label='Data')\n",
    "            \n",
    "            # Plot threshold\n",
    "            ax.axvline(threshold, color='green', linestyle='-', linewidth=2, \n",
    "                      label=f'Threshold ({percentile:.0f}th percentile) = {threshold:.2f}')\n",
    "            ax.axvline(0.01, color='red', linestyle='--', linewidth=1, alpha=0.5, label='Min value (0.01)')\n",
    "            \n",
    "            ax.set_xlabel('Velocity (m/s)', fontsize=12)\n",
    "            ax.set_ylabel('Density', fontsize=12)\n",
    "            ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "            ax.legend(fontsize=10)\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "        \n",
    "        print(f\"✅ Threshold found ({percentile:.0f}th percentile): {threshold:.3f}\")\n",
    "        \n",
    "        return threshold, fig\n",
    "    \n",
    "    def find_threshold_turning_correlation_based(self,\n",
    "                                                 running_velocities: np.ndarray,\n",
    "                                                 turning_velocities: np.ndarray,\n",
    "                                                 window_size_s: float = 1.0,\n",
    "                                                 correlation_threshold: float = 0.5,\n",
    "                                                 plot: bool = True,\n",
    "                                                 title: str = \"Turning Velocity Distribution (Correlation-based)\",\n",
    "                                                 plot_bins: int = 50,\n",
    "                                                 percentile: float = 10) -> Tuple[float, Optional[plt.Figure]]:\n",
    "        \"\"\"\n",
    "        Find turning threshold using correlation-based approach.\n",
    "        Uses periods where running and turning are highly correlated to define the threshold.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        running_velocities : np.ndarray\n",
    "            Running velocity values (Velocity_0X)\n",
    "        turning_velocities : np.ndarray\n",
    "            Turning velocity values (Motor_Velocity, absolute values)\n",
    "        window_size_s : float\n",
    "            Window size in seconds for rolling correlation calculation\n",
    "        correlation_threshold : float\n",
    "            Minimum correlation coefficient to consider as \"high correlation\" period\n",
    "        plot : bool\n",
    "            Whether to create a plot\n",
    "        title : str\n",
    "            Plot title\n",
    "        plot_bins : int\n",
    "            Number of bins for histogram\n",
    "        percentile : float\n",
    "            Percentile to use for threshold calculation on high-correlation periods\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        threshold : float\n",
    "            Calculated threshold\n",
    "        fig : Optional[plt.Figure]\n",
    "            Figure if plot=True, None otherwise\n",
    "        \"\"\"\n",
    "        # Remove NaN values (use views where possible for memory efficiency)\n",
    "        valid_mask = ~(np.isnan(running_velocities) | np.isnan(turning_velocities))\n",
    "        running_valid = running_velocities[valid_mask].astype(np.float32)  # Use float32 to save memory\n",
    "        turning_valid = turning_velocities[valid_mask].astype(np.float32)\n",
    "        \n",
    "        if len(running_valid) < 100:\n",
    "            print(f\"⚠️ Warning: Not enough valid data points ({len(running_valid)})\")\n",
    "            return 0.0, None\n",
    "        \n",
    "        # Note: Keep running_valid and turning_valid for plotting, clean up later\n",
    "        \n",
    "        # Estimate sampling rate from data length\n",
    "        # Assume reasonable sampling rate (typically 1000 Hz for this data)\n",
    "        # Calculate window size in samples - use at least 100 samples\n",
    "        # For typical 1000 Hz data, 1 second = 1000 samples\n",
    "        window_samples = max(100, int(window_size_s * 1000))  # Assume ~1000 Hz\n",
    "        \n",
    "        # Try to get actual sampling rate if sliced_data is available\n",
    "        if hasattr(self, 'sliced_data') and len(self.sliced_data) > 1:\n",
    "            try:\n",
    "                sampling_interval = pd.Timedelta(self.sliced_data.index.to_series().diff().median()).total_seconds()\n",
    "                if sampling_interval > 0:\n",
    "                    sampling_rate = 1.0 / sampling_interval\n",
    "                    window_samples = max(100, int(window_size_s * sampling_rate))\n",
    "                    print(f\"   Using rolling correlation window: {window_samples} samples ({window_size_s}s at {sampling_rate:.0f} Hz)\")\n",
    "                else:\n",
    "                    print(f\"   Using rolling correlation window: {window_samples} samples (estimated)\")\n",
    "            except:\n",
    "                print(f\"   Using rolling correlation window: {window_samples} samples (estimated)\")\n",
    "        else:\n",
    "            print(f\"   Using rolling correlation window: {window_samples} samples (estimated)\")\n",
    "        \n",
    "        # For plotting, estimate sampling interval\n",
    "        sampling_interval = 1.0 / 1000  # Default assumption\n",
    "        if hasattr(self, 'sliced_data') and len(self.sliced_data) > 1:\n",
    "            try:\n",
    "                sampling_interval = pd.Timedelta(self.sliced_data.index.to_series().diff().median()).total_seconds()\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Calculate rolling correlation efficiently using pandas (optimized in C)\n",
    "        print(\"   Calculating rolling correlation (optimized)...\")\n",
    "        n = len(running_valid)\n",
    "        \n",
    "        # Use pandas DataFrame rolling correlation - much faster than Python loop\n",
    "        # Only create DataFrame temporarily for correlation calculation\n",
    "        df_corr = pd.DataFrame({\n",
    "            'running': running_valid,\n",
    "            'turning': turning_valid\n",
    "        }, dtype=np.float32)  # Use float32 to save memory\n",
    "        \n",
    "        # Calculate rolling correlation using pandas (optimized implementation)\n",
    "        rolling_corr_series = df_corr['running'].rolling(\n",
    "            window=window_samples,\n",
    "            min_periods=max(10, window_samples // 4)\n",
    "        ).corr(df_corr['turning'])\n",
    "        \n",
    "        # Identify high-correlation periods directly from series (memory efficient)\n",
    "        high_corr_mask = (rolling_corr_series.fillna(0.0).abs() >= correlation_threshold).values\n",
    "        \n",
    "        # Extract turning velocities from high-correlation periods\n",
    "        turning_high_corr = turning_valid[high_corr_mask]\n",
    "        \n",
    "        # Clean up DataFrame to free memory\n",
    "        del df_corr, rolling_corr_series\n",
    "        gc.collect()\n",
    "        \n",
    "        print(f\"   ✓ Rolling correlation calculated ({n} samples)\")\n",
    "        \n",
    "        if len(turning_high_corr) < 100:\n",
    "            print(f\"⚠️ Warning: Not enough high-correlation data points ({len(turning_high_corr)}). \"\n",
    "                  f\"Try lowering correlation_threshold or increasing window_size_s.\")\n",
    "            # Fallback: use all data\n",
    "            turning_high_corr = turning_valid\n",
    "            print(f\"   Using all data as fallback\")\n",
    "        \n",
    "        # Filter values below 0.01 (same as running)\n",
    "        valid_turning = turning_high_corr[turning_high_corr >= 0.01]\n",
    "        \n",
    "        if len(valid_turning) < 100:\n",
    "            print(f\"⚠️ Warning: Not enough valid data points after filtering\")\n",
    "            return 0.0, None\n",
    "        \n",
    "        # Calculate threshold using percentile on high-correlation periods\n",
    "        threshold = np.percentile(valid_turning, percentile)\n",
    "        method = f\"{percentile:.0f}th percentile (high-correlation periods)\"\n",
    "        \n",
    "        # Create plot if requested\n",
    "        fig = None\n",
    "        if plot:\n",
    "            fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 10))\n",
    "            \n",
    "            # Plot 1: Time series of running and turning velocities (no correlation line)\n",
    "            time_axis = np.arange(len(running_valid)) * sampling_interval\n",
    "            ax1.plot(time_axis, np.abs(running_valid), color='blue', alpha=0.5, linewidth=0.5, label='|Running|')\n",
    "            ax1.plot(time_axis, turning_valid, color='red', alpha=0.5, linewidth=0.5, label='|Turning|')\n",
    "            ax1.set_xlabel('Time (s)', fontsize=11)\n",
    "            ax1.set_ylabel('Velocity (m/s or deg/s)', fontsize=11)\n",
    "            ax1.set_title(f'Velocity Time Series (Window: {window_size_s}s) - {self.mouse_name}', \n",
    "                         fontsize=12, fontweight='bold')\n",
    "            ax1.legend(loc='upper left', fontsize=9)\n",
    "            ax1.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Plot 2: Histogram of turning velocities (highlighting high-correlation periods)\n",
    "            ax2.hist(turning_valid, bins=plot_bins, density=True, alpha=0.3, \n",
    "                    color='gray', edgecolor='white', label='All Turning Velocities')\n",
    "            ax2.hist(turning_high_corr, bins=plot_bins, density=True, alpha=0.6, \n",
    "                    color='orange', edgecolor='white', label='High-Correlation Periods')\n",
    "            ax2.axvline(threshold, color='green', linestyle='-', linewidth=2, \n",
    "                       label=f'Threshold ({method}) = {threshold:.2f}')\n",
    "            ax2.axvline(0.01, color='red', linestyle='--', linewidth=1, alpha=0.5, label='Min value (0.01)')\n",
    "            ax2.set_xlabel('Velocity (deg/s)', fontsize=12)\n",
    "            ax2.set_ylabel('Density', fontsize=12)\n",
    "            ax2.set_title(title, fontsize=14, fontweight='bold')\n",
    "            ax2.legend(fontsize=10)\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # Clean up large arrays after plotting\n",
    "            del turning_valid, running_valid\n",
    "        \n",
    "        # Calculate statistics (before cleanup)\n",
    "        high_corr_pct = (np.sum(high_corr_mask) / len(high_corr_mask)) * 100 if len(high_corr_mask) > 0 else 0.0\n",
    "        print(f\"✅ Turning threshold found ({method}): {threshold:.3f}\")\n",
    "        print(f\"   High-correlation periods: {high_corr_pct:.1f}% of data\")\n",
    "        print(f\"   Correlation threshold: {correlation_threshold}, Window size: {window_size_s}s\")\n",
    "        \n",
    "        return threshold, fig\n",
    "    \n",
    "    def find_threshold_turning(self,\n",
    "                               velocities: np.ndarray,\n",
    "                               plot: bool = True,\n",
    "                               title: str = \"Turning Velocity Distribution\",\n",
    "                               plot_bins: int = 50) -> Tuple[float, Optional[plt.Figure]]:\n",
    "        \"\"\"\n",
    "        Find threshold for turning behavior using the median.\n",
    "        Only uses values above minimum threshold (0.01) to calculate the median.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        velocities : np.ndarray\n",
    "            Absolute velocity values\n",
    "        plot : bool\n",
    "            Whether to create a plot\n",
    "        title : str\n",
    "            Plot title\n",
    "        plot_bins : int\n",
    "            Number of bins for histogram\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        threshold : float\n",
    "            Calculated threshold (median of values above 0.01)\n",
    "        fig : Optional[plt.Figure]\n",
    "            Figure if plot=True, None otherwise\n",
    "        \"\"\"\n",
    "        # Filter valid velocities\n",
    "        valid_velocities = velocities[np.isfinite(velocities) & (velocities >= 0)]\n",
    "        \n",
    "        if len(valid_velocities) < 100:\n",
    "            print(f\"⚠️ Warning: Not enough valid data points ({len(valid_velocities)}) after filtering\")\n",
    "            return 0.0, None\n",
    "        \n",
    "        # Only use values above 0.01 (minimum threshold)\n",
    "        min_threshold = 0.01\n",
    "        valid_velocities_filtered = valid_velocities[valid_velocities >= min_threshold]\n",
    "        \n",
    "        if len(valid_velocities_filtered) < 100:\n",
    "            print(f\"⚠️ Warning: Not enough valid data points after filtering\")\n",
    "            return 0.0, None\n",
    "        \n",
    "        # Use median of values above minimum threshold as the turning threshold\n",
    "        threshold = np.median(valid_velocities_filtered)\n",
    "        method = f\"median (of values above {min_threshold})\"\n",
    "\n",
    "        # Create plot if requested\n",
    "        fig = None\n",
    "        if plot:\n",
    "            fig, ax = plt.subplots(figsize=(10, 6))\n",
    "            \n",
    "            # Plot histogram of all filtered velocities\n",
    "            ax.hist(valid_velocities_filtered, bins=plot_bins, density=True, alpha=0.6, \n",
    "                   color='gray', edgecolor='white', label='All velocities')\n",
    "            \n",
    "            # Plot threshold (median)\n",
    "            ax.axvline(threshold, color='green', linestyle='-', linewidth=2, \n",
    "                      label=f'Threshold (median) = {threshold:.2f}')\n",
    "            ax.axvline(min_threshold, color='red', linestyle='--', linewidth=1, alpha=0.5, label=f'Min threshold ({min_threshold})')\n",
    "            \n",
    "            ax.set_xlabel('Velocity (deg/s)', fontsize=12)\n",
    "            ax.set_ylabel('Density', fontsize=12)\n",
    "            ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "            ax.legend(fontsize=10)\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "        \n",
    "        print(f\"✅ Turning threshold found ({method}): {threshold:.3f}\")\n",
    "        \n",
    "        return threshold, fig\n",
    "    \n",
    "    def _calculate_min_bout_samples(self, sampling_interval: float) -> int:\n",
    "        \"\"\"\n",
    "        Calculate minimum bout samples from duration in seconds.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        sampling_interval : float\n",
    "            Sampling interval in seconds\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        int\n",
    "            Minimum number of samples required for a valid bout\n",
    "        \"\"\"\n",
    "        if sampling_interval > 0:\n",
    "            return int(round(self.min_bout_duration_s / sampling_interval))\n",
    "        return 0\n",
    "    \n",
    "    @staticmethod\n",
    "    def _detect_bouts_above_threshold(signal_values: np.ndarray,\n",
    "                                      threshold: float,\n",
    "                                      min_bout_samples: int = 0) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Detect behavioral bouts by identifying values above threshold and filtering by minimum duration.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        signal_values : np.ndarray\n",
    "            Signal values to analyze\n",
    "        threshold : float\n",
    "            Threshold value above which bouts are detected\n",
    "        min_bout_samples : int\n",
    "            Minimum number of consecutive samples required for a valid bout\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        np.ndarray\n",
    "            Boolean array indicating which samples are part of valid bouts\n",
    "        \"\"\"\n",
    "        # Simple threshold: values above threshold\n",
    "        above_threshold = signal_values > threshold\n",
    "        \n",
    "        # Apply minimum bout duration filter\n",
    "        if min_bout_samples > 0:\n",
    "            # Find contiguous regions above threshold\n",
    "            out = np.zeros_like(above_threshold, dtype=bool)\n",
    "            i = 0\n",
    "            while i < len(above_threshold):\n",
    "                if above_threshold[i]:\n",
    "                    # Find the end of this contiguous region\n",
    "                    start = i\n",
    "                    while i < len(above_threshold) and above_threshold[i]:\n",
    "                        i += 1\n",
    "                    end = i\n",
    "                    bout_length = end - start\n",
    "                    # Only keep if it meets minimum duration\n",
    "                    if bout_length >= min_bout_samples:\n",
    "                        out[start:end] = True\n",
    "                else:\n",
    "                    i += 1\n",
    "            return out\n",
    "        else:\n",
    "            return above_threshold\n",
    "\n",
    "    def analyze_running(self, save_dir: Path, suffix: str = \"\"):   \n",
    "        \"\"\"\n",
    "        Analyze running behavior from velocity_0x column.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        save_dir : Path\n",
    "            Directory to save outputs\n",
    "        suffix : str\n",
    "            Suffix to append to filenames (e.g., \"_first_block\", \"_last_block\")\n",
    "        \"\"\"\n",
    "        print(\"ANALYZING RUNNING BEHAVIOR\")\n",
    "        \n",
    "        velocities = self.sliced_data[\"Velocity_0X\"].values\n",
    "        \n",
    "        # Find robust threshold (positive velocities only)\n",
    "        threshold, fig = self.find_threshold_robust(\n",
    "            velocities,\n",
    "            positive_only=True,\n",
    "            plot=True,\n",
    "            title=f\"Running Velocity Distribution (robust) - {self.mouse_name}{suffix}\",\n",
    "            plot_bins=self.threshold_plot_bins,\n",
    "            percentile=self.running_percentile\n",
    "        )\n",
    "        # Save threshold plot\n",
    "        if fig is not None:\n",
    "            fig_path = save_dir / f\"{self.mouse_name}_running_distribution{suffix}.png\"\n",
    "            self._save_figure(fig, fig_path, \"running distribution plot\")\n",
    "            self.figures[f'running_distribution{suffix}'] = fig_path\n",
    "        \n",
    "        # Identify running bouts with threshold and min duration\n",
    "        sampling_interval = pd.Timedelta(self.sliced_data.index.to_series().diff().median()).total_seconds()\n",
    "        min_bout_samples = self._calculate_min_bout_samples(sampling_interval)\n",
    "        pos_velocities = velocities.copy()\n",
    "        pos_velocities[pos_velocities < 0] = 0\n",
    "        is_running = self._detect_bouts_above_threshold(\n",
    "            pos_velocities,\n",
    "            threshold=threshold,\n",
    "            min_bout_samples=min_bout_samples\n",
    "        )\n",
    "        \n",
    "        # Calculate statistics\n",
    "        running_velocities = velocities[is_running]\n",
    "        \n",
    "        if len(running_velocities) > 0:\n",
    "            avg_velocity = np.mean(running_velocities)\n",
    "            std_velocity = np.std(running_velocities)\n",
    "        else:\n",
    "            avg_velocity = 0.0\n",
    "            std_velocity = 0.0\n",
    "        \n",
    "        # Time calculations\n",
    "        total_time = len(self.sliced_data) * sampling_interval\n",
    "        running_time = np.sum(is_running) * sampling_interval\n",
    "        running_time_pct = (running_time / total_time) * 100\n",
    "        \n",
    "        # Distance calculation (total change in position during running bouts)\n",
    "        if \"Position_0X\" in self.sliced_data.columns:\n",
    "            forward_position = self.sliced_data[\"Position_0X\"].astype(float).to_numpy()\n",
    "            running_mask = is_running.astype(bool)\n",
    "            \n",
    "            # Calculate total distance as the change in position during running periods\n",
    "            if np.any(running_mask):\n",
    "                # Get the position at the end and start of running periods\n",
    "                running_positions = forward_position[running_mask]\n",
    "                if len(running_positions) > 0:\n",
    "                    # Total distance is the absolute difference between final and initial position during running\n",
    "                    travelled_distance = float(np.abs(running_positions[-1] - running_positions[0]))\n",
    "                else:\n",
    "                    travelled_distance = 0.0\n",
    "            else:\n",
    "                travelled_distance = 0.0\n",
    "        else:\n",
    "            travelled_distance = np.nan\n",
    "            print(\"⚠️ Warning: 'Position_0X' column not found; running distance set to NaN.\")\n",
    "        \n",
    "        # Store results with suffix in key\n",
    "        result_key = f'running{suffix}' if suffix else 'running'\n",
    "        self.results[result_key] = {\n",
    "            'threshold': threshold,\n",
    "            'avg_velocity': avg_velocity,\n",
    "            'std_velocity': std_velocity,\n",
    "            'running_time_seconds': running_time,\n",
    "            'running_time_percentage': running_time_pct,\n",
    "            'travelled_distance_m': travelled_distance,\n",
    "            'block_duration_seconds': total_time\n",
    "        }\n",
    "        \n",
    "        # Save CSV\n",
    "        df = pd.DataFrame([{\n",
    "            'Mouse': self.mouse_name,\n",
    "            'Average_Velocity_m_s': avg_velocity,\n",
    "            'SD_Velocity_m_s': std_velocity,\n",
    "            'Running_Time_Percentage': running_time_pct,\n",
    "            'Travelled_Distance_m': travelled_distance,\n",
    "            'Threshold_m_s': threshold\n",
    "        }])\n",
    "        csv_path = save_dir / f\"{self.mouse_name}_running_stats{suffix}.csv\"\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        print(f\"✅ Saved running statistics: {csv_path}\")\n",
    "\n",
    "        # Create and save thresholded time series plot (only positive values considered for thresholding)\n",
    "        time_seconds = (self.sliced_data.index - self.sliced_data.index[0]).total_seconds()\n",
    "        pos_vel = velocities.copy()\n",
    "        pos_vel[pos_vel < 0] = 0  # only positive shown for running\n",
    "        ts_path = save_dir / f\"{self.mouse_name}_running_thresholded_timeseries{suffix}.png\"\n",
    "        self._plot_thresholded_timeseries(\n",
    "            time_seconds=time_seconds,\n",
    "            values=pos_vel,\n",
    "            threshold=threshold,\n",
    "            title=f\"Running Velocity (positive only) with Threshold - {self.mouse_name}{suffix}\",\n",
    "            ylabel='Velocity_0X (m/s)',\n",
    "            save_path=ts_path,\n",
    "            mask_above=(pos_vel > threshold)\n",
    "        )\n",
    "        self.figures[f'running_thresholded_timeseries{suffix}'] = ts_path\n",
    "\n",
    "        # # Save histogram of raw positive running velocities\n",
    "        # hist_path = save_dir / f\"{self.mouse_name}_running_velocity_hist.png\"\n",
    "        # self._save_histogram(\n",
    "        #     values=pos_vel[pos_vel > 0],\n",
    "        #     bins=100,\n",
    "        #     threshold=threshold,\n",
    "        #     title=f\"Histogram of Running Velocity (positive) - {self.mouse_name}\",\n",
    "        #     xlabel='Velocity_0X (m/s)',\n",
    "        #     save_path=hist_path\n",
    "        # )\n",
    "        # self.figures['running_velocity_hist'] = hist_path\n",
    "        \n",
    "    def analyze_turning(self, save_dir: Path, plot: bool = True, suffix: str = \"\", column: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        Analyze turning behavior from the configured turning velocity column.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        save_dir : Path\n",
    "            Directory to save outputs\n",
    "        plot : bool\n",
    "            Whether to create plots\n",
    "        suffix : str\n",
    "            Suffix to append to filenames (e.g., \"_first_block\", \"_last_block\")\n",
    "        column : Optional[str]\n",
    "            Override the configured turning column for this analysis call.\n",
    "        \"\"\"\n",
    "        selected_column = column or self.turning_velocity_column\n",
    "        column_label = self._format_turning_source_label(selected_column)\n",
    "        print(f\"ANALYZING TURNING BEHAVIOR (column: {selected_column})\")\n",
    "        \n",
    "        # Check if selected column exists\n",
    "        if selected_column not in self.sliced_data.columns:\n",
    "            print(f\"⚠️ Warning: Column '{selected_column}' not found in data.\")\n",
    "            print(f\"   Available columns: {self.sliced_data.columns.tolist()}\")\n",
    "            return\n",
    "        \n",
    "        turning_values = self.sliced_data[selected_column].values\n",
    "        \n",
    "        # Use median as threshold\n",
    "        abs_velocities = np.abs(turning_values)\n",
    "        threshold, fig = self.find_threshold_turning(\n",
    "            abs_velocities,\n",
    "            plot=plot,\n",
    "            title=f\"Turning Velocity Distribution ({selected_column}) - {self.mouse_name}{suffix}\",\n",
    "            plot_bins=self.threshold_plot_bins\n",
    "        )\n",
    "        \n",
    "        # Save threshold plot\n",
    "        if fig is not None:\n",
    "            label_suffix = f\"_{column_label}\" if column_label != \"motor\" else \"\"\n",
    "            fig_path = save_dir / f\"{self.mouse_name}_turning_distribution{label_suffix}{suffix}.png\"\n",
    "            self._save_figure(fig, fig_path, \"turning distribution plot\")\n",
    "            self.figures[f'turning_distribution{suffix}'] = fig_path\n",
    "        \n",
    "        # Identify turning bouts with threshold and min duration\n",
    "        sampling_interval = pd.Timedelta(self.sliced_data.index.to_series().diff().median()).total_seconds()\n",
    "        min_bout_samples = self._calculate_min_bout_samples(sampling_interval)\n",
    "        abs_turn = np.abs(turning_values)\n",
    "        is_turning = self._detect_bouts_above_threshold(\n",
    "            abs_turn,\n",
    "            threshold=threshold,\n",
    "            min_bout_samples=min_bout_samples\n",
    "        )\n",
    "        \n",
    "        # Calculate statistics for turning bouts\n",
    "        turning_velocities = turning_values[is_turning]\n",
    "        \n",
    "        if len(turning_velocities) > 0:\n",
    "            avg_velocity = np.mean(np.abs(turning_velocities))\n",
    "            std_velocity = np.std(np.abs(turning_velocities))\n",
    "        else:\n",
    "            avg_velocity = 0.0\n",
    "            std_velocity = 0.0\n",
    "        \n",
    "        # Time calculations\n",
    "        total_time = len(self.sliced_data) * sampling_interval\n",
    "        turning_time = np.sum(is_turning) * sampling_interval\n",
    "        turning_time_pct = (turning_time / total_time) * 100\n",
    "        \n",
    "        # Distance calculation (total change in position during turning bouts)\n",
    "        if \"Position_0Y\" in self.sliced_data.columns:\n",
    "            turning_position = self.sliced_data[\"Position_0Y\"].astype(float).to_numpy()\n",
    "            turning_mask = is_turning.astype(bool)\n",
    "            \n",
    "            # Calculate total distance as the change in position during turning periods\n",
    "            if np.any(turning_mask):\n",
    "                # Get the position at the end and start of turning periods\n",
    "                turning_positions = turning_position[turning_mask]\n",
    "                if len(turning_positions) > 0:\n",
    "                    # Total distance is the absolute difference between final and initial position during turning\n",
    "                    turned_distance = float(np.abs(turning_positions[-1] - turning_positions[0]))\n",
    "                else:\n",
    "                    turned_distance = 0.0\n",
    "            else:\n",
    "                turned_distance = 0.0\n",
    "        else:\n",
    "            turned_distance = np.nan\n",
    "            print(\"⚠️ Warning: 'Position_0Y' column not found; turning distance set to NaN.\")\n",
    "        \n",
    "        # Turn direction percentages\n",
    "        left_turns = np.sum(turning_values[is_turning] < 0)\n",
    "        right_turns = np.sum(turning_values[is_turning] > 0)\n",
    "        total_turns = left_turns + right_turns\n",
    "        \n",
    "        if total_turns > 0:\n",
    "            left_pct = (left_turns / total_turns) * 100\n",
    "            right_pct = (right_turns / total_turns) * 100\n",
    "        else:\n",
    "            left_pct = 0.0\n",
    "            right_pct = 0.0\n",
    "        \n",
    "        # Store results with suffix in key\n",
    "        result_key = f'turning{suffix}' if suffix else 'turning'\n",
    "        self.results[result_key] = {\n",
    "            'threshold': threshold,\n",
    "            'avg_velocity': avg_velocity,\n",
    "            'std_velocity': std_velocity,\n",
    "            'turning_time_seconds': turning_time,\n",
    "            'turning_time_percentage': turning_time_pct,\n",
    "            'turned_distance_deg': turned_distance,\n",
    "            'left_percentage': left_pct,\n",
    "            'right_percentage': right_pct,\n",
    "            'block_duration_seconds': total_time,\n",
    "            'turning_column': selected_column\n",
    "        }\n",
    "        \n",
    "        # Save CSV\n",
    "        df = pd.DataFrame([{\n",
    "            'Mouse': self.mouse_name,\n",
    "            'Average_Velocity_deg_s': avg_velocity,\n",
    "            'SD_Velocity_deg_s': std_velocity,\n",
    "            'Turning_Time_Percentage': turning_time_pct,\n",
    "            'Turned_Distance_deg': turned_distance,\n",
    "            'Left_Turn_Percentage': left_pct,\n",
    "            'Right_Turn_Percentage': right_pct,\n",
    "            'Threshold_deg_s': threshold,\n",
    "            'Turning_Column': selected_column\n",
    "        }])\n",
    "        csv_label_suffix = f\"_{column_label}\" if column_label != \"motor\" else \"\"\n",
    "        csv_path = save_dir / f\"{self.mouse_name}_turning_stats{csv_label_suffix}{suffix}.csv\"\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        print(f\"✅ Saved turning statistics: {csv_path}\")\n",
    "        \n",
    "        # Create and save thresholded time series plot for turning (use absolute velocity for thresholding/plotting)\n",
    "        time_seconds = (self.sliced_data.index - self.sliced_data.index[0]).total_seconds()\n",
    "        ts_label_suffix = f\"_{column_label}\" if column_label != \"motor\" else \"\"\n",
    "        ts_path = save_dir / f\"{self.mouse_name}_turning_thresholded_timeseries{ts_label_suffix}{suffix}.png\"\n",
    "        self._plot_thresholded_timeseries(\n",
    "            time_seconds=time_seconds,\n",
    "            values=abs_turn,\n",
    "            threshold=threshold,\n",
    "            title=f\"Absolute Turning Velocity ({selected_column}) with Threshold - {self.mouse_name}{suffix}\",\n",
    "            ylabel=f'|{selected_column}|',\n",
    "            save_path=ts_path,\n",
    "            mask_above=(abs_turn > threshold)\n",
    "        )\n",
    "        self.figures[f'turning_thresholded_timeseries{suffix}'] = ts_path\n",
    "    \n",
    "    def analyze_platform_velocity(self, save_dir: Path, \n",
    "                                  encoder_column: str = \"Motor_Velocity\",\n",
    "                                  suffix: str = \"\"):\n",
    "        \"\"\"\n",
    "        Analyze platform velocity and cross-correlation with turning.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        save_dir : Path\n",
    "            Directory to save outputs\n",
    "        encoder_column : str\n",
    "            Column name for motor speed: Motor_velocity\n",
    "        suffix : str\n",
    "            Suffix to append to filenames (e.g., \"_first_block\", \"_last_block\")\n",
    "        \"\"\"\n",
    "        print(\"ANALYZING PLATFORM VELOCITY\")\n",
    "        \n",
    "        if encoder_column not in self.sliced_data.columns:\n",
    "            print(f\"⚠️ Warning: Column '{encoder_column}' not found in data.\")\n",
    "            print(f\"   Available columns: {self.sliced_data.columns.tolist()}\")\n",
    "            return\n",
    "        \n",
    "        motor_velocity = self.sliced_data[\"Motor_Velocity\"].values\n",
    "        turning_velocity = self.sliced_data[\"Velocity_0Y\"].values\n",
    "        \n",
    "        # Remove NaN values for correlation\n",
    "        valid_mask = ~(np.isnan(motor_velocity) | np.isnan(turning_velocity))\n",
    "        motor_valid = motor_velocity[valid_mask]\n",
    "        turning_valid = turning_velocity[valid_mask]\n",
    "        \n",
    "        if len(motor_valid) < 100:\n",
    "            print(f\"⚠️ Warning: Not enough valid data points for correlation\")\n",
    "            return\n",
    "        \n",
    "        # Cross-correlation\n",
    "        correlation = correlate(motor_valid, turning_valid, mode='full')\n",
    "        lags = np.arange(-len(motor_valid) + 1, len(motor_valid))\n",
    "        \n",
    "        # Find lag at maximum correlation\n",
    "        max_corr_idx = np.argmax(np.abs(correlation))\n",
    "        optimal_lag = lags[max_corr_idx]\n",
    "        \n",
    "        # Convert lag to time (samples to seconds)\n",
    "        sampling_interval = pd.Timedelta(self.sliced_data.index.to_series().diff().median()).total_seconds()\n",
    "        optimal_lag_time = optimal_lag * sampling_interval\n",
    "        \n",
    "        # Pearson correlation at optimal lag\n",
    "        if optimal_lag > 0:\n",
    "            enc_shifted = motor_valid[optimal_lag:]\n",
    "            turn_shifted = turning_valid[:-optimal_lag]\n",
    "        elif optimal_lag < 0:\n",
    "            enc_shifted = motor_valid[:optimal_lag]\n",
    "            turn_shifted = turning_valid[-optimal_lag:]\n",
    "        else:\n",
    "            enc_shifted = motor_valid\n",
    "            turn_shifted = turning_valid\n",
    "        \n",
    "        pearson_r, p_value = pearsonr(enc_shifted, turn_shifted)\n",
    "        \n",
    "        # Gain (divide one by the other) - use mean of absolute values\n",
    "        mean_encoder = np.mean(np.abs(enc_shifted))\n",
    "        mean_turning = np.mean(np.abs(turn_shifted))\n",
    "        \n",
    "        if mean_turning != 0:\n",
    "            gain = mean_encoder / mean_turning\n",
    "        else:\n",
    "            gain = np.nan\n",
    "        \n",
    "        # Store results with suffix in key\n",
    "        result_key = f'platform_velocity{suffix}' if suffix else 'platform_velocity'\n",
    "        self.results[result_key] = {\n",
    "            'optimal_lag_samples': optimal_lag,\n",
    "            'optimal_lag_seconds': optimal_lag_time,\n",
    "            'pearson_r': pearson_r,\n",
    "            'p_value': p_value,\n",
    "            'gain': gain,\n",
    "            'mean_motor_velocity': mean_encoder,\n",
    "            'mean_turning_velocity': mean_turning\n",
    "        }\n",
    "        \n",
    "        # Save CSV\n",
    "        df = pd.DataFrame([{\n",
    "            'Mouse': self.mouse_name,\n",
    "            'Optimal_Lag_Samples': optimal_lag,\n",
    "            'Optimal_Lag_Seconds': optimal_lag_time,\n",
    "            'Pearson_R': pearson_r,\n",
    "            'P_Value': p_value,\n",
    "            'Gain_Encoder_to_Turning': gain\n",
    "        }])\n",
    "        csv_path = save_dir / f\"{self.mouse_name}_platform_velocity_stats{suffix}.csv\"\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        print(f\"✅ Saved platform velocity statistics: {csv_path}\")\n",
    "        \n",
    "        # Create cross-correlation plot\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
    "        \n",
    "        # Plot cross-correlation\n",
    "        lag_times = lags * sampling_interval\n",
    "        ax1.plot(lag_times, correlation)\n",
    "        ax1.axvline(optimal_lag_time, color='r', linestyle='--', \n",
    "                   label=f'Optimal lag = {optimal_lag_time:.3f}s')\n",
    "        ax1.set_xlabel('Lag (seconds)', fontsize=12)\n",
    "        ax1.set_ylabel('Cross-correlation', fontsize=12)\n",
    "        ax1.set_title(f'Cross-correlation: flowY vs Motor Velocity - {self.mouse_name}{suffix}', \n",
    "                     fontsize=14, fontweight='bold')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot scatter at optimal lag\n",
    "        ax2.scatter(enc_shifted, turn_shifted, alpha=0.1, s=1)\n",
    "        ax2.set_xlabel('motor Velocity (degrees/s)', fontsize=12)\n",
    "        ax2.set_ylabel('flowY Velocity (degrees/s)', fontsize=12)\n",
    "        ax2.set_title(f'motor vs flowY (Lag={optimal_lag_time:.3f}s, r={pearson_r:.3f})', \n",
    "                     fontsize=12)\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        fig_path = save_dir / f\"{self.mouse_name}_cross_correlation{suffix}.png\"\n",
    "        self._save_figure(fig, fig_path, \"cross-correlation plot\")\n",
    "        self.figures[f'cross_correlation{suffix}'] = fig_path\n",
    "\n",
    "    def analyze_pupil_correlations(self, save_dir: Path, suffix: str = \"\") -> None:\n",
    "        \"\"\"\n",
    "        Correlate pupil diameter signals with forward velocity.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        save_dir : Path\n",
    "            Directory to save correlation CSV.\n",
    "        suffix : str\n",
    "            Suffix for interval-specific outputs.\n",
    "        \"\"\"\n",
    "        pupil_columns = [\n",
    "            (\"Pupil.Diameter_eye1\", \"eye1\"),\n",
    "            (\"Pupil.Diameter_eye2\", \"eye2\")\n",
    "        ]\n",
    "\n",
    "        rows = []\n",
    "        results_payload = {}\n",
    "\n",
    "        for column_name, eye_label in pupil_columns:\n",
    "            if column_name not in self.sliced_data.columns:\n",
    "                print(f\"ℹ️ Pupil column '{column_name}' not present, skipping correlation.\")\n",
    "                results_payload[f'{eye_label}_velocity_corr'] = np.nan\n",
    "                results_payload[f'{eye_label}_velocity_p_value'] = np.nan\n",
    "                results_payload[f'{eye_label}_valid_samples'] = 0\n",
    "                continue\n",
    "\n",
    "            subset = self.sliced_data[[column_name, \"Velocity_0X\"]].dropna()\n",
    "            sample_count = len(subset)\n",
    "\n",
    "            if sample_count < 30:\n",
    "                print(f\"⚠️ Not enough samples ({sample_count}) to correlate {column_name} with Velocity_0X.\")\n",
    "                corr_value = np.nan\n",
    "                p_value = np.nan\n",
    "            else:\n",
    "                corr_value, p_value = pearsonr(subset[column_name].values, subset[\"Velocity_0X\"].values)\n",
    "                print(f\"✅ Correlated {column_name} with Velocity_0X: r={corr_value:.3f}, p={p_value:.3e} (n={sample_count})\")\n",
    "\n",
    "            rows.append({\n",
    "                \"Eye\": eye_label,\n",
    "                \"Pupil_Column\": column_name,\n",
    "                \"Samples\": sample_count,\n",
    "                \"Pearson_r\": corr_value,\n",
    "                \"p_value\": p_value\n",
    "            })\n",
    "\n",
    "            results_payload[f'{eye_label}_velocity_corr'] = corr_value\n",
    "            results_payload[f'{eye_label}_velocity_p_value'] = p_value\n",
    "            results_payload[f'{eye_label}_valid_samples'] = sample_count\n",
    "\n",
    "        if rows:\n",
    "            df = pd.DataFrame(rows)\n",
    "            csv_path = save_dir / f\"{self.mouse_name}_pupil_velocity_correlation{suffix}.csv\"\n",
    "            df.to_csv(csv_path, index=False)\n",
    "            print(f\"✅ Saved pupil correlation stats: {csv_path}\")\n",
    "\n",
    "        result_key = f'pupil_metrics{suffix}' if suffix else 'pupil_metrics'\n",
    "        self.results[result_key] = results_payload\n",
    "\n",
    "    def summarize_saccade_types(self, save_dir: Path, suffix: str = \"\") -> None:\n",
    "        \"\"\"\n",
    "        Quantify orienting vs compensatory saccades within the sliced interval.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        save_dir : Path\n",
    "            Directory to save summary CSV.\n",
    "        suffix : str\n",
    "            Suffix for interval-specific outputs.\n",
    "        \"\"\"\n",
    "        interval_start = self.sliced_data.index[0]\n",
    "        interval_end = self.sliced_data.index[-1]\n",
    "        duration_seconds = (interval_end - interval_start).total_seconds()\n",
    "        duration_minutes = duration_seconds / 60 if duration_seconds > 0 else np.nan\n",
    "\n",
    "        def load_summary(video_key: str) -> Optional[pd.DataFrame]:\n",
    "            path = self.data_path / f\"{video_key}_saccade_summary.csv\"\n",
    "            if not path.exists():\n",
    "                print(f\"ℹ️ No saccade summary CSV for {video_key} at {path}\")\n",
    "                return None\n",
    "            try:\n",
    "                df = pd.read_csv(path)\n",
    "                if \"aeon_time\" not in df.columns:\n",
    "                    print(f\"⚠️ Column 'aeon_time' missing in {path.name}, skipping.\")\n",
    "                    return None\n",
    "                df[\"aeon_time\"] = pd.to_datetime(df[\"aeon_time\"])\n",
    "                return df\n",
    "            except Exception as exc:\n",
    "                print(f\"⚠️ Failed to load {path.name}: {exc}\")\n",
    "                return None\n",
    "\n",
    "        metrics_rows = []\n",
    "        combined_orienting = 0\n",
    "        combined_compensatory = 0\n",
    "        combined_total = 0\n",
    "\n",
    "        per_eye_results = {}\n",
    "\n",
    "        for video_key, eye_label in ((\"VideoData1\", \"eye1\"), (\"VideoData2\", \"eye2\")):\n",
    "            df = load_summary(video_key)\n",
    "            if df is None or df.empty:\n",
    "                per_eye_results[eye_label] = {\n",
    "                    \"total\": 0,\n",
    "                    \"orienting\": 0,\n",
    "                    \"compensatory\": 0,\n",
    "                    \"orienting_pct\": np.nan,\n",
    "                    \"compensatory_pct\": np.nan,\n",
    "                    \"orienting_rate_per_min\": np.nan,\n",
    "                    \"compensatory_rate_per_min\": np.nan\n",
    "                }\n",
    "                continue\n",
    "\n",
    "            mask = (df[\"aeon_time\"] >= interval_start) & (df[\"aeon_time\"] <= interval_end)\n",
    "            subset = df.loc[mask]\n",
    "\n",
    "            if subset.empty:\n",
    "                per_eye_results[eye_label] = {\n",
    "                    \"total\": 0,\n",
    "                    \"orienting\": 0,\n",
    "                    \"compensatory\": 0,\n",
    "                    \"orienting_pct\": np.nan,\n",
    "                    \"compensatory_pct\": np.nan,\n",
    "                    \"orienting_rate_per_min\": np.nan,\n",
    "                    \"compensatory_rate_per_min\": np.nan\n",
    "                }\n",
    "                continue\n",
    "\n",
    "            counts = subset[\"saccade_type\"].value_counts()\n",
    "            orienting = counts.get(\"orienting\", 0)\n",
    "            compensatory = counts.get(\"compensatory\", 0)\n",
    "            total = orienting + compensatory\n",
    "\n",
    "            orienting_pct = (orienting / total * 100) if total > 0 else np.nan\n",
    "            compensatory_pct = (compensatory / total * 100) if total > 0 else np.nan\n",
    "\n",
    "            orienting_rate = (orienting / duration_minutes) if duration_minutes and total > 0 else np.nan\n",
    "            compensatory_rate = (compensatory / duration_minutes) if duration_minutes and total > 0 else np.nan\n",
    "\n",
    "            combined_orienting += orienting\n",
    "            combined_compensatory += compensatory\n",
    "            combined_total += total\n",
    "\n",
    "            per_eye_results[eye_label] = {\n",
    "                \"total\": total,\n",
    "                \"orienting\": orienting,\n",
    "                \"compensatory\": compensatory,\n",
    "                \"orienting_pct\": orienting_pct,\n",
    "                \"compensatory_pct\": compensatory_pct,\n",
    "                \"orienting_rate_per_min\": orienting_rate,\n",
    "                \"compensatory_rate_per_min\": compensatory_rate\n",
    "            }\n",
    "\n",
    "            metrics_rows.append({\n",
    "                \"Eye\": eye_label,\n",
    "                \"Total_Saccades\": total,\n",
    "                \"Orienting_Count\": orienting,\n",
    "                \"Orienting_Percentage\": orienting_pct,\n",
    "                \"Orienting_Rate_per_min\": orienting_rate,\n",
    "                \"Compensatory_Count\": compensatory,\n",
    "                \"Compensatory_Percentage\": compensatory_pct,\n",
    "                \"Compensatory_Rate_per_min\": compensatory_rate,\n",
    "                \"Duration_seconds\": duration_seconds\n",
    "            })\n",
    "\n",
    "        if combined_total > 0:\n",
    "            combined_orienting_pct = combined_orienting / combined_total * 100\n",
    "            combined_compensatory_pct = combined_compensatory / combined_total * 100\n",
    "            combined_orienting_rate = (combined_orienting / duration_minutes) if duration_minutes else np.nan\n",
    "            combined_compensatory_rate = (combined_compensatory / duration_minutes) if duration_minutes else np.nan\n",
    "        else:\n",
    "            combined_orienting_pct = np.nan\n",
    "            combined_compensatory_pct = np.nan\n",
    "            combined_orienting_rate = np.nan\n",
    "            combined_compensatory_rate = np.nan\n",
    "\n",
    "        if metrics_rows:\n",
    "            metrics_rows.append({\n",
    "                \"Eye\": \"combined\",\n",
    "                \"Total_Saccades\": combined_total,\n",
    "                \"Orienting_Count\": combined_orienting,\n",
    "                \"Orienting_Percentage\": combined_orienting_pct,\n",
    "                \"Orienting_Rate_per_min\": combined_orienting_rate,\n",
    "                \"Compensatory_Count\": combined_compensatory,\n",
    "                \"Compensatory_Percentage\": combined_compensatory_pct,\n",
    "                \"Compensatory_Rate_per_min\": combined_compensatory_rate,\n",
    "                \"Duration_seconds\": duration_seconds\n",
    "            })\n",
    "            df_metrics = pd.DataFrame(metrics_rows)\n",
    "            csv_path = save_dir / f\"{self.mouse_name}_saccade_type_summary{suffix}.csv\"\n",
    "            df_metrics.to_csv(csv_path, index=False)\n",
    "            print(f\"✅ Saved saccade type summary: {csv_path}\")\n",
    "\n",
    "        result_key = f'saccade_metrics{suffix}' if suffix else 'saccade_metrics'\n",
    "        self.results[result_key] = {\n",
    "            \"duration_seconds\": duration_seconds,\n",
    "            \"eye1\": per_eye_results.get(\"eye1\", {}),\n",
    "            \"eye2\": per_eye_results.get(\"eye2\", {}),\n",
    "            \"combined\": {\n",
    "                \"total\": combined_total,\n",
    "                \"orienting\": combined_orienting,\n",
    "                \"compensatory\": combined_compensatory,\n",
    "                \"orienting_pct\": combined_orienting_pct,\n",
    "                \"compensatory_pct\": combined_compensatory_pct,\n",
    "                \"orienting_rate_per_min\": combined_orienting_rate,\n",
    "                \"compensatory_rate_per_min\": combined_compensatory_rate\n",
    "            }\n",
    "        }\n",
    "\n",
    "    \n",
    "    def run_full_analysis(self, output_dir: Path, encoder_column: str = \"Motor_Velocity\",\n",
    "                         experiment_day: str = None):\n",
    "        \"\"\"\n",
    "        Run complete analysis pipeline.\n",
    "        \n",
    "        For visual mismatch experiments, runs analysis twice:\n",
    "        1. First interval: until first \"Block timer elapsed\"\n",
    "        2. Second interval: until last \"Block timer elapsed\"\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        output_dir : Path\n",
    "            Directory to save all outputs\n",
    "        encoder_column : str\n",
    "            Column name for encoder velocity\n",
    "        experiment_day : str\n",
    "            Experiment day identifier (e.g., \"Day1\", \"Baseline\")\n",
    "        \"\"\"\n",
    "        # Create output directory\n",
    "        save_dir = data_dir / self.mouse_name\n",
    "        save_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Store experiment day for later use\n",
    "        self.experiment_day = experiment_day if experiment_day else \"Unknown\"\n",
    "        \n",
    "        # Check if this is a visual mismatch experiment\n",
    "        is_visual_mismatch = self._is_visual_mismatch_experiment()\n",
    "        \n",
    "        if is_visual_mismatch:\n",
    "            # Visual mismatch: analyze two intervals\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"VISUAL MISMATCH EXPERIMENT DETECTED\")\n",
    "            print(f\"STARTING FULL ANALYSIS FOR: {self.mouse_name}\")\n",
    "            print(f\"Experiment Day: {self.experiment_day}\")\n",
    "            print(f\"Output directory: {save_dir}\")\n",
    "            print(f\"{'='*60}\")\n",
    "            \n",
    "            # First interval: until first block timer elapsed\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"ANALYZING FIRST INTERVAL (until first Block timer elapsed)\")\n",
    "            print(f\"{'='*60}\")\n",
    "            self.slice_data_until_block_timer_elapsed(interval=\"first_block\")\n",
    "            self.analyze_running(save_dir, suffix=\"_first_block\")\n",
    "            self.analyze_turning(save_dir, suffix=\"_first_block\")\n",
    "            self.analyze_platform_velocity(save_dir, encoder_column, suffix=\"_first_block\")\n",
    "            self.analyze_pupil_correlations(save_dir, suffix=\"_first_block\")\n",
    "            self.summarize_saccade_types(save_dir, suffix=\"_first_block\")\n",
    "            \n",
    "            # Second interval: until last block timer elapsed\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"ANALYZING SECOND INTERVAL (until last Block timer elapsed)\")\n",
    "            print(f\"{'='*60}\")\n",
    "            self.slice_data_until_block_timer_elapsed(interval=\"last_block\")\n",
    "            self.analyze_running(save_dir, suffix=\"_last_block\")\n",
    "            self.analyze_turning(save_dir, suffix=\"_last_block\")\n",
    "            self.analyze_platform_velocity(save_dir, encoder_column, suffix=\"_last_block\")\n",
    "            self.analyze_pupil_correlations(save_dir, suffix=\"_last_block\")\n",
    "            self.summarize_saccade_types(save_dir, suffix=\"_last_block\")\n",
    "            \n",
    "        else:\n",
    "            # Standard analysis: first 10 minutes\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"STARTING FULL ANALYSIS FOR: {self.mouse_name}\")\n",
    "            print(f\"Experiment Day: {self.experiment_day}\")\n",
    "            print(f\"Output directory: {save_dir}\")\n",
    "            print(f\"{'='*60}\")\n",
    "            \n",
    "            # Step 1: Slice data\n",
    "            self.slice_data_until_block_timer_elapsed(interval=\"first_10min\")\n",
    "            \n",
    "            # Step 2: Analyze running\n",
    "            self.analyze_running(save_dir)\n",
    "            \n",
    "            # Step 3: Analyze turning\n",
    "            self.analyze_turning(save_dir)\n",
    "            \n",
    "            # Step 4: Analyze platform velocity\n",
    "            self.analyze_platform_velocity(save_dir, encoder_column)\n",
    "            \n",
    "            # Step 5: Analyze pupil correlations\n",
    "            self.analyze_pupil_correlations(save_dir)\n",
    "\n",
    "            # Step 6: Summarize saccade types\n",
    "            self.summarize_saccade_types(save_dir)\n",
    "        \n",
    "        # Save summary\n",
    "        summary_path = save_dir / f\"{self.mouse_name}_analysis_summary.json\"\n",
    "        with open(summary_path, 'w') as f:\n",
    "            import json\n",
    "            json.dump(self.results, f, indent=2, default=str)\n",
    "        print(f\"\\n✅ Saved analysis summary: {summary_path}\")\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"✅ ANALYSIS COMPLETE FOR: {self.mouse_name}\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "        \n",
    "        return self.results, self.figures\n",
    "    \n",
    "    def get_cohort_row_data(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Get data formatted for cohort-level CSV.\n",
    "        \n",
    "        For visual mismatch experiments, includes data for both intervals.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        Dict with Animal_ID, Experiment_Day, and all behavioral variables\n",
    "        \"\"\"\n",
    "        row_data = {\n",
    "            'Animal_ID': self.mouse_name,\n",
    "            'Experiment_Day': self.experiment_day\n",
    "        }\n",
    "        \n",
    "        # Helper function to add data with prefix\n",
    "        def add_behavioral_data(result_key, prefix):\n",
    "            if result_key in self.results:\n",
    "                data = self.results[result_key]\n",
    "                # Handle prefix: if empty, no prefix; otherwise add underscore\n",
    "                prefix_str = f\"{prefix}_\" if prefix else \"\"\n",
    "                \n",
    "                if 'running' in result_key:\n",
    "                    row_data.update({\n",
    "                        f'{prefix_str}running_threshold_m_per_s': data['threshold'],\n",
    "                        f'{prefix_str}running_velocity_avg_m_per_s': data['avg_velocity'],\n",
    "                        f'{prefix_str}running_velocity_sd_m_per_s': data['std_velocity'],\n",
    "                        f'{prefix_str}running_time_percentage': data['running_time_percentage'],\n",
    "                        f'{prefix_str}running_distance_travelled_m': data['travelled_distance_m'],\n",
    "                        f'{prefix_str}running_time_seconds': data['running_time_seconds'],\n",
    "                        f'{prefix_str}running_block_duration_seconds': data['block_duration_seconds']\n",
    "                    })\n",
    "                elif 'turning' in result_key:\n",
    "                    row_data.update({\n",
    "                        f'{prefix_str}turning_threshold_deg_per_s': data['threshold'],\n",
    "                        f'{prefix_str}turning_velocity_avg_deg_per_s': data['avg_velocity'],\n",
    "                        f'{prefix_str}turning_velocity_sd_deg_per_s': data['std_velocity'],\n",
    "                        f'{prefix_str}turning_time_percentage': data['turning_time_percentage'],\n",
    "                        f'{prefix_str}turning_distance_turned_deg': data['turned_distance_deg'],\n",
    "                        f'{prefix_str}turning_left_percentage': data['left_percentage'],\n",
    "                        f'{prefix_str}turning_right_percentage': data['right_percentage'],\n",
    "                        f'{prefix_str}turning_time_seconds': data['turning_time_seconds'],\n",
    "                        f'{prefix_str}turning_block_duration_seconds': data['block_duration_seconds']\n",
    "                    })\n",
    "                elif 'platform_velocity' in result_key:\n",
    "                    row_data.update({\n",
    "                        f'{prefix_str}platform_cross_corr_lag_samples': data['optimal_lag_samples'],\n",
    "                        f'{prefix_str}platform_cross_corr_lag_seconds': data['optimal_lag_seconds'],\n",
    "                        f'{prefix_str}platform_cross_corr_pearson_r': data['pearson_r'],\n",
    "                        f'{prefix_str}platform_cross_corr_p_value': data['p_value'],\n",
    "                        f'{prefix_str}platform_gain_encoder_to_turning': data['gain'],\n",
    "                        f'{prefix_str}platform_mean_motor_velocity_m_per_s': data['mean_motor_velocity'],\n",
    "                        f'{prefix_str}platform_mean_turning_velocity_m_per_s': data['mean_turning_velocity']\n",
    "                    })\n",
    "                elif 'pupil_metrics' in result_key:\n",
    "                    row_data.update({\n",
    "                        f'{prefix_str}pupil_eye1_velocity_corr': data.get('eye1_velocity_corr', np.nan),\n",
    "                        f'{prefix_str}pupil_eye1_velocity_p_value': data.get('eye1_velocity_p_value', np.nan),\n",
    "                        f'{prefix_str}pupil_eye1_valid_samples': data.get('eye1_valid_samples', 0),\n",
    "                        f'{prefix_str}pupil_eye2_velocity_corr': data.get('eye2_velocity_corr', np.nan),\n",
    "                        f'{prefix_str}pupil_eye2_velocity_p_value': data.get('eye2_velocity_p_value', np.nan),\n",
    "                        f'{prefix_str}pupil_eye2_valid_samples': data.get('eye2_valid_samples', 0)\n",
    "                    })\n",
    "                elif 'saccade_metrics' in result_key:\n",
    "                    eye1_metrics = data.get('eye1', {})\n",
    "                    eye2_metrics = data.get('eye2', {})\n",
    "                    combined_metrics = data.get('combined', {})\n",
    "\n",
    "                    row_data.update({\n",
    "                        f'{prefix_str}saccade_duration_seconds': data.get('duration_seconds', np.nan),\n",
    "                        f'{prefix_str}saccade_eye1_total': eye1_metrics.get('total', 0),\n",
    "                        f'{prefix_str}saccade_eye1_orienting_count': eye1_metrics.get('orienting', 0),\n",
    "                        f'{prefix_str}saccade_eye1_orienting_pct': eye1_metrics.get('orienting_pct', np.nan),\n",
    "                        f'{prefix_str}saccade_eye1_compensatory_count': eye1_metrics.get('compensatory', 0),\n",
    "                        f'{prefix_str}saccade_eye1_compensatory_pct': eye1_metrics.get('compensatory_pct', np.nan),\n",
    "                        f'{prefix_str}saccade_eye1_orienting_rate_per_min': eye1_metrics.get('orienting_rate_per_min', np.nan),\n",
    "                        f'{prefix_str}saccade_eye1_compensatory_rate_per_min': eye1_metrics.get('compensatory_rate_per_min', np.nan),\n",
    "                        f'{prefix_str}saccade_eye2_total': eye2_metrics.get('total', 0),\n",
    "                        f'{prefix_str}saccade_eye2_orienting_count': eye2_metrics.get('orienting', 0),\n",
    "                        f'{prefix_str}saccade_eye2_orienting_pct': eye2_metrics.get('orienting_pct', np.nan),\n",
    "                        f'{prefix_str}saccade_eye2_compensatory_count': eye2_metrics.get('compensatory', 0),\n",
    "                        f'{prefix_str}saccade_eye2_compensatory_pct': eye2_metrics.get('compensatory_pct', np.nan),\n",
    "                        f'{prefix_str}saccade_eye2_orienting_rate_per_min': eye2_metrics.get('orienting_rate_per_min', np.nan),\n",
    "                        f'{prefix_str}saccade_eye2_compensatory_rate_per_min': eye2_metrics.get('compensatory_rate_per_min', np.nan),\n",
    "                        f'{prefix_str}saccade_combined_total': combined_metrics.get('total', 0),\n",
    "                        f'{prefix_str}saccade_combined_orienting_count': combined_metrics.get('orienting', 0),\n",
    "                        f'{prefix_str}saccade_combined_orienting_pct': combined_metrics.get('orienting_pct', np.nan),\n",
    "                        f'{prefix_str}saccade_combined_compensatory_count': combined_metrics.get('compensatory', 0),\n",
    "                        f'{prefix_str}saccade_combined_compensatory_pct': combined_metrics.get('compensatory_pct', np.nan),\n",
    "                        f'{prefix_str}saccade_combined_orienting_rate_per_min': combined_metrics.get('orienting_rate_per_min', np.nan),\n",
    "                        f'{prefix_str}saccade_combined_compensatory_rate_per_min': combined_metrics.get('compensatory_rate_per_min', np.nan)\n",
    "                    })\n",
    "        \n",
    "        # Check if this is visual mismatch (has both intervals)\n",
    "        has_first_block = 'running_first_block' in self.results\n",
    "        has_last_block = 'running_last_block' in self.results\n",
    "        \n",
    "        if has_first_block and has_last_block:\n",
    "            # Visual mismatch: add both intervals\n",
    "            add_behavioral_data('running_first_block', 'first_block')\n",
    "            add_behavioral_data('turning_first_block', 'first_block')\n",
    "            add_behavioral_data('platform_velocity_first_block', 'first_block')\n",
    "            add_behavioral_data('pupil_metrics_first_block', 'first_block')\n",
    "            add_behavioral_data('saccade_metrics_first_block', 'first_block')\n",
    "            \n",
    "            add_behavioral_data('running_last_block', 'last_block')\n",
    "            add_behavioral_data('turning_last_block', 'last_block')\n",
    "            add_behavioral_data('platform_velocity_last_block', 'last_block')\n",
    "            add_behavioral_data('pupil_metrics_last_block', 'last_block')\n",
    "            add_behavioral_data('saccade_metrics_last_block', 'last_block')\n",
    "        else:\n",
    "            # Standard: add single interval data\n",
    "            add_behavioral_data('running', '')\n",
    "            add_behavioral_data('turning', '')\n",
    "            add_behavioral_data('platform_velocity', '')\n",
    "            add_behavioral_data('pupil_metrics', '')\n",
    "            add_behavioral_data('saccade_metrics', '')\n",
    "        \n",
    "        return row_data\n",
    "\n",
    "# Example usage with your existing code:\n",
    "def run_behavioral_analysis(loaded_data: Dict[Path, Dict], \n",
    "                           output_base_dir: Path,\n",
    "                           encoder_column: str = \"Motor_Velocity\",\n",
    "                           experiment_day: str = None,\n",
    "                           plot_bins: int = 50,\n",
    "                           min_bout_duration_s: float = 0.2,\n",
    "                           running_percentile: float = 10,\n",
    "                           turning_percentile: Optional[float] = None,\n",
    "                           turning_velocity_column: str = \"Motor_Velocity\"):\n",
    "    \"\"\"\n",
    "    Run behavioral analysis for all loaded data paths.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    loaded_data : Dict\n",
    "        Dictionary from your data loading code\n",
    "    output_base_dir : Path\n",
    "        Base directory for saving all outputs\n",
    "    encoder_column : str\n",
    "        Column name for encoder velocity data\n",
    "    experiment_day : str\n",
    "        Experiment day identifier (e.g., \"Day1\", \"Baseline\")\n",
    "    plot_bins : int\n",
    "        Number of bins for threshold plots\n",
    "    min_bout_duration_s : float\n",
    "        Minimum duration for a bout to be considered valid (seconds).\n",
    "        This is converted to samples internally based on the data sampling rate.\n",
    "    running_percentile : float\n",
    "        Percentile to use for running threshold calculation (default: 10)\n",
    "    turning_percentile : Optional[float]\n",
    "        Percentile to use on values below median for turning threshold (default: 25).\n",
    "        If None, uses 25th percentile.\n",
    "    turning_velocity_column : str\n",
    "        Column to use for turning analysis (e.g., \"Motor_Velocity\" or \"Velocity_0Y\").\n",
    "    \"\"\"\n",
    "    output_base_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    all_results = {}\n",
    "    cohort_data = []\n",
    "    \n",
    "    for data_path, data_dict in loaded_data.items():\n",
    "        try:\n",
    "            print(f\"\\n{'#'*60}\")\n",
    "            print(f\"Processing: {data_path.name}\")\n",
    "            print(f\"{'#'*60}\")\n",
    "            \n",
    "            # Determine experiment day from path if not provided\n",
    "            if experiment_day is None:\n",
    "                # Try to extract from path (customize this logic as needed)\n",
    "                exp_day = extract_experiment_day_from_path(data_path)\n",
    "            else:\n",
    "                exp_day = experiment_day\n",
    "            \n",
    "            analyzer = BehavioralAnalyzer(\n",
    "                data_path,\n",
    "                data_dict,\n",
    "                threshold_plot_bins=plot_bins,\n",
    "                min_bout_duration_s=min_bout_duration_s,\n",
    "                running_percentile=running_percentile,\n",
    "                turning_percentile=turning_percentile,\n",
    "                turning_velocity_column=turning_velocity_column\n",
    "            )\n",
    "            results, figures = analyzer.run_full_analysis(output_base_dir, encoder_column, exp_day)\n",
    "            \n",
    "            all_results[data_path] = {\n",
    "                'results': results,\n",
    "                'figures': figures,\n",
    "                'analyzer': analyzer\n",
    "            }\n",
    "            \n",
    "            # Collect data for cohort CSV\n",
    "            cohort_data.append(analyzer.get_cohort_row_data())\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ ERROR analyzing {data_path}: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "    \n",
    "    # Save cohort-level CSV\n",
    "    if cohort_data:\n",
    "        save_cohort_csv(cohort_data, output_base_dir)\n",
    "    \n",
    "    print(f\"\\n{'#'*60}\")\n",
    "    print(f\"✅ COMPLETED ANALYSIS FOR {len(all_results)}/{len(loaded_data)} DATASETS\")\n",
    "    print(f\"{'#'*60}\\n\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "def extract_experiment_day_from_path(data_path: Path) -> str:\n",
    "    \"\"\"\n",
    "    Extract experiment day from the data path.\n",
    "    Customize this function based on your naming conventions.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data_path : Path\n",
    "        Path to the processed data\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    str : Experiment day identifier\n",
    "    \"\"\"\n",
    "    # Example: Extract from parent directory name\n",
    "    # Adjust this logic based on your folder structure\n",
    "    path_parts = data_path.parts\n",
    "    \n",
    "    # Look for common day identifiers in path\n",
    "    for part in reversed(path_parts):\n",
    "        part_lower = part.lower()\n",
    "        if 'day' in part_lower or 'baseline' in part_lower or 'test' in part_lower:\n",
    "            return part\n",
    "    \n",
    "    # If no day found, try to extract from the immediate parent\n",
    "    parent_name = data_path.parent.name\n",
    "    if not parent_name.endswith('_processedData'):\n",
    "        return parent_name\n",
    "    \n",
    "    # Fallback: use grandparent name\n",
    "    return data_path.parent.parent.name\n",
    "\n",
    "# def save_cohort_csv(cohort_data: List[Dict[str, Any]], output_base_dir: Path):\n",
    "def save_cohort_csv(cohort_data: List[Dict[str, Any]], data_dir: Path):\n",
    "\n",
    "    \"\"\"\n",
    "    Save or append cohort-level behavioral data to CSV.\n",
    "    Creates file 2 levels above _processedData folder.\n",
    "    Appends new data but overwrites if Animal_ID + Experiment_Day already exists.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    cohort_data : List[Dict]\n",
    "        List of dictionaries containing behavioral data for each animal\n",
    "\n",
    "    \"\"\"\n",
    "    # Navigate to cohort level (2 levels above _processedData)\n",
    "    # Find the first data path to determine cohort directory\n",
    "    cohort_dir = data_dir.parent.parent if '_processedData' in str(data_dir) else cohort_data_dir\n",
    "    \n",
    "    # Create cohort CSV path\n",
    "    cohort_csv_path = cohort_dir / \"cohort_behavioral_analysis.csv\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"SAVING COHORT-LEVEL CSV\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Cohort CSV path: {cohort_csv_path}\")\n",
    "    \n",
    "    # Convert new data to DataFrame\n",
    "    new_df = pd.DataFrame(cohort_data)\n",
    "    \n",
    "    # Ensure Animal_ID and Experiment_Day are first columns\n",
    "    cols = ['Animal_ID', 'Experiment_Day'] + [col for col in new_df.columns \n",
    "                                               if col not in ['Animal_ID', 'Experiment_Day']]\n",
    "    new_df = new_df[cols]\n",
    "    \n",
    "    # Check if file exists\n",
    "    if cohort_csv_path.exists():\n",
    "        print(f\"📄 Existing cohort CSV found, loading...\")\n",
    "        existing_df = pd.read_csv(cohort_csv_path)\n",
    "        \n",
    "        # Check if columns match\n",
    "        existing_cols = set(existing_df.columns)\n",
    "        new_cols = set(new_df.columns)\n",
    "        \n",
    "        if existing_cols != new_cols:\n",
    "            print(f\"⚠️  Column mismatch detected:\")\n",
    "            print(f\"   Columns in existing file: {sorted(existing_cols)}\")\n",
    "            print(f\"   Columns in new data: {sorted(new_cols)}\")\n",
    "            \n",
    "            # Add missing columns to existing data\n",
    "            for col in new_cols - existing_cols:\n",
    "                existing_df[col] = np.nan\n",
    "                print(f\"   ➕ Added missing column: {col}\")\n",
    "            \n",
    "            # Add missing columns to new data\n",
    "            for col in existing_cols - new_cols:\n",
    "                new_df[col] = np.nan\n",
    "                print(f\"   ➕ Added missing column to new data: {col}\")\n",
    "        \n",
    "        # Remove rows with matching Animal_ID and Experiment_Day\n",
    "        for _, row in new_df.iterrows():\n",
    "            animal_id = row['Animal_ID']\n",
    "            exp_day = row['Experiment_Day']\n",
    "            \n",
    "            mask = (existing_df['Animal_ID'] == animal_id) & (existing_df['Experiment_Day'] == exp_day)\n",
    "            if mask.any():\n",
    "                print(f\"   🔄 Overwriting existing data for {animal_id} - {exp_day}\")\n",
    "                existing_df = existing_df[~mask]\n",
    "        \n",
    "        # Append new data\n",
    "        combined_df = pd.concat([existing_df, new_df], ignore_index=True)\n",
    "        \n",
    "        # Sort by Animal_ID and Experiment_Day\n",
    "        combined_df = combined_df.sort_values(['Animal_ID', 'Experiment_Day']).reset_index(drop=True)\n",
    "        \n",
    "        print(f\"   ✅ Appended {len(new_df)} rows to existing data\")\n",
    "        print(f\"   📊 Total rows in cohort CSV: {len(combined_df)}\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"📄 Creating new cohort CSV file...\")\n",
    "        combined_df = new_df\n",
    "        print(f\"   ✅ Created new file with {len(combined_df)} rows\")\n",
    "    \n",
    "    # Save to CSV\n",
    "    combined_df.to_csv(cohort_csv_path, index=False)\n",
    "    print(f\"✅ Saved cohort CSV: {cohort_csv_path}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e0e4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_behavioral_analysis(loaded_data, data_dir, encoder_column=\"Motor_Velocity\", min_bout_duration_s=1, running_percentile=10, turning_velocity_column= \"Motor_Velocity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260d4f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create DFs and plot figure for each data path\n",
    "#---------------------------------------------------\n",
    "# Dictionary to store analysis results for each data path\n",
    "data_path_variables = {}\n",
    "\n",
    "for idx, data_path in enumerate(data_paths, start=1):\n",
    "    print(f\"\\n--------- Processing analysis for data path {idx}/{len(data_paths)}: {data_path} ---------\")\n",
    "    \n",
    "    # Skip if data wasn't successfully loaded for this path\n",
    "    if data_path not in loaded_data:\n",
    "        print(f\"⚠️ Skipping analysis for {data_path} - data not loaded successfully\")\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        # Extract data from loaded_data dictionary\n",
    "        photometry_tracking_encoder_data = loaded_data[data_path][\"photometry_tracking_encoder_data\"]\n",
    "        camera_photodiode_data = loaded_data[data_path][\"camera_photodiode_data\"]\n",
    "        experiment_events = loaded_data[data_path][\"experiment_events\"]\n",
    "        mouse_name = loaded_data[data_path][\"mouse_name\"]\n",
    "        session_name = f\"{mouse_name}_{data_path.name}\"  # Assuming session_name is constructed this way\n",
    "        \n",
    "        # Create dataframe to analyze\n",
    "        df_to_analyze = photometry_tracking_encoder_data[\"Photodiode_int\"]  # Using downsampled values in common time grid\n",
    "        # df_to_analyze = camera_photodiode_data[\"Photodiode\"]  # Use async raw values if needed for troubleshooting\n",
    "        \n",
    "        # Determine halt times based on different conditions\n",
    "        if vestibular_mismatch or event_name == \"No halt\":  # Determine halt times based on experiment events\n",
    "            events_matching_name = experiment_events[experiment_events[\"Event\"] == event_name]\n",
    "            if events_matching_name.empty:\n",
    "                print(f\"⚠️ WARNING: No events found with name '{event_name}', skipping this data path\")\n",
    "                continue\n",
    "                \n",
    "            photodiode_halts = events_matching_name.index.tolist()\n",
    "            nearest_indices = photometry_tracking_encoder_data.index.get_indexer(photodiode_halts, method='nearest')\n",
    "            photodiode_halts = photometry_tracking_encoder_data.index[nearest_indices]  # Align to downsampled data time grid\n",
    "            print(f\"ℹ️ INFO: vestibular MM or 'No halt', no signal in the photodiode, using experiment events for MM times\")\n",
    "            photodiode_delay_min = photodiode_delay_avg = photodiode_delay_max = None\n",
    "        else:  # Determine exact halt times based on photodiode signal\n",
    "            try:\n",
    "                photodiode_halts, photodiode_delay_min, photodiode_delay_avg, photodiode_delay_max = process.analyze_photodiode(\n",
    "                    df_to_analyze, experiment_events, event_name, plot=True\n",
    "                )\n",
    "                print(f\"✅ Successfully analyzed photodiode signal for {data_path.name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ ERROR analyzing photodiode signal: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        # Store analysis results\n",
    "        data_path_variables[data_path] = {\n",
    "            \"photodiode_halts\": photodiode_halts,\n",
    "            \"photodiode_delay_min\": photodiode_delay_min,\n",
    "            \"photodiode_delay_avg\": photodiode_delay_avg,\n",
    "            \"photodiode_delay_max\": photodiode_delay_max,\n",
    "            \"session_name\": session_name\n",
    "        }\n",
    "        \n",
    "        # Plot figure if requested\n",
    "        if plot_fig1:\n",
    "            try:\n",
    "                # process.plot_figure_1(\n",
    "                #     photometry_tracking_encoder_data, \n",
    "                #     session_name, \n",
    "                #     save_path, \n",
    "                #     common_resampled_rate, \n",
    "                #     photodiode_halts, \n",
    "                #     save_figure=True, \n",
    "                #     show_figure=True, \n",
    "                #     downsample_factor=50\n",
    "                # )\n",
    "                # print(f\"✅ Successfully created figure 1 for {data_path.name}\")\n",
    "                pass\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ ERROR creating figure 1: {str(e)}\")\n",
    "        else:\n",
    "            print(f\"ℹ️ INFO: skipping figure 1 for {data_path.name}\")\n",
    "        \n",
    "        # Clean up to free memory\n",
    "        del df_to_analyze\n",
    "        gc.collect()\n",
    "        \n",
    "        print(f\"✅ Completed analysis for data path: {data_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ ERROR during analysis of {data_path}: {str(e)}\")\n",
    "\n",
    "print(f\"\\n✅ Finished analyzing all {len(data_path_variables)} successfully processed data paths\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0d16b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#separates RIGHT vs LEFT TURNS, creates heatmaps, and comprehensive (Mean +- sem) aligned data for each data path\n",
    "#----------------------------------------------------\n",
    "\"\"\"\n",
    "Refactored photometry analysis code for processing aligned behavioral data.\n",
    "Separates left vs right turns, creates heatmaps, and generates comprehensive plots.\n",
    "\"\"\"\n",
    "class PhotometryAnalyzer:\n",
    "    \"\"\"Class for analyzing photometry data with behavioral events.\"\"\"\n",
    "    \n",
    "    # Class constants\n",
    "    REQUIRED_COLUMNS = [\n",
    "        \"Time (s)\", \"Photodiode_int\", \"z_470\", \"z_560\", \n",
    "        \"dfF_470\", \"dfF_560\", \"Motor_Velocity\", \"Velocity_0X\", \"Velocity_0Y\"\n",
    "    ]\n",
    "    \n",
    "    FLUORESCENCE_CHANNELS = {\n",
    "        'z_470': {'color': 'cornflowerblue', 'label': 'z_470'},\n",
    "        'z_560': {'color': 'red', 'label': 'z_560'},\n",
    "        'dfF_470': {'color': 'blue', 'label': 'dfF_470'},\n",
    "        'dfF_560': {'color': 'orange', 'label': 'dfF_560'},\n",
    "        # Eye tracking channels\n",
    "        'Pupil.Diameter_eye1': {'color': 'purple', 'label': 'Pupil Diameter'},\n",
    "        'Ellipse.Center.X_eye1': {'color': 'magenta', 'label': 'Eye Position X'}\n",
    "    }\n",
    "    \n",
    "    def __init__(self, time_window: Tuple[float, float] = (time_window_start, time_window_end),\n",
    "                 video_saccade_summaries: Optional[Dict[str, pd.DataFrame]] = None,\n",
    "                 saccade_bin_size_s: float = saccade_bin_size_s):\n",
    "        \"\"\"\n",
    "        Initialize analyzer with time window parameters.\n",
    "        \n",
    "        Args:\n",
    "            time_window: Tuple of (start, end) times relative to event (seconds)\n",
    "            video_saccade_summaries: Dictionary with keys 'VideoData1', 'VideoData2' \n",
    "                                   containing saccade summary DataFrames\n",
    "            saccade_bin_size_s: Bin size in seconds for saccade density analysis \n",
    "                              (externalized from global parameter)\n",
    "        \"\"\"\n",
    "        self.time_window_start, self.time_window_end = time_window\n",
    "        self.video_saccade_summaries = video_saccade_summaries or {}\n",
    "        self.saccade_bin_size_s = saccade_bin_size_s\n",
    "    \n",
    "    def compute_saccade_density(self, \n",
    "                               window_data: pd.DataFrame, \n",
    "                               halt_time: pd.Timestamp,\n",
    "                               eye_key: str = \"VideoData1\") -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Compute saccade probability within the event window at specified bin resolution.\n",
    "        Uses the boolean saccade_event column to calculate probability (proportion of \n",
    "        samples with saccades in each bin).\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        window_data : pd.DataFrame\n",
    "            Aligned window data for single halt event\n",
    "        halt_time : pd.Timestamp\n",
    "            Timestamp of halt event\n",
    "        eye_key : str\n",
    "            Which eye to analyze ('VideoData1' or 'VideoData2')\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        pd.DataFrame\n",
    "            window_data with added column:\n",
    "            - saccade_probability_eye1: probability (0 to 1) of saccade in bin\n",
    "        \"\"\"\n",
    "        # Determine eye suffix from key\n",
    "        eye_suffix = \"eye1\" if \"1\" in eye_key else \"eye2\"\n",
    "        saccade_col_name = f\"saccade_event_{eye_suffix}\"\n",
    "        prob_col_name = f\"saccade_probability_{eye_suffix}\"\n",
    "        \n",
    "        # Initialize with zeros (ensures backward compatibility)\n",
    "        window_data[prob_col_name] = 0.0\n",
    "        \n",
    "        # Check if saccade boolean column exists\n",
    "        if saccade_col_name not in window_data.columns:\n",
    "            # No saccade data available\n",
    "            return window_data\n",
    "        \n",
    "        # Get relative times for binning\n",
    "        window_relative_times = window_data[\"Time (s)\"].values\n",
    "        saccade_bool = window_data[saccade_col_name].astype(float).values  # Convert True/False to 1/0\n",
    "        \n",
    "        # Create bin edges\n",
    "        bin_edges = np.arange(self.time_window_start, \n",
    "                             self.time_window_end + self.saccade_bin_size_s, \n",
    "                             self.saccade_bin_size_s)\n",
    "        \n",
    "        # Assign each sample to a bin\n",
    "        bin_indices = np.digitize(window_relative_times, bin_edges) - 1\n",
    "        \n",
    "        # Calculate probability for each bin (proportion of samples with saccades)\n",
    "        n_bins = len(bin_edges) - 1\n",
    "        bin_probabilities = np.zeros(n_bins)\n",
    "        \n",
    "        for bin_idx in range(n_bins):\n",
    "            # Find all samples in this bin\n",
    "            in_bin = (bin_indices == bin_idx)\n",
    "            if np.any(in_bin):\n",
    "                # Probability = mean of boolean values in bin (proportion of True values)\n",
    "                bin_probabilities[bin_idx] = saccade_bool[in_bin].mean()\n",
    "        \n",
    "        # Assign probabilities to each row in window_data\n",
    "        for idx, bin_idx in enumerate(bin_indices):\n",
    "            if 0 <= bin_idx < n_bins:\n",
    "                window_data.iloc[idx, window_data.columns.get_loc(prob_col_name)] = bin_probabilities[bin_idx]\n",
    "        \n",
    "        return window_data\n",
    "        \n",
    "    def process_aligned_data(self, df: pd.DataFrame, halt_time: pd.Timestamp) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Process a single halt event efficiently.\n",
    "        \n",
    "        Args:\n",
    "            df: Main dataframe with photometry and behavioral data\n",
    "            halt_time: Timestamp of the halt event\n",
    "            \n",
    "        Returns:\n",
    "            Windowed dataframe or None if no data in window\n",
    "        \"\"\"\n",
    "        window_start = halt_time + pd.Timedelta(seconds=self.time_window_start)\n",
    "        window_end = halt_time + pd.Timedelta(seconds=self.time_window_end)\n",
    "        mask = (df.index >= window_start) & (df.index <= window_end)\n",
    "        \n",
    "        if not mask.any():\n",
    "            return None\n",
    "        \n",
    "        window = df.loc[mask].copy()\n",
    "        window[\"Time (s)\"] = (window.index - halt_time).total_seconds()\n",
    "        window[\"Halt Time\"] = halt_time\n",
    "        \n",
    "        # Add saccade density (probability) if available\n",
    "        try:\n",
    "            window = self.compute_saccade_density(window, halt_time, eye_key=\"VideoData1\")\n",
    "        except Exception as e:\n",
    "            # Log error but don't fail the entire analysis\n",
    "            print(f\"⚠️ Warning: Error computing saccade density: {str(e)}\")\n",
    "            # Initialize saccade column with zeros for backward compatibility\n",
    "            window['saccade_probability_eye1'] = 0.0\n",
    "        \n",
    "        return window\n",
    "    \n",
    "    def separate_turns(self, aligned_df: pd.DataFrame) -> Tuple[List[pd.Timestamp], List[pd.Timestamp]]:\n",
    "        \"\"\"\n",
    "        Separate halt events into left and right turns based on motor velocity.\n",
    "        \n",
    "        Args:\n",
    "            aligned_df: Aligned dataframe with all halt events\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (left_turn_halts, right_turn_halts)\n",
    "        \"\"\"\n",
    "        left_turn_halts = []\n",
    "        right_turn_halts = []\n",
    "        \n",
    "        for halt in aligned_df[\"Halt Time\"].unique():\n",
    "            # Look at motor velocity in pre-event window\n",
    "            subset = aligned_df[\n",
    "                (aligned_df[\"Halt Time\"] == halt) & \n",
    "                (aligned_df[\"Time (s)\"] >= -1) & \n",
    "                (aligned_df[\"Time (s)\"] < 0)\n",
    "            ]\n",
    "            \n",
    "            if subset.empty:\n",
    "                continue\n",
    "                \n",
    "            mean_velocity = subset[\"Motor_Velocity\"].mean()\n",
    "            \n",
    "            if mean_velocity < 0:  # Negative = left turn\n",
    "                left_turn_halts.append(halt)\n",
    "            elif mean_velocity > 0:  # Positive = right turn\n",
    "                right_turn_halts.append(halt)\n",
    "        \n",
    "        return left_turn_halts, right_turn_halts\n",
    "    \n",
    "    def save_turn_data(self, aligned_df: pd.DataFrame, left_turns: List, right_turns: List, \n",
    "                      session_name: str, event_name: str, output_dir: Path) -> None:\n",
    "        \"\"\"Save separated turn data to CSV files only if turns are detected.\"\"\"\n",
    "        \n",
    "        # Only save left turns if there are any\n",
    "        if left_turns:\n",
    "            left_df = aligned_df[aligned_df[\"Halt Time\"].isin(left_turns)]\n",
    "            left_file = output_dir / f\"{session_name}_{event_name}_left_turns.csv\"\n",
    "            left_df.to_csv(left_file, index=False, float_format='%.4f')\n",
    "            print(f\"Saved {len(left_turns)} left turns to {left_file}\")\n",
    "        else:\n",
    "            print(f\"No left turns detected - no CSV file saved\")\n",
    "        \n",
    "        # Only save right turns if there are any\n",
    "        if right_turns:\n",
    "            right_df = aligned_df[aligned_df[\"Halt Time\"].isin(right_turns)]\n",
    "            right_file = output_dir / f\"{session_name}_{event_name}_right_turns.csv\"\n",
    "            right_df.to_csv(right_file, index=False, float_format='%.4f')\n",
    "            print(f\"Saved {len(right_turns)} right turns to {right_file}\")\n",
    "        else:\n",
    "            print(f\"No right turns detected - no CSV file saved\")\n",
    "    \n",
    "    def create_heatmap(self, pivot_data: pd.DataFrame, session_name: str, event_name: str, \n",
    "                      channel: str, save_path: Path, figsize: Tuple[int, int] = (10, 6)) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create and save normalized heatmap.\n",
    "        \n",
    "        Args:\n",
    "            pivot_data: Pivoted data (events x time)\n",
    "            session_name: Name of session\n",
    "            event_name: Name of event type\n",
    "            channel: Channel name (e.g., 'z_470')\n",
    "            save_path: Path to save figure\n",
    "            figsize: Figure size tuple\n",
    "            \n",
    "        Returns:\n",
    "            Normalized data used for heatmap\n",
    "        \"\"\"\n",
    "        # Baseline normalization\n",
    "        baseline_cols = (pivot_data.columns >= -1) & (pivot_data.columns < 0)\n",
    "        if baseline_cols.any():\n",
    "            baseline_means = pivot_data.loc[:, baseline_cols].mean(axis=1)\n",
    "            normalized_data = pivot_data.subtract(baseline_means, axis=0)\n",
    "        else:\n",
    "            normalized_data = pivot_data\n",
    "        \n",
    "        # Create figure\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        \n",
    "        sns.heatmap(\n",
    "            normalized_data, \n",
    "            cmap=\"RdBu_r\", \n",
    "            center=0, \n",
    "            ax=ax,\n",
    "            cbar_kws={'label': f'Normalized {channel}'},\n",
    "            rasterized=True\n",
    "        )\n",
    "        \n",
    "        ax.set_title(f\"Heatmap ({channel}) - {session_name}\")\n",
    "        ax.set_xlabel(\"Time (s)\")\n",
    "        ax.set_ylabel(\"Event\")\n",
    "        \n",
    "        # Optimize y-axis ticks\n",
    "        n_events = len(normalized_data.index)\n",
    "        y_positions = range(0, n_events, max(1, n_events // 10))\n",
    "        y_labels = [str(i+1) for i in y_positions]\n",
    "        ax.set_yticks(y_positions)\n",
    "        ax.set_yticklabels(y_labels)\n",
    "        \n",
    "        # Add event line at time 0\n",
    "        if 0 in normalized_data.columns:\n",
    "            zero_idx = list(normalized_data.columns).index(0)\n",
    "            ax.axvline(zero_idx, linestyle='--', color='black', alpha=0.7)\n",
    "        \n",
    "        # Optimize x-axis ticks\n",
    "        time_cols = normalized_data.columns\n",
    "        tick_indices = [i for i, val in enumerate(time_cols) \n",
    "                       if isinstance(val, (int, float)) and val % 2 == 0]\n",
    "        tick_labels = [f\"{int(time_cols[i])}\" for i in tick_indices]\n",
    "        \n",
    "        ax.set_xticks(tick_indices)\n",
    "        ax.set_xticklabels(tick_labels, rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path, dpi=200, bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "        \n",
    "        return normalized_data\n",
    "    \n",
    "    def create_line_collections(self, df: pd.DataFrame, channels: List[str]) -> Dict[str, LineCollection]:\n",
    "        \"\"\"Create line collections for individual traces.\"\"\"\n",
    "        line_collections = {}\n",
    "        \n",
    "        for channel in channels:\n",
    "            lines = []\n",
    "            for halt in df[\"Halt Time\"].unique():\n",
    "                subset = df[df[\"Halt Time\"] == halt]\n",
    "                time_vals = subset[\"Time (s)\"].values\n",
    "                channel_vals = subset[channel].values\n",
    "                lines.append(list(zip(time_vals, channel_vals)))\n",
    "            \n",
    "            color = self.FLUORESCENCE_CHANNELS.get(channel, {}).get('color', 'gray')\n",
    "            line_collections[channel] = LineCollection(lines, colors=color, alpha=0.3, linewidths=1)\n",
    "        \n",
    "        return line_collections\n",
    "    \n",
    "    def add_mean_sem_plot(self, ax: plt.Axes, df: pd.DataFrame, channels: List[str], \n",
    "                         turn_type: str, line_style: str = '-') -> None:\n",
    "        \"\"\"Add mean ± SEM traces to axis.\"\"\"\n",
    "        grouped = df.groupby(\"Time (s)\")\n",
    "        time_index = grouped.mean().index.values\n",
    "        \n",
    "        for channel in channels:\n",
    "            color = self.FLUORESCENCE_CHANNELS[channel]['color']\n",
    "            label = f\"{turn_type} {channel}\"\n",
    "            \n",
    "            mean_vals = grouped.mean()[channel]\n",
    "            sem_vals = grouped.sem()[channel]\n",
    "            \n",
    "            ax.plot(time_index, mean_vals, color=color, linestyle=line_style, \n",
    "                   linewidth=2, label=label)\n",
    "            ax.fill_between(time_index, mean_vals - sem_vals, mean_vals + sem_vals,\n",
    "                           color=color, alpha=0.2)\n",
    "    \n",
    "    def create_summary_plot(self, aligned_df: pd.DataFrame, session_name: str, \n",
    "                          event_name: str, save_path: Path) -> None:\n",
    "        \"\"\"\n",
    "        Create comprehensive summary plots comparing left vs right turns.\n",
    "        \n",
    "        Args:\n",
    "            aligned_df: Aligned dataframe with all events\n",
    "            session_name: Session identifier\n",
    "            event_name: Event type name\n",
    "            save_path: Path to save the plot\n",
    "        \"\"\"\n",
    "        # Separate turns\n",
    "        left_turns, right_turns = self.separate_turns(aligned_df)\n",
    "        \n",
    "        if not left_turns and not right_turns:\n",
    "            print(\"No valid turns found for summary plot\")\n",
    "            return\n",
    "        \n",
    "        # Create DataFrames for each turn type\n",
    "        left_df = aligned_df[aligned_df[\"Halt Time\"].isin(left_turns)]\n",
    "        right_df = aligned_df[aligned_df[\"Halt Time\"].isin(right_turns)]\n",
    "        \n",
    "        # Create figure\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(21, 6), sharex=True)\n",
    "        z_channels = ['z_470', 'z_560']\n",
    "        \n",
    "        # Left plot - Left turn traces\n",
    "        if not left_df.empty:\n",
    "            ax1 = axes[0]\n",
    "            left_collections = self.create_line_collections(left_df, z_channels + ['Motor_Velocity'])\n",
    "            \n",
    "            # Add fluorescence traces\n",
    "            for channel in z_channels:\n",
    "                ax1.add_collection(left_collections[channel])\n",
    "            \n",
    "            # Add motor velocity on secondary axis\n",
    "            ax1_motor = ax1.twinx()\n",
    "            ax1_motor.add_collection(left_collections['Motor_Velocity'])\n",
    "            ax1_motor.set_ylabel(\"Motor Velocity\", color='slategray')\n",
    "            ax1_motor.tick_params(axis='y', labelcolor='slategray')\n",
    "            ax1_motor.autoscale()\n",
    "            \n",
    "            ax1.set_title(f'Left Turn Traces (n={len(left_turns)})')\n",
    "            ax1.set_ylabel(\"Fluorescence (z-score)\")\n",
    "            ax1.axvline(0, linestyle='--', color='black', alpha=0.7)\n",
    "            ax1.autoscale()\n",
    "        \n",
    "        # Right plot - Right turn traces  \n",
    "        if not right_df.empty:\n",
    "            ax2 = axes[1]\n",
    "            right_collections = self.create_line_collections(right_df, z_channels + ['Motor_Velocity'])\n",
    "            \n",
    "            # Add fluorescence traces\n",
    "            for channel in z_channels:\n",
    "                ax2.add_collection(right_collections[channel])\n",
    "            \n",
    "            # Add motor velocity on secondary axis\n",
    "            ax2_motor = ax2.twinx()\n",
    "            ax2_motor.add_collection(right_collections['Motor_Velocity'])\n",
    "            ax2_motor.set_ylabel(\"Motor Velocity\", color='slategray')\n",
    "            ax2_motor.tick_params(axis='y', labelcolor='slategray')\n",
    "            ax2_motor.autoscale()\n",
    "            \n",
    "            ax2.set_title(f'Right Turn Traces (n={len(right_turns)})')\n",
    "            ax2.set_ylabel(\"Fluorescence (z-score)\")\n",
    "            ax2.axvline(0, linestyle='--', color='black', alpha=0.7)\n",
    "            ax2.autoscale()\n",
    "        \n",
    "        # Comparison plot - Mean ± SEM\n",
    "        ax3 = axes[2]\n",
    "        \n",
    "        if not left_df.empty:\n",
    "            self.add_mean_sem_plot(ax3, left_df, z_channels, \"Left\", '--')\n",
    "        if not right_df.empty:\n",
    "            self.add_mean_sem_plot(ax3, right_df, z_channels, \"Right\", '-')\n",
    "        \n",
    "        # Add motor velocity comparison\n",
    "        ax3_motor = ax3.twinx()\n",
    "        motor_color = 'slategray'\n",
    "        \n",
    "        if not left_df.empty:\n",
    "            left_motor_grouped = left_df.groupby(\"Time (s)\")\n",
    "            time_idx = left_motor_grouped.mean().index.values\n",
    "            mean_motor = left_motor_grouped.mean()[\"Motor_Velocity\"]\n",
    "            sem_motor = left_motor_grouped.sem()[\"Motor_Velocity\"]\n",
    "            \n",
    "            ax3_motor.plot(time_idx, mean_motor, color=motor_color, linestyle='--', \n",
    "                          linewidth=1.5, label=\"Left Motor\")\n",
    "            ax3_motor.fill_between(time_idx, mean_motor - sem_motor, mean_motor + sem_motor,\n",
    "                                  color=motor_color, alpha=0.2)\n",
    "        \n",
    "        if not right_df.empty:\n",
    "            right_motor_grouped = right_df.groupby(\"Time (s)\")\n",
    "            time_idx = right_motor_grouped.mean().index.values\n",
    "            mean_motor = right_motor_grouped.mean()[\"Motor_Velocity\"]\n",
    "            sem_motor = right_motor_grouped.sem()[\"Motor_Velocity\"]\n",
    "            \n",
    "            ax3_motor.plot(time_idx, mean_motor, color=motor_color, linestyle='-',\n",
    "                          linewidth=1.5, label=\"Right Motor\")\n",
    "            ax3_motor.fill_between(time_idx, mean_motor - sem_motor, mean_motor + sem_motor,\n",
    "                                  color=motor_color, alpha=0.2)\n",
    "        \n",
    "        ax3_motor.set_ylabel(\"Motor Velocity\", color=motor_color)\n",
    "        ax3_motor.tick_params(axis='y', labelcolor=motor_color)\n",
    "        ax3_motor.axhline(0, linestyle='--', color='gray', alpha=0.5)\n",
    "        ax3_motor.legend(loc='upper right')\n",
    "        \n",
    "        # Add pupil diameter if available\n",
    "        if \"Pupil.Diameter_eye1\" in aligned_df.columns:\n",
    "            ax3_pupil = ax3.twinx()\n",
    "            ax3_pupil.spines['right'].set_position(('outward', 60))\n",
    "            pupil_color = self.FLUORESCENCE_CHANNELS.get('Pupil.Diameter_eye1', {}).get('color', 'purple')\n",
    "            \n",
    "            if not left_df.empty and \"Pupil.Diameter_eye1\" in left_df.columns:\n",
    "                left_pupil_grouped = left_df.groupby(\"Time (s)\")\n",
    "                time_idx = left_pupil_grouped.mean().index.values\n",
    "                mean_pupil = left_pupil_grouped.mean()[\"Pupil.Diameter_eye1\"]\n",
    "                sem_pupil = left_pupil_grouped.sem()[\"Pupil.Diameter_eye1\"]\n",
    "                ax3_pupil.plot(time_idx, mean_pupil, color=pupil_color, linestyle='--', \n",
    "                              linewidth=1, alpha=0.7, label=\"Left Pupil\")\n",
    "                ax3_pupil.fill_between(time_idx, mean_pupil - sem_pupil, mean_pupil + sem_pupil,\n",
    "                                      color=pupil_color, alpha=0.1)\n",
    "            \n",
    "            if not right_df.empty and \"Pupil.Diameter_eye1\" in right_df.columns:\n",
    "                right_pupil_grouped = right_df.groupby(\"Time (s)\")\n",
    "                time_idx = right_pupil_grouped.mean().index.values\n",
    "                mean_pupil = right_pupil_grouped.mean()[\"Pupil.Diameter_eye1\"]\n",
    "                sem_pupil = right_pupil_grouped.sem()[\"Pupil.Diameter_eye1\"]\n",
    "                ax3_pupil.plot(time_idx, mean_pupil, color=pupil_color, linestyle='-',\n",
    "                              linewidth=1, alpha=0.7, label=\"Right Pupil\")\n",
    "                ax3_pupil.fill_between(time_idx, mean_pupil - sem_pupil, mean_pupil + sem_pupil,\n",
    "                                      color=pupil_color, alpha=0.1)\n",
    "            \n",
    "            ax3_pupil.set_ylabel(\"Pupil Diameter\", color=pupil_color, fontsize=9)\n",
    "            ax3_pupil.tick_params(axis='y', labelcolor=pupil_color, labelsize=8)\n",
    "        \n",
    "        # Add eye position X if available\n",
    "        if \"Ellipse.Center.X_eye1\" in aligned_df.columns:\n",
    "            ax3_eye = ax3.twinx()\n",
    "            ax3_eye.spines['right'].set_position(('outward', 120))\n",
    "            eye_color = self.FLUORESCENCE_CHANNELS.get('Ellipse.Center.X_eye1', {}).get('color', 'magenta')\n",
    "            \n",
    "            if not left_df.empty and \"Ellipse.Center.X_eye1\" in left_df.columns:\n",
    "                left_eye_grouped = left_df.groupby(\"Time (s)\")\n",
    "                time_idx = left_eye_grouped.mean().index.values\n",
    "                mean_eye = left_eye_grouped.mean()[\"Ellipse.Center.X_eye1\"]\n",
    "                sem_eye = left_eye_grouped.sem()[\"Ellipse.Center.X_eye1\"]\n",
    "                ax3_eye.plot(time_idx, mean_eye, color=eye_color, linestyle='--', \n",
    "                            linewidth=1, alpha=0.7, label=\"Left Eye X\")\n",
    "                ax3_eye.fill_between(time_idx, mean_eye - sem_eye, mean_eye + sem_eye,\n",
    "                                    color=eye_color, alpha=0.1)\n",
    "            \n",
    "            if not right_df.empty and \"Ellipse.Center.X_eye1\" in right_df.columns:\n",
    "                right_eye_grouped = right_df.groupby(\"Time (s)\")\n",
    "                time_idx = right_eye_grouped.mean().index.values\n",
    "                mean_eye = right_eye_grouped.mean()[\"Ellipse.Center.X_eye1\"]\n",
    "                sem_eye = right_eye_grouped.sem()[\"Ellipse.Center.X_eye1\"]\n",
    "                ax3_eye.plot(time_idx, mean_eye, color=eye_color, linestyle='-',\n",
    "                            linewidth=1, alpha=0.7, label=\"Right Eye X\")\n",
    "                ax3_eye.fill_between(time_idx, mean_eye - sem_eye, mean_eye + sem_eye,\n",
    "                                    color=eye_color, alpha=0.1)\n",
    "            \n",
    "            ax3_eye.set_ylabel(\"Eye Position X\", color=eye_color, fontsize=9)\n",
    "            ax3_eye.tick_params(axis='y', labelcolor=eye_color, labelsize=8)\n",
    "        \n",
    "        # Add saccade probability if available\n",
    "        if \"saccade_probability_eye1\" in aligned_df.columns:\n",
    "            ax3_saccade = ax3.twinx()\n",
    "            ax3_saccade.spines['right'].set_position(('outward', 180))\n",
    "            saccade_color = 'cyan'\n",
    "            \n",
    "            if not left_df.empty and \"saccade_probability_eye1\" in left_df.columns:\n",
    "                left_saccade_grouped = left_df.groupby(\"Time (s)\")\n",
    "                time_idx = left_saccade_grouped.mean().index.values\n",
    "                mean_saccade = left_saccade_grouped.mean()[\"saccade_probability_eye1\"]\n",
    "                # Use step plot to show binned probability as a continuous-looking signal\n",
    "                ax3_saccade.step(time_idx, mean_saccade, where='mid',\n",
    "                                color=saccade_color, alpha=0.6, linewidth=2, \n",
    "                                label=\"Left Turns Sac. Prob\")\n",
    "            \n",
    "            if not right_df.empty and \"saccade_probability_eye1\" in right_df.columns:\n",
    "                right_saccade_grouped = right_df.groupby(\"Time (s)\")\n",
    "                time_idx = right_saccade_grouped.mean().index.values\n",
    "                mean_saccade = right_saccade_grouped.mean()[\"saccade_probability_eye1\"]\n",
    "                # Overlay right turns with slightly different style\n",
    "                ax3_saccade.step(time_idx, mean_saccade, where='mid',\n",
    "                                color='darkblue', alpha=0.8, linewidth=2.5,\n",
    "                                linestyle='--', label=\"Right Turns Sac. Prob\")\n",
    "            \n",
    "            ax3_saccade.set_ylabel(\"Saccade Probability\", color=saccade_color, fontsize=9)\n",
    "            ax3_saccade.tick_params(axis='y', labelcolor=saccade_color, labelsize=8)\n",
    "            # Autoscale based on actual data range\n",
    "            y_min = aligned_df[\"saccade_probability_eye1\"].min()\n",
    "            y_max = aligned_df[\"saccade_probability_eye1\"].max()\n",
    "            if y_max > y_min:\n",
    "                ax3_saccade.set_ylim(y_min * 0.9, y_max * 1.1)  # Add 10% padding\n",
    "            ax3_saccade.legend(loc='upper right', fontsize=8)\n",
    "        \n",
    "        ax3.axvline(0, linestyle='--', color='black', alpha=0.7)\n",
    "        ax3.set_xlabel(\"Time (s)\")\n",
    "        ax3.set_ylabel(\"Fluorescence (z-score)\")\n",
    "        ax3.set_title(\"Mean ± SEM Comparison\")\n",
    "        ax3.legend(loc='upper left')\n",
    "        \n",
    "        # Format all x-axes\n",
    "        for ax in axes:\n",
    "            ax.set_xlabel(\"Time (s)\")\n",
    "        \n",
    "        fig.suptitle(f\"{session_name} - {event_name}\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path, dpi=200, bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "        plt.ioff()\n",
    "    \n",
    "    def process_session(self, data_path: Path, data: Dict[str, Any], \n",
    "                       variables: Dict[str, Any], event_name: str = \"halt\") -> None:\n",
    "        \"\"\"\n",
    "        Process a complete session of data.\n",
    "        \n",
    "        Args:\n",
    "            data_path: Path to the data directory\n",
    "            data: Dictionary containing loaded data\n",
    "            variables: Dictionary containing analysis variables\n",
    "            event_name: Name of the event type being analyzed\n",
    "        \"\"\"\n",
    "        print(f\"\\n--------- Processing: {data_path} ---------\")\n",
    "        \n",
    "        try:\n",
    "            # Extract data components\n",
    "            df = data[\"photometry_tracking_encoder_data\"]\n",
    "            halts = variables[\"photodiode_halts\"]\n",
    "            \n",
    "            # Get session name\n",
    "            session_name = variables.get(\"session_name\")\n",
    "            if not session_name:\n",
    "                mouse_name = data.get(\"mouse_name\", \"unknown_mouse\")\n",
    "                session_name = f\"{mouse_name}_{data_path.stem}\"\n",
    "                print(f\"No session_name found, using: {session_name}\")\n",
    "            \n",
    "            print(f\"Aligning {len(halts)} events for session '{session_name}'\")\n",
    "            \n",
    "            # Process all halt events\n",
    "            aligned_data = []\n",
    "            for halt_time in halts:\n",
    "                window_data = self.process_aligned_data(df, halt_time)\n",
    "                if window_data is not None:\n",
    "                    aligned_data.append(window_data)\n",
    "            \n",
    "            if not aligned_data:\n",
    "                print(f\"No aligned data generated for {session_name}, skipping\")\n",
    "                return\n",
    "            \n",
    "            # Combine all aligned data\n",
    "            aligned_df = pd.concat(aligned_data, ignore_index=True)\n",
    "            \n",
    "            # DIAGNOSTIC: Log which eye tracking columns are present\n",
    "            eye_cols = [col for col in aligned_df.columns if 'Pupil.' in col or 'Ellipse.Center' in col]\n",
    "            if eye_cols:\n",
    "                print(f\"✅ Eye tracking columns present: {eye_cols}\")\n",
    "            else:\n",
    "                print(f\"⚠️ No eye tracking columns found in aligned data\")\n",
    "            \n",
    "            # Create output directory\n",
    "            aligned_dir = data_path.parent / \"aligned_data\"\n",
    "            aligned_dir.mkdir(exist_ok=True)\n",
    "            \n",
    "            # Save main aligned data\n",
    "            aligned_file = aligned_dir / f\"{session_name}_{event_name}_aligned.csv\"\n",
    "            aligned_df.to_csv(aligned_file, index=False, float_format='%.4f')\n",
    "            print(f\"Saved aligned data to {aligned_file}\")\n",
    "            \n",
    "            # Separate and save turn data\n",
    "            left_turns, right_turns = self.separate_turns(aligned_df)\n",
    "            self.save_turn_data(aligned_df, left_turns, right_turns, \n",
    "                              session_name, event_name, aligned_dir)\n",
    "            \n",
    "            # Create summary plot\n",
    "            print(f\"📈 Creating plots for {session_name}\")\n",
    "            summary_path = data_path.parent / f\"{session_name}_{event_name}.pdf\"\n",
    "            self.create_summary_plot(aligned_df, session_name, event_name, summary_path)\n",
    "            print(f\"Saved summary plot to {summary_path}\")\n",
    "            \n",
    "            # Create heatmaps if sufficient data\n",
    "            unique_halts = aligned_df[\"Halt Time\"].nunique()\n",
    "            if unique_halts > 1:\n",
    "                self._create_all_heatmaps(aligned_df, session_name, event_name, data_path)\n",
    "            else:\n",
    "                print(\"Insufficient data for heatmaps (need >1 event)\")\n",
    "            \n",
    "            # Cleanup\n",
    "            del aligned_data, aligned_df\n",
    "            gc.collect()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"ERROR processing {data_path}: {str(e)}\")\n",
    "            traceback.print_exc()\n",
    "            gc.collect()\n",
    "    \n",
    "    def _create_all_heatmaps(self, aligned_df: pd.DataFrame, session_name: str, \n",
    "                           event_name: str, data_path: Path) -> None:\n",
    "        \"\"\"Create all heatmaps for different channels.\"\"\"\n",
    "        heatmap_channels = [\n",
    "            'z_470', 'z_560', 'dfF_470', 'dfF_560',\n",
    "            'Pupil.Diameter_eye1',      # NEW\n",
    "            'Ellipse.Center.X_eye1'     # NEW\n",
    "        ]\n",
    "        \n",
    "        for channel in heatmap_channels:\n",
    "            try:\n",
    "                # Check if channel exists in aligned_df\n",
    "                if channel not in aligned_df.columns:\n",
    "                    print(f\"ℹ️ Skipping {channel} heatmap (column not present)\")\n",
    "                    continue\n",
    "                \n",
    "                # Create pivot table\n",
    "                pivot_data = aligned_df.pivot_table(\n",
    "                    index=\"Halt Time\", \n",
    "                    columns=\"Time (s)\", \n",
    "                    values=channel, \n",
    "                    aggfunc='first'\n",
    "                )\n",
    "                \n",
    "                # Create heatmap\n",
    "                heatmap_path = data_path.parent / f\"{session_name}_{event_name}_heatmap_{channel}.pdf\"\n",
    "                self.create_heatmap(pivot_data, session_name, event_name, channel, heatmap_path)\n",
    "                print(f\"Saved {channel} heatmap\")\n",
    "                \n",
    "                # Cleanup\n",
    "                del pivot_data\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error creating {channel} heatmap: {e}\")\n",
    "        \n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "def main(data_paths: List[Path], loaded_data: Dict, data_path_variables: Dict, \n",
    "         event_name: str = \"halt\", time_window: Tuple[float, float] = (-5, 10),\n",
    "         saccade_bin_size_s: float = 0.1):\n",
    "    \"\"\"\n",
    "    Main processing function.\n",
    "    \n",
    "    Args:\n",
    "        data_paths: List of data directory paths\n",
    "        loaded_data: Dictionary of loaded data for each path\n",
    "        data_path_variables: Dictionary of analysis variables for each path\n",
    "        event_name: Name of event type (default: \"halt\")\n",
    "        time_window: Tuple of (start, end) times relative to event in seconds\n",
    "        saccade_bin_size_s: Bin size in seconds for saccade density analysis (default: 0.1 = 100ms)\n",
    "    \"\"\"\n",
    "    # Initialize analyzer (will be updated with saccade data per session)\n",
    "    analyzer = PhotometryAnalyzer(time_window, saccade_bin_size_s=saccade_bin_size_s)\n",
    "    \n",
    "    # Process each data path\n",
    "    for idx, data_path in enumerate(data_paths, start=1):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Processing {idx}/{len(data_paths)}: {data_path.name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        if data_path not in data_path_variables:\n",
    "            print(f\"Skipping {data_path} - no analysis data found\")\n",
    "            continue\n",
    "        \n",
    "        if data_path not in loaded_data:\n",
    "            print(f\"Skipping {data_path} - no loaded data found\")\n",
    "            continue\n",
    "        \n",
    "        # Update analyzer with saccade summaries for this session\n",
    "        video_saccade_summaries = loaded_data[data_path].get(\"video_saccade_summaries\", {})\n",
    "        analyzer.video_saccade_summaries = video_saccade_summaries\n",
    "        \n",
    "        # Process this session\n",
    "        analyzer.process_session(\n",
    "            data_path, \n",
    "            loaded_data[data_path], \n",
    "            data_path_variables[data_path], \n",
    "            event_name\n",
    "        )\n",
    "    \n",
    "    print(\"\\n✅ Finished processing all data paths.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3905dad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_window = (time_window_start, time_window_end)\n",
    "# saccade_bin_size_s is now externalized to the parameter cell at the top (line 140)\n",
    "\n",
    "main(data_paths, loaded_data, data_path_variables, \n",
    "     event_name=event_name, time_window=time_window,\n",
    "     saccade_bin_size_s=saccade_bin_size_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c3605b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASELINING\n",
    "#----------------------------------------------------\n",
    "def process_aligned_data_folders(data_dirs, baseline_window, event_name=event_name, plot_width=12, create_plots=True):\n",
    "    \"\"\"\n",
    "    Process all aligned_data folders and generate baseline plots.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data_dirs : list\n",
    "        List of Path objects pointing to your main data directories\n",
    "    baseline_window : tuple\n",
    "        Tuple of (start_time, end_time) for baseline window\n",
    "    event_name : str\n",
    "        Event name for file naming (default: \"halt\")\n",
    "    plot_width : int\n",
    "        Width of the plot in inches\n",
    "    create_plots : bool\n",
    "        Whether to create and save plots (default: True)\n",
    "    \"\"\"\n",
    "    \n",
    "    results = {\n",
    "        'processed': [],\n",
    "        'errors': [],\n",
    "        'total_folders': 0\n",
    "    }\n",
    "    \n",
    "    # Find all aligned_data folders\n",
    "    aligned_folders = []\n",
    "    for data_dir in data_dirs:\n",
    "        print(f\"Searching in: {data_dir}\")\n",
    "        # Find all aligned_data folders recursively\n",
    "        found_folders = list(data_dir.rglob(\"aligned_data\"))\n",
    "        aligned_folders.extend(found_folders)\n",
    "        print(f\"  Found {len(found_folders)} aligned_data folders\")\n",
    "    \n",
    "    results['total_folders'] = len(aligned_folders)\n",
    "    print(f\"\\nTotal aligned_data folders found: {len(aligned_folders)}\")\n",
    "    \n",
    "    for aligned_folder in aligned_folders:\n",
    "        try:\n",
    "            print(f\"\\n📁 Processing folder: {aligned_folder}\")\n",
    "            \n",
    "            # Find only the original aligned CSV files (exclude already processed baselined files and turn files)\n",
    "            all_csv_files = list(aligned_folder.glob(\"*.csv\"))\n",
    "            csv_files = [f for f in all_csv_files if not f.name.endswith('_baselined_data.csv') \n",
    "                        and not f.name.endswith('_left_turns.csv') \n",
    "                        and not f.name.endswith('_right_turns.csv')]\n",
    "            \n",
    "            if not csv_files:\n",
    "                print(f\"  ⚠️  No original aligned CSV files found in {aligned_folder}\")\n",
    "                print(f\"  Available files: {[f.name for f in all_csv_files]}\")\n",
    "                results['errors'].append({\n",
    "                    'folder': str(aligned_folder),\n",
    "                    'error': 'No original aligned CSV files found',\n",
    "                    'status': 'skipped'\n",
    "                })\n",
    "                continue\n",
    "            print(f\"  Found {len(csv_files)} aligned CSV files to process\")\n",
    "            \n",
    "            for csv_file in csv_files:\n",
    "                try:\n",
    "                    # Check if the CSV file name matches the event name\n",
    "                    if event_name not in csv_file.name:\n",
    "                        print(f\"    ⚠️ Skipping {csv_file.name} as it does not match the event name '{event_name}'\")\n",
    "                        continue\n",
    "                    \n",
    "                    print(f\"    📊 Processing: {csv_file.name}\")\n",
    "                    \n",
    "                    # Load the data\n",
    "                    aligned_df = pd.read_csv(csv_file)\n",
    "                    \n",
    "                    # Create aligned DataFrames for left and right turns\n",
    "                    # Replace '_aligned' with '_left_turns' and '_right_turns'\n",
    "                    left_turns_csv = csv_file.with_name(csv_file.stem.replace('_aligned', '_left_turns') + \".csv\")\n",
    "                    right_turns_csv = csv_file.with_name(csv_file.stem.replace('_aligned', '_right_turns') + \".csv\")\n",
    "                    \n",
    "                    left_turns_df = None\n",
    "                    right_turns_df = None\n",
    "                    \n",
    "                    if left_turns_csv.exists():\n",
    "                        print(f\"    📂 Found left turns CSV: {left_turns_csv.name}\")\n",
    "                        left_turns_df = pd.read_csv(left_turns_csv)\n",
    "                    else:\n",
    "                        print(f\"    ⚠️ Left turns CSV not found: {left_turns_csv.name}\")\n",
    "                    \n",
    "                    if right_turns_csv.exists():\n",
    "                        print(f\"    📂 Found right turns CSV: {right_turns_csv.name}\")\n",
    "                        right_turns_df = pd.read_csv(right_turns_csv)\n",
    "                    else:\n",
    "                        print(f\"    ⚠️ Right turns CSV not found: {right_turns_csv.name}\")\n",
    "                    \n",
    "                    # Clean up the mouse name (remove extra suffixes)\n",
    "                    mouse_name = csv_file.stem.replace('_aligned', '').replace('_downsampled_data_Apply halt: 2s', '').split('_')[0]\n",
    "                    # Get session name from the folder structure\n",
    "                    session_name = aligned_folder.parent.name\n",
    "                    \n",
    "                    # Check if required columns exist\n",
    "                    required_columns = [\"Time (s)\", \"Halt Time\", \"z_470\", \"z_560\", \"Motor_Velocity\", \n",
    "                                      \"Velocity_0X\", \"Velocity_0Y\", \"Photodiode_int\"]\n",
    "                    missing_columns = [col for col in required_columns if col not in aligned_df.columns]\n",
    "                    \n",
    "                    if missing_columns:\n",
    "                        print(f\"    ⚠️  Missing columns: {missing_columns}\")\n",
    "                        print(f\"    Available columns: {list(aligned_df.columns)}\")\n",
    "                        raise ValueError(f\"Missing required columns: {missing_columns}\")\n",
    "                    \n",
    "                    # Process the data and create plot\n",
    "                    fig = baseline_aligned_data_simple(\n",
    "                        aligned_df=aligned_df,\n",
    "                        left_turns_df=left_turns_df,\n",
    "                        right_turns_df=right_turns_df,\n",
    "                        baseline_window=baseline_window,\n",
    "                        mouse_name=mouse_name,\n",
    "                        session_name=session_name,\n",
    "                        event_name=event_name,\n",
    "                        output_folder=aligned_folder,\n",
    "                        csv_file=csv_file,\n",
    "                        plot_width=plot_width,\n",
    "                        create_plots=create_plots\n",
    "                    )\n",
    "                    \n",
    "                    results['processed'].append({\n",
    "                        'file': str(csv_file),\n",
    "                        'mouse_name': mouse_name,\n",
    "                        'session_name': session_name,\n",
    "                        'folder': str(aligned_folder),\n",
    "                        'status': 'success'\n",
    "                    })\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    error_info = {\n",
    "                        'file': str(csv_file),\n",
    "                        'error': str(e),\n",
    "                        'status': 'failed'\n",
    "                    }\n",
    "                    results['errors'].append(error_info)\n",
    "                    print(f\"    ❌ Error processing {csv_file.name}: {str(e)}\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            error_info = {\n",
    "                'folder': str(aligned_folder),\n",
    "                'error': str(e),\n",
    "                'status': 'failed'\n",
    "            }\n",
    "            results['errors'].append(error_info)\n",
    "            print(f\"❌ Error accessing {aligned_folder}: {str(e)}\")\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"PROCESSING SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Total aligned_data folders: {results['total_folders']}\")\n",
    "    print(f\"Successfully processed files: {len(results['processed'])}\")\n",
    "    print(f\"Errors encountered: {len(results['errors'])}\")\n",
    "    \n",
    "    if results['errors']:\n",
    "        print(f\"\\nErrors:\")\n",
    "        for error in results['errors']:\n",
    "            if 'file' in error:\n",
    "                print(f\"  - File {Path(error['file']).name}: {error['error']}\")\n",
    "            else:\n",
    "                print(f\"  - Folder {Path(error['folder']).name}: {error['error']}\")\n",
    "    \n",
    "    if results['processed']:\n",
    "        print(f\"\\nSuccessfully processed:\")\n",
    "        for proc in results['processed']:\n",
    "            print(f\"  - {proc['mouse_name']} in {Path(proc['folder']).parent.name}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def create_single_baseline_plot(mean_baseline_df, sem_baseline_df, mouse_name, session_name, \n",
    "                               event_name, output_folder, plot_width=12, suffix=\"\", turn_type=\"\"):\n",
    "    \"\"\"\n",
    "    Create a single baseline plot from aggregated mean and SEM data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    mean_baseline_df : pd.DataFrame\n",
    "        Mean values grouped by Time (s)\n",
    "    sem_baseline_df : pd.DataFrame\n",
    "        SEM values grouped by Time (s)\n",
    "    mouse_name : str\n",
    "        Mouse name for file naming\n",
    "    session_name : str\n",
    "        Session name for title\n",
    "    event_name : str\n",
    "        Event name for file naming\n",
    "    output_folder : Path\n",
    "        Output folder path\n",
    "    plot_width : int\n",
    "        Width of the plot in inches\n",
    "    suffix : str\n",
    "        Suffix for filename (\"\", \"_left_turns\", \"_right_turns\")\n",
    "    turn_type : str\n",
    "        Text to add to title (\"\", \"LEFT TURNS ONLY\", \"RIGHT TURNS ONLY\")\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    Path\n",
    "        Path to saved figure file\n",
    "    \"\"\"\n",
    "    \n",
    "    def get_symmetric_ylim(mean_data, sem_data):\n",
    "        max_abs_value = max(\n",
    "            abs(mean_data).max() + sem_data.max(),\n",
    "            abs(mean_data).min() - sem_data.min()\n",
    "        )\n",
    "        return (-max_abs_value, max_abs_value)\n",
    "    \n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=(plot_width, 6))\n",
    "    \n",
    "    # Photodiode\n",
    "    ax.plot(mean_baseline_df.index, mean_baseline_df[\"Photodiode_int\"], color='grey', alpha=0.8, linewidth=2)\n",
    "    ax.fill_between(mean_baseline_df.index,\n",
    "                    mean_baseline_df[\"Photodiode_int\"] - sem_baseline_df[\"Photodiode_int\"],\n",
    "                    mean_baseline_df[\"Photodiode_int\"] + sem_baseline_df[\"Photodiode_int\"],\n",
    "                    color='grey', alpha=0.2)\n",
    "    \n",
    "    ax.set_xlabel('Time (s) relative to halt')\n",
    "    ax.set_ylabel('Photodiode', color='grey')\n",
    "    title = f'Baselined Signals - {mouse_name} ({session_name})'\n",
    "    if turn_type:\n",
    "        title += f' - {turn_type}'\n",
    "    ax.set_title(title)\n",
    "    \n",
    "    # z_470 and z_560 (Fluorescence) - BASELINED\n",
    "    ax2 = ax.twinx()\n",
    "    ax2.plot(mean_baseline_df.index, mean_baseline_df[\"z_470_Baseline\"], color='green', alpha=0.8, linewidth=2, label='470nm')\n",
    "    ax2.fill_between(mean_baseline_df.index,\n",
    "                     mean_baseline_df[\"z_470_Baseline\"] - sem_baseline_df[\"z_470_Baseline\"],\n",
    "                     mean_baseline_df[\"z_470_Baseline\"] + sem_baseline_df[\"z_470_Baseline\"],\n",
    "                     color='green', alpha=0.2)\n",
    "    ax2.plot(mean_baseline_df.index, mean_baseline_df[\"z_560_Baseline\"], color='red', alpha=0.8, linewidth=2, label='560nm')\n",
    "    ax2.fill_between(mean_baseline_df.index,\n",
    "                     mean_baseline_df[\"z_560_Baseline\"] - sem_baseline_df[\"z_560_Baseline\"],\n",
    "                     mean_baseline_df[\"z_560_Baseline\"] + sem_baseline_df[\"z_560_Baseline\"],\n",
    "                     color='red', alpha=0.2)\n",
    "    ax2.set_ylabel('Fluorescence (z-score)', color='green')\n",
    "    ax2.set_ylim(get_symmetric_ylim(\n",
    "        pd.concat([mean_baseline_df[\"z_470_Baseline\"], mean_baseline_df[\"z_560_Baseline\"]]),\n",
    "        pd.concat([sem_baseline_df[\"z_470_Baseline\"], sem_baseline_df[\"z_560_Baseline\"]])\n",
    "    ))\n",
    "    ax2.yaxis.label.set_color('green')\n",
    "    \n",
    "    # Motor velocity (NOT baselined)\n",
    "    ax3 = ax.twinx()\n",
    "    ax3.spines['right'].set_position(('outward', 50))\n",
    "    ax3.plot(mean_baseline_df.index, mean_baseline_df[\"Motor_Velocity\"], color='#00008B', alpha=0.8, linewidth=2)\n",
    "    ax3.fill_between(mean_baseline_df.index,\n",
    "                     mean_baseline_df[\"Motor_Velocity\"] - sem_baseline_df[\"Motor_Velocity\"],\n",
    "                     mean_baseline_df[\"Motor_Velocity\"] + sem_baseline_df[\"Motor_Velocity\"],\n",
    "                     color='#00008B', alpha=0.2)\n",
    "    ax3.set_ylabel('Motor Velocity (deg/s²)', color='#00008B')\n",
    "    ax3.set_ylim(get_symmetric_ylim(mean_baseline_df[\"Motor_Velocity\"], sem_baseline_df[\"Motor_Velocity\"]))\n",
    "    ax3.yaxis.label.set_color('#00008B')\n",
    "    \n",
    "    # Running velocity (Velocity_0X) - BASELINED\n",
    "    ax4 = ax.twinx()\n",
    "    ax4.spines['right'].set_position(('outward', 100))\n",
    "    ax4.plot(mean_baseline_df.index, mean_baseline_df[\"Velocity_0X_Baseline\"] * 1000, color='orange', alpha=0.8, linewidth=2)\n",
    "    ax4.fill_between(mean_baseline_df.index,\n",
    "                     (mean_baseline_df[\"Velocity_0X_Baseline\"] - sem_baseline_df[\"Velocity_0X_Baseline\"]) * 1000,\n",
    "                     (mean_baseline_df[\"Velocity_0X_Baseline\"] + sem_baseline_df[\"Velocity_0X_Baseline\"]) * 1000,\n",
    "                     color='orange', alpha=0.2)\n",
    "    ax4.set_ylabel('Running velocity (mm/s²)', color='orange')\n",
    "    ax4.set_ylim(get_symmetric_ylim(mean_baseline_df[\"Velocity_0X_Baseline\"] * 1000, sem_baseline_df[\"Velocity_0X_Baseline\"] * 1000))\n",
    "    ax4.yaxis.label.set_color('orange')\n",
    "    \n",
    "    # Turning velocity (Velocity_0Y) (NOT baselined)\n",
    "    ax5 = ax.twinx()\n",
    "    ax5.spines['right'].set_position(('outward', 150))\n",
    "    ax5.plot(mean_baseline_df.index, mean_baseline_df[\"Velocity_0Y\"], color='#4682B4', alpha=0.8, linewidth=2)\n",
    "    ax5.fill_between(mean_baseline_df.index,\n",
    "                     mean_baseline_df[\"Velocity_0Y\"] - sem_baseline_df[\"Velocity_0Y\"],\n",
    "                     mean_baseline_df[\"Velocity_0Y\"] + sem_baseline_df[\"Velocity_0Y\"],\n",
    "                     color='#4682B4', alpha=0.2)\n",
    "    ax5.set_ylabel('Turning velocity (deg/s²)', color='#4682B4')\n",
    "    ax5.set_ylim(get_symmetric_ylim(mean_baseline_df[\"Velocity_0Y\"], sem_baseline_df[\"Velocity_0Y\"]))\n",
    "    ax5.yaxis.label.set_color('#4682B4')\n",
    "    \n",
    "    # Pupil diameter - BASELINED\n",
    "    if \"Pupil.Diameter_eye1_Baseline\" in mean_baseline_df.columns:\n",
    "        ax6 = ax.twinx()\n",
    "        ax6.spines['right'].set_position(('outward', 200))\n",
    "        ax6.plot(mean_baseline_df.index, mean_baseline_df[\"Pupil.Diameter_eye1_Baseline\"], \n",
    "                color='purple', alpha=0.8, linewidth=2, label='Pupil Diameter')\n",
    "        ax6.fill_between(mean_baseline_df.index,\n",
    "                        mean_baseline_df[\"Pupil.Diameter_eye1_Baseline\"] - sem_baseline_df[\"Pupil.Diameter_eye1_Baseline\"],\n",
    "                        mean_baseline_df[\"Pupil.Diameter_eye1_Baseline\"] + sem_baseline_df[\"Pupil.Diameter_eye1_Baseline\"],\n",
    "                        color='purple', alpha=0.2)\n",
    "        ax6.set_ylabel('Pupil Diameter (pixels)', color='purple')\n",
    "        ax6.set_ylim(get_symmetric_ylim(mean_baseline_df[\"Pupil.Diameter_eye1_Baseline\"], \n",
    "                                       sem_baseline_df[\"Pupil.Diameter_eye1_Baseline\"]))\n",
    "        ax6.yaxis.label.set_color('purple')\n",
    "    \n",
    "    # Eye position X - BASELINED\n",
    "    if \"Ellipse.Center.X_eye1_Baseline\" in mean_baseline_df.columns:\n",
    "        ax7 = ax.twinx()\n",
    "        ax7.spines['right'].set_position(('outward', 250))\n",
    "        ax7.plot(mean_baseline_df.index, mean_baseline_df[\"Ellipse.Center.X_eye1_Baseline\"], \n",
    "                color='magenta', alpha=0.8, linewidth=2, label='Eye Position X')\n",
    "        ax7.fill_between(mean_baseline_df.index,\n",
    "                        mean_baseline_df[\"Ellipse.Center.X_eye1_Baseline\"] - sem_baseline_df[\"Ellipse.Center.X_eye1_Baseline\"],\n",
    "                        mean_baseline_df[\"Ellipse.Center.X_eye1_Baseline\"] + sem_baseline_df[\"Ellipse.Center.X_eye1_Baseline\"],\n",
    "                        color='magenta', alpha=0.2)\n",
    "        ax7.set_ylabel('Eye Position X (pixels)', color='magenta')\n",
    "        ax7.set_ylim(get_symmetric_ylim(mean_baseline_df[\"Ellipse.Center.X_eye1_Baseline\"], \n",
    "                                       sem_baseline_df[\"Ellipse.Center.X_eye1_Baseline\"]))\n",
    "        ax7.yaxis.label.set_color('magenta')\n",
    "    \n",
    "    # Saccade probability (NOT baselined)\n",
    "    if \"saccade_probability_eye1\" in mean_baseline_df.columns:\n",
    "        ax8 = ax.twinx()\n",
    "        ax8.spines['right'].set_position(('outward', 300))\n",
    "        ax8.step(mean_baseline_df.index, \n",
    "                mean_baseline_df[\"saccade_probability_eye1\"],\n",
    "                where='mid', color='cyan', alpha=0.8, linewidth=2, label='Saccade Probability')\n",
    "        ax8.fill_between(mean_baseline_df.index,\n",
    "                        0, mean_baseline_df[\"saccade_probability_eye1\"],\n",
    "                        step='mid', color='cyan', alpha=0.2)\n",
    "        ax8.set_ylabel('Saccade Probability', color='cyan')\n",
    "        ax8.yaxis.label.set_color('cyan')\n",
    "        # Autoscale based on actual data range\n",
    "        y_min = mean_baseline_df[\"saccade_probability_eye1\"].min()\n",
    "        y_max = mean_baseline_df[\"saccade_probability_eye1\"].max()\n",
    "        if y_max > y_min:\n",
    "            ax8.set_ylim(y_min * 0.9, y_max * 1.1)\n",
    "    \n",
    "    # Add vertical line at event time (t=0)\n",
    "    ax.axvline(x=0, color='black', linestyle='--', alpha=0.5, linewidth=1)\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    \n",
    "    # Save the figure\n",
    "    figure_file = output_folder / f\"{session_name}_{event_name}_baselined{suffix}.pdf\"\n",
    "    fig.savefig(figure_file, format='pdf', bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    \n",
    "    return figure_file\n",
    "\n",
    "\n",
    "def baseline_aligned_data_simple(aligned_df, left_turns_df, right_turns_df, baseline_window, mouse_name, session_name, event_name, output_folder, csv_file, plot_width=12, create_plots=True):\n",
    "    \"\"\"\n",
    "    Simple baseline correction and plotting function.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    aligned_df : pd.DataFrame\n",
    "        Main aligned data\n",
    "    left_turns_df : pd.DataFrame or None\n",
    "        Left turns data\n",
    "    right_turns_df : pd.DataFrame or None\n",
    "        Right turns data\n",
    "    baseline_window : tuple\n",
    "        Tuple of (start_time, end_time) for baseline window\n",
    "    mouse_name : str\n",
    "        Mouse name for file naming\n",
    "    session_name : str\n",
    "        Session name for file naming\n",
    "    event_name : str\n",
    "        Event name for file naming\n",
    "    output_folder : Path\n",
    "        Output folder path\n",
    "    csv_file : Path\n",
    "        Original CSV file path\n",
    "    plot_width : int\n",
    "        Width of the plot in inches\n",
    "    create_plots : bool\n",
    "        Whether to create and save plots (default: True)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"      🔄 Performing baseline correction...\")\n",
    "\n",
    "    def baseline_dataframe(df, baseline_window, mouse_name, event_name, output_folder, suffix=\"\"):\n",
    "        \"\"\"Helper function to baseline a single dataframe\"\"\"\n",
    "        # Make a copy to avoid modifying the original data\n",
    "        df_copy = df.copy()\n",
    "        \n",
    "        # Calculate baseline values\n",
    "        baseline_df = df_copy[\n",
    "            (df_copy[\"Time (s)\"] >= baseline_window[0]) & \n",
    "            (df_copy[\"Time (s)\"] <= baseline_window[1])\n",
    "        ].groupby(\"Halt Time\").mean(numeric_only=True)\n",
    "        \n",
    "        # Create baseline-corrected columns (only for specified signals)\n",
    "        # Baselined: fluorescence, running velocity, pupil diameter, eye position\n",
    "        # NOT baselined: motor velocity, turning velocity, saccade probability\n",
    "        signals_to_baseline = [\n",
    "            \"z_470\", \"z_560\",                    # Fluorescence - baselined\n",
    "            \"Velocity_0X\",                       # Running velocity - baselined\n",
    "            \"Pupil.Diameter_eye1\",               # Pupil diameter - baselined\n",
    "            \"Ellipse.Center.X_eye1\"              # Eye position X - baselined\n",
    "        ]\n",
    "        \n",
    "        for signal_name in signals_to_baseline:\n",
    "            if signal_name in df_copy.columns:\n",
    "                df_copy[f\"{signal_name}_Baseline\"] = df_copy[signal_name] - df_copy[\"Halt Time\"].map(baseline_df[signal_name])\n",
    "            else:\n",
    "                if suffix:  # Only print warning for main data, not for each turn type\n",
    "                    pass\n",
    "                elif signal_name not in [\"Pupil.Diameter_eye1\", \"Ellipse.Center.X_eye1\"]:\n",
    "                    # Only warn for required columns, not optional eye tracking\n",
    "                    print(f\"      ⚠️  Column {signal_name} not found in data, skipping baseline correction...\")\n",
    "        \n",
    "        # Define the baseline data file path\n",
    "        if suffix:\n",
    "            baseline_data_file = output_folder / f\"{mouse_name}_{event_name}_{suffix}_baselined_data.csv\"\n",
    "        else:\n",
    "            baseline_data_file = output_folder / f\"{mouse_name}_{event_name}_baselined_data.csv\"\n",
    "        \n",
    "        # Save the baseline-corrected data\n",
    "        df_copy.to_csv(baseline_data_file, index=False)\n",
    "        print(f\"      💾 Saved {suffix} baseline data to: {baseline_data_file.name}\")\n",
    "        \n",
    "        return df_copy\n",
    "\n",
    "    # ---------------- Baseline Correction ----------------\n",
    "    # Process main aligned data\n",
    "    aligned_df_baselined = baseline_dataframe(aligned_df, baseline_window, mouse_name, event_name, output_folder)\n",
    "    \n",
    "    # Process left turns data if available\n",
    "    left_turns_df_baselined = None\n",
    "    if left_turns_df is not None:\n",
    "        print(f\"      🔄 Processing left turns data...\")\n",
    "        left_turns_df_baselined = baseline_dataframe(left_turns_df, baseline_window, mouse_name, event_name, output_folder, \"left_turns\")\n",
    "    \n",
    "    # Process right turns data if available\n",
    "    right_turns_df_baselined = None\n",
    "    if right_turns_df is not None:\n",
    "        print(f\"      🔄 Processing right turns data...\")\n",
    "        right_turns_df_baselined = baseline_dataframe(right_turns_df, baseline_window, mouse_name, event_name, output_folder, \"right_turns\")\n",
    "\n",
    "    # ---------------- Plotting ----------------\n",
    "    if create_plots:\n",
    "        print(f\"      📊 Creating baseline plots...\")\n",
    "        \n",
    "        # Plot 1: Combined (all events)\n",
    "        print(f\"      📊 Creating combined plot (all events)...\")\n",
    "        numeric_columns = aligned_df_baselined.select_dtypes(include=['number']).columns\n",
    "        mean_combined = aligned_df_baselined.groupby(\"Time (s)\")[numeric_columns].mean()\n",
    "        sem_combined = aligned_df_baselined.groupby(\"Time (s)\")[numeric_columns].sem()\n",
    "        \n",
    "        figure_file_combined = create_single_baseline_plot(\n",
    "            mean_baseline_df=mean_combined,\n",
    "            sem_baseline_df=sem_combined,\n",
    "            mouse_name=mouse_name,\n",
    "            session_name=session_name,\n",
    "            event_name=event_name,\n",
    "            output_folder=output_folder,\n",
    "            plot_width=plot_width,\n",
    "            suffix=\"\",\n",
    "            turn_type=\"\"\n",
    "        )\n",
    "        print(f\"      💾 Saved combined plot to: {figure_file_combined.name}\")\n",
    "        \n",
    "        # Plot 2: Left turns only\n",
    "        if left_turns_df_baselined is not None and len(left_turns_df_baselined) > 0:\n",
    "            print(f\"      📊 Creating left turns plot...\")\n",
    "            numeric_columns_left = left_turns_df_baselined.select_dtypes(include=['number']).columns\n",
    "            mean_left = left_turns_df_baselined.groupby(\"Time (s)\")[numeric_columns_left].mean()\n",
    "            sem_left = left_turns_df_baselined.groupby(\"Time (s)\")[numeric_columns_left].sem()\n",
    "            \n",
    "            figure_file_left = create_single_baseline_plot(\n",
    "                mean_baseline_df=mean_left,\n",
    "                sem_baseline_df=sem_left,\n",
    "                mouse_name=mouse_name,\n",
    "                session_name=session_name,\n",
    "                event_name=event_name,\n",
    "                output_folder=output_folder,\n",
    "                plot_width=plot_width,\n",
    "                suffix=\"_left_turns\",\n",
    "                turn_type=\"LEFT TURNS ONLY\"\n",
    "            )\n",
    "            print(f\"      💾 Saved left turns plot to: {figure_file_left.name}\")\n",
    "        else:\n",
    "            print(f\"      ℹ️  No left turns detected - skipping left turns plot\")\n",
    "        \n",
    "        # Plot 3: Right turns only\n",
    "        if right_turns_df_baselined is not None and len(right_turns_df_baselined) > 0:\n",
    "            print(f\"      📊 Creating right turns plot...\")\n",
    "            numeric_columns_right = right_turns_df_baselined.select_dtypes(include=['number']).columns\n",
    "            mean_right = right_turns_df_baselined.groupby(\"Time (s)\")[numeric_columns_right].mean()\n",
    "            sem_right = right_turns_df_baselined.groupby(\"Time (s)\")[numeric_columns_right].sem()\n",
    "            \n",
    "            figure_file_right = create_single_baseline_plot(\n",
    "                mean_baseline_df=mean_right,\n",
    "                sem_baseline_df=sem_right,\n",
    "                mouse_name=mouse_name,\n",
    "                session_name=session_name,\n",
    "                event_name=event_name,\n",
    "                output_folder=output_folder,\n",
    "                plot_width=plot_width,\n",
    "                suffix=\"_right_turns\",\n",
    "                turn_type=\"RIGHT TURNS ONLY\"\n",
    "            )\n",
    "            print(f\"      💾 Saved right turns plot to: {figure_file_right.name}\")\n",
    "        else:\n",
    "            print(f\"      ℹ️  No right turns detected - skipping right turns plot\")\n",
    "        \n",
    "        return figure_file_combined\n",
    "    else:\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b673f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Process all aligned_data folders\n",
    "    results = process_aligned_data_folders(\n",
    "        data_dirs=data_dirs,\n",
    "        baseline_window=baseline_window,\n",
    "        event_name=event_name,\n",
    "        plot_width=plot_width,\n",
    "        create_plots=True  # Ensure this is set to True to save plots\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n🎉 Processing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bd2894",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "aeon",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
