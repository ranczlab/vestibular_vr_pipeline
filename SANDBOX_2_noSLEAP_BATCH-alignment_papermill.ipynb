{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a329aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIXME: this notebook does not work with the papermill script yet, it throws an error because it does not find parquet files#\n",
    "#this notebook is supposed to be run using the papermill script 2_run_iterative_notebook2.py#\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "import plotly.express as px\n",
    "import plotly.subplots as sp\n",
    "import math\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy.stats import mode\n",
    "from scipy.integrate import cumulative_trapezoid\n",
    "from scipy.signal import correlate\n",
    "import json\n",
    "%config Completer.use_jedi = False  # Fixes autocomplete issues\n",
    "%config InlineBackend.figure_format = 'retina'  # Improves plot resolution\n",
    "\n",
    "import gc # garbage collector for removing large variables from memory instantly \n",
    "import importlib #for force updating changed packages \n",
    "\n",
    "#import harp\n",
    "import harp_resources.process\n",
    "import harp_resources.utils\n",
    "from harp_resources import process, utils # Reassign to maintain direct references for force updating \n",
    "#from sleap import load_and_process as lp\n",
    "import papermill as pm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429681bc",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# IMPORTANT: Make sure this cell has the \"parameters\" tag in the Jupyter notebook\n",
    "# In Jupyter: View -> Cell Toolbar -> Tags -> Add \"parameters\" tag to this cell\n",
    "\n",
    "# Define default values (these will be overridden by papermill)\n",
    "Data_path = None\n",
    "Time_window_start = 1\n",
    "Time_window_end = 1\n",
    "Baseline_window = (-1, 0)\n",
    "Plot_width = 1\n",
    "Event_name = \"Apply halt: 2s\"\n",
    "Vestibular_mismatch = False\n",
    "Common_resampled_rate = 1\n",
    "Plot_fig1 = False\n",
    "Has_sleap = False\n",
    "\n",
    "# Now convert the parameters to the variable names used in your notebook\n",
    "data_path = Path(Data_path) if Data_path is not None else Path(\".\")\n",
    "# data_path = Path('~/Desktop/RANCZLAB-NAS/data/ONIX/20250409_Cohort3_rotation/Visual_mismatch_day3/B6J2780-2025-04-25T11-51-53_processedData/downsampled_data/').expanduser()\n",
    "time_window_start = Time_window_start\n",
    "time_window_end = Time_window_end\n",
    "baseline_window = Baseline_window\n",
    "plot_width = Plot_width\n",
    "event_name = Event_name\n",
    "vestibular_mismatch = Vestibular_mismatch\n",
    "common_resampled_rate = Common_resampled_rate\n",
    "plot_fig1 = Plot_fig1\n",
    "has_sleap = Has_sleap\n",
    "\n",
    "# For debugging - print out to confirm parameters were received\n",
    "print(f\"Parameters received:\")\n",
    "print(f\"data_path = {data_path}\")\n",
    "print(f\"time_window_start = {time_window_start}\")\n",
    "print(f\"time_window_end = {time_window_end}\")\n",
    "print(f\"event_name = {event_name}\")\n",
    "print(f\"has_sleap = {has_sleap}\")\n",
    "\n",
    "# For the loop issue, define a list with a single element\n",
    "data_paths = [data_path]\n",
    "idx = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e9c1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------\n",
    "# load downsampled data and save in dictionary\n",
    "#-------------------------------\n",
    "loaded_data = {}  # Dictionary to store loaded data for each path\n",
    "\n",
    "photometry_tracking_encoder_data = pd.read_parquet(data_path / \"photometry_tracking_encoder_data.parquet\", engine=\"pyarrow\")\n",
    "camera_photodiode_data = pd.read_parquet(data_path / \"camera_photodiode_data.parquet\", engine=\"pyarrow\")\n",
    "experiment_events = pd.read_parquet(data_path / \"experiment_events.parquet\", engine=\"pyarrow\")\n",
    "photometry_info = pd.read_parquet(data_path / \"photometry_info.parquet\", engine=\"pyarrow\")\n",
    "session_settings = pd.read_parquet(data_path / \"session_settings.parquet\", engine=\"pyarrow\")\n",
    "session_settings[\"metadata\"] = session_settings[\"metadata\"].apply(process.safe_from_json)\n",
    "\n",
    "print(f\"✅ Finished loading all parquet files\")\n",
    "\n",
    "# Calculate time differences between event_name events\n",
    "\n",
    "event_times = experiment_events[experiment_events[\"Event\"] == event_name].index\n",
    "if len(event_times) > 1:\n",
    "    time_diffs = event_times.to_series().diff().dropna().dt.total_seconds()\n",
    "            # Print the 5 shortest time differences\n",
    "            # print(\"5 shortest time differences between events:\")\n",
    "            # print(time_diffs.nsmallest(5))\n",
    "if (time_diffs < 10).any():\n",
    "    print(f\"⚠️ Warning: Some '{event_name}' events are less than 10 seconds apart. Consider applying a filter to events.\")\n",
    "else:\n",
    "    print(f\"ℹ️ INFO: Found {len(event_times)} events with name '{event_name}' - not enough to calculate differences\")\n",
    "\n",
    "mouse_name = process.check_exp_events(experiment_events, photometry_info, verbose = True)\n",
    "\n",
    "# Store all loaded data in the dictionary\n",
    "loaded_data[data_path] = {\n",
    "    \"photometry_tracking_encoder_data\": photometry_tracking_encoder_data,\n",
    "    \"camera_photodiode_data\": camera_photodiode_data,\n",
    "    \"experiment_events\": experiment_events,\n",
    "    \"photometry_info\": photometry_info,\n",
    "    \"session_settings\": session_settings,\n",
    "    \"mouse_name\": mouse_name\n",
    "    }\n",
    "#---------------------------------------------------\n",
    "# create DFs and plot figure if True\n",
    "#---------------------------------------------------\n",
    "\n",
    "data_path_variables = {}\n",
    "\n",
    "print(f\"\\n--------- Processing analysis for data path {idx}/{len(data_paths)}: {data_path} ---------\")\n",
    "\n",
    "# Load the data from the dictionary\n",
    "photometry_tracking_encoder_data = loaded_data[data_path][\"photometry_tracking_encoder_data\"]\n",
    "camera_photodiode_data = loaded_data[data_path][\"camera_photodiode_data\"]\n",
    "experiment_events = loaded_data[data_path][\"experiment_events\"]\n",
    "mouse_name = loaded_data[data_path][\"mouse_name\"]\n",
    "session_name = f\"{mouse_name}_{data_path.name}\"  # Assuming session_name is constructed this way\n",
    "      \n",
    "\n",
    "df_to_analyze = photometry_tracking_encoder_data[\"Photodiode_int\"] #using downsampled values in common time grid \n",
    "#df_to_analyze = camera_photodiode_data[\"Photodiode\"] #use async raw values if needed for troubleshooting, but the nearest indices needs to be found , see couple of lines below\n",
    "\n",
    "if vestibular_mismatch or event_name == \"No halt\": #determine halt times based on experiment events \n",
    "    photodiode_halts = experiment_events[experiment_events[\"Event\"] == event_name].index.tolist()\n",
    "    nearest_indices = photometry_tracking_encoder_data.index.get_indexer(photodiode_halts, method='nearest')\n",
    "    photodiode_halts = photometry_tracking_encoder_data.index[nearest_indices] # as experiment events timestamps are not in the same time grid as downsampled data\n",
    "    print (\"ℹ️ INFO: vestibular MM or 'No halt', no signal in the photodiode, using experiment events for MM times\")\n",
    "    photodiode_delay_min = photodiode_delay_avg = photodiode_delay_max = None\n",
    "\n",
    "else: #determine exact halt times based on photodiode signal\n",
    "    photodiode_halts, photodiode_delay_min, photodiode_delay_avg, photodiode_delay_max = process.analyze_photodiode(df_to_analyze, experiment_events, event_name, plot = True)\n",
    "# nearest_indices = photometry_tracking_encoder_data.index.get_indexer(photodiode_halts, method='nearest')\n",
    "# photodiode_halts = photometry_tracking_encoder_data.index[nearest_indices]\n",
    "if plot_fig1:\n",
    "    process.plot_figure_1(photometry_tracking_encoder_data, session_name, save_path, common_resampled_rate, photodiode_halts, save_figure = True, show_figure = True, downsample_factor=50)\n",
    "else: \n",
    "    print (\"ℹ️ INFO: skipping figure 1\")\n",
    "del df_to_analyze\n",
    "gc.collect()\n",
    "None\n",
    "\n",
    "# Store analysis results\n",
    "data_path_variables[data_path] = {\n",
    "    \"photodiode_halts\": photodiode_halts,\n",
    "    \"photodiode_delay_min\": photodiode_delay_min,\n",
    "    \"photodiode_delay_avg\": photodiode_delay_avg,\n",
    "    \"photodiode_delay_max\": photodiode_delay_max,\n",
    "    \"session_name\": session_name\n",
    "}\n",
    "# Define the selected indices\n",
    "#selected_indices = [0, 1, 11, 16]\n",
    "selected_indices = list(range(len(photodiode_halts)))\n",
    "\n",
    "print(f\"\\n✅ Finished analyzing all {len(data_path_variables)} successfully processed data paths\")\n",
    "\n",
    "#---------------------------------------------------\n",
    "# Create aligned data and plot comprehensive figures for each data path\n",
    "#---------------------------------------------------\n",
    "\n",
    "print(f\"Using time window: {time_window_start}s to {time_window_end}s relative to halt\")\n",
    "\n",
    "\n",
    "# Extract data\n",
    "data = loaded_data[data_path]\n",
    "vars_ = data_path_variables[data_path]\n",
    "        \n",
    "df = data[\"photometry_tracking_encoder_data\"]\n",
    "halts = vars_[\"photodiode_halts\"]\n",
    "        \n",
    "session_name = vars_.get(\"session_name\")\n",
    "if not session_name:\n",
    "    mouse_name = data.get(\"mouse_name\", \"unknown_mouse\")\n",
    "    session_name = f\"{mouse_name}_{data_path.stem}\"\n",
    "    print(f\"⚠️ No session_name found, using: {session_name}\")\n",
    "\n",
    "event_name = event_name\n",
    "print(f\"Aligning {len(halts)} events for session '{session_name}'\")\n",
    "\n",
    "    # Align data to each halt event\n",
    "aligned_data = []\n",
    "for i, halt_time in enumerate(halts):\n",
    "    window = df.loc[\n",
    "        (df.index >= halt_time + pd.Timedelta(seconds=time_window_start)) &\n",
    "        (df.index <= halt_time + pd.Timedelta(seconds=time_window_end))\n",
    "    ].copy()\n",
    "\n",
    "    if window.empty:\n",
    "        print(f\"⚠️ No data in window around halt {halt_time}\")\n",
    "        continue\n",
    "    window[\"Time (s)\"] = (window.index - halt_time).total_seconds()\n",
    "    window[\"Halt Time\"] = halt_time\n",
    "    aligned_data.append(window)\n",
    "\n",
    "if not aligned_data:\n",
    "    print(f\"⚠️ No aligned data generated for {session_name}, skipping\")\n",
    "\n",
    "aligned_df = pd.concat(aligned_data, ignore_index=True)\n",
    "\n",
    "# Save CSV in same folder as data\n",
    "aligned_dir = data_path.parent / \"aligned_data\"\n",
    "aligned_dir.mkdir(exist_ok=True)\n",
    "\n",
    "aligned_file = aligned_dir / f\"{session_name}_{event_name}_aligned.csv\"\n",
    "aligned_df.to_csv(aligned_file, index=False)\n",
    "print(f\"✅ Saved aligned data to {aligned_file}\")\n",
    "\n",
    "# Compute mean and standard error of the mean (SEM)\n",
    "mean_df = aligned_df.groupby(\"Time (s)\").mean()\n",
    "sem_df = aligned_df.groupby(\"Time (s)\").sem()  \n",
    "\n",
    "# Create figure for the two plots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6), sharex=True)\n",
    "\n",
    "## Plot 1: Individual Traces\n",
    "ax1 = axes[0]\n",
    "for halt in aligned_df[\"Halt Time\"].unique():\n",
    "    subset = aligned_df[aligned_df[\"Halt Time\"] == halt]\n",
    "    ax1.plot(subset[\"Time (s)\"], subset[\"Photodiode_int\"], color='grey', alpha=0.5)\n",
    "\n",
    "ax1.set_title('Photodiode, z_470, and z_560')\n",
    "ax1.set_xlabel(\"Time (s)\")\n",
    "ax1.set_ylabel(\"Photodiode\")\n",
    "\n",
    "ax1_2 = ax1.twinx()\n",
    "for halt in aligned_df[\"Halt Time\"].unique():\n",
    "    subset = aligned_df[aligned_df[\"Halt Time\"] == halt]\n",
    "    ax1_2.plot(subset[\"Time (s)\"], subset[\"z_470\"], color='green', alpha=0.5)\n",
    "    ax1_2.plot(subset[\"Time (s)\"], subset[\"z_560\"], color='red', alpha=0.5)\n",
    "\n",
    "ax1_2.set_ylabel(\"Fluorescence (z-score)\", color='green')\n",
    "\n",
    "## Plot 2: Mean + SEM\n",
    "ax2 = axes[1]\n",
    "ax2.plot(mean_df.index, mean_df[\"Photodiode_int\"], color='grey')\n",
    "ax2.fill_between(mean_df.index, mean_df[\"Photodiode_int\"] - sem_df[\"Photodiode_int\"],\n",
    "                         mean_df[\"Photodiode_int\"] + sem_df[\"Photodiode_int\"], color='grey', alpha=0.2)\n",
    "ax2.set_xlabel(\"Time (s)\")\n",
    "ax2.set_ylabel(\"Photodiode\")\n",
    "ax2.set_title(\"Mean & SEM\")\n",
    "\n",
    "ax2_2 = ax2.twinx()\n",
    "ax2_2.plot(mean_df.index, mean_df[\"z_470\"], color='green')\n",
    "ax2_2.fill_between(mean_df.index, mean_df[\"z_470\"] - sem_df[\"z_470\"], \n",
    "                           mean_df[\"z_470\"] + sem_df[\"z_470\"], color='green', alpha=0.2)\n",
    "\n",
    "ax2_2.plot(mean_df.index, mean_df[\"z_560\"], color='red')\n",
    "ax2_2.fill_between(mean_df.index, mean_df[\"z_560\"] - sem_df[\"z_560\"], \n",
    "                           mean_df[\"z_560\"] + sem_df[\"z_560\"], color='red', alpha=0.2)\n",
    "\n",
    "ax2_2.set_ylabel(\"Fluorescence (z-score)\", color='green')\n",
    "\n",
    "# Save figure in same folder as data\n",
    "fig.suptitle(f\"{session_name} - {event_name}\")\n",
    "fig.tight_layout()\n",
    "figure_file = data_path.parent / f\"{session_name}_{event_name}.pdf\"\n",
    "fig.savefig(figure_file, dpi=300)\n",
    "plt.close(fig)\n",
    "print(f\"✅ Saved figure to {figure_file}\")\n",
    "\n",
    "\n",
    "print(\"\\n✅ Finished all data paths.\")\n",
    "\n",
    "#-------------------------------\n",
    "# Baseline correction and plotting\n",
    "#-------------------------------\n",
    "\n",
    "# Perform baseline correction\n",
    "baseline_start, baseline_end = baseline_window\n",
    "baseline_data = aligned_df[\n",
    "    (aligned_df[\"Time (s)\"] >= baseline_start) & \n",
    "    (aligned_df[\"Time (s)\"] <= baseline_end)\n",
    "]\n",
    "\n",
    "# Compute baseline mean for each signal\n",
    "baseline_means = baseline_data.groupby(\"Halt Time\").mean()\n",
    "\n",
    "# Subtract baseline mean from each signal\n",
    "aligned_df[\"Photodiode_int_Baseline\"] = aligned_df[\"Photodiode_int\"] - aligned_df[\"Halt Time\"].map(baseline_means[\"Photodiode_int\"])\n",
    "aligned_df[\"z_470_Baseline\"] = aligned_df[\"z_470\"] - aligned_df[\"Halt Time\"].map(baseline_means[\"z_470\"])\n",
    "aligned_df[\"z_560_Baseline\"] = aligned_df[\"z_560\"] - aligned_df[\"Halt Time\"].map(baseline_means[\"z_560\"])\n",
    "\n",
    "# Compute mean and SEM for baseline-corrected signals\n",
    "mean_baseline_df = aligned_df.groupby(\"Time (s)\").mean()\n",
    "sem_baseline_df = aligned_df.groupby(\"Time (s)\").sem()\n",
    "\n",
    "# Plot baseline-corrected signals\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "# Plot Photodiode signal\n",
    "ax.plot(mean_baseline_df.index, mean_baseline_df[\"Photodiode_int_Baseline\"], color='grey', alpha=0.8)\n",
    "ax.fill_between(mean_baseline_df.index, \n",
    "                mean_baseline_df[\"Photodiode_int_Baseline\"] - sem_baseline_df[\"Photodiode_int_Baseline\"], \n",
    "                mean_baseline_df[\"Photodiode_int_Baseline\"] + sem_baseline_df[\"Photodiode_int_Baseline\"], \n",
    "                color='grey', alpha=0.2)\n",
    "\n",
    "ax.set_xlabel('Time (s) relative to halt')\n",
    "ax.set_ylabel('Photodiode (Baseline Corrected)', color='grey')\n",
    "ax.set_title(f'Baselined Mean & SEM of All Signals - {session_name}')\n",
    "\n",
    "# Fluorescence signals (z_470 and z_560)\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(mean_baseline_df.index, mean_baseline_df[\"z_470_Baseline\"], color='green', linestyle='-', alpha=0.8)\n",
    "ax2.fill_between(mean_baseline_df.index, \n",
    "                 mean_baseline_df[\"z_470_Baseline\"] - sem_baseline_df[\"z_470_Baseline\"], \n",
    "                 mean_baseline_df[\"z_470_Baseline\"] + sem_baseline_df[\"z_470_Baseline\"], \n",
    "                 color='green', alpha=0.2)\n",
    "ax2.plot(mean_baseline_df.index, mean_baseline_df[\"z_560_Baseline\"], color='red', linestyle='-', alpha=0.8)\n",
    "ax2.fill_between(mean_baseline_df.index, \n",
    "                 mean_baseline_df[\"z_560_Baseline\"] - sem_baseline_df[\"z_560_Baseline\"], \n",
    "                 mean_baseline_df[\"z_560_Baseline\"] + sem_baseline_df[\"z_560_Baseline\"], \n",
    "                 color='red', alpha=0.2)\n",
    "ax2.set_ylabel('Fluorescence (z-score, Baseline Corrected)', color='green')\n",
    "\n",
    "# Adjust layout and save the figure\n",
    "fig.tight_layout()\n",
    "baseline_figure_file = aligned_dir / f\"{session_name}_{event_name}_baseline_corrected.pdf\"\n",
    "fig.savefig(baseline_figure_file, dpi=300)\n",
    "plt.close(fig)\n",
    "print(f\"✅ Saved baseline-corrected figure to {baseline_figure_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7c038f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aeon2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
