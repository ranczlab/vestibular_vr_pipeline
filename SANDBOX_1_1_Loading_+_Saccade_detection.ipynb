{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.io as pio\n",
    "import gc\n",
    "\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.colors import LogNorm\n",
    "from fastkde.fastKDE import fastKDE\n",
    "from scipy.stats import linregress\n",
    "from scipy.signal import butter, filtfilt\n",
    "from scipy.signal import correlate\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "from harp_resources import process, utils\n",
    "from sleap import load_and_process as lp\n",
    "\n",
    "# symbols to use ✅ ℹ️ ⚠️ ❗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up variables and load data \n",
    "############################################################################################################\n",
    "\n",
    "plot_timeseries = False\n",
    "score_cutoff = 0.2 # for filtering out inferred points with low confidence, they get interpolated \n",
    "outlier_sd_threshold = 10 # for removing outliers from the data, they get interpolated \n",
    "NaNs_removed = False # for checking if NaNs already removed in the notebook\n",
    "cutoff = 10  # Hz for pupil diameter filtering \n",
    "\n",
    "# for saccades\n",
    "framerate = 59.77  # Hz (in the future, should come from saved data)\n",
    "threshold = 65  # px/s FIXME make this adaptive\n",
    "refractory_period = pd.Timedelta(milliseconds=100)  # msec, using pd.Timedelta for datetime index\n",
    "plot_saccade_detection_QC = True\n",
    "\n",
    "data_path = Path('/Users/rancze/Documents/Data/vestVR/20250409_Cohort3_rotation/Visual_mismatch_day4/B6J2782-2025-04-28T14-22-03') \n",
    "#data_path = Path('/Users/rancze/Documents/Data/vestVR/Cohort1/No_iso_correction/Visual_mismatch_day3/B6J2717-2024-12-10T12-17-03') # sleap data 2 is duplicate of sleap data 1\n",
    "save_path = data_path.parent / f\"{data_path.name}_processedData\"\n",
    "\n",
    "\n",
    "print (\"\\n❗ 20251025 NOT sure I understand this Ede ---- if SleapData.csv was already saved in the VideoData folder, this may break. Delete the file if you want to rerun processing\\n\")\n",
    "VideoData1, VideoData2, VideoData1_Has_Sleap, VideoData2_Has_Sleap = lp.load_videography_data(data_path)\n",
    "\n",
    "columns_of_interest = ['left.x','left.y','center.x','center.y','right.x','right.y','p1.x','p1.y','p2.x','p2.y','p3.x','p3.y','p4.x','p4.y','p5.x','p5.y','p6.x','p6.y','p7.x','p7.y','p8.x','p8.y']\n",
    "\n",
    "if VideoData1_Has_Sleap:\n",
    "    VideoData1 = VideoData1.drop(columns=['track']) # drop the track column as it is empty\n",
    "    coordinates_dict1_raw=lp.get_coordinates_dict(VideoData1, columns_of_interest)\n",
    "    FPS_1 = 1 / VideoData1[\"Seconds\"].diff().mean()  # frame rate for VideoData1 TODO where to save it, is it useful?\n",
    "    print(f\"FPS_1: {FPS_1}\")\n",
    "\n",
    "if VideoData2_Has_Sleap:\n",
    "    VideoData2 = VideoData2.drop(columns=['track']) # drop the track column as it is empty\n",
    "    coordinates_dict2_raw=lp.get_coordinates_dict(VideoData2, columns_of_interest)\n",
    "    FPS_2 = 1 / VideoData2[\"Seconds\"].diff().mean()  # frame rate for VideoData2\n",
    "    print(f\"FPS_2: {FPS_2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot timeseries of coordinates in browser for both VideoData1 and VideoData2\n",
    "############################################################################################################\n",
    "if plot_timeseries:\n",
    "    print(f'⚠️ Check for long discontinuities and outliers in the data, we will try to deal with them later')\n",
    "    print(f'ℹ️ Figures open in browser window, takes a bit of time.')\n",
    "\n",
    "    # Helper list variables\n",
    "    subplot_titles = (\n",
    "        \"X coordinates for pupil centre and left-right eye corner\",\n",
    "        \"Y coordinates for pupil centre and left-right eye corner\",\n",
    "        \"X coordinates for iris points\",\n",
    "        \"Y coordinates for iris points\"\n",
    "    )\n",
    "    eye_x = ['left.x', 'center.x', 'right.x']\n",
    "    eye_y = ['left.y', 'center.y', 'right.y']\n",
    "    iris_x = ['p1.x', 'p2.x', 'p3.x', 'p4.x', 'p5.x', 'p6.x', 'p7.x', 'p8.x']\n",
    "    iris_y = ['p1.y', 'p2.y', 'p3.y', 'p4.y', 'p5.y', 'p6.y', 'p7.y', 'p8.y']\n",
    "\n",
    "    # --- VideoData1 ---\n",
    "    if VideoData1_Has_Sleap:\n",
    "        fig1 = make_subplots(\n",
    "            rows=4, cols=1,\n",
    "            shared_xaxes=True,\n",
    "            vertical_spacing=0.05,\n",
    "            subplot_titles=subplot_titles\n",
    "        )\n",
    "\n",
    "        # Row 1: left.x, center.x, right.x\n",
    "        for col in eye_x:\n",
    "            fig1.add_trace(go.Scatter(x=VideoData1['Seconds'], y=VideoData1[col], mode='lines', name=col), row=1, col=1)\n",
    "        # Row 2: left.y, center.y, right.y\n",
    "        for col in eye_y:\n",
    "            fig1.add_trace(go.Scatter(x=VideoData1['Seconds'], y=VideoData1[col], mode='lines', name=col), row=2, col=1)\n",
    "        # Row 3: p1.x ... p8.x\n",
    "        for col in iris_x:\n",
    "            fig1.add_trace(go.Scatter(x=VideoData1['Seconds'], y=VideoData1[col], mode='lines', name=col), row=3, col=1)\n",
    "        # Row 4: p1.y ... p8.y\n",
    "        for col in iris_y:\n",
    "            fig1.add_trace(go.Scatter(x=VideoData1['Seconds'], y=VideoData1[col], mode='lines', name=col), row=4, col=1)\n",
    "\n",
    "        fig1.update_layout(\n",
    "            height=1200,\n",
    "            title_text=\"Time series subplots for coordinates [VideoData1]\",\n",
    "            showlegend=True\n",
    "        )\n",
    "        fig1.update_xaxes(title_text=\"Seconds\", row=4, col=1)\n",
    "        fig1.update_yaxes(title_text=\"X Position\", row=1, col=1)\n",
    "        fig1.update_yaxes(title_text=\"Y Position\", row=2, col=1)\n",
    "        fig1.update_yaxes(title_text=\"X Position\", row=3, col=1)\n",
    "        fig1.update_yaxes(title_text=\"Y Position\", row=4, col=1)\n",
    "\n",
    "        fig1.show(renderer='browser')\n",
    "\n",
    "    # --- VideoData2 ---\n",
    "    if VideoData2_Has_Sleap:\n",
    "        fig2 = make_subplots(\n",
    "            rows=4, cols=1,\n",
    "            shared_xaxes=True,\n",
    "            vertical_spacing=0.05,\n",
    "            subplot_titles=subplot_titles\n",
    "        )\n",
    "        # Row 1: left.x, center.x, right.x\n",
    "        for col in eye_x:\n",
    "            fig2.add_trace(go.Scatter(x=VideoData2['Seconds'], y=VideoData2[col], mode='lines', name=col), row=1, col=1)\n",
    "        # Row 2: left.y, center.y, right.y\n",
    "        for col in eye_y:\n",
    "            fig2.add_trace(go.Scatter(x=VideoData2['Seconds'], y=VideoData2[col], mode='lines', name=col), row=2, col=1)\n",
    "        # Row 3: p1.x ... p8.x\n",
    "        for col in iris_x:\n",
    "            fig2.add_trace(go.Scatter(x=VideoData2['Seconds'], y=VideoData2[col], mode='lines', name=col), row=3, col=1)\n",
    "        # Row 4: p1.y ... p8.y\n",
    "        for col in iris_y:\n",
    "            fig2.add_trace(go.Scatter(x=VideoData2['Seconds'], y=VideoData2[col], mode='lines', name=col), row=4, col=1)\n",
    "\n",
    "        fig2.update_layout(\n",
    "            height=1200,\n",
    "            title_text=\"Time series subplots for coordinates [VideoData2]\",\n",
    "            showlegend=True\n",
    "        )\n",
    "        fig2.update_xaxes(title_text=\"Seconds\", row=4, col=1)\n",
    "        fig2.update_yaxes(title_text=\"X Position\", row=1, col=1)\n",
    "        fig2.update_yaxes(title_text=\"Y Position\", row=2, col=1)\n",
    "        fig2.update_yaxes(title_text=\"X Position\", row=3, col=1)\n",
    "        fig2.update_yaxes(title_text=\"Y Position\", row=4, col=1)\n",
    "\n",
    "        fig2.show(renderer='browser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QC plot XY coordinate distributions to visualize outliers \n",
    "############################################################################################################\n",
    "\n",
    "columns_of_interest = ['left', 'right', 'center', 'p1', 'p2', 'p3', 'p4', 'p5', 'p6', 'p7', 'p8']\n",
    "\n",
    "# Filter out NaN values and calculate the min and max values for X and Y coordinates for both dict1 and dict2\n",
    "\n",
    "def min_max_dict(coordinates_dict):\n",
    "    x_min = min([coordinates_dict[f'{col}.x'][~np.isnan(coordinates_dict[f'{col}.x'])].min() for col in columns_of_interest])\n",
    "    x_max = max([coordinates_dict[f'{col}.x'][~np.isnan(coordinates_dict[f'{col}.x'])].max() for col in columns_of_interest])\n",
    "    y_min = min([coordinates_dict[f'{col}.y'][~np.isnan(coordinates_dict[f'{col}.y'])].min() for col in columns_of_interest])\n",
    "    y_max = max([coordinates_dict[f'{col}.y'][~np.isnan(coordinates_dict[f'{col}.y'])].max() for col in columns_of_interest])\n",
    "    return x_min, x_max, y_min, y_max\n",
    "\n",
    "# Only plot panels for 1 and 2 if VideoData1_Has_Sleap and/or VideoData2_Has_Sleap are true\n",
    "\n",
    "# Compute min/max as before for global axes limits\n",
    "x_min1, x_max1, y_min1, y_max1 = min_max_dict(coordinates_dict1_raw)\n",
    "x_min2, x_max2, y_min2, y_max2 = min_max_dict(coordinates_dict2_raw)\n",
    "\n",
    "# Use global min and max for consistency across subplots\n",
    "x_min = min(x_min1, x_min2)\n",
    "x_max = max(x_max1, x_max2)\n",
    "y_min = min(y_min1, y_min2)\n",
    "y_max = max(y_max1, y_max2)\n",
    "\n",
    "# Create the figure and axes\n",
    "fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(18, 12))\n",
    "fig.suptitle('XY coordinate distribution of different points for VideoData1 (_dict1) and VideoData2 (_dict2) before outlier removal and NaN interpolation', fontsize=14)\n",
    "\n",
    "# Define colormap for p1-p8\n",
    "colors = ['blue', 'green', 'red', 'cyan', 'magenta', 'yellow', 'black', 'orange']\n",
    "\n",
    "# Panel 1: left, right, center (dict1)\n",
    "if 'VideoData1_Has_Sleap' in globals() and VideoData1_Has_Sleap:\n",
    "    ax[0, 0].set_title('VideoData1 (_dict1): left, right, center')\n",
    "    ax[0, 0].scatter(coordinates_dict1_raw['left.x'], coordinates_dict1_raw['left.y'], color='black', label='left', s=10)\n",
    "    ax[0, 0].scatter(coordinates_dict1_raw['right.x'], coordinates_dict1_raw['right.y'], color='grey', label='right', s=10)\n",
    "    ax[0, 0].scatter(coordinates_dict1_raw['center.x'], coordinates_dict1_raw['center.y'], color='red', label='center', s=10)\n",
    "    ax[0, 0].set_xlim([x_min, x_max])\n",
    "    ax[0, 0].set_ylim([y_min, y_max])\n",
    "    ax[0, 0].set_xlabel('x coordinates (pixels)')\n",
    "    ax[0, 0].set_ylabel('y coordinates (pixels)')\n",
    "    ax[0, 0].legend(loc='upper right')\n",
    "else:\n",
    "    ax[0, 0].axis('off')\n",
    "\n",
    "# Panel 2: p1 to p8 (dict1)\n",
    "if 'VideoData1_Has_Sleap' in globals() and VideoData1_Has_Sleap:\n",
    "    ax[0, 1].set_title('VideoData1 (_dict1): p1 to p8')\n",
    "    for idx, col in enumerate(columns_of_interest[3:]):\n",
    "        ax[0, 1].scatter(coordinates_dict1_raw[f'{col}.x'], coordinates_dict1_raw[f'{col}.y'], color=colors[idx], label=col, s=5)\n",
    "    ax[0, 1].set_xlim([x_min, x_max])\n",
    "    ax[0, 1].set_ylim([y_min, y_max])\n",
    "    ax[0, 1].set_xlabel('x coordinates (pixels)')\n",
    "    ax[0, 1].set_ylabel('y coordinates (pixels)')\n",
    "    ax[0, 1].legend(loc='upper right')\n",
    "else:\n",
    "    ax[0, 1].axis('off')\n",
    "\n",
    "# Panel 3: left, right, center (dict2)\n",
    "if 'VideoData2_Has_Sleap' in globals() and VideoData2_Has_Sleap:\n",
    "    ax[1, 0].set_title('VideoData2 (_dict2): left, right, center')\n",
    "    ax[1, 0].scatter(coordinates_dict2_raw['left.x'], coordinates_dict2_raw['left.y'], color='black', label='left', s=10)\n",
    "    ax[1, 0].scatter(coordinates_dict2_raw['right.x'], coordinates_dict2_raw['right.y'], color='grey', label='right', s=10)\n",
    "    ax[1, 0].scatter(coordinates_dict2_raw['center.x'], coordinates_dict2_raw['center.y'], color='red', label='center', s=10)\n",
    "    ax[1, 0].set_xlim([x_min, x_max])\n",
    "    ax[1, 0].set_ylim([y_min, y_max])\n",
    "    ax[1, 0].set_xlabel('x coordinates (pixels)')\n",
    "    ax[1, 0].set_ylabel('y coordinates (pixels)')\n",
    "    ax[1, 0].legend(loc='upper right')\n",
    "else:\n",
    "    ax[1, 0].axis('off')\n",
    "\n",
    "# Panel 4: p1 to p8 (dict2)\n",
    "if 'VideoData2_Has_Sleap' in globals() and VideoData2_Has_Sleap:\n",
    "    ax[1, 1].set_title('VideoData2 (_dict2): p1 to p8')\n",
    "    for idx, col in enumerate(columns_of_interest[3:]):\n",
    "        ax[1, 1].scatter(coordinates_dict2_raw[f'{col}.x'], coordinates_dict2_raw[f'{col}.y'], color=colors[idx], label=col, s=5)\n",
    "    ax[1, 1].set_xlim([x_min, x_max])\n",
    "    ax[1, 1].set_ylim([y_min, y_max])\n",
    "    ax[1, 1].set_xlabel('x coordinates (pixels)')\n",
    "    ax[1, 1].set_ylabel('y coordinates (pixels)')\n",
    "    ax[1, 1].legend(loc='upper right')\n",
    "else:\n",
    "    ax[1, 1].axis('off')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.96])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QC for consecutive NaN and low confidence inference frames TODO - not sure what to do, what threshold to use to send it back to SLEAP inference \n",
    "############################################################################################################\n",
    "columns_of_interest = ['left.x','left.y','center.x','center.y','right.x','right.y','p1.x','p1.y','p2.x','p2.y','p3.x','p3.y','p4.x','p4.y','p5.x','p5.y','p6.x','p6.y','p7.x','p7.y','p8.x','p8.y']\n",
    "\n",
    "# VideoData1 NaN analysis\n",
    "if 'VideoData1_Has_Sleap' in globals() and VideoData1_Has_Sleap:\n",
    "    all_nan_df = VideoData1[VideoData1[columns_of_interest].isnull().all(1)]\n",
    "    all_nan_index_array = all_nan_df.index.values\n",
    "\n",
    "    # print the groups of sequential NaNs\n",
    "    group_counts = {'1-5': 0, '6-10': 0, '>10': 0}\n",
    "    i = 1\n",
    "    for group in lp.find_sequential_groups(all_nan_index_array):\n",
    "        #print(f'NaN frame group {i} with {len(group)} elements')\n",
    "        if 1 <= len(group) <= 5:\n",
    "            group_counts['1-5'] += 1\n",
    "        elif 6 <= len(group) <= 10:\n",
    "            group_counts['6-10'] += 1\n",
    "        else:\n",
    "            group_counts['>10'] += 1\n",
    "            print(f'\\n⚠️ VideoData1 Framegroup {i} has {len(group)} consecutive all NaN frames  with indices {group}. If this is a long group, consider rerunning SLEAP inference.')\n",
    "        i += 1\n",
    "\n",
    "    print(f\"\\nVideoData1 - Framegroups with 1-5 consecutive all NaN frames: {group_counts['1-5']}\")\n",
    "    print(f\"VideoData1 - Framegroups with 6-10 consecutive all NaN frames: {group_counts['6-10']}\")\n",
    "    print(f\"VideoData1 - Framegroups with >10 consecutive all NaN frames: {group_counts['>10']}\")\n",
    "\n",
    "# VideoData2 NaN analysis\n",
    "if 'VideoData2_Has_Sleap' in globals() and VideoData2_Has_Sleap:\n",
    "    all_nan_df2 = VideoData2[VideoData2[columns_of_interest].isnull().all(1)]\n",
    "    all_nan_index_array2 = all_nan_df2.index.values\n",
    "\n",
    "    # print the groups of sequential NaNs for VideoData2\n",
    "    group_counts2 = {'1-5': 0, '6-10': 0, '>10': 0}\n",
    "    i = 1\n",
    "    for group in lp.find_sequential_groups(all_nan_index_array2):\n",
    "        #print(f'NaN frame group {i} with {len(group)} elements')\n",
    "        if 1 <= len(group) <= 5:\n",
    "            group_counts2['1-5'] += 1\n",
    "        elif 6 <= len(group) <= 10:\n",
    "            group_counts2['6-10'] += 1\n",
    "        else:\n",
    "            group_counts2['>10'] += 1\n",
    "            print(f'\\n⚠️ VideoData2 Framegroup {i} has {len(group)} consecutive all NaN frames  with indices {group}. If this is a long group, consider rerunning SLEAP inference.')\n",
    "        i += 1\n",
    "\n",
    "    print(f\"\\nVideoData2 - Framegroups with 1-5 consecutive all NaN frames: {group_counts2['1-5']}\")\n",
    "    print(f\"VideoData2 - Framegroups with 6-10 consecutive all NaN frames: {group_counts2['6-10']}\")\n",
    "    print(f\"VideoData2 - Framegroups with >10 consecutive all NaN frames: {group_counts2['>10']}\")\n",
    "\n",
    "############################################################################################################\n",
    "# check if we can use some filtering on scores to remove bad frames\n",
    "############################################################################################################\n",
    "\n",
    "columns_of_interest = ['left.score','center.score','right.score','p1.score','p2.score','p3.score','p4.score','p5.score','p6.score','p7.score','p8.score']\n",
    "\n",
    "# VideoData1 confidence score analysis\n",
    "if 'VideoData1_Has_Sleap' in globals() and VideoData1_Has_Sleap:\n",
    "    total_points1 = len(VideoData1)\n",
    "    print(f'\\nℹ️ VideoData1 - Top 3 columns with most frames below {score_cutoff} confidence score:')\n",
    "\n",
    "    # Calculate statistics for all columns\n",
    "    video1_stats = []\n",
    "    for col in columns_of_interest:\n",
    "        count_below_threshold = (VideoData1[col] < score_cutoff).sum()\n",
    "        percentage_below_threshold = (count_below_threshold / total_points1) * 100\n",
    "\n",
    "        # Find the longest consecutive series below threshold\n",
    "        below_threshold = VideoData1[col] < score_cutoff\n",
    "        longest_series = 0\n",
    "        current_series = 0\n",
    "\n",
    "        for value in below_threshold:\n",
    "            if value:\n",
    "                current_series += 1\n",
    "                if current_series > longest_series:\n",
    "                    longest_series = current_series\n",
    "            else:\n",
    "                current_series = 0\n",
    "\n",
    "        video1_stats.append((col, count_below_threshold, percentage_below_threshold, longest_series))\n",
    "\n",
    "    # Sort by count_below_threshold and show top 3\n",
    "    video1_stats.sort(key=lambda x: x[1], reverse=True)\n",
    "    for i, (col, count, percentage, longest) in enumerate(video1_stats[:3]):\n",
    "        print(f\"VideoData1 - #{i+1}: {col} | Values below {score_cutoff}: {count} ({percentage:.2f}%) | Longest consecutive frame series: {longest}\")\n",
    "\n",
    "# VideoData2 confidence score analysis\n",
    "if 'VideoData2_Has_Sleap' in globals() and VideoData2_Has_Sleap:\n",
    "    total_points2 = len(VideoData2)\n",
    "    print(f'\\nℹ️ VideoData2 - Top 3 columns with most frames below {score_cutoff} confidence score:')\n",
    "\n",
    "    # Calculate statistics for all columns\n",
    "    video2_stats = []\n",
    "    for col in columns_of_interest:\n",
    "        count_below_threshold = (VideoData2[col] < score_cutoff).sum()\n",
    "        percentage_below_threshold = (count_below_threshold / total_points2) * 100\n",
    "        \n",
    "        # Find the longest consecutive series below threshold\n",
    "        below_threshold = VideoData2[col] < score_cutoff\n",
    "        longest_series = 0\n",
    "        current_series = 0\n",
    "        \n",
    "        for value in below_threshold:\n",
    "            if value:\n",
    "                current_series += 1\n",
    "                if current_series > longest_series:\n",
    "                    longest_series = current_series\n",
    "            else:\n",
    "                current_series = 0\n",
    "        \n",
    "        video2_stats.append((col, count_below_threshold, percentage_below_threshold, longest_series))\n",
    "\n",
    "    # Sort by count_below_threshold and show top 3\n",
    "    video2_stats.sort(key=lambda x: x[1], reverse=True)\n",
    "    for i, (col, count, percentage, longest) in enumerate(video2_stats[:3]):\n",
    "        print(f\"VideoData2 - #{i+1}: {col} | Values below {score_cutoff}: {count} ({percentage:.2f}%) | Longest consecutive frame series: {longest}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca1279e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frame_idx</th>\n",
       "      <th>Seconds</th>\n",
       "      <th>instance.score</th>\n",
       "      <th>left.x</th>\n",
       "      <th>left.y</th>\n",
       "      <th>left.score</th>\n",
       "      <th>right.x</th>\n",
       "      <th>right.y</th>\n",
       "      <th>right.score</th>\n",
       "      <th>p1.x</th>\n",
       "      <th>...</th>\n",
       "      <th>p6.score</th>\n",
       "      <th>p7.x</th>\n",
       "      <th>p7.y</th>\n",
       "      <th>p7.score</th>\n",
       "      <th>p8.x</th>\n",
       "      <th>p8.y</th>\n",
       "      <th>p8.score</th>\n",
       "      <th>center.x</th>\n",
       "      <th>center.y</th>\n",
       "      <th>center.score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>345439.465792</td>\n",
       "      <td>NaN</td>\n",
       "      <td>384.904218</td>\n",
       "      <td>211.864399</td>\n",
       "      <td>NaN</td>\n",
       "      <td>224.606827</td>\n",
       "      <td>202.062801</td>\n",
       "      <td>NaN</td>\n",
       "      <td>319.325667</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>298.933187</td>\n",
       "      <td>217.689423</td>\n",
       "      <td>NaN</td>\n",
       "      <td>313.458057</td>\n",
       "      <td>211.856610</td>\n",
       "      <td>NaN</td>\n",
       "      <td>298.782703</td>\n",
       "      <td>196.624016</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>345439.482464</td>\n",
       "      <td>10.885521</td>\n",
       "      <td>384.003204</td>\n",
       "      <td>211.951065</td>\n",
       "      <td>1.006772</td>\n",
       "      <td>223.652267</td>\n",
       "      <td>200.493408</td>\n",
       "      <td>0.951371</td>\n",
       "      <td>319.940155</td>\n",
       "      <td>...</td>\n",
       "      <td>0.975107</td>\n",
       "      <td>299.665833</td>\n",
       "      <td>216.380905</td>\n",
       "      <td>0.994031</td>\n",
       "      <td>312.544525</td>\n",
       "      <td>211.654114</td>\n",
       "      <td>0.998158</td>\n",
       "      <td>299.698944</td>\n",
       "      <td>196.055771</td>\n",
       "      <td>0.986040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>345439.499136</td>\n",
       "      <td>10.858869</td>\n",
       "      <td>383.827301</td>\n",
       "      <td>211.965302</td>\n",
       "      <td>1.012721</td>\n",
       "      <td>223.537964</td>\n",
       "      <td>200.404343</td>\n",
       "      <td>0.923593</td>\n",
       "      <td>319.805450</td>\n",
       "      <td>...</td>\n",
       "      <td>0.978168</td>\n",
       "      <td>299.556458</td>\n",
       "      <td>216.301315</td>\n",
       "      <td>0.988012</td>\n",
       "      <td>312.416077</td>\n",
       "      <td>211.621201</td>\n",
       "      <td>1.002317</td>\n",
       "      <td>299.618073</td>\n",
       "      <td>195.993195</td>\n",
       "      <td>0.984853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>345439.515808</td>\n",
       "      <td>10.803942</td>\n",
       "      <td>383.982910</td>\n",
       "      <td>212.067245</td>\n",
       "      <td>1.014064</td>\n",
       "      <td>223.598831</td>\n",
       "      <td>200.535294</td>\n",
       "      <td>0.932378</td>\n",
       "      <td>319.791901</td>\n",
       "      <td>...</td>\n",
       "      <td>0.968120</td>\n",
       "      <td>299.531006</td>\n",
       "      <td>216.480255</td>\n",
       "      <td>0.982161</td>\n",
       "      <td>312.406189</td>\n",
       "      <td>211.812073</td>\n",
       "      <td>1.002717</td>\n",
       "      <td>299.581818</td>\n",
       "      <td>196.190887</td>\n",
       "      <td>0.970574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>345439.532480</td>\n",
       "      <td>10.870998</td>\n",
       "      <td>383.926697</td>\n",
       "      <td>212.136185</td>\n",
       "      <td>1.005561</td>\n",
       "      <td>223.600067</td>\n",
       "      <td>200.479523</td>\n",
       "      <td>0.955708</td>\n",
       "      <td>319.839600</td>\n",
       "      <td>...</td>\n",
       "      <td>0.971014</td>\n",
       "      <td>299.619537</td>\n",
       "      <td>216.427963</td>\n",
       "      <td>0.994664</td>\n",
       "      <td>312.462708</td>\n",
       "      <td>211.723083</td>\n",
       "      <td>1.003096</td>\n",
       "      <td>299.651886</td>\n",
       "      <td>196.121872</td>\n",
       "      <td>0.982007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   frame_idx        Seconds  instance.score      left.x      left.y  \\\n",
       "0          0  345439.465792             NaN  384.904218  211.864399   \n",
       "1          1  345439.482464       10.885521  384.003204  211.951065   \n",
       "2          2  345439.499136       10.858869  383.827301  211.965302   \n",
       "3          3  345439.515808       10.803942  383.982910  212.067245   \n",
       "4          4  345439.532480       10.870998  383.926697  212.136185   \n",
       "\n",
       "   left.score     right.x     right.y  right.score        p1.x  ...  p6.score  \\\n",
       "0         NaN  224.606827  202.062801          NaN  319.325667  ...       NaN   \n",
       "1    1.006772  223.652267  200.493408     0.951371  319.940155  ...  0.975107   \n",
       "2    1.012721  223.537964  200.404343     0.923593  319.805450  ...  0.978168   \n",
       "3    1.014064  223.598831  200.535294     0.932378  319.791901  ...  0.968120   \n",
       "4    1.005561  223.600067  200.479523     0.955708  319.839600  ...  0.971014   \n",
       "\n",
       "         p7.x        p7.y  p7.score        p8.x        p8.y  p8.score  \\\n",
       "0  298.933187  217.689423       NaN  313.458057  211.856610       NaN   \n",
       "1  299.665833  216.380905  0.994031  312.544525  211.654114  0.998158   \n",
       "2  299.556458  216.301315  0.988012  312.416077  211.621201  1.002317   \n",
       "3  299.531006  216.480255  0.982161  312.406189  211.812073  1.002717   \n",
       "4  299.619537  216.427963  0.994664  312.462708  211.723083  1.003096   \n",
       "\n",
       "     center.x    center.y  center.score  \n",
       "0  298.782703  196.624016           NaN  \n",
       "1  299.698944  196.055771      0.986040  \n",
       "2  299.618073  195.993195      0.984853  \n",
       "3  299.581818  196.190887      0.970574  \n",
       "4  299.651886  196.121872      0.982007  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VideoData1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Centering ===\n",
      "VideoData1 - Centering on median pupil centre: \n",
      "Mean center.x: 304.25169372558594, Mean center.y: 195.6934967041016\n",
      "VideoData2 - Centering on median pupil centre: \n",
      "Mean center.x: 264.44810485839844, Mean center.y: 231.88140869140625\n",
      "\n",
      "=== Score-based Filtering ===\n",
      "VideoData1 - A total number of 7492 low-confidence coordinate values were replaced by interpolation\n",
      "VideoData2 - A total number of 10472 low-confidence coordinate values were replaced by interpolation\n",
      "\n",
      "=== Outlier Analysis ===\n",
      "VideoData1 - Channel with the maximum number of outliers: right.x, Number of outliers: 343\n",
      "VideoData1 - A total number of 500 outliers were replaced by interpolation\n",
      "VideoData2 - Channel with the maximum number of outliers: left.x, Number of outliers: 176\n",
      "VideoData2 - A total number of 291 outliers were replaced by interpolation\n"
     ]
    }
   ],
   "source": [
    "# center coordinates on median pupil centre, removing outliers and (TODO - low confidence inference points)\n",
    "############################################################################################################\n",
    "\n",
    "columns_of_interest = ['left.x','left.y','center.x','center.y','right.x','right.y','p1.x','p1.y','p2.x','p2.y','p3.x','p3.y','p4.x','p4.y','p5.x','p5.y','p6.x','p6.y','p7.x','p7.y','p8.x','p8.y']\n",
    "\n",
    "print(\"=== Centering ===\")\n",
    "# VideoData1 processing\n",
    "if 'VideoData1_Has_Sleap' in globals() and VideoData1_Has_Sleap:\n",
    "    # Calculate the mean of the center x and y points\n",
    "    mean_center_x1 = VideoData1['center.x'].median()\n",
    "    mean_center_y1 = VideoData1['center.y'].median()\n",
    "\n",
    "    print(f\"VideoData1 - Centering on median pupil centre: \\nMean center.x: {mean_center_x1}, Mean center.y: {mean_center_y1}\")\n",
    "\n",
    "    # Translate the coordinates\n",
    "    for col in columns_of_interest:\n",
    "        if '.x' in col:\n",
    "            VideoData1[col] = VideoData1[col] - mean_center_x1\n",
    "        elif '.y' in col:\n",
    "            VideoData1[col] = VideoData1[col] - mean_center_y1\n",
    "\n",
    "    VideoData1_centered = VideoData1.copy()\n",
    "\n",
    "# VideoData2 processing\n",
    "if 'VideoData2_Has_Sleap' in globals() and VideoData2_Has_Sleap:\n",
    "    # Calculate the mean of the center x and y points\n",
    "    mean_center_x2 = VideoData2['center.x'].median()\n",
    "    mean_center_y2 = VideoData2['center.y'].median()\n",
    "\n",
    "    print(f\"VideoData2 - Centering on median pupil centre: \\nMean center.x: {mean_center_x2}, Mean center.y: {mean_center_y2}\")\n",
    "\n",
    "    # Translate the coordinates\n",
    "    for col in columns_of_interest:\n",
    "        if '.x' in col:\n",
    "            VideoData2[col] = VideoData2[col] - mean_center_x2\n",
    "        elif '.y' in col:\n",
    "            VideoData2[col] = VideoData2[col] - mean_center_y2\n",
    "\n",
    "    VideoData2_centered = VideoData2.copy()\n",
    "\n",
    "############################################################################################################\n",
    "# remove low confidence points (score < threshold)\n",
    "############################################################################################################\n",
    "print(\"\\n=== Score-based Filtering ===\")\n",
    "print(f\"Score threshold: {score_cutoff}\")\n",
    "# List of point names (without .x, .y, .score)\n",
    "point_names = ['left', 'right', 'center', 'p1', 'p2', 'p3', 'p4', 'p5', 'p6', 'p7', 'p8']\n",
    "\n",
    "# VideoData1 score-based filtering\n",
    "if 'VideoData1_Has_Sleap' in globals() and VideoData1_Has_Sleap:\n",
    "    total_low_score1 = 0\n",
    "    low_score_counts1 = {}\n",
    "    for point in point_names:\n",
    "        if f'{point}.score' in VideoData1.columns:\n",
    "            # Find indices where score is below threshold\n",
    "            low_score_mask = VideoData1[f'{point}.score'] < score_cutoff\n",
    "            low_score_count = low_score_mask.sum()\n",
    "            low_score_counts1[f'{point}.x'] = low_score_count\n",
    "            low_score_counts1[f'{point}.y'] = low_score_count\n",
    "            total_low_score1 += low_score_count * 2  # *2 because we're removing both x and y\n",
    "            \n",
    "            # Set x and y to NaN for low confidence points\n",
    "            VideoData1.loc[low_score_mask, f'{point}.x'] = np.nan\n",
    "            VideoData1.loc[low_score_mask, f'{point}.y'] = np.nan\n",
    "    \n",
    "    # Find the channel with the maximum number of low-score points\n",
    "    max_low_score_channel1 = max(low_score_counts1, key=low_score_counts1.get)\n",
    "    max_low_score_count1 = low_score_counts1[max_low_score_channel1]\n",
    "    \n",
    "    # Print the channel with the maximum number of low-score points\n",
    "    print(f\"VideoData1 - Channel with the maximum number of low-confidence points: {max_low_score_channel1}, Number of low-confidence points: {max_low_score_count1}\")\n",
    "    print(f\"VideoData1 - A total number of {total_low_score1} low-confidence coordinate values were replaced by interpolation\")\n",
    "\n",
    "# VideoData2 score-based filtering\n",
    "if 'VideoData2_Has_Sleap' in globals() and VideoData2_Has_Sleap:\n",
    "    total_low_score2 = 0\n",
    "    low_score_counts2 = {}\n",
    "    for point in point_names:\n",
    "        if f'{point}.score' in VideoData2.columns:\n",
    "            # Find indices where score is below threshold\n",
    "            low_score_mask = VideoData2[f'{point}.score'] < score_cutoff\n",
    "            low_score_count = low_score_mask.sum()\n",
    "            low_score_counts2[f'{point}.x'] = low_score_count\n",
    "            low_score_counts2[f'{point}.y'] = low_score_count\n",
    "            total_low_score2 += low_score_count * 2  # *2 because we're removing both x and y\n",
    "            \n",
    "            # Set x and y to NaN for low confidence points\n",
    "            VideoData2.loc[low_score_mask, f'{point}.x'] = np.nan\n",
    "            VideoData2.loc[low_score_mask, f'{point}.y'] = np.nan\n",
    "    \n",
    "    # Find the channel with the maximum number of low-score points\n",
    "    max_low_score_channel2 = max(low_score_counts2, key=low_score_counts2.get)\n",
    "    max_low_score_count2 = low_score_counts2[max_low_score_channel2]\n",
    "    \n",
    "    # Print the channel with the maximum number of low-score points\n",
    "    print(f\"VideoData2 - Channel with the maximum number of low-confidence points: {max_low_score_channel2}, Number of low-confidence points: {max_low_score_count2}\")\n",
    "    print(f\"VideoData2 - A total number of {total_low_score2} low-confidence coordinate values were replaced by interpolation\")\n",
    "\n",
    "############################################################################################################\n",
    "# remove outliers (x times SD)\n",
    "# then interpolates on all NaN values (skipped frames, low confidence inference points, outliers)\n",
    "############################################################################################################\n",
    "if not NaNs_removed:\n",
    "    print(\"\\n=== Outlier Analysis ===\")\n",
    "\n",
    "    # VideoData1 outlier analysis and interpolation\n",
    "    if 'VideoData1_Has_Sleap' in globals() and VideoData1_Has_Sleap:\n",
    "        # Calculate the standard deviation for each column of interest\n",
    "        std_devs1 = {col: VideoData1[col].std() for col in columns_of_interest}\n",
    "\n",
    "        # Calculate the number of outliers for each column\n",
    "        outliers1 = {col: ((VideoData1[col] - VideoData1[col].mean()).abs() > 10 * std_devs1[col]).sum() for col in columns_of_interest}\n",
    "\n",
    "        # Find the channel with the maximum number of outliers\n",
    "        max_outliers_channel1 = max(outliers1, key=outliers1.get)\n",
    "        max_outliers_count1 = outliers1[max_outliers_channel1]\n",
    "\n",
    "        # Print the channel with the maximum number of outliers and the number\n",
    "        print(f\"VideoData1 - Channel with the maximum number of outliers: {max_outliers_channel1}, Number of outliers: {max_outliers_count1}\")\n",
    "\n",
    "        # Print the total number of outliers\n",
    "        total_outliers1 = sum(outliers1.values())\n",
    "        print(f\"VideoData1 - A total number of {total_outliers1} outliers were replaced by interpolation\")\n",
    "\n",
    "        # Replace outliers by interpolating between the previous and subsequent non-NaN value\n",
    "        for col in columns_of_interest:\n",
    "            outlier_indices = VideoData1[((VideoData1[col] - VideoData1[col].mean()).abs() > outlier_sd_threshold * std_devs1[col])].index\n",
    "            VideoData1.loc[outlier_indices, col] = np.nan\n",
    "\n",
    "        #VideoData1.interpolate(inplace=True)\n",
    "        VideoData1 = VideoData1.interpolate(method='linear', limit_direction='both')\n",
    "\n",
    "    # VideoData2 outlier analysis and interpolation\n",
    "    if 'VideoData2_Has_Sleap' in globals() and VideoData2_Has_Sleap:\n",
    "        # Calculate the standard deviation for each column of interest\n",
    "        std_devs2 = {col: VideoData2[col].std() for col in columns_of_interest}\n",
    "\n",
    "        # Calculate the number of outliers for each column\n",
    "        outliers2 = {col: ((VideoData2[col] - VideoData2[col].mean()).abs() > 10 * std_devs2[col]).sum() for col in columns_of_interest}\n",
    "\n",
    "        # Find the channel with the maximum number of outliers\n",
    "        max_outliers_channel2 = max(outliers2, key=outliers2.get)\n",
    "        max_outliers_count2 = outliers2[max_outliers_channel2]\n",
    "\n",
    "        # Print the channel with the maximum number of outliers and the number\n",
    "        print(f\"VideoData2 - Channel with the maximum number of outliers: {max_outliers_channel2}, Number of outliers: {max_outliers_count2}\")\n",
    "\n",
    "        # Print the total number of outliers\n",
    "        total_outliers2 = sum(outliers2.values())\n",
    "        print(f\"VideoData2 - A total number of {total_outliers2} outliers were replaced by interpolation\")\n",
    "\n",
    "        # Replace outliers by interpolating between the previous and subsequent non-NaN value\n",
    "        for col in columns_of_interest:\n",
    "            outlier_indices = VideoData2[((VideoData2[col] - VideoData2[col].mean()).abs() > outlier_sd_threshold * std_devs2[col])].index\n",
    "            VideoData2.loc[outlier_indices, col] = np.nan\n",
    "\n",
    "        #VideoData2.interpolate(inplace=True)\n",
    "        VideoData2 = VideoData2.interpolate(method='linear', limit_direction='both')\n",
    "        \n",
    "        NaNs_removed = True\n",
    "else:\n",
    "    print(\"=== Interpolation already done, skipping ===\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QC plot timeseries of interpolation corrected NaN and (TODO low confidence coordinates in browser \n",
    "############################################################################################################\n",
    "\n",
    "if plot_timeseries:\n",
    "    print(f'ℹ️ Figure opens in browser window, takes a bit of time.')\n",
    "    \n",
    "    # VideoData1 QC Plot\n",
    "    if 'VideoData1_Has_Sleap' in globals() and VideoData1_Has_Sleap:\n",
    "        fig1 = make_subplots(\n",
    "            rows=4, cols=1,\n",
    "            shared_xaxes=True,\n",
    "            vertical_spacing=0.05,\n",
    "            subplot_titles=(\n",
    "                \"VideoData1 - X coordinates for pupil centre and left-right eye corner\",\n",
    "                \"VideoData1 - Y coordinates for pupil centre and left-right eye corner\",\n",
    "                \"VideoData1 - X coordinates for iris points\",\n",
    "                \"VideoData1 - Y coordinates for iris points\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Row 1: Plot left.x, center.x, right.x\n",
    "        fig1.add_trace(go.Scatter(x=VideoData1['Seconds'], y=VideoData1['left.x'], mode='lines', name='left.x'), row=1, col=1)\n",
    "        fig1.add_trace(go.Scatter(x=VideoData1['Seconds'], y=VideoData1['center.x'], mode='lines', name='center.x'), row=1, col=1)\n",
    "        fig1.add_trace(go.Scatter(x=VideoData1['Seconds'], y=VideoData1['right.x'], mode='lines', name='right.x'), row=1, col=1)\n",
    "\n",
    "        # Row 2: Plot left.y, center.y, right.y\n",
    "        fig1.add_trace(go.Scatter(x=VideoData1['Seconds'], y=VideoData1['left.y'], mode='lines', name='left.y'), row=2, col=1)\n",
    "        fig1.add_trace(go.Scatter(x=VideoData1['Seconds'], y=VideoData1['center.y'], mode='lines', name='center.y'), row=2, col=1)\n",
    "        fig1.add_trace(go.Scatter(x=VideoData1['Seconds'], y=VideoData1['right.y'], mode='lines', name='right.y'), row=2, col=1)\n",
    "\n",
    "        # Row 3: Plot p.x coordinates for p1 to p8\n",
    "        for col in ['p1.x', 'p2.x', 'p3.x', 'p4.x', 'p5.x', 'p6.x', 'p7.x', 'p8.x']:\n",
    "            fig1.add_trace(go.Scatter(x=VideoData1['Seconds'], y=VideoData1[col], mode='lines', name=col), row=3, col=1)\n",
    "\n",
    "        # Row 4: Plot p.y coordinates for p1 to p8\n",
    "        for col in ['p1.y', 'p2.y', 'p3.y', 'p4.y', 'p5.y', 'p6.y', 'p7.y', 'p8.y']:\n",
    "            fig1.add_trace(go.Scatter(x=VideoData1['Seconds'], y=VideoData1[col], mode='lines', name=col), row=4, col=1)\n",
    "\n",
    "        fig1.update_layout(\n",
    "            height=1200,\n",
    "            title_text=\"VideoData1 - Time series subplots for coordinates (QC after interpolation)\",\n",
    "            showlegend=True\n",
    "        )\n",
    "        fig1.update_xaxes(title_text=\"Seconds\", row=4, col=1)\n",
    "        fig1.update_yaxes(title_text=\"X Position\", row=1, col=1)\n",
    "        fig1.update_yaxes(title_text=\"Y Position\", row=2, col=1)\n",
    "        fig1.update_yaxes(title_text=\"X Position\", row=3, col=1)\n",
    "        fig1.update_yaxes(title_text=\"Y Position\", row=4, col=1)\n",
    "\n",
    "        fig1.show(renderer='browser')\n",
    "    \n",
    "    # VideoData2 QC Plot\n",
    "    if 'VideoData2_Has_Sleap' in globals() and VideoData2_Has_Sleap:\n",
    "        fig2 = make_subplots(\n",
    "            rows=4, cols=1,\n",
    "            shared_xaxes=True,\n",
    "            vertical_spacing=0.05,\n",
    "            subplot_titles=(\n",
    "                \"VideoData2 - X coordinates for pupil centre and left-right eye corner\",\n",
    "                \"VideoData2 - Y coordinates for pupil centre and left-right eye corner\",\n",
    "                \"VideoData2 - X coordinates for iris points\",\n",
    "                \"VideoData2 - Y coordinates for iris points\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Row 1: Plot left.x, center.x, right.x\n",
    "        fig2.add_trace(go.Scatter(x=VideoData2['Seconds'], y=VideoData2['left.x'], mode='lines', name='left.x'), row=1, col=1)\n",
    "        fig2.add_trace(go.Scatter(x=VideoData2['Seconds'], y=VideoData2['center.x'], mode='lines', name='center.x'), row=1, col=1)\n",
    "        fig2.add_trace(go.Scatter(x=VideoData2['Seconds'], y=VideoData2['right.x'], mode='lines', name='right.x'), row=1, col=1)\n",
    "\n",
    "        # Row 2: Plot left.y, center.y, right.y\n",
    "        fig2.add_trace(go.Scatter(x=VideoData2['Seconds'], y=VideoData2['left.y'], mode='lines', name='left.y'), row=2, col=1)\n",
    "        fig2.add_trace(go.Scatter(x=VideoData2['Seconds'], y=VideoData2['center.y'], mode='lines', name='center.y'), row=2, col=1)\n",
    "        fig2.add_trace(go.Scatter(x=VideoData2['Seconds'], y=VideoData2['right.y'], mode='lines', name='right.y'), row=2, col=1)\n",
    "\n",
    "        # Row 3: Plot p.x coordinates for p1 to p8\n",
    "        for col in ['p1.x', 'p2.x', 'p3.x', 'p4.x', 'p5.x', 'p6.x', 'p7.x', 'p8.x']:\n",
    "            fig2.add_trace(go.Scatter(x=VideoData2['Seconds'], y=VideoData2[col], mode='lines', name=col), row=3, col=1)\n",
    "\n",
    "        # Row 4: Plot p.y coordinates for p1 to p8\n",
    "        for col in ['p1.y', 'p2.y', 'p3.y', 'p4.y', 'p5.y', 'p6.y', 'p7.y', 'p8.y']:\n",
    "            fig2.add_trace(go.Scatter(x=VideoData2['Seconds'], y=VideoData2[col], mode='lines', name=col), row=4, col=1)\n",
    "\n",
    "        fig2.update_layout(\n",
    "            height=1200,\n",
    "            title_text=\"VideoData2 - Time series subplots for coordinates (QC after interpolation)\",\n",
    "            showlegend=True\n",
    "        )\n",
    "        fig2.update_xaxes(title_text=\"Seconds\", row=4, col=1)\n",
    "        fig2.update_yaxes(title_text=\"X Position\", row=1, col=1)\n",
    "        fig2.update_yaxes(title_text=\"Y Position\", row=2, col=1)\n",
    "        fig2.update_yaxes(title_text=\"X Position\", row=3, col=1)\n",
    "        fig2.update_yaxes(title_text=\"Y Position\", row=4, col=1)\n",
    "\n",
    "        fig2.show(renderer='browser')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QC plot XY coordinate distributions after NaN and ( TODO - low confidence inference points) are interpolated \n",
    "##############################################################################################################\n",
    "\n",
    "columns_of_interest = ['left.x','left.y','center.x','center.y','right.x','right.y','p1.x','p1.y','p2.x','p2.y','p3.x','p3.y','p4.x','p4.y','p5.x','p5.y','p6.x','p6.y','p7.x','p7.y','p8.x','p8.y']\n",
    "\n",
    "# Create coordinates_dict for both datasets\n",
    "if VideoData1_Has_Sleap:\n",
    "    coordinates_dict1_processed = lp.get_coordinates_dict(VideoData1, columns_of_interest)\n",
    "if VideoData2_Has_Sleap:\n",
    "    coordinates_dict2_processed = lp.get_coordinates_dict(VideoData2, columns_of_interest)\n",
    "\n",
    "columns_of_interest = ['left', 'right', 'center', 'p1', 'p2', 'p3', 'p4', 'p5', 'p6', 'p7', 'p8']\n",
    "\n",
    "# Filter out NaN values and calculate the min and max values for X and Y coordinates for both dict1 and dict2\n",
    "def min_max_dict(coordinates_dict):\n",
    "    x_min = min([coordinates_dict[f'{col}.x'][~np.isnan(coordinates_dict[f'{col}.x'])].min() for col in columns_of_interest])\n",
    "    x_max = max([coordinates_dict[f'{col}.x'][~np.isnan(coordinates_dict[f'{col}.x'])].max() for col in columns_of_interest])\n",
    "    y_min = min([coordinates_dict[f'{col}.y'][~np.isnan(coordinates_dict[f'{col}.y'])].min() for col in columns_of_interest])\n",
    "    y_max = max([coordinates_dict[f'{col}.y'][~np.isnan(coordinates_dict[f'{col}.y'])].max() for col in columns_of_interest])\n",
    "    return x_min, x_max, y_min, y_max\n",
    "\n",
    "if VideoData1_Has_Sleap:\n",
    "    x_min1, x_max1, y_min1, y_max1 = min_max_dict(coordinates_dict1_processed)\n",
    "if VideoData2_Has_Sleap:\n",
    "    x_min2, x_max2, y_min2, y_max2 = min_max_dict(coordinates_dict2_processed)\n",
    "\n",
    "# Use global min and max for consistency across subplots\n",
    "if VideoData1_Has_Sleap and VideoData2_Has_Sleap:\n",
    "    x_min = min(x_min1, x_min2)\n",
    "    x_max = max(x_max1, x_max2)\n",
    "    y_min = min(y_min1, y_min2)\n",
    "    y_max = max(y_max1, y_max2)\n",
    "elif VideoData1_Has_Sleap:\n",
    "    x_min, x_max, y_min, y_max = x_min1, x_max1, y_min1, y_max1\n",
    "elif VideoData2_Has_Sleap:\n",
    "    x_min, x_max, y_min, y_max = x_min2, x_max2, y_min2, y_max2\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(18, 12))\n",
    "fig.suptitle('XY coordinate distribution of different points for VideoData1 and VideoData2 post outlier removal and NaN interpolation)', fontsize=14)\n",
    "\n",
    "# Define colormap for p1-p8\n",
    "colors = ['blue', 'green', 'red', 'cyan', 'magenta', 'yellow', 'black', 'orange']\n",
    "\n",
    "# Panel 1: left, right, center (VideoData1)\n",
    "if VideoData1_Has_Sleap:\n",
    "    ax[0, 0].set_title('VideoData1: left, right, center')\n",
    "    ax[0, 0].scatter(coordinates_dict1_processed['left.x'], coordinates_dict1_processed['left.y'], color='black', label='left', s=10)\n",
    "    ax[0, 0].scatter(coordinates_dict1_processed['right.x'], coordinates_dict1_processed['right.y'], color='grey', label='right', s=10)\n",
    "    ax[0, 0].scatter(coordinates_dict1_processed['center.x'], coordinates_dict1_processed['center.y'], color='red', label='center', s=10)\n",
    "    ax[0, 0].set_xlim([x_min, x_max])\n",
    "    ax[0, 0].set_ylim([y_min, y_max])\n",
    "    ax[0, 0].set_xlabel('x coordinates (pixels)')\n",
    "    ax[0, 0].set_ylabel('y coordinates (pixels)')\n",
    "    ax[0, 0].legend(loc='upper right')\n",
    "\n",
    "    # Panel 2: p1 to p8 (VideoData1)\n",
    "    ax[0, 1].set_title('VideoData1: p1 to p8')\n",
    "    for idx, col in enumerate(columns_of_interest[3:]):\n",
    "        ax[0, 1].scatter(coordinates_dict1_processed[f'{col}.x'], coordinates_dict1_processed[f'{col}.y'], color=colors[idx], label=col, s=5)\n",
    "    ax[0, 1].set_xlim([x_min, x_max])\n",
    "    ax[0, 1].set_ylim([y_min, y_max])\n",
    "    ax[0, 1].set_xlabel('x coordinates (pixels)')\n",
    "    ax[0, 1].set_ylabel('y coordinates (pixels)')\n",
    "    ax[0, 1].legend(loc='upper right')\n",
    "\n",
    "# Panel 3: left, right, center (VideoData2)\n",
    "if VideoData2_Has_Sleap:\n",
    "    ax[1, 0].set_title('VideoData2: left, right, center')\n",
    "    ax[1, 0].scatter(coordinates_dict2_processed['left.x'], coordinates_dict2_processed['left.y'], color='black', label='left', s=10)\n",
    "    ax[1, 0].scatter(coordinates_dict2_processed['right.x'], coordinates_dict2_processed['right.y'], color='grey', label='right', s=10)\n",
    "    ax[1, 0].scatter(coordinates_dict2_processed['center.x'], coordinates_dict2_processed['center.y'], color='red', label='center', s=10)\n",
    "    ax[1, 0].set_xlim([x_min, x_max])\n",
    "    ax[1, 0].set_ylim([y_min, y_max])\n",
    "    ax[1, 0].set_xlabel('x coordinates (pixels)')\n",
    "    ax[1, 0].set_ylabel('y coordinates (pixels)')\n",
    "    ax[1, 0].legend(loc='upper right')\n",
    "\n",
    "    # Panel 4: p1 to p8 (VideoData2)\n",
    "    ax[1, 1].set_title('VideoData2: p1 to p8')\n",
    "    for idx, col in enumerate(columns_of_interest[3:]):\n",
    "        ax[1, 1].scatter(coordinates_dict2_processed[f'{col}.x'], coordinates_dict2_processed[f'{col}.y'], color=colors[idx], label=col, s=5)\n",
    "    ax[1, 1].set_xlim([x_min, x_max])\n",
    "    ax[1, 1].set_ylim([y_min, y_max])\n",
    "    ax[1, 1].set_xlabel('x coordinates (pixels)')\n",
    "    ax[1, 1].set_ylabel('y coordinates (pixels)')\n",
    "    ax[1, 1].legend(loc='upper right')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.96])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit ellipses on the 8 points to determine pupil centre and diameter\n",
    "############################################################################################################\n",
    "\n",
    "columns_of_interest = ['left.x','left.y','center.x','center.y','right.x','right.y','p1.x','p1.y','p2.x','p2.y','p3.x','p3.y','p4.x','p4.y','p5.x','p5.y','p6.x','p6.y','p7.x','p7.y','p8.x','p8.y']\n",
    "\n",
    "# VideoData1 processing\n",
    "if VideoData1_Has_Sleap:\n",
    "    print(\"=== VideoData1 Ellipse Fitting for Pupil Diameter ===\")\n",
    "    coordinates_dict1_processed = lp.get_coordinates_dict(VideoData1, columns_of_interest)\n",
    "\n",
    "    theta1 = lp.find_horizontal_axis_angle(VideoData1, 'left', 'center')\n",
    "    center_point1 = lp.get_left_right_center_point(coordinates_dict1_processed)\n",
    "\n",
    "    columns_of_interest_reformatted = ['left', 'right', 'center', 'p1', 'p2', 'p3', 'p4', 'p5', 'p6', 'p7', 'p8']\n",
    "    remformatted_coordinates_dict1 = lp.get_reformatted_coordinates_dict(coordinates_dict1_processed, columns_of_interest_reformatted)\n",
    "    centered_coordinates_dict1 = lp.get_centered_coordinates_dict(remformatted_coordinates_dict1, center_point1)\n",
    "    rotated_coordinates_dict1 = lp.get_rotated_coordinates_dict(centered_coordinates_dict1, theta1)\n",
    "\n",
    "    columns_of_interest_ellipse = ['p1', 'p2', 'p3', 'p4', 'p5', 'p6', 'p7', 'p8']\n",
    "    ellipse_parameters_data1, ellipse_center_points_data1 = lp.get_fitted_ellipse_parameters(rotated_coordinates_dict1, columns_of_interest_ellipse)\n",
    "\n",
    "    average_diameter1 = np.mean([ellipse_parameters_data1[:,0], ellipse_parameters_data1[:,1]], axis=0)\n",
    "\n",
    "    SleapVideoData1 = process.convert_arrays_to_dataframe(['Seconds', 'Ellipse.Diameter', 'Ellipse.Angle', 'Ellipse.Center.X', 'Ellipse.Center.Y'], [VideoData1['Seconds'].values, average_diameter1, ellipse_parameters_data1[:,2], ellipse_center_points_data1[:,0], ellipse_center_points_data1[:,1]])\n",
    "\n",
    "# VideoData2 processing\n",
    "if VideoData2_Has_Sleap:\n",
    "    print(\"=== VideoData2 Ellipse Fitting for Pupil Diameter ===\")\n",
    "    coordinates_dict2_processed = lp.get_coordinates_dict(VideoData2, columns_of_interest)\n",
    "\n",
    "    theta2 = lp.find_horizontal_axis_angle(VideoData2, 'left', 'center')\n",
    "    center_point2 = lp.get_left_right_center_point(coordinates_dict2_processed)\n",
    "\n",
    "    columns_of_interest_reformatted = ['left', 'right', 'center', 'p1', 'p2', 'p3', 'p4', 'p5', 'p6', 'p7', 'p8']\n",
    "    remformatted_coordinates_dict2 = lp.get_reformatted_coordinates_dict(coordinates_dict2_processed, columns_of_interest_reformatted)\n",
    "    centered_coordinates_dict2 = lp.get_centered_coordinates_dict(remformatted_coordinates_dict2, center_point2)\n",
    "    rotated_coordinates_dict2 = lp.get_rotated_coordinates_dict(centered_coordinates_dict2, theta2)\n",
    "\n",
    "    columns_of_interest_ellipse = ['p1', 'p2', 'p3', 'p4', 'p5', 'p6', 'p7', 'p8']\n",
    "    ellipse_parameters_data2, ellipse_center_points_data2 = lp.get_fitted_ellipse_parameters(rotated_coordinates_dict2, columns_of_interest_ellipse)\n",
    "\n",
    "    average_diameter2 = np.mean([ellipse_parameters_data2[:,0], ellipse_parameters_data2[:,1]], axis=0)\n",
    "\n",
    "    SleapVideoData2 = process.convert_arrays_to_dataframe(['Seconds', 'Ellipse.Diameter', 'Ellipse.Angle', 'Ellipse.Center.X', 'Ellipse.Center.Y'], [VideoData2['Seconds'].values, average_diameter2, ellipse_parameters_data2[:,2], ellipse_center_points_data2[:,0], ellipse_center_points_data2[:,1]])\n",
    "\n",
    "############################################################################################################\n",
    "# some aggressive filtering of the pupil diameter\n",
    "############################################################################################################\n",
    "\n",
    "# VideoData1 filtering\n",
    "if VideoData1_Has_Sleap:\n",
    "    print(\"\\n=== VideoData1 Filtering ===\")\n",
    "    # Butterworth filter parameters\n",
    "    fs1 = 1 / np.median(np.diff(SleapVideoData1['Seconds']))  # Sampling frequency (Hz)\n",
    "    order = 6\n",
    "\n",
    "    b1, a1 = butter(order, cutoff / (0.5 * fs1), btype='low')\n",
    "    SleapVideoData1['Ellipse.Diameter.Filt'] = filtfilt(b1, a1, SleapVideoData1['Ellipse.Diameter'])\n",
    "\n",
    "    SleapVideoData1['Ellipse.Diameter'] = SleapVideoData1['Ellipse.Diameter'].rolling(window=12, center=True, min_periods=1).median()\n",
    "\n",
    "# VideoData2 filtering\n",
    "if VideoData2_Has_Sleap:\n",
    "    print(\"=== VideoData2 Filtering ===\")\n",
    "    fs2 = 1 / np.median(np.diff(SleapVideoData2['Seconds']))  # Sampling frequency (Hz)\n",
    "    order = 6\n",
    "\n",
    "    b2, a2 = butter(order, cutoff / (0.5 * fs2), btype='low')\n",
    "    SleapVideoData2['Ellipse.Diameter.Filt'] = filtfilt(b2, a2, SleapVideoData2['Ellipse.Diameter'])\n",
    "\n",
    "    SleapVideoData2['Ellipse.Diameter'] = SleapVideoData2['Ellipse.Diameter'].rolling(window=12, center=True, min_periods=1).median()\n",
    "\n",
    "print(\"✅ Done calculating pupil diameter and angle for both VideoData1 and VideoData2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-correlate pupil diameter for left and right eye \n",
    "############################################################################################################\n",
    "\n",
    "if VideoData1_Has_Sleap and VideoData2_Has_Sleap:\n",
    "    # Create subplots for both comparison and cross-correlation\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=1,\n",
    "        subplot_titles=[\"Pupil Diameter Comparison\", \"Cross-Correlation Analysis\"],\n",
    "        vertical_spacing=0.15\n",
    "    )\n",
    "\n",
    "    # Add SleapVideoData1 pupil diameter\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=SleapVideoData1['Seconds'],\n",
    "            y=SleapVideoData1['Ellipse.Diameter'],\n",
    "            mode='lines',\n",
    "            name=\"VideoData1 Pupil Diameter\",\n",
    "            line=dict(color='blue')\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "    # Add SleapVideoData2 pupil diameter\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=SleapVideoData2['Seconds'],\n",
    "            y=SleapVideoData2['Ellipse.Diameter'],\n",
    "            mode='lines',\n",
    "            name=\"VideoData2 Pupil Diameter\",\n",
    "            line=dict(color='red')\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "    # Cross-correlation analysis\n",
    "    print(\"=== Cross-Correlation Analysis ===\")\n",
    "\n",
    "    # Get pupil diameter data\n",
    "    pupil1 = SleapVideoData1['Ellipse.Diameter'].values\n",
    "    pupil2 = SleapVideoData2['Ellipse.Diameter'].values\n",
    "\n",
    "    # Handle different lengths by using the shorter dataset length\n",
    "    min_length = min(len(pupil1), len(pupil2))\n",
    "\n",
    "    # Truncate both datasets to the same length\n",
    "    pupil1_truncated = pupil1[:min_length]\n",
    "    pupil2_truncated = pupil2[:min_length]\n",
    "\n",
    "    # Remove NaN values for correlation, shouldn't be necessary as we filtered NaNs out before\n",
    "    valid_mask1 = ~np.isnan(pupil1_truncated)\n",
    "    valid_mask2 = ~np.isnan(pupil2_truncated)\n",
    "    valid_mask = valid_mask1 & valid_mask2\n",
    "\n",
    "    pupil1_clean = pupil1_truncated[valid_mask]\n",
    "    pupil2_clean = pupil2_truncated[valid_mask]\n",
    "\n",
    "    # Check if we have enough data\n",
    "    if len(pupil1_clean) < 2 or len(pupil2_clean) < 2:\n",
    "        print(\"❌ Error: Not enough valid data points for correlation analysis\")\n",
    "    else:\n",
    "        # Calculate cross-correlation\n",
    "        try:\n",
    "            correlation = correlate(pupil1_clean, pupil2_clean, mode='full')\n",
    "            \n",
    "            # Calculate lags (in samples)\n",
    "            lags = np.arange(-len(pupil2_clean) + 1, len(pupil1_clean))\n",
    "            \n",
    "            # Convert lags to time (assuming same sampling rate)\n",
    "            dt = np.median(np.diff(SleapVideoData1['Seconds']))\n",
    "            lag_times = lags * dt\n",
    "            \n",
    "            # Find peak correlation and corresponding lag\n",
    "            peak_idx = np.argmax(correlation)\n",
    "            peak_correlation = correlation[peak_idx]\n",
    "            peak_lag_samples = lags[peak_idx]\n",
    "            peak_lag_time = lag_times[peak_idx]\n",
    "            peak_lag_time_display = peak_lag_time # for final QC figure \n",
    "            \n",
    "            print(f\"Peak lag (time): {peak_lag_time:.4f} seconds\")\n",
    "\n",
    "        \n",
    "            # Normalize correlation to [-1, 1] range\n",
    "            norm_factor = np.sqrt(np.sum(pupil1_clean**2) * np.sum(pupil2_clean**2))\n",
    "            if norm_factor > 0:\n",
    "                correlation_normalized = correlation / norm_factor\n",
    "                peak_correlation_normalized = correlation_normalized[peak_idx]\n",
    "                print(f\"Peak normalized correlation: {peak_correlation_normalized:.4f}\")\n",
    "            else:\n",
    "                print(\"❌ Error: Cannot normalize correlation (zero variance)\")\n",
    "                correlation_normalized = correlation\n",
    "                peak_correlation_normalized = 0\n",
    "            \n",
    "            # Plot cross-correlation\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=lag_times,\n",
    "                    y=correlation_normalized,\n",
    "                    mode='lines',\n",
    "                    name=\"Cross-Correlation\",\n",
    "                    line=dict(color='green')\n",
    "                ),\n",
    "                row=2, col=1\n",
    "            )\n",
    "            \n",
    "            # Add vertical line at peak\n",
    "            fig.add_vline(\n",
    "                x=peak_lag_time,\n",
    "                line_dash=\"dash\",\n",
    "                line_color=\"red\",\n",
    "                annotation_text=f\"Peak: {peak_correlation_normalized:.3f}\",\n",
    "                row=2, col=1\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error in cross-correlation calculation: {e}\")\n",
    "            # Add empty trace to maintain plot structure\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=[0], y=[0],\n",
    "                    mode='lines',\n",
    "                    name=\"Cross-Correlation (Error)\",\n",
    "                    line=dict(color='gray')\n",
    "                ),\n",
    "                row=2, col=1\n",
    "            )\n",
    "\n",
    "    # Update axes labels\n",
    "    fig.update_xaxes(title_text=\"Time (seconds)\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Pupil Diameter\", row=1, col=1)\n",
    "    fig.update_xaxes(title_text=\"Lag (seconds)\", row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"Normalized Correlation\", row=2, col=1)\n",
    "\n",
    "    fig.update_layout(\n",
    "        height=800,\n",
    "        width=1000,\n",
    "        title_text=\"SLEAP Pupil Diameter Analysis: Comparison & Cross-Correlation\"\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    # Additional correlation statistics\n",
    "    if len(pupil1_clean) >= 2 and len(pupil2_clean) >= 2:\n",
    "        try:\n",
    "            # Calculate Pearson correlation coefficient\n",
    "            pearson_r, pearson_p = pearsonr(pupil1_clean, pupil2_clean)\n",
    "            pearson_r_display = pearson_r\n",
    "            pearson_p_display = pearson_p\n",
    "            \n",
    "            print(f\"\\n=== Additional Statistics ===\")\n",
    "            print(f\"Pearson correlation coefficient: {pearson_r:.2f}\")\n",
    "\n",
    "            # Handle extremely small p-values\n",
    "            if pearson_p < 1e-300:\n",
    "                print(f'Pearson p-value: < 1e-300 (extremely significant)')\n",
    "            else:\n",
    "                print(f'Pearson p-value: {pearson_p:.5e}')\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error in additional statistics: {e}\")\n",
    "            pearson_r_display = None\n",
    "            pearson_p_display = None\n",
    "    else:\n",
    "        print(\"❌ Cannot calculate additional statistics - insufficient data\")\n",
    "else:\n",
    "    print(\"Only one eye is present, no pupil diameter cross-correlation can be done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if Second values match 1:1 between VideoData and SleapVideoData then merge them into VideoData\n",
    "############################################################################################################\n",
    "\n",
    "if VideoData1_Has_Sleap is True:\n",
    "    if VideoData1['Seconds'].equals(SleapVideoData1['Seconds']) is False:\n",
    "        print(\"❗ Video1: The 'Seconds' columns DO NOT correspond 1:1 between the two DataFrames. This should not happen\")\n",
    "    else:\n",
    "        VideoData1 = VideoData1.merge(SleapVideoData1, on='Seconds', how='outer')\n",
    "        del SleapVideoData1\n",
    "\n",
    "if VideoData2_Has_Sleap is True:\n",
    "    if VideoData2['Seconds'].equals(SleapVideoData2['Seconds']) is False:\n",
    "        print(\"❗ Video2: The 'Seconds' columns DO NOT correspond 1:1 between the two DataFrames. This should not happen\")\n",
    "    else:\n",
    "        VideoData2 = VideoData2.merge(SleapVideoData2, on='Seconds', how='outer')\n",
    "        del SleapVideoData2\n",
    "gc.collect()\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare SLEAP center.x and .y with fitted ellipse centre distributions for both VideoData1 and VideoData2\n",
    "############################################################################################################\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1) Compute correlations for VideoData1\n",
    "# ------------------------------------------------------------------\n",
    "if VideoData1_Has_Sleap is True:\n",
    "    print(\"=== VideoData1 Analysis ===\")\n",
    "    slope_x1, intercept_x1, r_value_x1, p_value_x1, std_err_x1 = linregress(\n",
    "        VideoData1[\"Ellipse.Center.X\"], \n",
    "        VideoData1[\"center.x\"]\n",
    "    )\n",
    "    r_squared_x1 = r_value_x1**2\n",
    "    print(f\"VideoData1 - R^2 between center point and ellipse center X data: {r_squared_x1:.4f}\")\n",
    "\n",
    "    slope_y1, intercept_y1, r_value_y1, p_value_y1, std_err_y1 = linregress(\n",
    "        VideoData1[\"Ellipse.Center.Y\"], \n",
    "        VideoData1[\"center.y\"]\n",
    "    )\n",
    "    r_squared_y1 = r_value_y1**2\n",
    "    print(f\"VideoData1 - R^2 between center point and ellipse center Y data: {r_squared_y1:.4f}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2) Compute correlations for VideoData2\n",
    "# ------------------------------------------------------------------\n",
    "if VideoData2_Has_Sleap is True:\n",
    "    print(\"\\n=== VideoData2 Analysis ===\")\n",
    "    slope_x2, intercept_x2, r_value_x2, p_value_x2, std_err_x2 = linregress(\n",
    "        VideoData2[\"Ellipse.Center.X\"], \n",
    "        VideoData2[\"center.x\"]\n",
    "    )\n",
    "    r_squared_x2 = r_value_x2**2\n",
    "    print(f\"VideoData2 - R^2 between center point and ellipse center X data: {r_squared_x2:.4f}\")\n",
    "\n",
    "    slope_y2, intercept_y2, r_value_y2, p_value_y2, std_err_y2 = linregress(\n",
    "        VideoData2[\"Ellipse.Center.Y\"], \n",
    "        VideoData2[\"center.y\"]\n",
    "    )\n",
    "    r_squared_y2 = r_value_y2**2\n",
    "    print(f\"VideoData2 - R^2 between center point and ellipse center Y data: {r_squared_y2:.4f}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3) Create subplots for VideoData1\n",
    "# ------------------------------------------------------------------\n",
    "fig1, ax1 = plt.subplots(nrows=1, ncols=3, figsize=(30, 6))\n",
    "fig1.suptitle('VideoData1: Center Point vs Ellipse Center Analysis', fontsize=16)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4) Scatter plots + linear fits for VideoData1\n",
    "# ------------------------------------------------------------------\n",
    "# (a) For X data - VideoData1\n",
    "if VideoData1_Has_Sleap is True:\n",
    "    ax1[0].scatter(\n",
    "        VideoData1[\"Ellipse.Center.X\"], \n",
    "        VideoData1[\"center.x\"], \n",
    "        alpha=0.5, \n",
    "        label=\"Data points\",\n",
    "        color='blue'\n",
    "    )\n",
    "    ax1[0].plot(\n",
    "        VideoData1[\"Ellipse.Center.X\"],\n",
    "        intercept_x1 + slope_x1 * VideoData1[\"Ellipse.Center.X\"],\n",
    "        \"r\",\n",
    "        label=f\"Fitted line (R^2={r_squared_x1:.2f})\"\n",
    "    )\n",
    "    ax1[0].set_xlabel(\"Ellipse.Center.X\")\n",
    "    ax1[0].set_ylabel(\"center.x\")\n",
    "    ax1[0].set_title(\"VideoData1: Ellipse.Center.X vs center.x (with linear fit)\")\n",
    "    ax1[0].legend()\n",
    "\n",
    "    # (b) For Y data - VideoData1\n",
    "    ax1[1].scatter(\n",
    "        VideoData1[\"Ellipse.Center.Y\"], \n",
    "        VideoData1[\"center.y\"], \n",
    "        alpha=0.5, \n",
    "        label=\"Data points\",\n",
    "        color='blue'\n",
    "    )\n",
    "    ax1[1].plot(\n",
    "        VideoData1[\"Ellipse.Center.Y\"],\n",
    "        intercept_y1 + slope_y1 * VideoData1[\"Ellipse.Center.Y\"],\n",
    "        \"r\",\n",
    "        label=f\"Fitted line (R^2={r_squared_y1:.2f})\"\n",
    "    )\n",
    "    ax1[1].set_xlabel(\"Ellipse.Center.Y\")\n",
    "    ax1[1].set_ylabel(\"center.y\")\n",
    "    ax1[1].set_title(\"VideoData1: Ellipse.Center.Y vs center.y (with linear fit)\")\n",
    "    ax1[1].legend()\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 5) 2D KDE for VideoData1\n",
    "# ------------------------------------------------------------------\n",
    "if VideoData1_Has_Sleap is True:\n",
    "    # Ellipse.Center (blue)\n",
    "    x_ellipse1 = VideoData1[\"Ellipse.Center.X\"].to_numpy()\n",
    "    y_ellipse1 = VideoData1[\"Ellipse.Center.Y\"].to_numpy()\n",
    "    data_ellipse1 = np.vstack([x_ellipse1, y_ellipse1])\n",
    "\n",
    "    fkde_ellipse1 = fastKDE(data_ellipse1)\n",
    "    pdf_ellipse1 = fkde_ellipse1.pdf\n",
    "    x_axis_ellipse1, y_axis_ellipse1 = fkde_ellipse1.axes\n",
    "\n",
    "    im_ellipse1 = ax1[2].imshow(\n",
    "        pdf_ellipse1,\n",
    "        extent=[x_axis_ellipse1.min(), x_axis_ellipse1.max(), \n",
    "                y_axis_ellipse1.min(), y_axis_ellipse1.max()],\n",
    "        aspect=\"auto\",\n",
    "        origin=\"lower\",\n",
    "        cmap=\"Blues\",\n",
    "        alpha=0.4,\n",
    "        norm=LogNorm(),\n",
    "        zorder=1\n",
    "    )\n",
    "\n",
    "    cbar_ellipse1 = plt.colorbar(im_ellipse1, ax=ax1[2], orientation=\"vertical\", fraction=0.046, pad=0.04)\n",
    "    cbar_ellipse1.set_label(\"Ellipse.Center Density (log scale)\")\n",
    "\n",
    "    # Center (red)\n",
    "    x_center1 = VideoData1[\"center.x\"].to_numpy()\n",
    "    y_center1 = VideoData1[\"center.y\"].to_numpy()\n",
    "    data_center1 = np.vstack([x_center1, y_center1])\n",
    "\n",
    "    fkde_center1 = fastKDE(data_center1)\n",
    "    pdf_center1 = fkde_center1.pdf\n",
    "    x_axis_center1, y_axis_center1 = fkde_center1.axes\n",
    "\n",
    "    im_center1 = ax1[2].imshow(\n",
    "        pdf_center1,\n",
    "        extent=[x_axis_center1.min(), x_axis_center1.max(), \n",
    "                y_axis_center1.min(), y_axis_center1.max()],\n",
    "        aspect=\"auto\",\n",
    "        origin=\"lower\",\n",
    "        cmap=\"Reds\",\n",
    "        alpha=0.8,\n",
    "        norm=LogNorm(),\n",
    "        zorder=2\n",
    "    )\n",
    "\n",
    "    cbar_center1 = plt.colorbar(im_center1, ax=ax1[2], orientation=\"vertical\", fraction=0.046, pad=0.04)\n",
    "    cbar_center1.set_label(\"Center Density (log scale)\")\n",
    "\n",
    "    ax1[2].set_xlabel(\"X coordinates\")\n",
    "    ax1[2].set_ylabel(\"Y coordinates\")\n",
    "    ax1[2].set_title(\"VideoData1: Probability distribution of X-Y pairs\")\n",
    "\n",
    "    legend_elements1 = [\n",
    "        Line2D([0], [0], color=\"blue\", lw=4, label=\"Ellipse.Center\"),\n",
    "        Line2D([0], [0], color=\"red\",  lw=4, label=\"center.xy\")\n",
    "    ]\n",
    "    ax1[2].legend(handles=legend_elements1, loc=\"upper right\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 6) Create subplots for VideoData2\n",
    "# ------------------------------------------------------------------\n",
    "fig2, ax2 = plt.subplots(nrows=1, ncols=3, figsize=(30, 6))\n",
    "fig2.suptitle('VideoData2: Center Point vs Ellipse Center Analysis', fontsize=16)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 7) Scatter plots + linear fits for VideoData2\n",
    "# ------------------------------------------------------------------\n",
    "if VideoData2_Has_Sleap:\n",
    "    # (a) For X data - VideoData2\n",
    "    ax2[0].scatter(\n",
    "        VideoData2[\"Ellipse.Center.X\"], \n",
    "        VideoData2[\"center.x\"], \n",
    "        alpha=0.5, \n",
    "        label=\"Data points\",\n",
    "        color='green'\n",
    "    )\n",
    "    ax2[0].plot(\n",
    "        VideoData2[\"Ellipse.Center.X\"],\n",
    "        intercept_x2 + slope_x2 * VideoData2[\"Ellipse.Center.X\"],\n",
    "        \"r\",\n",
    "        label=f\"Fitted line (R^2={r_squared_x2:.2f})\"\n",
    "    )\n",
    "    ax2[0].set_xlabel(\"Ellipse.Center.X\")\n",
    "    ax2[0].set_ylabel(\"center.x\")\n",
    "    ax2[0].set_title(\"VideoData2: Ellipse.Center.X vs center.x (with linear fit)\")\n",
    "    ax2[0].legend()\n",
    "\n",
    "    # (b) For Y data - VideoData2\n",
    "    ax2[1].scatter(\n",
    "        VideoData2[\"Ellipse.Center.Y\"], \n",
    "        VideoData2[\"center.y\"], \n",
    "        alpha=0.5, \n",
    "        label=\"Data points\",\n",
    "        color='green'\n",
    "    )\n",
    "    ax2[1].plot(\n",
    "        VideoData2[\"Ellipse.Center.Y\"],\n",
    "        intercept_y2 + slope_y2 * VideoData2[\"Ellipse.Center.Y\"],\n",
    "        \"r\",\n",
    "        label=f\"Fitted line (R^2={r_squared_y2:.2f})\"\n",
    "    )\n",
    "    ax2[1].set_xlabel(\"Ellipse.Center.Y\")\n",
    "    ax2[1].set_ylabel(\"center.y\")\n",
    "    ax2[1].set_title(\"VideoData2: Ellipse.Center.Y vs center.y (with linear fit)\")\n",
    "    ax2[1].legend()\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 8) 2D KDE for VideoData2\n",
    "    # ------------------------------------------------------------------\n",
    "    # Ellipse.Center (blue)\n",
    "    x_ellipse2 = VideoData2[\"Ellipse.Center.X\"].to_numpy()\n",
    "    y_ellipse2 = VideoData2[\"Ellipse.Center.Y\"].to_numpy()\n",
    "    data_ellipse2 = np.vstack([x_ellipse2, y_ellipse2])\n",
    "\n",
    "    fkde_ellipse2 = fastKDE(data_ellipse2)\n",
    "    pdf_ellipse2 = fkde_ellipse2.pdf\n",
    "    x_axis_ellipse2, y_axis_ellipse2 = fkde_ellipse2.axes\n",
    "\n",
    "    im_ellipse2 = ax2[2].imshow(\n",
    "        pdf_ellipse2,\n",
    "        extent=[x_axis_ellipse2.min(), x_axis_ellipse2.max(), \n",
    "                y_axis_ellipse2.min(), y_axis_ellipse2.max()],\n",
    "        aspect=\"auto\",\n",
    "        origin=\"lower\",\n",
    "        cmap=\"Blues\",\n",
    "        alpha=0.4,\n",
    "        norm=LogNorm(),\n",
    "        zorder=1\n",
    "    )\n",
    "\n",
    "    cbar_ellipse2 = plt.colorbar(im_ellipse2, ax=ax2[2], orientation=\"vertical\", fraction=0.046, pad=0.04)\n",
    "    cbar_ellipse2.set_label(\"Ellipse.Center Density (log scale)\")\n",
    "\n",
    "    # Center (red)\n",
    "    x_center2 = VideoData2[\"center.x\"].to_numpy()\n",
    "    y_center2 = VideoData2[\"center.y\"].to_numpy()\n",
    "    data_center2 = np.vstack([x_center2, y_center2])\n",
    "\n",
    "    fkde_center2 = fastKDE(data_center2)\n",
    "    pdf_center2 = fkde_center2.pdf\n",
    "    x_axis_center2, y_axis_center2 = fkde_center2.axes\n",
    "\n",
    "    im_center2 = ax2[2].imshow(\n",
    "        pdf_center2,\n",
    "        extent=[x_axis_center2.min(), x_axis_center2.max(), \n",
    "                y_axis_center2.min(), y_axis_center2.max()],\n",
    "        aspect=\"auto\",\n",
    "        origin=\"lower\",\n",
    "        cmap=\"Reds\",\n",
    "        alpha=0.8,\n",
    "        norm=LogNorm(),\n",
    "        zorder=2\n",
    "    )\n",
    "\n",
    "    cbar_center2 = plt.colorbar(im_center2, ax=ax2[2], orientation=\"vertical\", fraction=0.046, pad=0.04)\n",
    "    cbar_center2.set_label(\"Center Density (log scale)\")\n",
    "\n",
    "    ax2[2].set_xlabel(\"X coordinates\")\n",
    "    ax2[2].set_ylabel(\"Y coordinates\")\n",
    "    ax2[2].set_title(\"VideoData2: Probability distribution of X-Y pairs\")\n",
    "\n",
    "    legend_elements2 = [\n",
    "        Line2D([0], [0], color=\"blue\", lw=4, label=\"Ellipse.Center\"),\n",
    "        Line2D([0], [0], color=\"red\",  lw=4, label=\"center.xy\")\n",
    "    ]\n",
    "    ax2[2].legend(handles=legend_elements2, loc=\"upper right\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ae8d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make and save summary QC plot using matplotlib with scatter plots for 2D distributions\n",
    "\n",
    "# Initialize the statistics variables (these are calculated in Cell 11)\n",
    "try:\n",
    "    pearson_r_display\n",
    "except NameError:\n",
    "    pearson_r_display = None\n",
    "    pearson_p_display = None\n",
    "    peak_lag_time_display = None\n",
    "    print(\"⚠️ Note: Statistics not found. They should be calculated in Cell 11.\")\n",
    "\n",
    "# Calculate correlation for Ellipse.Center.X between VideoData1 and VideoData2 (if both exist)\n",
    "pearson_r_center = None\n",
    "pearson_p_center = None\n",
    "peak_lag_time_center = None\n",
    "\n",
    "if VideoData1_Has_Sleap and VideoData2_Has_Sleap:\n",
    "    # Get the Center.X data\n",
    "    center_x1 = VideoData1['Ellipse.Center.X'].values\n",
    "    center_x2 = VideoData2['Ellipse.Center.X'].values\n",
    "    \n",
    "    min_length = min(len(center_x1), len(center_x2))\n",
    "    center_x1_truncated = center_x1[:min_length]\n",
    "    center_x2_truncated = center_x2[:min_length]\n",
    "    \n",
    "    valid_mask1 = ~np.isnan(center_x1_truncated)\n",
    "    valid_mask2 = ~np.isnan(center_x2_truncated)\n",
    "    valid_mask = valid_mask1 & valid_mask2\n",
    "    \n",
    "    center_x1_clean = center_x1_truncated[valid_mask]\n",
    "    center_x2_clean = center_x2_truncated[valid_mask]\n",
    "    \n",
    "    if len(center_x1_clean) >= 2 and len(center_x2_clean) >= 2:\n",
    "        try:\n",
    "            # Calculate Pearson correlation\n",
    "            pearson_r_center, pearson_p_center = pearsonr(center_x1_clean, center_x2_clean)\n",
    "            \n",
    "            # Calculate cross-correlation for peak lag\n",
    "            correlation = correlate(center_x1_clean, center_x2_clean, mode='full')\n",
    "            lags = np.arange(-len(center_x2_clean) + 1, len(center_x1_clean))\n",
    "            dt = np.median(np.diff(VideoData1['Seconds']))\n",
    "            lag_times = lags * dt\n",
    "            peak_idx = np.argmax(correlation)\n",
    "            peak_lag_time_center = lag_times[peak_idx]\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error calculating Ellipse.Center.X correlation stats: {e}\")\n",
    "\n",
    "# Create the QC summary figure using matplotlib with custom grid layout\n",
    "fig = plt.figure(figsize=(20, 18))\n",
    "fig.suptitle(str(data_path), fontsize=16, y=0.995)\n",
    "\n",
    "# Create a grid layout:\n",
    "# - Top row (full width): VideoData1 Time Series\n",
    "# - Second row (full width): VideoData2 Time Series  \n",
    "# - Third row (two columns): 2D scatter plots (VideoData1 left, VideoData2 right)\n",
    "# - Fourth row (two columns): Pupil diameter (left), Ellipse.Center.X correlation (right)\n",
    "\n",
    "gs = fig.add_gridspec(4, 2, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# Panel 1: VideoData1 center coordinates - Time Series (full width)\n",
    "if VideoData1_Has_Sleap:\n",
    "    ax1 = fig.add_subplot(gs[0, :])\n",
    "    ax1.plot(VideoData1_centered['Seconds'], VideoData1_centered['center.x'],\n",
    "            linewidth=0.5, c='blue', alpha=0.6, label='Centered Center.X')\n",
    "    ax1.plot(VideoData1['Seconds'], VideoData1['Ellipse.Center.X'],\n",
    "            linewidth=0.5, c='red', alpha=0.6, label='Ellipse Center.X')\n",
    "    ax1.set_xlabel('Time (s)')\n",
    "    ax1.set_ylabel('Position (pixels)')\n",
    "    ax1.set_title('VideoData1 - center.X Time Series')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Panel 2: VideoData2 center coordinates - Time Series (full width)\n",
    "if VideoData2_Has_Sleap:\n",
    "    ax2 = fig.add_subplot(gs[1, :])\n",
    "    ax2.plot(VideoData2_centered['Seconds'], VideoData2_centered['center.x'],\n",
    "            linewidth=0.5, c='blue', alpha=0.6, label='Centered Center.X')\n",
    "    ax2.plot(VideoData2['Seconds'], VideoData2['Ellipse.Center.X'],\n",
    "            linewidth=0.5, c='red', alpha=0.6, label='Ellipse Center.X')\n",
    "    ax2.set_xlabel('Time (s)')\n",
    "    ax2.set_ylabel('Position (pixels)')\n",
    "    ax2.set_title('VideoData2 - center.X Time Series')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Panel 3: VideoData1 center coordinates - Scatter plot (left half)\n",
    "if VideoData1_Has_Sleap:\n",
    "    ax3 = fig.add_subplot(gs[2, 0])\n",
    "    \n",
    "    # Ellipse.Center (blue)\n",
    "    x_ellipse1 = VideoData1['Ellipse.Center.X'].to_numpy()\n",
    "    y_ellipse1 = VideoData1['Ellipse.Center.Y'].to_numpy()\n",
    "    mask1 = ~(np.isnan(x_ellipse1) | np.isnan(y_ellipse1))\n",
    "    \n",
    "    ax3.scatter(x_ellipse1[mask1], y_ellipse1[mask1],\n",
    "               s=1, alpha=0.3, c='blue', label='Ellipse.Center')\n",
    "    \n",
    "    # Center (red) - from centered data\n",
    "    x_center1 = VideoData1_centered['center.x'].to_numpy()\n",
    "    y_center1 = VideoData1_centered['center.y'].to_numpy()\n",
    "    mask2 = ~(np.isnan(x_center1) | np.isnan(y_center1))\n",
    "    \n",
    "    ax3.scatter(x_center1[mask2], y_center1[mask2],\n",
    "               s=1, alpha=0.3, c='red', label='Centered center.X')\n",
    "    \n",
    "    ax3.set_xlabel('Center X (pixels)')\n",
    "    ax3.set_ylabel('Center Y (pixels)')\n",
    "    ax3.set_title('VideoData1 - Center X-Y Distribution (center.X vs Ellipse)')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Panel 4: VideoData2 center coordinates - Scatter plot (right half)\n",
    "if VideoData2_Has_Sleap:\n",
    "    ax4 = fig.add_subplot(gs[2, 1])\n",
    "    \n",
    "    # Ellipse.Center (blue)\n",
    "    x_ellipse2 = VideoData2['Ellipse.Center.X'].to_numpy()\n",
    "    y_ellipse2 = VideoData2['Ellipse.Center.Y'].to_numpy()\n",
    "    mask3 = ~(np.isnan(x_ellipse2) | np.isnan(y_ellipse2))\n",
    "    \n",
    "    ax4.scatter(x_ellipse2[mask3], y_ellipse2[mask3],\n",
    "               s=1, alpha=0.3, c='blue', label='Ellipse.Center')\n",
    "    \n",
    "    # Center (red) - from centered data\n",
    "    x_center2 = VideoData2_centered['center.x'].to_numpy()\n",
    "    y_center2 = VideoData2_centered['center.y'].to_numpy()\n",
    "    mask4 = ~(np.isnan(x_center2) | np.isnan(y_center2))\n",
    "    \n",
    "    ax4.scatter(x_center2[mask4], y_center2[mask4],\n",
    "               s=1, alpha=0.3, c='red', label='center.X Center')\n",
    "    \n",
    "    ax4.set_xlabel('Center X (pixels)')\n",
    "    ax4.set_ylabel('Center Y (pixels)')\n",
    "    ax4.set_title('VideoData2 - Center X-Y Distribution (center.X vs Ellipse)')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# Panel 5: Pupil diameter comparison (bottom left)\n",
    "ax5 = fig.add_subplot(gs[3, 0])\n",
    "if VideoData1_Has_Sleap and VideoData2_Has_Sleap:\n",
    "    ax5.plot(VideoData1['Seconds'], VideoData1['Ellipse.Diameter.Filt'],\n",
    "            linewidth=0.5, c='#FF7F00', alpha=0.6, label='VideoData1 Diameter')\n",
    "    ax5.plot(VideoData2['Seconds'], VideoData2['Ellipse.Diameter.Filt'],\n",
    "            linewidth=0.5, c='#9370DB', alpha=0.6, label='VideoData2 Diameter')\n",
    "elif VideoData1_Has_Sleap:\n",
    "    ax5.plot(VideoData1['Seconds'], VideoData1['Ellipse.Diameter.Filt'],\n",
    "            linewidth=0.5, c='#FF7F00', alpha=0.6, label='VideoData1 Diameter')\n",
    "elif VideoData2_Has_Sleap:\n",
    "    ax5.plot(VideoData2['Seconds'], VideoData2['Ellipse.Diameter.Filt'],\n",
    "            linewidth=0.5, c='#9370DB', alpha=0.6, label='VideoData2 Diameter')\n",
    "\n",
    "ax5.set_xlabel('Time (s)')\n",
    "ax5.set_ylabel('Diameter (pixels)')\n",
    "ax5.set_title('Pupil Diameter Comparison')\n",
    "ax5.legend()\n",
    "ax5.grid(True, alpha=0.3)\n",
    "\n",
    "# Add statistics text to Panel 5\n",
    "if pearson_r_display is not None and pearson_p_display is not None and peak_lag_time_display is not None:\n",
    "    stats_text = (f'Pearson r = {pearson_r_display:.4f}\\n'\n",
    "                  f'Pearson p = {pearson_p_display:.4e}\\n'\n",
    "                  f'Peak lag = {peak_lag_time_display:.4f} s')\n",
    "    ax5.text(0.98, 0.98, stats_text, transform=ax5.transAxes,\n",
    "            verticalalignment='top', horizontalalignment='right',\n",
    "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8),\n",
    "            fontsize=10, family='monospace')\n",
    "else:\n",
    "    ax5.text(0.5, 0.5, 'Statistics not available\\n(See Cell 11 for correlation analysis)', \n",
    "            transform=ax5.transAxes, ha='center', va='center', fontsize=10)\n",
    "\n",
    "# Panel 6: Ellipse.Center.X comparison (bottom right) with dual y-axis\n",
    "ax6 = fig.add_subplot(gs[3, 1])\n",
    "ax6_twin = ax6.twinx()  # Create a second y-axis\n",
    "\n",
    "if VideoData1_Has_Sleap and VideoData2_Has_Sleap:\n",
    "    # Plot the individual traces\n",
    "    ax6.plot(VideoData1['Seconds'], VideoData1['Ellipse.Center.X'],\n",
    "            linewidth=0.5, c='#FF7F00', alpha=0.6, label='VideoData1 Ellipse.Center.X')\n",
    "    ax6.plot(VideoData2['Seconds'], VideoData2['Ellipse.Center.X'],\n",
    "            linewidth=0.5, c='#9370DB', alpha=0.6, label='VideoData2 Ellipse.Center.X')\n",
    "    \n",
    "    # Plot the difference on the right axis\n",
    "    # Align the data to the same length and normalize for fair comparison\n",
    "    min_length = min(len(VideoData1), len(VideoData2))\n",
    "    \n",
    "    # Normalize data (z-score) to account for different scales\n",
    "    center_x1_aligned = VideoData1['Ellipse.Center.X'].iloc[:min_length]\n",
    "    center_x2_aligned = VideoData2['Ellipse.Center.X'].iloc[:min_length]\n",
    "    \n",
    "    # Calculate mean and std for normalization\n",
    "    mean1 = center_x1_aligned.mean()\n",
    "    std1 = center_x1_aligned.std()\n",
    "    mean2 = center_x2_aligned.mean()\n",
    "    std2 = center_x2_aligned.std()\n",
    "    \n",
    "    # Normalize both datasets\n",
    "    center_x1_norm = (center_x1_aligned - mean1) / std1\n",
    "    center_x2_norm = (center_x2_aligned - mean2) / std2\n",
    "    \n",
    "    # Calculate difference of normalized data\n",
    "    center_x_diff = center_x1_norm - center_x2_norm\n",
    "    seconds_aligned = VideoData1['Seconds'].iloc[:min_length]\n",
    "    ax6_twin.plot(seconds_aligned, center_x_diff,\n",
    "                  linewidth=0.5, c='green', alpha=0.6, label='Difference (normalized)')\n",
    "    \n",
    "elif VideoData1_Has_Sleap:\n",
    "    ax6.plot(VideoData1['Seconds'], VideoData1['Ellipse.Center.X'],\n",
    "            linewidth=0.5, c='#FF7F00', alpha=0.6, label='VideoData1 Ellipse.Center.X')\n",
    "elif VideoData2_Has_Sleap:\n",
    "    ax6.plot(VideoData2['Seconds'], VideoData2['Ellipse.Center.X'],\n",
    "            linewidth=0.5, c='#9370DB', alpha=0.6, label='VideoData2 Ellipse.Center.X')\n",
    "\n",
    "ax6.set_xlabel('Time (s)')\n",
    "ax6.set_ylabel('Center X (pixels)', color='black')\n",
    "ax6.set_title('Ellipse.Center.X Comparison')\n",
    "ax6.tick_params(axis='y', labelcolor='black')\n",
    "if VideoData1_Has_Sleap and VideoData2_Has_Sleap:\n",
    "    ax6_twin.set_ylabel('Normalized Difference (z-score)', color='green')\n",
    "    ax6_twin.tick_params(axis='y', labelcolor='green')\n",
    "\n",
    "# Combine legends from both axes\n",
    "lines1, labels1 = ax6.get_legend_handles_labels()\n",
    "if VideoData1_Has_Sleap and VideoData2_Has_Sleap:\n",
    "    lines2, labels2 = ax6_twin.get_legend_handles_labels()\n",
    "    ax6.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "else:\n",
    "    ax6.legend(loc='upper left')\n",
    "\n",
    "ax6.grid(True, alpha=0.3)\n",
    "\n",
    "# Add statistics text to Panel 6\n",
    "if pearson_r_center is not None and pearson_p_center is not None and peak_lag_time_center is not None:\n",
    "    stats_text = (f'Pearson r = {pearson_r_center:.4f}\\n'\n",
    "                  f'Pearson p = {pearson_p_center:.4e}\\n'\n",
    "                  f'Peak lag = {peak_lag_time_center:.4f} s')\n",
    "    ax6.text(0.98, 0.98, stats_text, transform=ax6.transAxes,\n",
    "            verticalalignment='top', horizontalalignment='right',\n",
    "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8),\n",
    "            fontsize=10, family='monospace')\n",
    "else:\n",
    "    ax6.text(0.5, 0.5, 'Statistics not available\\n(both eyes required)', \n",
    "            transform=ax6.transAxes, ha='center', va='center', fontsize=10)\n",
    "\n",
    "# Save as PDF\n",
    "save_path.mkdir(parents=True, exist_ok=True)\n",
    "pdf_path = save_path / \"Eye_data_QC.pdf\"\n",
    "plt.savefig(pdf_path, dpi=300, bbox_inches='tight', format='pdf')\n",
    "print(f\"✅ QC figure saved to: {pdf_path}\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d37438a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interactive time series plots using plotly for browser viewing\n",
    "\n",
    "# Create subplots for the time series (3 rows now instead of 2)\n",
    "# Need to enable secondary_y for the third panel\n",
    "fig = make_subplots(\n",
    "    rows=3, cols=1,\n",
    "    shared_xaxes=True,\n",
    "    vertical_spacing=0.08,\n",
    "    subplot_titles=('VideoData1 - center.X Time Series', \n",
    "                    'VideoData2 - center.X Time Series',\n",
    "                    'Ellipse.Center.X Comparison with Difference'),\n",
    "    specs=[[{}], [{}], [{\"secondary_y\": True}]]  # Enable secondary_y for row 3\n",
    ")\n",
    "\n",
    "# Panel 1: VideoData1 center coordinates - Time Series\n",
    "if VideoData1_Has_Sleap:\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=VideoData1_centered['Seconds'],\n",
    "        y=VideoData1_centered['center.x'],\n",
    "        mode='lines',\n",
    "        name='Centered Center.X',\n",
    "        line=dict(color='blue', width=0.5),\n",
    "        opacity=0.6\n",
    "    ), row=1, col=1)\n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=VideoData1['Seconds'],\n",
    "        y=VideoData1['Ellipse.Center.X'],\n",
    "        mode='lines',\n",
    "        name='Ellipse Center.X',\n",
    "        line=dict(color='red', width=0.5),\n",
    "        opacity=0.6\n",
    "    ), row=1, col=1)\n",
    "\n",
    "# Panel 2: VideoData2 center coordinates - Time Series\n",
    "if VideoData2_Has_Sleap:\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=VideoData2_centered['Seconds'],\n",
    "        y=VideoData2_centered['center.x'],\n",
    "        mode='lines',\n",
    "        name='Centered Center.X',\n",
    "        line=dict(color='blue', width=0.5),\n",
    "        opacity=0.6\n",
    "    ), row=2, col=1)\n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=VideoData2['Seconds'],\n",
    "        y=VideoData2['Ellipse.Center.X'],\n",
    "        mode='lines',\n",
    "        name='Ellipse Center.X',\n",
    "        line=dict(color='red', width=0.5),\n",
    "        opacity=0.6\n",
    "    ), row=2, col=1)\n",
    "\n",
    "# Panel 3: Ellipse.Center.X Comparison with difference\n",
    "if VideoData1_Has_Sleap and VideoData2_Has_Sleap:\n",
    "    # Plot the individual traces\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=VideoData1['Seconds'],\n",
    "        y=VideoData1['Ellipse.Center.X'],\n",
    "        mode='lines',\n",
    "        name='VideoData1 Ellipse.Center.X',\n",
    "        line=dict(color='#FF7F00', width=0.5),  # Orange\n",
    "        opacity=0.6\n",
    "    ), row=3, col=1)\n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=VideoData2['Seconds'],\n",
    "        y=VideoData2['Ellipse.Center.X'],\n",
    "        mode='lines',\n",
    "        name='VideoData2 Ellipse.Center.X',\n",
    "        line=dict(color='#9370DB', width=0.5),  # Purple\n",
    "        opacity=0.6\n",
    "    ), row=3, col=1)\n",
    "    \n",
    "    # Plot the difference on secondary y-axis\n",
    "    # Align the data to the same length and normalize for fair comparison\n",
    "    min_length = min(len(VideoData1), len(VideoData2))\n",
    "    \n",
    "    # Normalize data (z-score) to account for different scales\n",
    "    center_x1_aligned = VideoData1['Ellipse.Center.X'].iloc[:min_length]\n",
    "    center_x2_aligned = VideoData2['Ellipse.Center.X'].iloc[:min_length]\n",
    "    \n",
    "    # Calculate mean and std for normalization\n",
    "    mean1 = center_x1_aligned.mean()\n",
    "    std1 = center_x1_aligned.std()\n",
    "    mean2 = center_x2_aligned.mean()\n",
    "    std2 = center_x2_aligned.std()\n",
    "    \n",
    "    # Normalize both datasets\n",
    "    center_x1_norm = (center_x1_aligned - mean1) / std1\n",
    "    center_x2_norm = (center_x2_aligned - mean2) / std2\n",
    "    \n",
    "    # Calculate difference of normalized data\n",
    "    center_x_diff = center_x1_norm - center_x2_norm\n",
    "    seconds_aligned = VideoData1['Seconds'].iloc[:min_length]\n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=seconds_aligned,\n",
    "        y=center_x_diff,\n",
    "        mode='lines',\n",
    "        name='Difference (normalized)',\n",
    "        line=dict(color='green', width=0.5),\n",
    "        opacity=0.6\n",
    "    ), row=3, col=1, secondary_y=True)\n",
    "    \n",
    "elif VideoData1_Has_Sleap:\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=VideoData1['Seconds'],\n",
    "        y=VideoData1['Ellipse.Center.X'],\n",
    "        mode='lines',\n",
    "        name='VideoData1 Ellipse.Center.X',\n",
    "        line=dict(color='#FF7F00', width=0.5),\n",
    "        opacity=0.6\n",
    "    ), row=3, col=1)\n",
    "elif VideoData2_Has_Sleap:\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=VideoData2['Seconds'],\n",
    "        y=VideoData2['Ellipse.Center.X'],\n",
    "        mode='lines',\n",
    "        name='VideoData2 Ellipse.Center.X',\n",
    "        line=dict(color='#9370DB', width=0.5),\n",
    "        opacity=0.6\n",
    "    ), row=3, col=1)\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    height=1200,  # Increased height for 3 panels\n",
    "    title_text=f'{data_path} - Eye Tracking Time Series QC',\n",
    "    showlegend=True,\n",
    "    hovermode='x unified'\n",
    ")\n",
    "\n",
    "# Update axes\n",
    "fig.update_xaxes(title_text=\"Time (s)\", row=3, col=1)\n",
    "fig.update_yaxes(title_text=\"Position (pixels)\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Position (pixels)\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"Center X (pixels)\", row=3, col=1)\n",
    "\n",
    "# Update secondary y-axis for difference plot\n",
    "if VideoData1_Has_Sleap and VideoData2_Has_Sleap:\n",
    "    fig.update_yaxes(title_text=\"Normalized Difference (z-score)\", row=3, col=1, secondary_y=True)\n",
    "\n",
    "# Show in browser\n",
    "fig.show(renderer='browser')\n",
    "\n",
    "# Also save as HTML\n",
    "save_path.mkdir(parents=True, exist_ok=True)\n",
    "html_path = save_path / \"Eye_data_QC_time_series.html\"\n",
    "fig.write_html(html_path)\n",
    "print(f\"✅ Interactive time series plot saved to: {html_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as df to csv to be loaded in the photometry/harp/etc. analysis notebook \n",
    "############################################################################################################\n",
    "# reindex to aeon datetime to be done in the other notebook\n",
    " \n",
    "if VideoData1_Has_Sleap:\n",
    "    # Save  DataFrame as CSV to proper path and filename\n",
    "    save_path1 = save_path / \"Video_Sleap_Data1\" / \"Video_Sleap_Data1_1904-01-01T00-00-00.csv\"\n",
    "    save_path1.parent.mkdir(parents=True, exist_ok=True)\n",
    "    #save_path1.parent.mkdir(parents=True, exist_ok=True)\n",
    "    VideoData1.to_csv(save_path1)\n",
    "\n",
    "if VideoData2_Has_Sleap:\n",
    "    # Save  DataFrame as CSV to proper path and filename\n",
    "    save_path2 = save_path / \"Video_Sleap_Data2\" / \"Video_Sleap_Data2_1904-01-01T00-00-00.csv\"\n",
    "    save_path2.parent.mkdir(parents=True, exist_ok=True)\n",
    "    #save_path2.parent.mkdir(parents=True, exist_ok=True)\n",
    "    VideoData2.to_csv(save_path2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "# Saccade detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391f03ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "VideoData1[[\"Seconds\", \"Ellipse.Center.X\"]].head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fefdc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Preprocess:  smooth\n",
    "df = VideoData1[[\"Ellipse.Center.X\", \"Seconds\"]].copy()\n",
    "\n",
    "df['X_smooth'] = (\n",
    "    df['Ellipse.Center.X']\n",
    "      .rolling(window=5, center=True)\n",
    "      .median()\n",
    "      .bfill()\n",
    "      .ffill()\n",
    ")\n",
    "\n",
    "# 2. Compute instantaneous velocity\n",
    "#   dt in seconds\n",
    "df['dt'] = df['Seconds'].diff()\n",
    "#   vel = dX / dt\n",
    "df['vel_x'] = df['X_smooth'].diff() / df['dt']\n",
    "\n",
    "# --- Plot smoothed trace and on separate y axis plot velocity---\n",
    "# Create subplots with shared x-axis for synchronized zooming\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=1,\n",
    "    shared_xaxes=True,  # This ensures x-axis zoom synchronization\n",
    "    vertical_spacing=0.1,\n",
    "    subplot_titles=('X Position (px)', 'Velocity (px/s)')\n",
    ")\n",
    "\n",
    "# Add X_smooth to the first subplot\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=df['Seconds'],\n",
    "        y=df['X_smooth'],\n",
    "        mode='lines',\n",
    "        name='Smoothed X',\n",
    "        line=dict(color='blue')\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Add velocity to the second subplot\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=df['Seconds'],\n",
    "        y=df['vel_x'],\n",
    "        mode='lines',\n",
    "        name='Velocity',\n",
    "        line=dict(color='red')\n",
    "    ),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='Smoothed X and Velocity Traces (Synchronized Zoom)',\n",
    "    height=600,  # Adjust height for two subplots\n",
    "    showlegend=True,\n",
    "    legend=dict(x=0.01, y=0.99)\n",
    ")\n",
    "\n",
    "# Update x-axes\n",
    "fig.update_xaxes(title_text=\"Time (s)\", row=2, col=1)\n",
    "\n",
    "# Update y-axes\n",
    "fig.update_yaxes(title_text=\"X Position (px)\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Velocity (px/s)\", row=2, col=1)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fe6434",
   "metadata": {},
   "source": [
    "# more complex approach "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9356780",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_saccades_refined(df, k=3, pct=0.2, refractory=0.100):\n",
    "    \"\"\"\n",
    "    df must have columns: Seconds, X_smooth, vel_x.\n",
    "    Returns DataFrame with one row per saccade:\n",
    "      start_time, end_time, peak_vel, amplitude, direction\n",
    "    \"\"\"\n",
    "    # 1) compute global thresholds\n",
    "    abs_vel = df['vel_x'].abs().dropna()\n",
    "    vel_thresh = abs_vel.mean() + k * abs_vel.std()\n",
    "    pos_thresh, neg_thresh =  vel_thresh, -vel_thresh\n",
    "\n",
    "    # 2) label rough segments\n",
    "    df = df.copy()\n",
    "    df['is_pos'] = df['vel_x'] > pos_thresh\n",
    "    df['is_neg'] = df['vel_x'] < neg_thresh\n",
    "\n",
    "    df['id_pos'] = (df['is_pos'] & ~df['is_pos'].shift(fill_value=False)).cumsum().where(df['is_pos'], 0)\n",
    "    df['id_neg'] = (df['is_neg'] & ~df['is_neg'].shift(fill_value=False)).cumsum().where(df['is_neg'], 0)\n",
    "\n",
    "    events = []\n",
    "    for direction, seg_id_col, thr in [\n",
    "        ('pos','id_pos', pos_thresh),\n",
    "        ('neg','id_neg', neg_thresh)\n",
    "    ]:\n",
    "        for seg_id, seg in df.groupby(seg_id_col):\n",
    "            if seg_id == 0: \n",
    "                continue\n",
    "\n",
    "            t = seg['Seconds'].values\n",
    "            v = seg['vel_x'].values\n",
    "            # a) raw indices of first crossing\n",
    "            #    (we know seg[0] is already above thr)\n",
    "            #    so look for the first index i where v[i] > thr (or < thr)\n",
    "            if direction=='pos':\n",
    "                idx_cross = np.nonzero(v > thr)[0][0]\n",
    "            else:\n",
    "                idx_cross = np.nonzero(v < thr)[0][0]\n",
    "\n",
    "            # b) interpolate to find sub-sample start crossing\n",
    "            if idx_cross>0:\n",
    "                t0, v0 = t[idx_cross-1], v[idx_cross-1]\n",
    "                t1, v1 = t[idx_cross],   v[idx_cross]\n",
    "                t_start_rough = t0 + (thr - v0)*(t1-t0)/(v1-v0)\n",
    "            else:\n",
    "                # already above threshold at first sample\n",
    "                t_start_rough = t[0]\n",
    "\n",
    "            # c) find the peak (or plateau) within this segment\n",
    "            if direction=='pos':\n",
    "                peak_i = np.argmax(v)\n",
    "            else:\n",
    "                peak_i = np.argmin(v)\n",
    "            v_peak = v[peak_i]\n",
    "\n",
    "            # d) define 20%-of-peak threshold (preserve sign)\n",
    "            thr20 = pct * abs(v_peak) * np.sign(v_peak)\n",
    "\n",
    "            # e) find raw end: first idx after peak where |v| < |thr20|\n",
    "            after_peak = np.where(np.abs(v[peak_i:]) < abs(thr20))[0]\n",
    "            if len(after_peak)==0:\n",
    "                # didn’t return to baseline within segment—skip\n",
    "                continue\n",
    "            idx_end = peak_i + after_peak[0]\n",
    "\n",
    "            # f) interpolate to get sub-sample end time\n",
    "            if idx_end>0:\n",
    "                t0_e, v0_e = t[idx_end-1], v[idx_end-1]\n",
    "                t1_e, v1_e = t[idx_end],   v[idx_end]\n",
    "                t_end = t0_e + (thr20 - v0_e)*(t1_e-t0_e)/(v1_e-v0_e)\n",
    "            else:\n",
    "                t_end = t[0]\n",
    "\n",
    "            # g) walk backwards from peak to find when |v| last < |thr20|\n",
    "            before_peak = np.where(np.abs(v[:peak_i][::-1]) < abs(thr20))[0]\n",
    "            if len(before_peak)==0:\n",
    "                # can’t find a clean baseline before peak → use rough start\n",
    "                t_start = t_start_rough\n",
    "            else:\n",
    "                j = peak_i - before_peak[0]  # this is first index > thr20 going backward\n",
    "                # j-1 is last below-thr20\n",
    "                t0_s, v0_s = t[j-1], v[j-1]\n",
    "                t1_s, v1_s = t[j],   v[j]\n",
    "                t_start = t0_s + (thr20 - v0_s)*(t1_s-t0_s)/(v1_s-v0_s)\n",
    "\n",
    "            # h) compute amplitude using interpolation of X_smooth\n",
    "            X = df[['Seconds','X_smooth']].dropna()\n",
    "            amp = np.interp(t_end,   X['Seconds'], X['X_smooth']) \\\n",
    "                - np.interp(t_start, X['Seconds'], X['X_smooth'])\n",
    "\n",
    "            events.append({\n",
    "                'start_time': t_start,\n",
    "                'end_time':   t_end,\n",
    "                'peak_vel':   v_peak,\n",
    "                'amplitude':  amp,\n",
    "                'direction':  direction\n",
    "            })\n",
    "\n",
    "    # 3) make DataFrame & apply refractory\n",
    "    sacc = pd.DataFrame(events).sort_values('start_time').reset_index(drop=True)\n",
    "\n",
    "    keep = []\n",
    "    last_end = -np.inf\n",
    "    for _, row in sacc.iterrows():\n",
    "        if row['start_time'] - last_end >= refractory:\n",
    "            keep.append(row)\n",
    "            last_end = row['end_time']\n",
    "    return pd.DataFrame(keep)\n",
    "\n",
    "\n",
    "# ——————————————————————————————\n",
    "# USAGE:\n",
    "sacc_refined = detect_saccades_refined(df, k=3, pct=0.4, refractory=0.100)\n",
    "print(sacc_refined)\n",
    "\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# ── 1) Define window and subset ───────────────────────────────\n",
    "t0_rel, t1_rel = 0, 1600\n",
    "t0 = df['Seconds'].min() + t0_rel\n",
    "t1 = df['Seconds'].min() + t1_rel\n",
    "\n",
    "df_win = df[(df['Seconds'] >= t0) & (df['Seconds'] <= t1)]\n",
    "s = sacc_refined[\n",
    "    (sacc_refined['start_time'] >= t0) &\n",
    "    (sacc_refined['end_time']   <= t1)\n",
    "]\n",
    "\n",
    "# ── 2) Interpolate to get X_smooth & vel_x at exact start/end ──\n",
    "x_start = np.interp(s['start_time'], df_win['Seconds'], df_win['X_smooth'])\n",
    "x_end   = np.interp(s['end_time'],   df_win['Seconds'], df_win['X_smooth'])\n",
    "v_start = np.interp(s['start_time'], df_win['Seconds'], df_win['vel_x'])\n",
    "v_end   = np.interp(s['end_time'],   df_win['Seconds'], df_win['vel_x'])\n",
    "\n",
    "# ── 3) Build 2-row subplot (shared x-axis) ────────────────────\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=1, shared_xaxes=True,\n",
    "    vertical_spacing=0.05,\n",
    "    row_heights=[0.6,0.4],\n",
    "    subplot_titles=(\"Position (px)\", \"Velocity (px/s)\")\n",
    ")\n",
    "\n",
    "# Position trace\n",
    "fig.add_trace(go.Scattergl(\n",
    "    x=df_win['Seconds'], y=df_win['X_smooth'],\n",
    "    mode='lines', line=dict(width=1), name='Position'\n",
    "), row=1, col=1)\n",
    "\n",
    "# Position: start markers\n",
    "fig.add_trace(go.Scattergl(\n",
    "    x=s['start_time'], y=x_start,\n",
    "    mode='markers',\n",
    "    marker=dict(symbol='triangle-up', size=8, color='red'),\n",
    "    name='Sac start'\n",
    "), row=1, col=1)\n",
    "\n",
    "# Position: end markers\n",
    "fig.add_trace(go.Scattergl(\n",
    "    x=s['end_time'], y=x_end,\n",
    "    mode='markers',\n",
    "    marker=dict(symbol='triangle-down', size=8, color='blue'),\n",
    "    name='Sac end'\n",
    "), row=1, col=1)\n",
    "\n",
    "\n",
    "# Velocity trace\n",
    "fig.add_trace(go.Scattergl(\n",
    "    x=df_win['Seconds'], y=df_win['vel_x'],\n",
    "    mode='lines', line=dict(width=1), name='Velocity'\n",
    "), row=2, col=1)\n",
    "\n",
    "# Velocity: start markers (hide legend duplicates)\n",
    "fig.add_trace(go.Scattergl(\n",
    "    x=s['start_time'], y=v_start,\n",
    "    mode='markers',\n",
    "    marker=dict(symbol='triangle-up', size=8, color='red'),\n",
    "    showlegend=False\n",
    "), row=2, col=1)\n",
    "\n",
    "# Velocity: end markers\n",
    "fig.add_trace(go.Scattergl(\n",
    "    x=s['end_time'], y=v_end,\n",
    "    mode='markers',\n",
    "    marker=dict(symbol='triangle-down', size=8, color='blue'),\n",
    "    showlegend=False\n",
    "), row=2, col=1)\n",
    "\n",
    "\n",
    "# ── 4) Layout tweaks ────────────────────────────────────────────\n",
    "fig.update_layout(\n",
    "    title=f\"QC: {t0_rel}–{t1_rel} s Window\",\n",
    "    height=500,\n",
    "    legend=dict(orientation='h', yanchor='bottom', y=1.02, xanchor='right', x=1),\n",
    "    margin=dict(l=50, r=50, t=60, b=40),\n",
    ")\n",
    "\n",
    "# shared x-axis formatting\n",
    "fig.update_xaxes(title_text=\"Time (s)\", range=[t0, t1], row=2, col=1)\n",
    "fig.update_xaxes(showticklabels=False, row=1, col=1)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16af88d5",
   "metadata": {},
   "source": [
    "# simple approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18699bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Compute symmetric thresholds\n",
    "abs_vel = df['vel_x'].abs().dropna()\n",
    "k = 3  # times std to use as threshold\n",
    "vel_thresh = abs_vel.mean() + k * abs_vel.std()\n",
    "pos_thresh =  vel_thresh\n",
    "neg_thresh = -vel_thresh\n",
    "print(f\"pos_thresh = {pos_thresh:.1f}, neg_thresh = {neg_thresh:.1f} px/s\")\n",
    "\n",
    "# 2. Masks for each direction\n",
    "df['is_sac_pos'] = df['vel_x'] > pos_thresh\n",
    "df['is_sac_neg'] = df['vel_x'] < neg_thresh\n",
    "\n",
    "# 3. Label contiguous runs\n",
    "df['sac_pos_id'] = (\n",
    "    (df['is_sac_pos'] & ~df['is_sac_pos'].shift(fill_value=False))\n",
    "    .cumsum()\n",
    "    .where(df['is_sac_pos'], 0)\n",
    ")\n",
    "df['sac_neg_id'] = (\n",
    "    (df['is_sac_neg'] & ~df['is_sac_neg'].shift(fill_value=False))\n",
    "    .cumsum()\n",
    "    .where(df['is_sac_neg'], 0)\n",
    ")\n",
    "\n",
    "# 4. Aggregate each into summaries, then concat\n",
    "def summarize(df, col_id, direction):\n",
    "    return (\n",
    "        df[df[col_id] > 0]\n",
    "        .groupby(col_id)\n",
    "        .agg(\n",
    "            start_time=('Seconds', 'first'),\n",
    "            end_time  =('Seconds', 'last'),\n",
    "            peak_vel  =('vel_x', lambda x: x.max() if direction=='pos' else x.min()),\n",
    "            amplitude =('X_smooth', lambda x: (x.max()-x.min()) if direction=='pos' else (x.min()-x.max()))\n",
    "        )\n",
    "        .assign(direction=direction)\n",
    "    )\n",
    "\n",
    "sac_pos = summarize(df, 'sac_pos_id', 'pos')\n",
    "sac_neg = summarize(df, 'sac_neg_id', 'neg')\n",
    "\n",
    "saccades = pd.concat([sac_pos, sac_neg], ignore_index=True)\n",
    "print(f\"Detected {len(saccades)} saccades ({sac_pos.shape[0]} pos, {sac_neg.shape[0]} neg)\")\n",
    "\n",
    "# Add a refractory period of 100 ms to saccade detection\n",
    "refractory_period = 0.100  # seconds (100 ms)\n",
    "\n",
    "# Sort saccades by start_time just in case\n",
    "saccades = saccades.sort_values('start_time').reset_index(drop=True)\n",
    "\n",
    "# Keep only saccades that are at least 100 ms apart\n",
    "filtered_saccades = []\n",
    "last_end_time = -np.inf\n",
    "\n",
    "for idx, row in saccades.iterrows():\n",
    "    if row['start_time'] - last_end_time >= refractory_period:\n",
    "        filtered_saccades.append(row)\n",
    "        last_end_time = row['end_time']\n",
    "\n",
    "filtered_saccades = pd.DataFrame(filtered_saccades)\n",
    "print(f\"After applying 100 ms refractory period: {len(filtered_saccades)} saccades remain\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1350db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# 1) Define window and subset\n",
    "t0_rel, t1_rel = 0, 1800\n",
    "t0 = df['Seconds'].min() + t0_rel\n",
    "t1 = df['Seconds'].min() + t1_rel\n",
    "\n",
    "df_win = df[(df['Seconds'] >= t0) & (df['Seconds'] <= t1)]\n",
    "sacs_win = filtered_saccades[\n",
    "    (filtered_saccades['start_time'] >= t0) &\n",
    "    (filtered_saccades['start_time'] <= t1)\n",
    "]\n",
    "\n",
    "pos = sacs_win[sacs_win['direction']=='pos']\n",
    "neg = sacs_win[sacs_win['direction']=='neg']\n",
    "\n",
    "# Pre‐compute the X_smooth at each saccade start for the position markers\n",
    "x_pos_starts = np.interp(pos['start_time'], df_win['Seconds'], df_win['X_smooth'])\n",
    "x_neg_starts = np.interp(neg['start_time'], df_win['Seconds'], df_win['X_smooth'])\n",
    "\n",
    "# 2) Build 2-row subplot\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=1,\n",
    "    shared_xaxes=True,\n",
    "    vertical_spacing=0.05,\n",
    "    row_heights=[0.6, 0.4],\n",
    "    subplot_titles=(\"Position (px)\", \"Velocity (px/s)\")\n",
    ")\n",
    "\n",
    "# 3) Position trace + saccade markers\n",
    "fig.add_trace(\n",
    "    go.Scattergl(\n",
    "        x=df_win['Seconds'],\n",
    "        y=df_win['X_smooth'],\n",
    "        mode='lines',\n",
    "        line=dict(width=1),\n",
    "        name='Position'\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "# Pos saccade starts on pos plot\n",
    "fig.add_trace(\n",
    "    go.Scattergl(\n",
    "        x=pos['start_time'],\n",
    "        y=x_pos_starts,\n",
    "        mode='markers',\n",
    "        marker=dict(symbol='triangle-up', size=8, color='red'),\n",
    "        showlegend=False\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "# Neg saccade starts on pos plot\n",
    "fig.add_trace(\n",
    "    go.Scattergl(\n",
    "        x=neg['start_time'],\n",
    "        y=x_neg_starts,\n",
    "        mode='markers',\n",
    "        marker=dict(symbol='triangle-down', size=8, color='blue'),\n",
    "        showlegend=False\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# 4) Velocity trace + saccade markers\n",
    "fig.add_trace(\n",
    "    go.Scattergl(\n",
    "        x=df_win['Seconds'],\n",
    "        y=df_win['vel_x'],\n",
    "        mode='lines',\n",
    "        line=dict(width=1),\n",
    "        name='Velocity'\n",
    "    ),\n",
    "    row=2, col=1\n",
    ")\n",
    "# Pos saccade starts on vel plot\n",
    "fig.add_trace(\n",
    "    go.Scattergl(\n",
    "        x=pos['start_time'],\n",
    "        y=[pos_thresh]*len(pos),\n",
    "        mode='markers',\n",
    "        marker=dict(symbol='triangle-up', size=8, color='red'),\n",
    "        name='Pos sacc start'\n",
    "    ),\n",
    "    row=2, col=1\n",
    ")\n",
    "# Neg saccade starts on vel plot\n",
    "fig.add_trace(\n",
    "    go.Scattergl(\n",
    "        x=neg['start_time'],\n",
    "        y=[neg_thresh]*len(neg),\n",
    "        mode='markers',\n",
    "        marker=dict(symbol='triangle-down', size=8, color='blue'),\n",
    "        name='Neg sacc start'\n",
    "    ),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# 5) Layout\n",
    "fig.update_layout(\n",
    "    title=f\"QC: {t0_rel}–{t1_rel} s Window\",\n",
    "    height=500,\n",
    "    legend=dict(orientation='h', yanchor='bottom', y=1.02, xanchor='right', x=1),\n",
    "    margin=dict(l=50, r=50, t=60, b=40),\n",
    ")\n",
    "\n",
    "# 6) X‐axis formatting\n",
    "fig.update_xaxes(title_text=\"Time (s)\", range=[t0, t1], row=2, col=1)\n",
    "fig.update_xaxes(showticklabels=False, row=1, col=1)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a52b50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040b2c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# ══════════════════════════════════════════════════════════════════════════════════\n",
    "# ROBUST SACCADE THRESHOLD DETERMINATION\n",
    "# ══════════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "# 1. ANALYZE VELOCITY DISTRIBUTION\n",
    "vel_abs = np.abs(df['vel_x'].dropna())\n",
    "\n",
    "# Basic statistics\n",
    "vel_mean = np.mean(vel_abs)\n",
    "vel_median = np.median(vel_abs)\n",
    "vel_std = np.std(vel_abs)\n",
    "vel_mad = stats.median_abs_deviation(vel_abs)  # More robust than std\n",
    "\n",
    "print(f\"Velocity Statistics:\")\n",
    "print(f\"Mean absolute velocity: {vel_mean:.2f} px/s\")\n",
    "print(f\"Median absolute velocity: {vel_median:.2f} px/s\")\n",
    "print(f\"Standard deviation: {vel_std:.2f} px/s\")\n",
    "print(f\"Median Absolute Deviation (MAD): {vel_mad:.2f} px/s\")\n",
    "\n",
    "# 2. MULTIPLE THRESHOLD APPROACHES\n",
    "# Approach A: Standard deviation based (classical)\n",
    "threshold_std_3 = vel_mean + 3 * vel_std\n",
    "threshold_std_4 = vel_mean + 4 * vel_std\n",
    "threshold_std_5 = vel_mean + 5 * vel_std\n",
    "\n",
    "# Approach B: MAD-based (more robust to outliers)\n",
    "threshold_mad_3 = vel_median + 3 * vel_mad\n",
    "threshold_mad_4 = vel_median + 4 * vel_mad\n",
    "threshold_mad_5 = vel_median + 5 * vel_mad\n",
    "\n",
    "# Approach C: Percentile-based\n",
    "threshold_p95 = np.percentile(vel_abs, 95)\n",
    "threshold_p97 = np.percentile(vel_abs, 97)\n",
    "threshold_p99 = np.percentile(vel_abs, 99)\n",
    "\n",
    "# Approach D: Otsu's method (automatic threshold selection)\n",
    "def otsu_threshold(data, n_bins=512):\n",
    "    \"\"\"Find optimal threshold using Otsu's method\"\"\"\n",
    "    hist, bin_edges = np.histogram(data, bins=n_bins)\n",
    "    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "    \n",
    "    # Normalize histogram\n",
    "    hist = hist.astype(float) / hist.sum()\n",
    "    \n",
    "    # Calculate cumulative sums\n",
    "    cum_sum = np.cumsum(hist)\n",
    "    cum_mean = np.cumsum(hist * bin_centers)\n",
    "    \n",
    "    # Calculate between-class variance\n",
    "    total_mean = cum_mean[-1]\n",
    "    between_var = np.zeros_like(cum_sum)\n",
    "    \n",
    "    for i in range(len(cum_sum)):\n",
    "        if cum_sum[i] > 0 and cum_sum[i] < 1:\n",
    "            w0 = cum_sum[i]\n",
    "            w1 = 1 - w0\n",
    "            mu0 = cum_mean[i] / w0 if w0 > 0 else 0\n",
    "            mu1 = (total_mean - cum_mean[i]) / w1 if w1 > 0 else 0\n",
    "            between_var[i] = w0 * w1 * (mu0 - mu1) ** 2\n",
    "    \n",
    "    # Find threshold that maximizes between-class variance\n",
    "    optimal_idx = np.argmax(between_var)\n",
    "    return bin_centers[optimal_idx]\n",
    "\n",
    "threshold_otsu = otsu_threshold(vel_abs)\n",
    "\n",
    "# 3. DISPLAY ALL THRESHOLDS\n",
    "print(f\"\\nProposed Thresholds:\")\n",
    "print(f\"Standard deviation based:\")\n",
    "print(f\"  3σ: {threshold_std_3:.2f} px/s\")\n",
    "print(f\"  4σ: {threshold_std_4:.2f} px/s\")\n",
    "print(f\"  5σ: {threshold_std_5:.2f} px/s\")\n",
    "print(f\"MAD-based (robust):\")\n",
    "print(f\"  3×MAD: {threshold_mad_3:.2f} px/s\")\n",
    "print(f\"  4×MAD: {threshold_mad_4:.2f} px/s\")\n",
    "print(f\"  5×MAD: {threshold_mad_5:.2f} px/s\")\n",
    "print(f\"Percentile-based:\")\n",
    "print(f\"  95th percentile: {threshold_p95:.2f} px/s\")\n",
    "print(f\"  97th percentile: {threshold_p97:.2f} px/s\")\n",
    "print(f\"  99th percentile: {threshold_p99:.2f} px/s\")\n",
    "print(f\"Otsu's method: {threshold_otsu:.2f} px/s\")\n",
    "\n",
    "# 4. VISUALIZATION\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=[\n",
    "        'Velocity Distribution (Linear Scale)',\n",
    "        'Velocity Distribution (Log Scale)', \n",
    "        'Cumulative Distribution',\n",
    "        'Time Series with Threshold Candidates'\n",
    "    ],\n",
    "    specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    "           [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
    ")\n",
    "\n",
    "# Plot 1: Histogram (linear)\n",
    "fig.add_trace(\n",
    "    go.Histogram(x=vel_abs, nbinsx=100, name='Velocity', opacity=0.7),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Add threshold lines\n",
    "thresholds = {\n",
    "    '3σ': threshold_std_3,\n",
    "    '4σ': threshold_std_4, \n",
    "    '3×MAD': threshold_mad_3,\n",
    "    '99th %ile': threshold_p99,\n",
    "    'Otsu': threshold_otsu\n",
    "}\n",
    "\n",
    "colors = ['red', 'orange', 'blue', 'green', 'purple']\n",
    "for i, (name, thresh) in enumerate(thresholds.items()):\n",
    "    fig.add_vline(x=thresh, line_dash=\"dash\", line_color=colors[i], \n",
    "                  annotation_text=f\"{name}: {thresh:.1f}\", row=1, col=1)\n",
    "\n",
    "# Plot 2: Histogram (log scale)\n",
    "fig.add_trace(\n",
    "    go.Histogram(x=vel_abs, nbinsx=100, name='Velocity (log)', opacity=0.7),\n",
    "    row=1, col=2\n",
    ")\n",
    "fig.update_yaxes(type=\"log\", row=1, col=2)\n",
    "\n",
    "# Plot 3: Cumulative distribution\n",
    "sorted_vel = np.sort(vel_abs)\n",
    "cumulative = np.arange(1, len(sorted_vel) + 1) / len(sorted_vel)\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=sorted_vel, y=cumulative*100, mode='lines', name='CDF'),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# Plot 4: Time series with threshold candidates\n",
    "time_subset = df['Seconds'].iloc[::10]  # Subsample for visibility\n",
    "vel_subset = np.abs(df['vel_x'].iloc[::10])\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=time_subset, y=vel_subset, mode='lines', \n",
    "               name='|Velocity|', line=dict(color='lightgray')),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "for i, (name, thresh) in enumerate(thresholds.items()):\n",
    "    fig.add_hline(y=thresh, line_dash=\"dash\", line_color=colors[i],\n",
    "                  annotation_text=f\"{name}\", row=2, col=2)\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    height=800,\n",
    "    title_text=\"Saccade Threshold Analysis\",\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title_text=\"Absolute Velocity (px/s)\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"Absolute Velocity (px/s)\", row=1, col=2)\n",
    "fig.update_xaxes(title_text=\"Absolute Velocity (px/s)\", row=2, col=1)\n",
    "fig.update_xaxes(title_text=\"Time (s)\", row=2, col=2)\n",
    "\n",
    "fig.update_yaxes(title_text=\"Count\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Count (log)\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Cumulative %\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"Velocity (px/s)\", row=2, col=2)\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# 5. RECOMMENDATION\n",
    "print(f\"\\n🎯 RECOMMENDATIONS:\")\n",
    "print(f\"1. For CONSERVATIVE detection (fewer false positives): Use 4-5σ or 4-5×MAD\")\n",
    "print(f\"2. For SENSITIVE detection (catch more saccades): Use 3σ or 3×MAD\")\n",
    "print(f\"3. For AUTOMATIC threshold: Use Otsu's method ({threshold_otsu:.1f} px/s)\")\n",
    "print(f\"4. MAD-based thresholds are more robust to outliers than σ-based\")\n",
    "print(f\"5. Consider your data characteristics and validation requirements\")\n",
    "\n",
    "# Suggested threshold (you can modify this logic)\n",
    "if threshold_mad_3 > 20:  # Reasonable minimum for eye movements\n",
    "    suggested_threshold = threshold_mad_3\n",
    "    method_used = \"3×MAD (robust)\"\n",
    "else:\n",
    "    suggested_threshold = threshold_std_3\n",
    "    method_used = \"3σ (classical)\"\n",
    "\n",
    "print(f\"\\n✅ SUGGESTED THRESHOLD: {suggested_threshold:.1f} px/s ({method_used})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80a7f72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ffbfa8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153cdf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add X_smooth (or X_interp) on primary y-axis\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=df['Seconds'],\n",
    "    y=df['X_smooth'],\n",
    "    mode='lines',\n",
    "    name='Smoothed X',\n",
    "    line=dict(color='blue')\n",
    "))\n",
    "\n",
    "# Add velocity on secondary y-axis\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=df['Seconds'],\n",
    "    y=df['vel_x'],\n",
    "    mode='lines',\n",
    "    name='Velocity',\n",
    "    line=dict(color='red'),\n",
    "    yaxis='y2'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Smoothed X and Velocity Traces',\n",
    "    xaxis_title='Time (s)',\n",
    "    yaxis=dict(\n",
    "        title='X Position (px)',\n",
    "        title=dict(font=dict(color='blue')),\n",
    "        tickfont=dict(color='blue')\n",
    "    ),\n",
    "    yaxis2=dict(\n",
    "        title='Velocity (px/s)',\n",
    "        title=dict(font=dict(color='red')),\n",
    "        tickfont=dict(color='red'),\n",
    "        overlaying='y',\n",
    "        side='right'\n",
    "    ),\n",
    "    legend=dict(x=0.01, y=0.99)\n",
    ")\n",
    "\n",
    "fig.show(renderer='browser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f4c795",
   "metadata": {},
   "source": [
    "# PRE_ANR code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect saccades \n",
    "df = VideoData1.copy()\n",
    "\n",
    "df.index = pd.to_datetime(df.index * (1 / framerate), unit='s')\n",
    "\n",
    "# 1) Compute velocity (units/s). Filter/diff/filter. Because sample rate is 1000 Hz, diff is * 1000.\n",
    "window_size = int(round(2 / framerate * 1000))\n",
    "df[\"velocity\"] = df[\"Ellipse.Center.X\"].rolling(window=window_size, center=True, min_periods=1).mean()\n",
    "df[\"velocity\"] = df[\"velocity\"].diff() * 1000\n",
    "window_size = int(round(4 / framerate * 1000))\n",
    "df[\"velocity\"] = df[\"velocity\"].rolling(window=window_size, center=True, min_periods=1).mean()\n",
    "\n",
    "# 2) Define a velocity threshold for saccades (adjust as needed)\n",
    "# implement adaptive filter in the future \n",
    "\n",
    "# 3) Create a boolean mask for samples exceeding the threshold\n",
    "df[\"is_saccade\"] = df[\"velocity\"].abs() > threshold\n",
    "\n",
    "# 4) Group consecutive saccade samples to form saccade events.\n",
    "#    Label each contiguous \"True\" block with a unique ID.\n",
    "df[\"saccade_id\"] = (df[\"is_saccade\"] & ~df[\"is_saccade\"].shift(fill_value=False)).cumsum() * df[\"is_saccade\"]\n",
    "\n",
    "# 5) Extract saccade onset times and basic details for each saccade.\n",
    "saccade_events = []\n",
    "for sacc_id, group in df.groupby(\"saccade_id\"):\n",
    "    if sacc_id == 0:\n",
    "        continue\n",
    "    saccade_time = group.index[0]\n",
    "    peak_time = group[\"velocity\"].abs().idxmax()  # Save the time when the absolute velocity peaks\n",
    "    peak_velocity = group[\"velocity\"].abs().max()\n",
    "    direction = \"positive\" if group[\"velocity\"].mean() > 0 else \"negative\"\n",
    "    \n",
    "    saccade_events.append({\n",
    "        \"saccade_id\": sacc_id,\n",
    "        \"saccade_time\": saccade_time,\n",
    "        \"peak_time\": peak_time,         # New column for peak time\n",
    "        \"peak_velocity\": peak_velocity,\n",
    "        \"direction\": direction\n",
    "    })\n",
    "\n",
    "# 6) Apply a refractory period of 50 ms: if 2 saccade events occur within 50 ms, keep only the first.\n",
    "filtered_saccade_events = []\n",
    "last_event_time = None  # Initialize as None\n",
    "\n",
    "for event in saccade_events:\n",
    "    if last_event_time is None or (event[\"saccade_time\"] - last_event_time) >= refractory_period:\n",
    "        filtered_saccade_events.append(event)\n",
    "        last_event_time = event[\"saccade_time\"]\n",
    "\n",
    "\n",
    "# 7) For each filtered saccade event, calculate the baseline and relative peak.\n",
    "frame_duration = 1 / framerate  # seconds per frame\n",
    "for event in filtered_saccade_events:\n",
    "    saccade_time = event[\"saccade_time\"]\n",
    "    # Baseline: average the data for 3 frames immediately BEFORE the saccade onset.\n",
    "    baseline_start = saccade_time - pd.Timedelta(seconds=3 * frame_duration)\n",
    "    baseline = df.loc[baseline_start:saccade_time, \"Ellipse.Center.X\"].mean()\n",
    "    event[\"baseline\"] = baseline\n",
    "\n",
    "    # Relative peak: in the next 40 ms after the saccade onset, measure the peak change relative to the baseline.\n",
    "    window_end = saccade_time + pd.Timedelta(milliseconds=500)\n",
    "    saccade_window = df.loc[saccade_time:window_end, \"Ellipse.Center.X\"]\n",
    "    if event[\"direction\"] == \"positive\":\n",
    "        relative_peak = saccade_window.max() - baseline\n",
    "    else:\n",
    "        relative_peak = baseline - saccade_window.min()\n",
    "    event[\"relative_peak\"] = relative_peak\n",
    "\n",
    "# Create a DataFrame of the filtered saccade events including the new metrics.\n",
    "results_df = pd.DataFrame(filtered_saccade_events)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot_saccade_detection_QC:\n",
    "    pio.renderers.default = 'browser'\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=df.index, y=df[\"velocity\"], mode=\"lines\", name=\"Velocity\", line=dict(color=\"lightgrey\", width=1)))\n",
    "    fig.add_trace(go.Scatter(x=df.index, y=df[\"Ellipse.Center.X\"], mode=\"lines\", name=\"Ellipse.Center.X\", line=dict(color=\"darkgrey\", width=1), yaxis=\"y2\"))\n",
    "    if not results_df.empty:\n",
    "        pos_df = results_df[results_df[\"direction\"] == \"positive\"]\n",
    "        neg_df = results_df[results_df[\"direction\"] == \"negative\"]\n",
    "        if not pos_df.empty:\n",
    "            pos_starts = pos_df[\"saccade_time\"]\n",
    "            pos_y = df.loc[pos_starts, \"Ellipse.Center.X\"]\n",
    "            fig.add_trace(go.Scatter(x=pos_starts, y=pos_y, mode=\"markers\",\n",
    "                                     marker=dict(symbol=\"circle-open\", size=10, line=dict(width=2, color=\"red\")),\n",
    "                                     name=\"Positive Saccade Onsets\", yaxis=\"y2\"))\n",
    "        if not neg_df.empty:\n",
    "            neg_starts = neg_df[\"saccade_time\"]\n",
    "            neg_y = df.loc[neg_starts, \"Ellipse.Center.X\"]\n",
    "            fig.add_trace(go.Scatter(x=neg_starts, y=neg_y, mode=\"markers\",\n",
    "                                     marker=dict(symbol=\"circle-open\", size=10, line=dict(width=2, color=\"blue\")),\n",
    "                                     name=\"Negative Saccade Onsets\", yaxis=\"y2\"))\n",
    "    fig.update_layout(\n",
    "        title=\"Velocity and Ellipse.Center.X\",\n",
    "        xaxis_title=\"Time\",\n",
    "        yaxis=dict(title=\"Velocity\"),\n",
    "        yaxis2=dict(title=\"Ellipse.Center.X\", overlaying=\"y\", side=\"right\")\n",
    "    )\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################################\n",
    "# INVESTIGATE issue of long stretches of consecutive very low inference predicition scores \n",
    "############################################################################################################\n",
    "\n",
    "\n",
    "score_cutoff = 0.0000001\n",
    "columns_of_interest = ['left.score','center.score','right.score','p1.score','p2.score','p3.score','p4.score','p5.score','p6.score','p7.score','p8.score']\n",
    "total_points = len(VideoData1)\n",
    "\n",
    "\n",
    "for col in columns_of_interest:\n",
    "    count_below_threshold = (VideoData1[col] < score_cutoff).sum()\n",
    "    percentage_below_threshold = (count_below_threshold / total_points) * 100\n",
    "    \n",
    "    # Find the longest consecutive series below threshold\n",
    "    below_threshold = VideoData1[col] < score_cutoff\n",
    "    longest_series = 0\n",
    "    current_series = 0\n",
    "    \n",
    "    for value in below_threshold:\n",
    "        if value:\n",
    "            current_series += 1\n",
    "            if current_series > longest_series:\n",
    "                longest_series = current_series\n",
    "        else:\n",
    "            current_series = 0\n",
    "    \n",
    "    print(f\"Column: {col} | Values below {score_cutoff}: {count_below_threshold} ({percentage_below_threshold:.2f}%) | Longest consecutive frame series: {longest_series}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "pympler_memory_df = utils.get_pympler_memory_usage(top_n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "aeon",
   "language": "python",
   "name": "aeon"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
