{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d5e1da2",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8a50323f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.io as pio\n",
    "import gc\n",
    "import io\n",
    "from contextlib import redirect_stdout\n",
    "\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.colors import LogNorm\n",
    "from fastkde.fastKDE import fastKDE\n",
    "from scipy.stats import linregress\n",
    "from scipy.signal import butter, filtfilt\n",
    "from scipy.signal import correlate\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "from harp_resources import process, utils\n",
    "from sleap import load_and_process as lp\n",
    "from sleap import processing_functions as pf\n",
    "from sleap import saccade_processing as sp\n",
    "from sleap.saccade_processing import analyze_eye_video_saccades\n",
    "from sleap.visualization import plot_all_saccades_overlay, plot_saccade_amplitude_qc\n",
    "from sleap.annotation_gui import launch_annotation_gui\n",
    "from sleap.ml_feature_extraction import extract_experiment_id\n",
    "from sleap.annotation_storage import load_annotations, print_annotation_stats\n",
    "\n",
    "# Reload modules to pick up latest changes (useful after code updates)\n",
    "# Set force_reload_modules = True to always reload, or False to use cached versions\n",
    "force_reload_modules = True  # Set to False for faster execution when modules haven't changed\n",
    "if force_reload_modules:\n",
    "    import importlib\n",
    "    import sleap.load_and_process\n",
    "    import sleap.processing_functions\n",
    "    import sleap.saccade_processing\n",
    "    import sleap.visualization\n",
    "    import sleap.annotation_gui\n",
    "    import sleap.ml_feature_extraction\n",
    "    import sleap.annotation_storage\n",
    "    importlib.reload(sleap.load_and_process)\n",
    "    importlib.reload(sleap.processing_functions)\n",
    "    importlib.reload(sleap.saccade_processing)\n",
    "    importlib.reload(sleap.visualization)\n",
    "    importlib.reload(sleap.annotation_gui)\n",
    "    importlib.reload(sleap.ml_feature_extraction)\n",
    "    importlib.reload(sleap.annotation_storage)\n",
    "    # Re-import aliases after reload\n",
    "    lp = sleap.load_and_process\n",
    "    pf = sleap.processing_functions\n",
    "    sp = sleap.saccade_processing\n",
    "    from sleap.saccade_processing import analyze_eye_video_saccades\n",
    "    from sleap.visualization import plot_all_saccades_overlay, plot_saccade_amplitude_qc\n",
    "    from sleap.annotation_gui import launch_annotation_gui\n",
    "    from sleap.ml_feature_extraction import extract_experiment_id\n",
    "    from sleap.annotation_storage import load_annotations, print_annotation_stats\n",
    "\n",
    "def get_eye_label(key):\n",
    "    \"\"\"Return mapped user-viewable eye label for video key.\"\"\"\n",
    "    return VIDEO_LABELS.get(key, key)\n",
    "\n",
    "NaNs_removed = False # keep as false here, it is to checking if NaNs already removed if the notebook cell is rerun\n",
    "\n",
    "\n",
    "# symbols to use ‚úÖ ‚ÑπÔ∏è ‚ö†Ô∏è ‚ùó"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d376aeb8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ÑπÔ∏è Make sure the SLEAP files follow this convention: VideoData1_1904-01-01T00-00-00.sleap.csv\n",
      "\n",
      "Outputs of SLEAP found in VideoData1: True\n",
      "Outputs of SLEAP found in VideoData2: True\n",
      "\n",
      "üìã LOADING 2 VideoData1 file(s) and 2 VideoData2 file(s)\n",
      "\n",
      "‚ÑπÔ∏è VideoData1 SLEAP file 1 (VideoData1_1904-01-04T23-00-00.sleap.csv): first frame_idx = 0\n",
      "   ‚ö†Ô∏è Found 15 dropped frames within processed range [0-8785] in VideoData1_1904-01-04T23-00-00.sleap.csv. Filling gaps.\n",
      "   ‚ö†Ô∏è Found 48 dropped frames within processed range [0-107201] in VideoData1_1904-01-05T00-00-00.sleap.csv. Filling gaps.\n",
      "‚ÑπÔ∏è VideoData2 SLEAP file 1 (VideoData2_1904-01-04T23-00-00.sleap.csv): first frame_idx = 0\n",
      "   ‚ö†Ô∏è Found 21 dropped frames within processed range [0-8787] in VideoData2_1904-01-04T23-00-00.sleap.csv. Filling gaps.\n",
      "   ‚ö†Ô∏è Found 31 dropped frames within processed range [0-107201] in VideoData2_1904-01-05T00-00-00.sleap.csv. Filling gaps.\n",
      "‚úÖ Loaded 17 manual blinks for VideoData1 from Video1_manual_blinks.csv\n",
      "‚úÖ Loaded 41 manual blinks for VideoData2 from Video2_manual_blinks.csv\n",
      "\n",
      "VideoData1 (L: Left): FPS = 59.56704791768507\n",
      "VideoData2 (R: Right): FPS = 59.56705598182681\n"
     ]
    }
   ],
   "source": [
    "# set up variables and load data \n",
    "############################################################################################################\n",
    "\n",
    "# User-editable friendly labels for plotting and console output:\n",
    "\n",
    "debug = False  # Set to True to enable debug output across all cells (file loading, processing, etc.)\n",
    "plot_saccade_detection_QC = False\n",
    "\n",
    "video1_eye = 'L'  # Options: 'L' or 'R'; which eye does VideoData1 represent? ('L' = Left, 'R' = Right)\n",
    "plot_QC_timeseries = False\n",
    "score_cutoff = 0.2 # for filtering out inferred points with low confidence, they get interpolated \n",
    "outlier_sd_threshold = 10 # for removing outliers from the data, they get interpolated \n",
    "\n",
    "# Pupil diameter filter settings (Butterworth low-pass)\n",
    "pupil_filter_cutoff_hz = 10  # Hz\n",
    "pupil_filter_order = 6\n",
    "\n",
    "# Parameters for blink detection\n",
    "min_blink_duration_ms = 50  # minimum blink duration in milliseconds\n",
    "blink_merge_window_ms = 100  # NOT CURRENTLY USED: merge window was removed to preserve good data between separate blinks\n",
    "long_blink_warning_ms = 2000  # warn if blinks exceed this duration (in ms) - user should verify these are real blinks\n",
    "blink_instance_score_threshold = 3.8  # hard threshold for blink detection - frames with instance.score below this value are considered blinks, calculated as 9 pupil points *0.2 + left/right as 1   \n",
    "\n",
    "# for saccades\n",
    "refractory_period = 0.1  # sec\n",
    "## Separate adaptive saccade threshold (k) for each video:\n",
    "k1 = 4.2  # for VideoData1 (L) - 3-6 works well\n",
    "k2 = 4.5  # for VideoData2 (R) - 3-6 works well \n",
    "\n",
    "# for adaptive saccade threshold - Number of standard deviations (adjustable: 2-4 range works well) \n",
    "onset_offset_fraction = 0.2  # to determine saccade onset and offset, i.e. o.2 is 20% of the peak velocity\n",
    "\n",
    "# Saccade detection parameters (time-based for FPS independence)\n",
    "pre_saccade_window_time = 0.15  # Time (seconds) before threshold crossing to extract\n",
    "post_saccade_window_time = 0.5  # Time (seconds) after threshold crossing to extract\n",
    "baseline_window_start_time = -0.06  # Start time (seconds) relative to threshold crossing for baseline window (e.g., -0.1 = 100ms before)\n",
    "baseline_window_end_time = -0.02  # End time (seconds) relative to threshold crossing for baseline window (e.g., -0.02 = 20ms before)\n",
    "smoothing_window_time = 0.08  # Time (seconds) for position smoothing window (rolling median)\n",
    "peak_width_time = 0.005  # Minimum peak width (seconds) for find_peaks - typically 5-20ms for saccades\n",
    "min_saccade_duration = 0.2  # Minimum saccade segment duration (seconds) - segments shorter than this are excluded (typically truncated at recording edges)\n",
    "\n",
    "# Backward compatibility: old point-based parameters (deprecated, will be converted automatically)\n",
    "n_before = None  # Deprecated: use pre_saccade_window_time instead\n",
    "n_after = None  # Deprecated: use post_saccade_window_time instead\n",
    "baseline_n_points = None  # Deprecated: use baseline_window_start_time and baseline_window_end_time instead\n",
    "baseline_window_time = None  # Deprecated: use baseline_window_start_time and baseline_window_end_time instead\n",
    "saccade_smoothing_window = None  # Deprecated: use smoothing_window_time instead\n",
    "saccade_peak_width = None  # Deprecated: use peak_width_time instead\n",
    "\n",
    "# Parameters for orienting vs compensatory saccade classification\n",
    "classify_orienting_compensatory = True  # Set to True to classify saccades as orienting vs compensatory\n",
    "bout_window = 1.5  # Time window (seconds) for grouping saccades into bouts\n",
    "pre_saccade_window = 0.3  # Time window (seconds) before saccade onset to analyze\n",
    "max_intersaccade_interval_for_classification = 5.0  # Maximum time (seconds) to extend post-saccade window until next saccade for classification\n",
    "pre_saccade_velocity_threshold = 50.0  # Velocity threshold (px/s) for detecting pre-saccade drift\n",
    "pre_saccade_drift_threshold = 10.0  # Position drift threshold (px) before saccade for compensatory classification\n",
    "post_saccade_variance_threshold = 100.0  # Position variance threshold (px¬≤) after saccade for orienting classification\n",
    "post_saccade_position_change_threshold_percent = 50.0  # Position change threshold (% of saccade amplitude) - if post-saccade change > amplitude * this%, classify as compensatory\n",
    "\n",
    "# Adaptive threshold parameters (percentile-based)\n",
    "use_adaptive_thresholds = True  # Set to True to use adaptive thresholds based on feature distributions, False to use fixed thresholds\n",
    "adaptive_percentile_pre_velocity = 75  # Percentile for pre-saccade velocity threshold (upper percentile for compensatory detection)\n",
    "adaptive_percentile_pre_drift = 75  # Percentile for pre-saccade drift threshold (upper percentile for compensatory detection)\n",
    "adaptive_percentile_post_variance = 25  # Percentile for post-saccade variance threshold (lower percentile for orienting detection - low variance = stable)\n",
    "\n",
    "video2_eye = 'R' if video1_eye == 'L' else 'L' # Automatically assign eye for VideoData2\n",
    "eye_fullname = {'L': 'Left', 'R': 'Right'} # Map for full names (used in labels)\n",
    "# Update VIDEO_LABELS based on selection\n",
    "VIDEO_LABELS = {\n",
    "    'VideoData1': f\"VideoData1 ({video1_eye}: {eye_fullname[video1_eye]})\",\n",
    "    'VideoData2': f\"VideoData2 ({video2_eye}: {eye_fullname[video2_eye]})\"\n",
    "}\n",
    "\n",
    "data_path = Path('/Users/rancze/Documents/Data/vestVR/20250409_Cohort3_rotation/Visual_mismatch_day4/B6J2782-2025-04-28T14-22-03') \n",
    "#data_path = Path('/Users/rancze/Documents/Data/vestVR/Cohort1/No_iso_correction/Visual_mismatch_day3/B6J2717-2024-12-10T12-17-03') # only has sleap data 1\n",
    "save_path = data_path.parent / f\"{data_path.name}_processedData\"\n",
    "\n",
    "VideoData1, VideoData2, VideoData1_Has_Sleap, VideoData2_Has_Sleap = lp.load_videography_data(data_path, debug=debug)\n",
    "\n",
    "# Load manual blink data if available\n",
    "manual_blinks_v1 = pf.load_manual_blinks(data_path, video_number=1)\n",
    "manual_blinks_v2 = pf.load_manual_blinks(data_path, video_number=2)\n",
    "\n",
    "columns_of_interest = ['left.x','left.y','center.x','center.y','right.x','right.y','p1.x','p1.y','p2.x','p2.y','p3.x','p3.y','p4.x','p4.y','p5.x','p5.y','p6.x','p6.y','p7.x','p7.y','p8.x','p8.y']\n",
    "\n",
    "if VideoData1_Has_Sleap:\n",
    "    VideoData1 = VideoData1.drop(columns=['track']) # drop the track column as it is empty\n",
    "    coordinates_dict1_raw=lp.get_coordinates_dict(VideoData1, columns_of_interest)\n",
    "    FPS_1 = 1 / VideoData1[\"Seconds\"].diff().mean()  # frame rate for VideoData1 TODO where to save it, is it useful?\n",
    "    print ()\n",
    "    print(f\"{get_eye_label('VideoData1')}: FPS = {FPS_1}\")\n",
    "\n",
    "if VideoData2_Has_Sleap:\n",
    "    VideoData2 = VideoData2.drop(columns=['track']) # drop the track column as it is empty\n",
    "    coordinates_dict2_raw=lp.get_coordinates_dict(VideoData2, columns_of_interest)\n",
    "    FPS_2 = 1 / VideoData2[\"Seconds\"].diff().mean()  # frame rate for VideoData2\n",
    "    print(f\"{get_eye_label('VideoData2')}: FPS = {FPS_2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ef176ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot timeseries of coordinates in browser for both VideoData1 and VideoData2\n",
    "############################################################################################################\n",
    "if plot_QC_timeseries:\n",
    "    print(f'‚ö†Ô∏è Check for long discontinuities and outliers in the data, we will try to deal with them later')\n",
    "    print(f'‚ÑπÔ∏è Figures open in browser window, takes a bit of time.')\n",
    "\n",
    "    # Helper list variables\n",
    "    subplot_titles = (\n",
    "        \"X coordinates for pupil centre and left-right eye corner\",\n",
    "        \"Y coordinates for pupil centre and left-right eye corner\",\n",
    "        \"X coordinates for iris points\",\n",
    "        \"Y coordinates for iris points\"\n",
    "    )\n",
    "    eye_x = ['left.x', 'center.x', 'right.x']\n",
    "    eye_y = ['left.y', 'center.y', 'right.y']\n",
    "    iris_x = ['p1.x', 'p2.x', 'p3.x', 'p4.x', 'p5.x', 'p6.x', 'p7.x', 'p8.x']\n",
    "    iris_y = ['p1.y', 'p2.y', 'p3.y', 'p4.y', 'p5.y', 'p6.y', 'p7.y', 'p8.y']\n",
    "\n",
    "    # --- VideoData1 ---\n",
    "    if VideoData1_Has_Sleap:\n",
    "        fig1 = make_subplots(\n",
    "            rows=4, cols=1,\n",
    "            shared_xaxes=True,\n",
    "            vertical_spacing=0.05,\n",
    "            subplot_titles=subplot_titles\n",
    "        )\n",
    "\n",
    "        # Row 1: left.x, center.x, right.x\n",
    "        for col in eye_x:\n",
    "            fig1.add_trace(go.Scatter(x=VideoData1['Seconds'], y=VideoData1[col], mode='lines', name=col), row=1, col=1)\n",
    "        # Row 2: left.y, center.y, right.y\n",
    "        for col in eye_y:\n",
    "            fig1.add_trace(go.Scatter(x=VideoData1['Seconds'], y=VideoData1[col], mode='lines', name=col), row=2, col=1)\n",
    "        # Row 3: p1.x ... p8.x\n",
    "        for col in iris_x:\n",
    "            fig1.add_trace(go.Scatter(x=VideoData1['Seconds'], y=VideoData1[col], mode='lines', name=col), row=3, col=1)\n",
    "        # Row 4: p1.y ... p8.y\n",
    "        for col in iris_y:\n",
    "            fig1.add_trace(go.Scatter(x=VideoData1['Seconds'], y=VideoData1[col], mode='lines', name=col), row=4, col=1)\n",
    "\n",
    "        fig1.update_layout(\n",
    "            height=1200,\n",
    "            title_text=f\"Time series subplots for coordinates [{get_eye_label('VideoData1')}]\",\n",
    "            showlegend=True\n",
    "        )\n",
    "        fig1.update_xaxes(title_text=\"Seconds\", row=4, col=1)\n",
    "        fig1.update_yaxes(title_text=\"X Position\", row=1, col=1)\n",
    "        fig1.update_yaxes(title_text=\"Y Position\", row=2, col=1)\n",
    "        fig1.update_yaxes(title_text=\"X Position\", row=3, col=1)\n",
    "        fig1.update_yaxes(title_text=\"Y Position\", row=4, col=1)\n",
    "\n",
    "        fig1.show(renderer='browser')\n",
    "\n",
    "    # --- VideoData2 ---\n",
    "    if VideoData2_Has_Sleap:\n",
    "        fig2 = make_subplots(\n",
    "            rows=4, cols=1,\n",
    "            shared_xaxes=True,\n",
    "            vertical_spacing=0.05,\n",
    "            subplot_titles=subplot_titles\n",
    "        )\n",
    "        # Row 1: left.x, center.x, right.x\n",
    "        for col in eye_x:\n",
    "            fig2.add_trace(go.Scatter(x=VideoData2['Seconds'], y=VideoData2[col], mode='lines', name=col), row=1, col=1)\n",
    "        # Row 2: left.y, center.y, right.y\n",
    "        for col in eye_y:\n",
    "            fig2.add_trace(go.Scatter(x=VideoData2['Seconds'], y=VideoData2[col], mode='lines', name=col), row=2, col=1)\n",
    "        # Row 3: p1.x ... p8.x\n",
    "        for col in iris_x:\n",
    "            fig2.add_trace(go.Scatter(x=VideoData2['Seconds'], y=VideoData2[col], mode='lines', name=col), row=3, col=1)\n",
    "        # Row 4: p1.y ... p8.y\n",
    "        for col in iris_y:\n",
    "            fig2.add_trace(go.Scatter(x=VideoData2['Seconds'], y=VideoData2[col], mode='lines', name=col), row=4, col=1)\n",
    "\n",
    "        fig2.update_layout(\n",
    "            height=1200,\n",
    "            title_text=f\"Time series subplots for coordinates [{get_eye_label('VideoData2')}]\",\n",
    "            showlegend=True\n",
    "        )\n",
    "        fig2.update_xaxes(title_text=\"Seconds\", row=4, col=1)\n",
    "        fig2.update_yaxes(title_text=\"X Position\", row=1, col=1)\n",
    "        fig2.update_yaxes(title_text=\"Y Position\", row=2, col=1)\n",
    "        fig2.update_yaxes(title_text=\"X Position\", row=3, col=1)\n",
    "        fig2.update_yaxes(title_text=\"Y Position\", row=4, col=1)\n",
    "\n",
    "        fig2.show(renderer='browser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e5565d1",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# QC plot XY coordinate distributions to visualize outliers \n",
    "############################################################################################################\n",
    "\n",
    "if plot_QC_timeseries:\n",
    "    columns_of_interest = ['left', 'right', 'center', 'p1', 'p2', 'p3', 'p4', 'p5', 'p6', 'p7', 'p8']\n",
    "\n",
    "    # Filter out NaN values and calculate the min and max values for X and Y coordinates for both dict1 and dict2\n",
    "\n",
    "    def min_max_dict(coordinates_dict):\n",
    "        x_min = min([coordinates_dict[f'{col}.x'][~np.isnan(coordinates_dict[f'{col}.x'])].min() for col in columns_of_interest])\n",
    "        x_max = max([coordinates_dict[f'{col}.x'][~np.isnan(coordinates_dict[f'{col}.x'])].max() for col in columns_of_interest])\n",
    "        y_min = min([coordinates_dict[f'{col}.y'][~np.isnan(coordinates_dict[f'{col}.y'])].min() for col in columns_of_interest])\n",
    "        y_max = max([coordinates_dict[f'{col}.y'][~np.isnan(coordinates_dict[f'{col}.y'])].max() for col in columns_of_interest])\n",
    "        return x_min, x_max, y_min, y_max\n",
    "\n",
    "    # Only plot panels for 1 and 2 if VideoData1_Has_Sleap and/or VideoData2_Has_Sleap are true\n",
    "\n",
    "    # Compute min/max as before for global axes limits\n",
    "    if VideoData1_Has_Sleap:\n",
    "        x_min1, x_max1, y_min1, y_max1 = pf.min_max_dict(coordinates_dict1_raw, columns_of_interest)\n",
    "    if VideoData2_Has_Sleap:\n",
    "        x_min2, x_max2, y_min2, y_max2 = pf.min_max_dict(coordinates_dict2_raw, columns_of_interest)\n",
    "\n",
    "    # Use global min and max for consistency only if both VideoData1_Has_Sleap and VideoData2_Has_Sleap are True\n",
    "    if VideoData1_Has_Sleap and VideoData2_Has_Sleap:\n",
    "        x_min = min(x_min1, x_min2)\n",
    "        x_max = max(x_max1, x_max2)\n",
    "        y_min = min(y_min1, y_min2)\n",
    "        y_max = max(y_max1, y_max2)\n",
    "    elif VideoData1_Has_Sleap:\n",
    "        x_min, x_max, y_min, y_max = x_min1, x_max1, y_min1, y_max1\n",
    "    elif VideoData2_Has_Sleap:\n",
    "        x_min, x_max, y_min, y_max = x_min2, x_max2, y_min2, y_max2\n",
    "    else:\n",
    "        raise ValueError(\"Neither VideoData1 nor VideoData2 has Sleap data available.\")\n",
    "\n",
    "    # Create the figure and axes\n",
    "    fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(18, 12))\n",
    "    fig.suptitle(\n",
    "        f\"XY coordinate distribution of different points for {get_eye_label('VideoData1')} and {get_eye_label('VideoData2')} before outlier removal and NaN interpolation\", \n",
    "        fontsize=14\n",
    "    )\n",
    "\n",
    "    # Define colormap for p1-p8\n",
    "    colors = ['blue', 'green', 'red', 'cyan', 'magenta', 'yellow', 'black', 'orange']\n",
    "\n",
    "    # Panel 1: left, right, center (dict1)\n",
    "    if 'VideoData1_Has_Sleap' in globals() and VideoData1_Has_Sleap:\n",
    "        ax[0, 0].set_title(f\"{get_eye_label('VideoData1')}: left, right, center\")\n",
    "        ax[0, 0].scatter(coordinates_dict1_raw['left.x'], coordinates_dict1_raw['left.y'], color='black', label='left', s=10)\n",
    "        ax[0, 0].scatter(coordinates_dict1_raw['right.x'], coordinates_dict1_raw['right.y'], color='grey', label='right', s=10)\n",
    "        ax[0, 0].scatter(coordinates_dict1_raw['center.x'], coordinates_dict1_raw['center.y'], color='red', label='center', s=10)\n",
    "        ax[0, 0].set_xlim([x_min, x_max])\n",
    "        ax[0, 0].set_ylim([y_min, y_max])\n",
    "        ax[0, 0].set_xlabel('x coordinates (pixels)')\n",
    "        ax[0, 0].set_ylabel('y coordinates (pixels)')\n",
    "        ax[0, 0].legend(loc='upper right')\n",
    "    else:\n",
    "        ax[0, 0].axis('off')\n",
    "\n",
    "    # Panel 2: p1 to p8 (dict1)\n",
    "    if 'VideoData1_Has_Sleap' in globals() and VideoData1_Has_Sleap:\n",
    "        ax[0, 1].set_title(f\"{get_eye_label('VideoData1')}: p1 to p8\")\n",
    "        for idx, col in enumerate(columns_of_interest[3:]):\n",
    "            ax[0, 1].scatter(coordinates_dict1_raw[f'{col}.x'], coordinates_dict1_raw[f'{col}.y'], color=colors[idx], label=col, s=5)\n",
    "        ax[0, 1].set_xlim([x_min, x_max])\n",
    "        ax[0, 1].set_ylim([y_min, y_max])\n",
    "        ax[0, 1].set_xlabel('x coordinates (pixels)')\n",
    "        ax[0, 1].set_ylabel('y coordinates (pixels)')\n",
    "        ax[0, 1].legend(loc='upper right')\n",
    "    else:\n",
    "        ax[0, 1].axis('off')\n",
    "\n",
    "    # Panel 3: left, right, center (dict2)\n",
    "    if 'VideoData2_Has_Sleap' in globals() and VideoData2_Has_Sleap:\n",
    "        ax[1, 0].set_title(f\"{get_eye_label('VideoData2')}: left, right, center\")\n",
    "        ax[1, 0].scatter(coordinates_dict2_raw['left.x'], coordinates_dict2_raw['left.y'], color='black', label='left', s=10)\n",
    "        ax[1, 0].scatter(coordinates_dict2_raw['right.x'], coordinates_dict2_raw['right.y'], color='grey', label='right', s=10)\n",
    "        ax[1, 0].scatter(coordinates_dict2_raw['center.x'], coordinates_dict2_raw['center.y'], color='red', label='center', s=10)\n",
    "        ax[1, 0].set_xlim([x_min, x_max])\n",
    "        ax[1, 0].set_ylim([y_min, y_max])\n",
    "        ax[1, 0].set_xlabel('x coordinates (pixels)')\n",
    "        ax[1, 0].set_ylabel('y coordinates (pixels)')\n",
    "        ax[1, 0].legend(loc='upper right')\n",
    "    else:\n",
    "        ax[1, 0].axis('off')\n",
    "\n",
    "    # Panel 4: p1 to p8 (dict2)\n",
    "    if 'VideoData2_Has_Sleap' in globals() and VideoData2_Has_Sleap:\n",
    "        ax[1, 1].set_title(f\"{get_eye_label('VideoData2')}: p1 to p8\")\n",
    "        for idx, col in enumerate(columns_of_interest[3:]):\n",
    "            ax[1, 1].scatter(coordinates_dict2_raw[f'{col}.x'], coordinates_dict2_raw[f'{col}.y'], color=colors[idx], label=col, s=5)\n",
    "        ax[1, 1].set_xlim([x_min, x_max])\n",
    "        ax[1, 1].set_ylim([y_min, y_max])\n",
    "        ax[1, 1].set_xlabel('x coordinates (pixels)')\n",
    "        ax[1, 1].set_ylabel('y coordinates (pixels)')\n",
    "        ax[1, 1].legend(loc='upper right')\n",
    "    else:\n",
    "        ax[1, 1].axis('off')\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.96])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4722b94d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ÑπÔ∏è Debug output suppressed. Set debug=True to see detailed confidence score analysis.\n",
      "\n",
      "=== Centering coordinates to the median pupil centre ===\n",
      "VideoData1 (L: Left) - Centering on median pupil centre: \n",
      "Mean center.x: 304.25169372558594, Mean center.y: 195.6934967041016\n",
      "VideoData2 (R: Right) - Centering on median pupil centre: \n",
      "Mean center.x: 264.44810485839844, Mean center.y: 231.88140869140625\n"
     ]
    }
   ],
   "source": [
    "# Center coordinates, filter low-confidence points, remove outliers, and interpolate\n",
    "############################################################################################################\n",
    "\n",
    "# Detect and print confidence scores analysis (runs before any filtering)\n",
    "#########\n",
    "\n",
    "if not debug:\n",
    "    print(\"‚ÑπÔ∏è Debug output suppressed. Set debug=True to see detailed confidence score analysis.\")\n",
    "\n",
    "score_columns = ['left.score','center.score','right.score','p1.score','p2.score','p3.score','p4.score','p5.score','p6.score','p7.score','p8.score']\n",
    "\n",
    "# VideoData1 confidence score analysis\n",
    "if debug and 'VideoData1_Has_Sleap' in globals() and VideoData1_Has_Sleap:\n",
    "    pf.analyze_confidence_scores(VideoData1, score_columns, score_cutoff, get_eye_label('VideoData1'), debug=debug)\n",
    "\n",
    "# VideoData2 confidence score analysis\n",
    "if debug and 'VideoData2_Has_Sleap' in globals() and VideoData2_Has_Sleap:\n",
    "    pf.analyze_confidence_scores(VideoData2, score_columns, score_cutoff, get_eye_label('VideoData2'), debug=debug)\n",
    "\n",
    "\n",
    "print ()\n",
    "print(\"=== Centering coordinates to the median pupil centre ===\")\n",
    "# Reset columns_of_interest to full coordinate column names (needed after QC plotting redefined it)\n",
    "columns_of_interest = ['left.x','left.y','center.x','center.y','right.x','right.y','p1.x','p1.y','p2.x','p2.y','p3.x','p3.y','p4.x','p4.y','p5.x','p5.y','p6.x','p6.y','p7.x','p7.y','p8.x','p8.y']\n",
    "# VideoData1 processing\n",
    "if 'VideoData1_Has_Sleap' in globals() and VideoData1_Has_Sleap:\n",
    "    VideoData1_centered = pf.center_coordinates_to_median(VideoData1, columns_of_interest, get_eye_label('VideoData1'))\n",
    "\n",
    "# VideoData2 processing\n",
    "if 'VideoData2_Has_Sleap' in globals() and VideoData2_Has_Sleap:\n",
    "    VideoData2_centered = pf.center_coordinates_to_median(VideoData2, columns_of_interest, get_eye_label('VideoData2'))\n",
    "\n",
    "############################################################################################################\n",
    "# remove low confidence points (score < threshold)\n",
    "############################################################################################################\n",
    "if not NaNs_removed:\n",
    "    if debug:\n",
    "        print(\"\\n=== Score-based Filtering - point scores below threshold are replaced by interpolation ===\")\n",
    "        print(f\"Score threshold: {score_cutoff}\")\n",
    "    # List of point names (without .x, .y, .score)\n",
    "    point_names = ['left', 'right', 'center', 'p1', 'p2', 'p3', 'p4', 'p5', 'p6', 'p7', 'p8']\n",
    "\n",
    "    # VideoData1 score-based filtering\n",
    "    if 'VideoData1_Has_Sleap' in globals() and VideoData1_Has_Sleap:\n",
    "        pf.filter_low_confidence_points(VideoData1, point_names, score_cutoff, get_eye_label('VideoData1'), debug=debug)\n",
    "\n",
    "    # VideoData2 score-based filtering\n",
    "    if 'VideoData2_Has_Sleap' in globals() and VideoData2_Has_Sleap:\n",
    "        pf.filter_low_confidence_points(VideoData2, point_names, score_cutoff, get_eye_label('VideoData2'), debug=debug)\n",
    "\n",
    "    ############################################################################################################\n",
    "    # remove outliers (x times SD)\n",
    "    # then interpolates on all NaN values (skipped frames, low confidence inference points, outliers)\n",
    "    ############################################################################################################\n",
    "\n",
    "    if debug:\n",
    "        print(\"\\n=== Outlier Analysis - outlier points are replaced by interpolation ===\")\n",
    "\n",
    "    # Reset columns_of_interest to full coordinate column names (needed after QC plotting redefined it)\n",
    "    columns_of_interest = ['left.x','left.y','center.x','center.y','right.x','right.y','p1.x','p1.y','p2.x','p2.y','p3.x','p3.y','p4.x','p4.y','p5.x','p5.y','p6.x','p6.y','p7.x','p7.y','p8.x','p8.y']\n",
    "\n",
    "    # VideoData1 outlier analysis and interpolation\n",
    "    if 'VideoData1_Has_Sleap' in globals() and VideoData1_Has_Sleap:\n",
    "        outlier_results_v1 = pf.remove_outliers_and_interpolate(\n",
    "            VideoData1, columns_of_interest, outlier_sd_threshold, \n",
    "            get_eye_label('VideoData1'), debug=debug\n",
    "        )\n",
    "        VideoData1 = outlier_results_v1['video_data_interpolated']\n",
    "\n",
    "    # VideoData2 outlier analysis and interpolation\n",
    "    if 'VideoData2_Has_Sleap' in globals() and VideoData2_Has_Sleap:\n",
    "        outlier_results_v2 = pf.remove_outliers_and_interpolate(\n",
    "            VideoData2, columns_of_interest, outlier_sd_threshold, \n",
    "            get_eye_label('VideoData2'), debug=debug\n",
    "        )\n",
    "        VideoData2 = outlier_results_v2['video_data_interpolated']\n",
    "\n",
    "    # Set flag after both VideoData1 and VideoData2 processing is complete\n",
    "    NaNs_removed = True\n",
    "else:\n",
    "    print(\"=== Interpolation already done, skipping ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "719e2885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ÑπÔ∏è Debug output suppressed. Set debug=True to see detailed instance score distribution analysis.\n"
     ]
    }
   ],
   "source": [
    "# Instance.score distribution and hard threshold for blink detection\n",
    "############################################################################################################\n",
    "# Plotting the distribution of instance scores and using hard threshold for blink detection.\n",
    "# When instance score is low, that's typically because of a blink or similar occlusion, as there are long sequences of low scores.\n",
    "# Frames with instance.score below the hard threshold are considered potential blinks.\n",
    "\n",
    "if not debug:\n",
    "    print(\"‚ÑπÔ∏è Debug output suppressed. Set debug=True to see detailed instance score distribution analysis.\")\n",
    "\n",
    "if debug:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"INSTANCE.SCORE DISTRIBUTION AND BLINK DETECTION THRESHOLD\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\nHard threshold: instance.score < {blink_instance_score_threshold}\")\n",
    "    print(f\"  Frames with instance.score below this threshold will be considered potential blinks.\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "# Only analyze for dataset(s) that exist\n",
    "has_v1 = \"VideoData1_Has_Sleap\" in globals() and VideoData1_Has_Sleap\n",
    "has_v2 = \"VideoData2_Has_Sleap\" in globals() and VideoData2_Has_Sleap\n",
    "\n",
    "# Get FPS for time calculations\n",
    "fps_1_for_threshold = None\n",
    "fps_2_for_threshold = None\n",
    "if has_v1:\n",
    "    fps_1_for_threshold = FPS_1 if 'FPS_1' in globals() else (1 / VideoData1[\"Seconds\"].diff().mean() if has_v1 else None)\n",
    "if has_v2:\n",
    "    fps_2_for_threshold = FPS_2 if 'FPS_2' in globals() else (1 / VideoData2[\"Seconds\"].diff().mean() if has_v2 else None)\n",
    "\n",
    "# Plot combined histograms\n",
    "if debug and (has_v1 or has_v2):\n",
    "    pf.plot_instance_score_distributions_combined(\n",
    "        VideoData1 if has_v1 else None,\n",
    "        VideoData2 if has_v2 else None,\n",
    "        blink_instance_score_threshold,\n",
    "        has_v1=has_v1,\n",
    "        has_v2=has_v2\n",
    "    )\n",
    "\n",
    "# Report the statistics for available VideoData\n",
    "# Always show key stats: number/percentile below threshold and longest consecutive segment\n",
    "if has_v1:\n",
    "    pf.analyze_instance_score_distribution(\n",
    "        VideoData1, blink_instance_score_threshold, fps_1_for_threshold,\n",
    "        get_eye_label('VideoData1'), debug=debug, plot=False  # Already plotted above\n",
    "    )\n",
    "\n",
    "if has_v2:\n",
    "    pf.analyze_instance_score_distribution(\n",
    "        VideoData2, blink_instance_score_threshold, fps_2_for_threshold,\n",
    "        get_eye_label('VideoData2'), debug=debug, plot=False  # Already plotted above\n",
    "    )\n",
    "\n",
    "if debug:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"Note: This threshold will be used for blink detection in the next cell.\")\n",
    "    print(\"      Frames with instance.score below this threshold are considered potential blinks.\")\n",
    "    print(\"=\" * 80)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bea68fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ÑπÔ∏è Debug output suppressed. Set debug=True to see detailed blink detection information.\n",
      "VideoData1 (L: Left) - Found 41 blink segments\n",
      "  After filtering <4 frames: 30 blink segment(s), 11 short segment(s) will be interpolated\n",
      "  After merging blinks within 10 frames: 17 blink bout(s)\n",
      "\n",
      "   ‚ö†Ô∏è WARNING: Found 1 blink(s) longer than 2000ms:\n",
      "      Blink 21: frames 80092-80220, duration 2133.2ms - Please verify this is a real blink in the video\n",
      "VideoData2 (R: Right) - Found 53 blink segments\n",
      "  After filtering <4 frames: 43 blink segment(s), 10 short segment(s) will be interpolated\n",
      "  After merging blinks within 10 frames: 31 blink bout(s)\n",
      "\n",
      "‚úÖ Blink detection complete. Blink periods remain as NaN (not interpolated).\n",
      "\n",
      "‚úÖ Blink detection results (VideoData1) saved to: /Users/rancze/Documents/Data/vestVR/20250409_Cohort3_rotation/Visual_mismatch_day4/B6J2782-2025-04-28T14-22-03/blink_detection_VideoData1.csv\n",
      "   Saved 30 blinks\n",
      "\n",
      "‚úÖ Blink detection results (VideoData2) saved to: /Users/rancze/Documents/Data/vestVR/20250409_Cohort3_rotation/Visual_mismatch_day4/B6J2782-2025-04-28T14-22-03/blink_detection_VideoData2.csv\n",
      "   Saved 43 blinks\n",
      "\n",
      "================================================================================\n",
      "üìπ MANUAL QC CHECK:\n",
      "================================================================================\n",
      "For instructions on how to prepare videos for manual blink detection QC,\n",
      "see: https://github.com/ranczlab/vestibular_vr_pipeline/issues/86\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Blink detection output saved to: /Users/rancze/Documents/Data/vestVR/20250409_Cohort3_rotation/Visual_mismatch_day4/B6J2782-2025-04-28T14-22-03/blink_detection_QC.txt\n"
     ]
    }
   ],
   "source": [
    "# Blink detection using instance.score - mark blinks and set coordinates to NaN (keep them as NaN, no interpolation)\n",
    "############################################################################################################\n",
    "\n",
    "if not debug:\n",
    "    print(\"‚ÑπÔ∏è Debug output suppressed. Set debug=True to see detailed blink detection information.\")\n",
    "\n",
    "# Capture all print output to save to file\n",
    "\n",
    "class TeeOutput:\n",
    "    \"\"\"Output to both stdout and a string buffer\"\"\"\n",
    "    def __init__(self, stdout, buffer):\n",
    "        self.stdout = stdout\n",
    "        self.buffer = buffer\n",
    "    \n",
    "    def write(self, s):\n",
    "        self.stdout.write(s)\n",
    "        self.buffer.write(s)\n",
    "    \n",
    "    def flush(self):\n",
    "        self.stdout.flush()\n",
    "        self.buffer.flush()\n",
    "\n",
    "output_buffer = io.StringIO()\n",
    "original_stdout = sys.stdout\n",
    "sys.stdout = TeeOutput(original_stdout, output_buffer)\n",
    "\n",
    "# Run blink detection code with output captured\n",
    "if debug:\n",
    "    print(\"\\n=== Blink Detection ===\")\n",
    "\n",
    "# VideoData1 blink detection\n",
    "if 'VideoData1_Has_Sleap' in globals() and VideoData1_Has_Sleap:\n",
    "    # Get FPS if available, otherwise will be calculated in function\n",
    "    fps_1 = FPS_1 if 'FPS_1' in globals() else None\n",
    "    \n",
    "    # Get manual blinks if available\n",
    "    manual_blinks_for_v1 = manual_blinks_v1 if 'manual_blinks_v1' in globals() and manual_blinks_v1 is not None else None\n",
    "    \n",
    "    # Run blink detection\n",
    "    blink_results_v1 = pf.detect_blinks_for_video(\n",
    "        video_data=VideoData1,\n",
    "        columns_of_interest=columns_of_interest,\n",
    "        blink_instance_score_threshold=blink_instance_score_threshold,\n",
    "        long_blink_warning_ms=long_blink_warning_ms,\n",
    "        min_frames_threshold=4,\n",
    "        merge_window_frames=10,\n",
    "        fps=fps_1,\n",
    "        video_label=get_eye_label('VideoData1'),\n",
    "        manual_blinks=manual_blinks_for_v1,\n",
    "        debug=debug\n",
    "    )\n",
    "    \n",
    "    # Extract results to maintain compatibility with existing variable names\n",
    "    blink_segments_v1 = blink_results_v1['blink_segments']\n",
    "    short_blink_segments_v1 = blink_results_v1['short_blink_segments']\n",
    "    blink_bouts_v1 = blink_results_v1['blink_bouts']\n",
    "    all_blink_segments_v1 = blink_results_v1['all_blink_segments']\n",
    "    fps_1 = blink_results_v1['fps']  # Update fps_1 with calculated value\n",
    "    FPS_1 = fps_1  # Also update global FPS_1 for use elsewhere\n",
    "    long_blinks_warnings_v1 = blink_results_v1['long_blinks_warnings']\n",
    "\n",
    "# VideoData2 blink detection\n",
    "if 'VideoData2_Has_Sleap' in globals() and VideoData2_Has_Sleap:\n",
    "    # Get FPS if available, otherwise will be calculated in function\n",
    "    fps_2 = FPS_2 if 'FPS_2' in globals() else None\n",
    "    \n",
    "    # Get manual blinks if available\n",
    "    manual_blinks_for_v2 = manual_blinks_v2 if 'manual_blinks_v2' in globals() and manual_blinks_v2 is not None else None\n",
    "    \n",
    "    # Run blink detection\n",
    "    blink_results_v2 = pf.detect_blinks_for_video(\n",
    "        video_data=VideoData2,\n",
    "        columns_of_interest=columns_of_interest,\n",
    "        blink_instance_score_threshold=blink_instance_score_threshold,\n",
    "        long_blink_warning_ms=long_blink_warning_ms,\n",
    "        min_frames_threshold=4,\n",
    "        merge_window_frames=10,\n",
    "        fps=fps_2,\n",
    "        video_label=get_eye_label('VideoData2'),\n",
    "        manual_blinks=manual_blinks_for_v2,\n",
    "        debug=debug\n",
    "    )\n",
    "    \n",
    "    # Extract results to maintain compatibility with existing variable names\n",
    "    blink_segments_v2 = blink_results_v2['blink_segments']\n",
    "    short_blink_segments_v2 = blink_results_v2['short_blink_segments']\n",
    "    blink_bouts_v2 = blink_results_v2['blink_bouts']\n",
    "    all_blink_segments_v2 = blink_results_v2['all_blink_segments']\n",
    "    fps_2 = blink_results_v2['fps']  # Update fps_2 with calculated value\n",
    "    FPS_2 = fps_2  # Also update global FPS_2 for use elsewhere\n",
    "    long_blinks_warnings_v2 = blink_results_v2['long_blinks_warnings']\n",
    "\n",
    "print(\"\\n‚úÖ Blink detection complete. Blink periods remain as NaN (not interpolated).\")\n",
    "\n",
    "# Compare blink bout timing between VideoData1 and VideoData2 (between eyes)\n",
    "if ('VideoData1_Has_Sleap' in globals() and VideoData1_Has_Sleap and \n",
    "    'VideoData2_Has_Sleap' in globals() and VideoData2_Has_Sleap):\n",
    "    if debug:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"BLINK BOUT TIMING COMPARISON: VideoData1 vs VideoData2 (Between Eyes)\")\n",
    "        print(\"=\"*80)\n",
    "    \n",
    "    # Get blink bout frame ranges for both videos (if they exist)\n",
    "    # Check if blink_bouts variables exist (they are created during blink detection)\n",
    "    try:\n",
    "        has_bouts_v1 = 'blink_bouts_v1' in globals() and len(blink_bouts_v1) > 0\n",
    "    except:\n",
    "        has_bouts_v1 = False\n",
    "    \n",
    "    try:\n",
    "        has_bouts_v2 = 'blink_bouts_v2' in globals() and len(blink_bouts_v2) > 0\n",
    "    except:\n",
    "        has_bouts_v2 = False\n",
    "    \n",
    "    if has_bouts_v1 and has_bouts_v2:\n",
    "        \n",
    "        # Convert bout indices to frame numbers\n",
    "        bouts_v1 = []\n",
    "        for i, bout in enumerate(blink_bouts_v1, 1):\n",
    "            start_idx = bout['start_idx']\n",
    "            end_idx = bout['end_idx']\n",
    "            if 'frame_idx' in VideoData1.columns:\n",
    "                start_frame = int(VideoData1['frame_idx'].iloc[start_idx])\n",
    "                end_frame = int(VideoData1['frame_idx'].iloc[end_idx])\n",
    "            else:\n",
    "                start_frame = start_idx\n",
    "                end_frame = end_idx\n",
    "            bouts_v1.append({\n",
    "                'num': i,\n",
    "                'start_frame': start_frame,\n",
    "                'end_frame': end_frame,\n",
    "                'start_idx': start_idx,\n",
    "                'end_idx': end_idx,\n",
    "                'length': bout['length']\n",
    "            })\n",
    "        \n",
    "        bouts_v2 = []\n",
    "        for i, bout in enumerate(blink_bouts_v2, 1):\n",
    "            start_idx = bout['start_idx']\n",
    "            end_idx = bout['end_idx']\n",
    "            if 'frame_idx' in VideoData2.columns:\n",
    "                start_frame = int(VideoData2['frame_idx'].iloc[start_idx])\n",
    "                end_frame = int(VideoData2['frame_idx'].iloc[end_idx])\n",
    "            else:\n",
    "                start_frame = start_idx\n",
    "                end_frame = end_idx\n",
    "            bouts_v2.append({\n",
    "                'num': i,\n",
    "                'start_frame': start_frame,\n",
    "                'end_frame': end_frame,\n",
    "                'start_idx': start_idx,\n",
    "                'end_idx': end_idx,\n",
    "                'length': bout['length']\n",
    "            })\n",
    "        \n",
    "        # Find concurrent bouts (overlapping in time, synchronized by Seconds)\n",
    "        concurrent_bouts = []\n",
    "        v1_independent = []\n",
    "        v2_independent = []\n",
    "        \n",
    "        v2_matched = set()  # Track which VideoData2 bouts have been matched\n",
    "        \n",
    "        for bout1 in bouts_v1:\n",
    "            # Get time range for bout1\n",
    "            v1_start_time = VideoData1['Seconds'].iloc[bout1['start_idx']]\n",
    "            v1_end_time = VideoData1['Seconds'].iloc[bout1['end_idx']]\n",
    "            \n",
    "            found_match = False\n",
    "            for bout2 in bouts_v2:\n",
    "                # Get time range for bout2\n",
    "                v2_start_time = VideoData2['Seconds'].iloc[bout2['start_idx']]\n",
    "                v2_end_time = VideoData2['Seconds'].iloc[bout2['end_idx']]\n",
    "                \n",
    "                # Check if bouts overlap in time (any overlapping time period)\n",
    "                overlap_start_time = max(v1_start_time, v2_start_time)\n",
    "                overlap_end_time = min(v1_end_time, v2_end_time)\n",
    "                \n",
    "                if overlap_start_time <= overlap_end_time:\n",
    "                    # Concurrent - they overlap in time\n",
    "                    # Calculate overlap duration\n",
    "                    overlap_duration = overlap_end_time - overlap_start_time\n",
    "                    \n",
    "                    concurrent_bouts.append({\n",
    "                        'v1_num': bout1['num'],\n",
    "                        'v1_start_frame': bout1['start_frame'],\n",
    "                        'v1_end_frame': bout1['end_frame'],\n",
    "                        'v1_start_time': v1_start_time,\n",
    "                        'v1_end_time': v1_end_time,\n",
    "                        'v2_num': bout2['num'],\n",
    "                        'v2_start_frame': bout2['start_frame'],\n",
    "                        'v2_end_frame': bout2['end_frame'],\n",
    "                        'v2_start_time': v2_start_time,\n",
    "                        'v2_end_time': v2_end_time,\n",
    "                        'overlap_start_time': overlap_start_time,\n",
    "                        'overlap_end_time': overlap_end_time,\n",
    "                        'overlap_duration': overlap_duration\n",
    "                    })\n",
    "                    v2_matched.add(bout2['num'])\n",
    "                    found_match = True\n",
    "                    break\n",
    "            \n",
    "            if not found_match:\n",
    "                v1_independent.append(bout1)\n",
    "        \n",
    "        # Find VideoData2 bouts that don't have matches\n",
    "        for bout2 in bouts_v2:\n",
    "            if bout2['num'] not in v2_matched:\n",
    "                v2_independent.append(bout2)\n",
    "        \n",
    "        # Calculate statistics\n",
    "        total_v1_bouts = len(bouts_v1)\n",
    "        total_v2_bouts = len(bouts_v2)\n",
    "        total_concurrent = len(concurrent_bouts)\n",
    "        total_v1_independent = len(v1_independent)\n",
    "        total_v2_independent = len(v2_independent)\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"\\nBlink bout counts:\")\n",
    "            print(f\"  VideoData1: {total_v1_bouts} blink bout(s)\")\n",
    "            print(f\"  VideoData2: {total_v2_bouts} blink bout(s)\")\n",
    "            print(f\"  Concurrent: {total_concurrent} bout(s) (overlapping frames)\")\n",
    "            print(f\"  VideoData1 only: {total_v1_independent} bout(s)\")\n",
    "            print(f\"  VideoData2 only: {total_v2_independent} bout(s)\")\n",
    "            \n",
    "            if total_v1_bouts > 0 and total_v2_bouts > 0:\n",
    "                concurrent_pct_v1 = (total_concurrent / total_v1_bouts) * 100\n",
    "                concurrent_pct_v2 = (total_concurrent / total_v2_bouts) * 100\n",
    "                print(f\"\\nConcurrency percentage:\")\n",
    "                print(f\"  {concurrent_pct_v1:.1f}% of VideoData1 bouts are concurrent with VideoData2\")\n",
    "                print(f\"  {concurrent_pct_v2:.1f}% of VideoData2 bouts are concurrent with VideoData1\")\n",
    "                \n",
    "                # Calculate timing offsets for concurrent bouts\n",
    "                if len(concurrent_bouts) > 0:\n",
    "                    time_offsets_ms = []\n",
    "                    for cb in concurrent_bouts:\n",
    "                        # Calculate offset from start times (already in Seconds)\n",
    "                        offset_ms = (cb['v1_start_time'] - cb['v2_start_time']) * 1000\n",
    "                        time_offsets_ms.append(offset_ms)\n",
    "                        cb['time_offset_ms'] = offset_ms\n",
    "                    \n",
    "                    mean_offset = np.mean(time_offsets_ms)\n",
    "                    std_offset = np.std(time_offsets_ms)\n",
    "                    print(f\"\\nTiming offset for concurrent bouts:\")\n",
    "                    print(f\"  Mean offset (VideoData1 - VideoData2): {mean_offset:.2f} ms\")\n",
    "                    print(f\"  Std offset: {std_offset:.2f} ms\")\n",
    "                    print(f\"  Range: {min(time_offsets_ms):.2f} to {max(time_offsets_ms):.2f} ms\")\n",
    "            \n",
    "            # Visualization removed per request\n",
    "            print(\"=\"*80)\n",
    "    elif has_bouts_v1 or has_bouts_v2:\n",
    "        print(f\"\\n‚ö†Ô∏è Cannot compare blink bouts - only one eye has blink bouts detected:\")\n",
    "        if has_bouts_v1:\n",
    "            print(f\"  VideoData1: {len(blink_bouts_v1)} blink bout(s)\")\n",
    "        else:\n",
    "            print(f\"  VideoData1: 0 blink bout(s)\")\n",
    "        if has_bouts_v2:\n",
    "            print(f\"  VideoData2: {len(blink_bouts_v2)} blink bout(s)\")\n",
    "        else:\n",
    "            print(f\"  VideoData2: 0 blink bout(s)\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è Cannot compare blink bouts - neither video has blink bouts detected\")\n",
    "\n",
    "# Save blink detection results to CSV files\n",
    "if 'VideoData1_Has_Sleap' in globals() and VideoData1_Has_Sleap:\n",
    "    if len(blink_segments_v1) > 0:\n",
    "        # Collect blink information\n",
    "        blink_data_v1 = []\n",
    "        manual_blinks_for_csv = None\n",
    "        if 'manual_blinks_v1' in globals() and manual_blinks_v1 is not None:\n",
    "            manual_blinks_for_csv = manual_blinks_v1\n",
    "            \n",
    "        for i, blink in enumerate(blink_segments_v1, 1):\n",
    "            start_idx = blink['start_idx']\n",
    "            end_idx = blink['end_idx']\n",
    "            \n",
    "            # Get actual frame numbers from frame_idx column\n",
    "            if 'frame_idx' in VideoData1.columns:\n",
    "                first_frame = int(VideoData1['frame_idx'].iloc[start_idx])\n",
    "                last_frame = int(VideoData1['frame_idx'].iloc[end_idx])\n",
    "            else:\n",
    "                first_frame = start_idx\n",
    "                last_frame = end_idx\n",
    "            \n",
    "            # Check if this blink matches a manual one (using function from processing_functions)\n",
    "            matches_manual = pf.check_manual_match(first_frame, last_frame, manual_blinks_for_csv)\n",
    "            \n",
    "            blink_data_v1.append({\n",
    "                'blink_number': i,\n",
    "                'first_frame': first_frame,\n",
    "                'last_frame': last_frame,\n",
    "                'matches_manual': matches_manual\n",
    "            })\n",
    "        \n",
    "        # Create DataFrame and save to CSV\n",
    "        blink_df_v1 = pd.DataFrame(blink_data_v1)\n",
    "        blink_csv_path_v1 = data_path / \"blink_detection_VideoData1.csv\"\n",
    "        blink_df_v1.to_csv(blink_csv_path_v1, index=False)\n",
    "        print(f\"\\n‚úÖ Blink detection results (VideoData1) saved to: {blink_csv_path_v1}\")\n",
    "        print(f\"   Saved {len(blink_data_v1)} blinks\")\n",
    "\n",
    "if 'VideoData2_Has_Sleap' in globals() and VideoData2_Has_Sleap:\n",
    "    if len(blink_segments_v2) > 0:\n",
    "        # Collect blink information\n",
    "        blink_data_v2 = []\n",
    "        manual_blinks_for_csv = None\n",
    "        if 'manual_blinks_v2' in globals() and manual_blinks_v2 is not None:\n",
    "            manual_blinks_for_csv = manual_blinks_v2\n",
    "            \n",
    "        for i, blink in enumerate(blink_segments_v2, 1):\n",
    "            start_idx = blink['start_idx']\n",
    "            end_idx = blink['end_idx']\n",
    "            \n",
    "            # Get actual frame numbers from frame_idx column\n",
    "            if 'frame_idx' in VideoData2.columns:\n",
    "                first_frame = int(VideoData2['frame_idx'].iloc[start_idx])\n",
    "                last_frame = int(VideoData2['frame_idx'].iloc[end_idx])\n",
    "            else:\n",
    "                first_frame = start_idx\n",
    "                last_frame = end_idx\n",
    "            \n",
    "            # Check if this blink matches a manual one (using function from processing_functions)\n",
    "            matches_manual = pf.check_manual_match(first_frame, last_frame, manual_blinks_for_csv)\n",
    "            \n",
    "            blink_data_v2.append({\n",
    "                'blink_number': i,\n",
    "                'first_frame': first_frame,\n",
    "                'last_frame': last_frame,\n",
    "                'matches_manual': matches_manual\n",
    "            })\n",
    "        \n",
    "        # Create DataFrame and save to CSV\n",
    "        blink_df_v2 = pd.DataFrame(blink_data_v2)\n",
    "        blink_csv_path_v2 = data_path / \"blink_detection_VideoData2.csv\"\n",
    "        blink_df_v2.to_csv(blink_csv_path_v2, index=False)\n",
    "        print(f\"\\n‚úÖ Blink detection results (VideoData2) saved to: {blink_csv_path_v2}\")\n",
    "        print(f\"   Saved {len(blink_data_v2)} blinks\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìπ MANUAL QC CHECK:\")\n",
    "print(\"=\"*80)\n",
    "print(\"For instructions on how to prepare videos for manual blink detection QC,\")\n",
    "print(\"see: https://github.com/ranczlab/vestibular_vr_pipeline/issues/86\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Restore original stdout and save captured output to file\n",
    "sys.stdout = original_stdout\n",
    "\n",
    "# Get the captured output\n",
    "captured_output = output_buffer.getvalue()\n",
    "\n",
    "# Save to file in data_path folder\n",
    "output_file = data_path / \"blink_detection_QC.txt\"\n",
    "output_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "with open(output_file, 'w') as f:\n",
    "    f.write(captured_output)\n",
    "\n",
    "print(f\"\\n‚úÖ Blink detection output saved to: {output_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad4361a9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# QC plot timeseries of interpolation corrected NaN and (TODO low confidence coordinates in browser \n",
    "############################################################################################################\n",
    "\n",
    "if plot_QC_timeseries:\n",
    "    print(f'‚ÑπÔ∏è Figure opens in browser window, takes a bit of time.')\n",
    "    \n",
    "    # VideoData1 QC Plot\n",
    "    if 'VideoData1_Has_Sleap' in globals() and VideoData1_Has_Sleap:\n",
    "        fig1 = make_subplots(\n",
    "            rows=4, cols=1,\n",
    "            shared_xaxes=True,\n",
    "            vertical_spacing=0.05,\n",
    "            subplot_titles=(\n",
    "                f\"VideoData1 - X coordinates for pupil centre and left-right eye corner\",\n",
    "                f\"VideoData1 - Y coordinates for pupil centre and left-right eye corner\",\n",
    "                f\"VideoData1 - X coordinates for iris points\",\n",
    "                f\"VideoData1 - Y coordinates for iris points\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Row 1: Plot left.x, center.x, right.x\n",
    "        fig1.add_trace(go.Scatter(x=VideoData1['Seconds'], y=VideoData1['left.x'], mode='lines', name='left.x'), row=1, col=1)\n",
    "        fig1.add_trace(go.Scatter(x=VideoData1['Seconds'], y=VideoData1['center.x'], mode='lines', name='center.x'), row=1, col=1)\n",
    "        fig1.add_trace(go.Scatter(x=VideoData1['Seconds'], y=VideoData1['right.x'], mode='lines', name='right.x'), row=1, col=1)\n",
    "\n",
    "        # Row 2: Plot left.y, center.y, right.y\n",
    "        fig1.add_trace(go.Scatter(x=VideoData1['Seconds'], y=VideoData1['left.y'], mode='lines', name='left.y'), row=2, col=1)\n",
    "        fig1.add_trace(go.Scatter(x=VideoData1['Seconds'], y=VideoData1['center.y'], mode='lines', name='center.y'), row=2, col=1)\n",
    "        fig1.add_trace(go.Scatter(x=VideoData1['Seconds'], y=VideoData1['right.y'], mode='lines', name='right.y'), row=2, col=1)\n",
    "\n",
    "        # Row 3: Plot p.x coordinates for p1 to p8\n",
    "        for col in ['p1.x', 'p2.x', 'p3.x', 'p4.x', 'p5.x', 'p6.x', 'p7.x', 'p8.x']:\n",
    "            fig1.add_trace(go.Scatter(x=VideoData1['Seconds'], y=VideoData1[col], mode='lines', name=col), row=3, col=1)\n",
    "\n",
    "        # Row 4: Plot p.y coordinates for p1 to p8\n",
    "        for col in ['p1.y', 'p2.y', 'p3.y', 'p4.y', 'p5.y', 'p6.y', 'p7.y', 'p8.y']:\n",
    "            fig1.add_trace(go.Scatter(x=VideoData1['Seconds'], y=VideoData1[col], mode='lines', name=col), row=4, col=1)\n",
    "\n",
    "        fig1.update_layout(\n",
    "            height=1200,\n",
    "            title_text=f\"VideoData1 - Time series subplots for coordinates (QC after interpolation)\",\n",
    "            showlegend=True\n",
    "        )\n",
    "        fig1.update_xaxes(title_text=\"Seconds\", row=4, col=1)\n",
    "        fig1.update_yaxes(title_text=\"X Position\", row=1, col=1)\n",
    "        fig1.update_yaxes(title_text=\"Y Position\", row=2, col=1)\n",
    "        fig1.update_yaxes(title_text=\"X Position\", row=3, col=1)\n",
    "        fig1.update_yaxes(title_text=\"Y Position\", row=4, col=1)\n",
    "\n",
    "        fig1.show(renderer='browser')\n",
    "    \n",
    "    # VideoData2 QC Plot\n",
    "    if 'VideoData2_Has_Sleap' in globals() and VideoData2_Has_Sleap:\n",
    "        fig2 = make_subplots(\n",
    "            rows=4, cols=1,\n",
    "            shared_xaxes=True,\n",
    "            vertical_spacing=0.05,\n",
    "            subplot_titles=(\n",
    "                f\"VideoData2 - X coordinates for pupil centre and left-right eye corner\",\n",
    "                f\"VideoData2 - Y coordinates for pupil centre and left-right eye corner\",\n",
    "                f\"VideoData2 - X coordinates for iris points\",\n",
    "                f\"VideoData2 - Y coordinates for iris points\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Row 1: Plot left.x, center.x, right.x\n",
    "        fig2.add_trace(go.Scatter(x=VideoData2['Seconds'], y=VideoData2['left.x'], mode='lines', name='left.x'), row=1, col=1)\n",
    "        fig2.add_trace(go.Scatter(x=VideoData2['Seconds'], y=VideoData2['center.x'], mode='lines', name='center.x'), row=1, col=1)\n",
    "        fig2.add_trace(go.Scatter(x=VideoData2['Seconds'], y=VideoData2['right.x'], mode='lines', name='right.x'), row=1, col=1)\n",
    "\n",
    "        # Row 2: Plot left.y, center.y, right.y\n",
    "        fig2.add_trace(go.Scatter(x=VideoData2['Seconds'], y=VideoData2['left.y'], mode='lines', name='left.y'), row=2, col=1)\n",
    "        fig2.add_trace(go.Scatter(x=VideoData2['Seconds'], y=VideoData2['center.y'], mode='lines', name='center.y'), row=2, col=1)\n",
    "        fig2.add_trace(go.Scatter(x=VideoData2['Seconds'], y=VideoData2['right.y'], mode='lines', name='right.y'), row=2, col=1)\n",
    "\n",
    "        # Row 3: Plot p.x coordinates for p1 to p8\n",
    "        for col in ['p1.x', 'p2.x', 'p3.x', 'p4.x', 'p5.x', 'p6.x', 'p7.x', 'p8.x']:\n",
    "            fig2.add_trace(go.Scatter(x=VideoData2['Seconds'], y=VideoData2[col], mode='lines', name=col), row=3, col=1)\n",
    "\n",
    "        # Row 4: Plot p.y coordinates for p1 to p8\n",
    "        for col in ['p1.y', 'p2.y', 'p3.y', 'p4.y', 'p5.y', 'p6.y', 'p7.y', 'p8.y']:\n",
    "            fig2.add_trace(go.Scatter(x=VideoData2['Seconds'], y=VideoData2[col], mode='lines', name=col), row=4, col=1)\n",
    "\n",
    "        fig2.update_layout(\n",
    "            height=1200,\n",
    "            title_text=f\"VideoData2 - Time series subplots for coordinates (QC after interpolation)\",\n",
    "            showlegend=True\n",
    "        )\n",
    "        fig2.update_xaxes(title_text=\"Seconds\", row=4, col=1)\n",
    "        fig2.update_yaxes(title_text=\"X Position\", row=1, col=1)\n",
    "        fig2.update_yaxes(title_text=\"Y Position\", row=2, col=1)\n",
    "        fig2.update_yaxes(title_text=\"X Position\", row=3, col=1)\n",
    "        fig2.update_yaxes(title_text=\"Y Position\", row=4, col=1)\n",
    "\n",
    "        fig2.show(renderer='browser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac2d21bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QC plot XY coordinate distributions after NaN and ( TODO - low confidence inference points) are interpolated \n",
    "##############################################################################################################\n",
    "\n",
    "if plot_QC_timeseries:\n",
    "    columns_of_interest = ['left.x','left.y','center.x','center.y','right.x','right.y','p1.x','p1.y','p2.x','p2.y','p3.x','p3.y','p4.x','p4.y','p5.x','p5.y','p6.x','p6.y','p7.x','p7.y','p8.x','p8.y']\n",
    "\n",
    "    # Create coordinates_dict for both datasets\n",
    "    if VideoData1_Has_Sleap:\n",
    "        coordinates_dict1_processed = lp.get_coordinates_dict(VideoData1, columns_of_interest)\n",
    "    if VideoData2_Has_Sleap:\n",
    "        coordinates_dict2_processed = lp.get_coordinates_dict(VideoData2, columns_of_interest)\n",
    "\n",
    "    columns_of_interest = ['left', 'right', 'center', 'p1', 'p2', 'p3', 'p4', 'p5', 'p6', 'p7', 'p8']\n",
    "\n",
    "    # Filter out NaN values and calculate the min and max values for X and Y coordinates for both dict1 and dict2\n",
    "    def min_max_dict(coordinates_dict):\n",
    "        x_min = min([coordinates_dict[f'{col}.x'][~np.isnan(coordinates_dict[f'{col}.x'])].min() for col in columns_of_interest])\n",
    "        x_max = max([coordinates_dict[f'{col}.x'][~np.isnan(coordinates_dict[f'{col}.x'])].max() for col in columns_of_interest])\n",
    "        y_min = min([coordinates_dict[f'{col}.y'][~np.isnan(coordinates_dict[f'{col}.y'])].min() for col in columns_of_interest])\n",
    "        y_max = max([coordinates_dict[f'{col}.y'][~np.isnan(coordinates_dict[f'{col}.y'])].max() for col in columns_of_interest])\n",
    "        return x_min, x_max, y_min, y_max\n",
    "\n",
    "    if VideoData1_Has_Sleap:\n",
    "        x_min1, x_max1, y_min1, y_max1 = min_max_dict(coordinates_dict1_processed)\n",
    "    if VideoData2_Has_Sleap:\n",
    "        x_min2, x_max2, y_min2, y_max2 = min_max_dict(coordinates_dict2_processed)\n",
    "\n",
    "    # Use global min and max for consistency across subplots\n",
    "    if VideoData1_Has_Sleap and VideoData2_Has_Sleap:\n",
    "        x_min = min(x_min1, x_min2)\n",
    "        x_max = max(x_max1, x_max2)\n",
    "        y_min = min(y_min1, y_min2)\n",
    "        y_max = max(y_max1, y_max2)\n",
    "    elif VideoData1_Has_Sleap:\n",
    "        x_min, x_max, y_min, y_max = x_min1, x_max1, y_min1, y_max1\n",
    "    elif VideoData2_Has_Sleap:\n",
    "        x_min, x_max, y_min, y_max = x_min2, x_max2, y_min2, y_max2\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(18, 12))\n",
    "    fig.suptitle(\n",
    "        f\"XY coordinate distribution of different points for {get_eye_label('VideoData1')} and {get_eye_label('VideoData2')} post outlier removal and NaN interpolation\",\n",
    "        fontsize=14\n",
    "    )\n",
    "\n",
    "    # Define colormap for p1-p8\n",
    "    colors = ['blue', 'green', 'red', 'cyan', 'magenta', 'yellow', 'black', 'orange']\n",
    "\n",
    "    # Panel 1: left, right, center (VideoData1)\n",
    "    if VideoData1_Has_Sleap:\n",
    "        ax[0, 0].set_title(f\"{get_eye_label('VideoData1')}: left, right, center\")\n",
    "        ax[0, 0].scatter(coordinates_dict1_processed['left.x'], coordinates_dict1_processed['left.y'], color='black', label='left', s=10)\n",
    "        ax[0, 0].scatter(coordinates_dict1_processed['right.x'], coordinates_dict1_processed['right.y'], color='grey', label='right', s=10)\n",
    "        ax[0, 0].scatter(coordinates_dict1_processed['center.x'], coordinates_dict1_processed['center.y'], color='red', label='center', s=10)\n",
    "        ax[0, 0].set_xlim([x_min, x_max])\n",
    "        ax[0, 0].set_ylim([y_min, y_max])\n",
    "        ax[0, 0].set_xlabel('x coordinates (pixels)')\n",
    "        ax[0, 0].set_ylabel('y coordinates (pixels)')\n",
    "        ax[0, 0].legend(loc='upper right')\n",
    "\n",
    "        # Panel 2: p1 to p8 (VideoData1)\n",
    "        ax[0, 1].set_title(f\"{get_eye_label('VideoData1')}: p1 to p8\")\n",
    "        for idx, col in enumerate(columns_of_interest[3:]):\n",
    "            ax[0, 1].scatter(coordinates_dict1_processed[f'{col}.x'], coordinates_dict1_processed[f'{col}.y'], color=colors[idx], label=col, s=5)\n",
    "        ax[0, 1].set_xlim([x_min, x_max])\n",
    "        ax[0, 1].set_ylim([y_min, y_max])\n",
    "        ax[0, 1].set_xlabel('x coordinates (pixels)')\n",
    "        ax[0, 1].set_ylabel('y coordinates (pixels)')\n",
    "        ax[0, 1].legend(loc='upper right')\n",
    "\n",
    "    # Panel 3: left, right, center (VideoData2)\n",
    "    if VideoData2_Has_Sleap:\n",
    "        ax[1, 0].set_title(f\"{get_eye_label('VideoData2')}: left, right, center\")\n",
    "        ax[1, 0].scatter(coordinates_dict2_processed['left.x'], coordinates_dict2_processed['left.y'], color='black', label='left', s=10)\n",
    "        ax[1, 0].scatter(coordinates_dict2_processed['right.x'], coordinates_dict2_processed['right.y'], color='grey', label='right', s=10)\n",
    "        ax[1, 0].scatter(coordinates_dict2_processed['center.x'], coordinates_dict2_processed['center.y'], color='red', label='center', s=10)\n",
    "        ax[1, 0].set_xlim([x_min, x_max])\n",
    "        ax[1, 0].set_ylim([y_min, y_max])\n",
    "        ax[1, 0].set_xlabel('x coordinates (pixels)')\n",
    "        ax[1, 0].set_ylabel('y coordinates (pixels)')\n",
    "        ax[1, 0].legend(loc='upper right')\n",
    "\n",
    "        # Panel 4: p1 to p8 (VideoData2)\n",
    "        ax[1, 1].set_title(f\"{get_eye_label('VideoData2')}: p1 to p8\")\n",
    "        for idx, col in enumerate(columns_of_interest[3:]):\n",
    "            ax[1, 1].scatter(coordinates_dict2_processed[f'{col}.x'], coordinates_dict2_processed[f'{col}.y'], color=colors[idx], label=col, s=5)\n",
    "        ax[1, 1].set_xlim([x_min, x_max])\n",
    "        ax[1, 1].set_ylim([y_min, y_max])\n",
    "        ax[1, 1].set_xlabel('x coordinates (pixels)')\n",
    "        ax[1, 1].set_ylabel('y coordinates (pixels)')\n",
    "        ax[1, 1].legend(loc='upper right')\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.96])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e6f672a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== VideoData1 Ellipse Fitting for Pupil Diameter ===\n",
      "=== VideoData2 Ellipse Fitting for Pupil Diameter ===\n",
      "\n",
      "=== Filtering pupil diameter for VideoData1  ===\n",
      "=== Filtering pupil diameter for VideoData1 ===\n",
      "‚úÖ Done calculating pupil diameter and angle for both VideoData1 and VideoData2\n"
     ]
    }
   ],
   "source": [
    "# fit ellipses on the 8 points to determine pupil centre and diameter\n",
    "############################################################################################################\n",
    "\n",
    "columns_of_interest = ['left.x','left.y','center.x','center.y','right.x','right.y','p1.x','p1.y','p2.x','p2.y','p3.x','p3.y','p4.x','p4.y','p5.x','p5.y','p6.x','p6.y','p7.x','p7.y','p8.x','p8.y']\n",
    "\n",
    "# VideoData1 processing\n",
    "if VideoData1_Has_Sleap:\n",
    "    print(f\"=== VideoData1 Ellipse Fitting for Pupil Diameter ===\")\n",
    "    coordinates_dict1_processed = lp.get_coordinates_dict(VideoData1, columns_of_interest)\n",
    "\n",
    "    theta1 = lp.find_horizontal_axis_angle(VideoData1, 'left', 'center')\n",
    "    center_point1 = lp.get_left_right_center_point(coordinates_dict1_processed)\n",
    "\n",
    "    columns_of_interest_reformatted = ['left', 'right', 'center', 'p1', 'p2', 'p3', 'p4', 'p5', 'p6', 'p7', 'p8']\n",
    "    remformatted_coordinates_dict1 = lp.get_reformatted_coordinates_dict(coordinates_dict1_processed, columns_of_interest_reformatted)\n",
    "    centered_coordinates_dict1 = lp.get_centered_coordinates_dict(remformatted_coordinates_dict1, center_point1)\n",
    "    rotated_coordinates_dict1 = lp.get_rotated_coordinates_dict(centered_coordinates_dict1, theta1)\n",
    "\n",
    "    columns_of_interest_ellipse = ['p1', 'p2', 'p3', 'p4', 'p5', 'p6', 'p7', 'p8']\n",
    "    ellipse_parameters_data1, ellipse_center_points_data1 = lp.get_fitted_ellipse_parameters(rotated_coordinates_dict1, columns_of_interest_ellipse)\n",
    "\n",
    "    average_diameter1 = np.mean([ellipse_parameters_data1[:,0], ellipse_parameters_data1[:,1]], axis=0)\n",
    "\n",
    "    SleapVideoData1 = process.convert_arrays_to_dataframe(['Seconds', 'Ellipse.Diameter', 'Ellipse.Angle', 'Ellipse.Center.X', 'Ellipse.Center.Y'], [VideoData1['Seconds'].values, average_diameter1, ellipse_parameters_data1[:,2], ellipse_center_points_data1[:,0], ellipse_center_points_data1[:,1]])\n",
    "\n",
    "# VideoData2 processing\n",
    "if VideoData2_Has_Sleap:\n",
    "    print(f\"=== VideoData2 Ellipse Fitting for Pupil Diameter ===\")\n",
    "    coordinates_dict2_processed = lp.get_coordinates_dict(VideoData2, columns_of_interest)\n",
    "\n",
    "    theta2 = lp.find_horizontal_axis_angle(VideoData2, 'left', 'center')\n",
    "    center_point2 = lp.get_left_right_center_point(coordinates_dict2_processed)\n",
    "\n",
    "    columns_of_interest_reformatted = ['left', 'right', 'center', 'p1', 'p2', 'p3', 'p4', 'p5', 'p6', 'p7', 'p8']\n",
    "    remformatted_coordinates_dict2 = lp.get_reformatted_coordinates_dict(coordinates_dict2_processed, columns_of_interest_reformatted)\n",
    "    centered_coordinates_dict2 = lp.get_centered_coordinates_dict(remformatted_coordinates_dict2, center_point2)\n",
    "    rotated_coordinates_dict2 = lp.get_rotated_coordinates_dict(centered_coordinates_dict2, theta2)\n",
    "\n",
    "    columns_of_interest_ellipse = ['p1', 'p2', 'p3', 'p4', 'p5', 'p6', 'p7', 'p8']\n",
    "    ellipse_parameters_data2, ellipse_center_points_data2 = lp.get_fitted_ellipse_parameters(rotated_coordinates_dict2, columns_of_interest_ellipse)\n",
    "\n",
    "    average_diameter2 = np.mean([ellipse_parameters_data2[:,0], ellipse_parameters_data2[:,1]], axis=0)\n",
    "\n",
    "    SleapVideoData2 = process.convert_arrays_to_dataframe(['Seconds', 'Ellipse.Diameter', 'Ellipse.Angle', 'Ellipse.Center.X', 'Ellipse.Center.Y'], [VideoData2['Seconds'].values, average_diameter2, ellipse_parameters_data2[:,2], ellipse_center_points_data2[:,0], ellipse_center_points_data2[:,1]])\n",
    "\n",
    "############################################################################################################\n",
    "# Filter pupil diameter using 10 Hz Butterworth low-pass filter\n",
    "############################################################################################################\n",
    "\n",
    "# VideoData1 filtering\n",
    "if VideoData1_Has_Sleap:\n",
    "    print(f\"\\n=== Filtering pupil diameter for VideoData1  ===\")\n",
    "    # Butterworth filter parameters - pupil_filter_cutoff_hz low-pass filter\n",
    "    fs1 = 1 / np.median(np.diff(SleapVideoData1['Seconds']))  # Sampling frequency (Hz)\n",
    "    order = pupil_filter_order\n",
    "\n",
    "    b1, a1 = butter(order, pupil_filter_cutoff_hz / (0.5 * fs1), btype='low')\n",
    "    \n",
    "    # Handle NaN values before filtering (from blink detection)\n",
    "    # Replace NaN with forward-fill for filtering purposes only (to avoid filtfilt issues)\n",
    "    diameter_data = SleapVideoData1['Ellipse.Diameter'].copy()\n",
    "    # Use ffill() and bfill() instead of deprecated fillna(method='ffill')\n",
    "    diameter_data_filled = diameter_data.ffill().bfill()\n",
    "    \n",
    "    # Apply Butterworth filter\n",
    "    if not diameter_data_filled.isna().all():\n",
    "        filtered = filtfilt(b1, a1, diameter_data_filled)\n",
    "        # Restore NaN values at original NaN positions (from blinks)\n",
    "        filtered = pd.Series(filtered, index=diameter_data.index)\n",
    "        filtered[diameter_data.isna()] = np.nan\n",
    "        SleapVideoData1['Ellipse.Diameter.Filt'] = filtered\n",
    "    else:\n",
    "        # If all values are NaN, just copy\n",
    "        SleapVideoData1['Ellipse.Diameter.Filt'] = diameter_data\n",
    "\n",
    "# VideoData2 filtering\n",
    "if VideoData2_Has_Sleap:\n",
    "    print(f\"=== Filtering pupil diameter for VideoData1 ===\")\n",
    "    # Butterworth filter parameters - pupil_filter_cutoff_hz low-pass filter\n",
    "    fs2 = 1 / np.median(np.diff(SleapVideoData2['Seconds']))  # Sampling frequency (Hz)\n",
    "    order = pupil_filter_order\n",
    "\n",
    "    b2, a2 = butter(order, pupil_filter_cutoff_hz / (0.5 * fs2), btype='low')\n",
    "    \n",
    "    # Handle NaN values before filtering (from blink detection)\n",
    "    # Replace NaN with forward-fill for filtering purposes only (to avoid filtfilt issues)\n",
    "    diameter_data = SleapVideoData2['Ellipse.Diameter'].copy()\n",
    "    # Use ffill() and bfill() instead of deprecated fillna(method='ffill')\n",
    "    diameter_data_filled = diameter_data.ffill().bfill()\n",
    "    \n",
    "    # Apply Butterworth filter\n",
    "    if not diameter_data_filled.isna().all():\n",
    "        filtered = filtfilt(b2, a2, diameter_data_filled)\n",
    "        # Restore NaN values at original NaN positions (from blinks)\n",
    "        filtered = pd.Series(filtered, index=diameter_data.index)\n",
    "        filtered[diameter_data.isna()] = np.nan\n",
    "        SleapVideoData2['Ellipse.Diameter.Filt'] = filtered\n",
    "    else:\n",
    "        # If all values are NaN, just copy\n",
    "        SleapVideoData2['Ellipse.Diameter.Filt'] = diameter_data\n",
    "\n",
    "print(\"‚úÖ Done calculating pupil diameter and angle for both VideoData1 and VideoData2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ca86f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Cross-Correlation Analysis ===\n",
      "Applied z-score normalization to pupil diameter signals (accounts for different camera magnifications)\n",
      "  VideoData1: mean=20.90, std=3.60\n",
      "  VideoData2: mean=22.40, std=2.35\n",
      "Peak lag (time): -0.0167 seconds\n",
      "Peak normalized correlation: 0.7541\n",
      "\n",
      "=== Additional Statistics ===\n",
      "Pearson correlation coefficient: 0.75\n",
      "Pearson p-value: < 1e-300 (extremely significant)\n"
     ]
    }
   ],
   "source": [
    "# cross-correlate pupil diameter for left and right eye \n",
    "############################################################################################################\n",
    "\n",
    "if VideoData1_Has_Sleap and VideoData2_Has_Sleap:\n",
    "    # # Create subplots for both comparison and cross-correlation\n",
    "    # fig = make_subplots(\n",
    "    #     rows=2, cols=1,\n",
    "    #     subplot_titles=[\"Pupil Diameter Comparison\", \"Cross-Correlation Analysis\"],\n",
    "    #     vertical_spacing=0.15\n",
    "    # )\n",
    "\n",
    "    # # Add SleapVideoData1 pupil diameter\n",
    "    # fig.add_trace(\n",
    "    #     go.Scatter(\n",
    "    #         x=SleapVideoData1['Seconds'],\n",
    "    #         y=SleapVideoData1['Ellipse.Diameter'],\n",
    "    #         mode='lines',\n",
    "    #         name=f\"VideoData1 Pupil Diameter\",\n",
    "    #         line=dict(color='blue')\n",
    "    #     ),\n",
    "    #     row=1, col=1\n",
    "    # )\n",
    "\n",
    "    # # Add SleapVideoData2 pupil diameter\n",
    "    # fig.add_trace(\n",
    "    #     go.Scatter(\n",
    "    #         x=SleapVideoData2['Seconds'],\n",
    "    #         y=SleapVideoData2['Ellipse.Diameter'],\n",
    "    #         mode='lines',\n",
    "    #         name=f\"VideoData2 Pupil Diameter\",\n",
    "    #         line=dict(color='red')\n",
    "    #     ),\n",
    "    #     row=1, col=1\n",
    "    # )\n",
    "\n",
    "    # Cross-correlation analysis\n",
    "    print(\"=== Cross-Correlation Analysis ===\")\n",
    "\n",
    "    # Get pupil diameter data\n",
    "    # Use filtered diameter data (with NaN restored at blink positions)\n",
    "    pupil1 = SleapVideoData1['Ellipse.Diameter.Filt'].values\n",
    "    pupil2 = SleapVideoData2['Ellipse.Diameter.Filt'].values\n",
    "\n",
    "    # Handle different lengths by using the shorter dataset length\n",
    "    min_length = min(len(pupil1), len(pupil2))\n",
    "\n",
    "    # Truncate both datasets to the same length (preserving time alignment)\n",
    "    pupil1_truncated = pupil1[:min_length]\n",
    "    pupil2_truncated = pupil2[:min_length]\n",
    "\n",
    "    # Remove NaN values for correlation - preserve time alignment by only keeping pairs where BOTH are valid\n",
    "    # This ensures cross-correlation is computed on temporally aligned data\n",
    "    valid_mask1 = ~np.isnan(pupil1_truncated)\n",
    "    valid_mask2 = ~np.isnan(pupil2_truncated)\n",
    "    valid_mask = valid_mask1 & valid_mask2  # Only use indices where both arrays have valid data\n",
    "\n",
    "    # Extract aligned pairs (preserves temporal alignment)\n",
    "    pupil1_clean = pupil1_truncated[valid_mask]\n",
    "    pupil2_clean = pupil2_truncated[valid_mask]\n",
    "\n",
    "    # Check if we have enough data\n",
    "    if len(pupil1_clean) < 2 or len(pupil2_clean) < 2:\n",
    "        print(\"‚ùå Error: Not enough valid data points for correlation analysis\")\n",
    "    else:\n",
    "        # Z-score normalize both signals before cross-correlation\n",
    "        # This accounts for different camera magnifications/orientations by comparing relative changes\n",
    "        # Formula: z = (x - mean) / std\n",
    "        pupil1_mean = np.mean(pupil1_clean)\n",
    "        pupil1_std = np.std(pupil1_clean)\n",
    "        pupil2_mean = np.mean(pupil2_clean)\n",
    "        pupil2_std = np.std(pupil2_clean)\n",
    "        \n",
    "        if pupil1_std > 0 and pupil2_std > 0:\n",
    "            pupil1_z = (pupil1_clean - pupil1_mean) / pupil1_std\n",
    "            pupil2_z = (pupil2_clean - pupil2_mean) / pupil2_std\n",
    "            print(f\"Applied z-score normalization to pupil diameter signals (accounts for different camera magnifications)\")\n",
    "            print(f\"  VideoData1: mean={pupil1_mean:.2f}, std={pupil1_std:.2f}\")\n",
    "            print(f\"  VideoData2: mean={pupil2_mean:.2f}, std={pupil2_std:.2f}\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Warning: Zero variance detected, using raw signals (no normalization)\")\n",
    "            pupil1_z = pupil1_clean\n",
    "            pupil2_z = pupil2_clean\n",
    "        \n",
    "        # Calculate cross-correlation using z-scored signals\n",
    "        try:\n",
    "            correlation = correlate(pupil1_z, pupil2_z, mode='full')\n",
    "            \n",
    "            # Calculate lags (in samples)\n",
    "            lags = np.arange(-len(pupil2_z) + 1, len(pupil1_z))\n",
    "            \n",
    "            # Convert lags to time (assuming same sampling rate)\n",
    "            dt = np.median(np.diff(SleapVideoData1['Seconds']))\n",
    "            lag_times = lags * dt\n",
    "            \n",
    "            # Find peak correlation and corresponding lag\n",
    "            peak_idx = np.argmax(correlation)\n",
    "            peak_correlation = correlation[peak_idx]\n",
    "            peak_lag_samples = lags[peak_idx]\n",
    "            peak_lag_time = lag_times[peak_idx]\n",
    "            peak_lag_time_display = peak_lag_time # for final QC figure \n",
    "            \n",
    "            print(f\"Peak lag (time): {peak_lag_time:.4f} seconds\")\n",
    "\n",
    "        \n",
    "            # Normalize correlation to [-1, 1] range (for z-scored signals, this is standard normalization)\n",
    "            norm_factor = np.sqrt(np.sum(pupil1_z**2) * np.sum(pupil2_z**2))\n",
    "            if norm_factor > 0:\n",
    "                correlation_normalized = correlation / norm_factor\n",
    "                peak_correlation_normalized = correlation_normalized[peak_idx]\n",
    "                print(f\"Peak normalized correlation: {peak_correlation_normalized:.4f}\")\n",
    "            else:\n",
    "                print(\"‚ùå Error: Cannot normalize correlation (zero variance)\")\n",
    "                correlation_normalized = correlation\n",
    "                peak_correlation_normalized = 0\n",
    "            \n",
    "            # # Plot cross-correlation\n",
    "            # fig.add_trace(\n",
    "            #     go.Scatter(\n",
    "            #         x=lag_times,\n",
    "            #         y=correlation_normalized,\n",
    "            #         mode='lines',\n",
    "            #         name=\"Cross-Correlation\",\n",
    "            #         line=dict(color='green')\n",
    "            #     ),\n",
    "            #     row=2, col=1\n",
    "            # )\n",
    "            \n",
    "            # # Add vertical line at peak\n",
    "            # fig.add_vline(\n",
    "            #     x=peak_lag_time,\n",
    "            #     line_dash=\"dash\",\n",
    "            #     line_color=\"red\",\n",
    "            #     annotation_text=f\"Peak: {peak_correlation_normalized:.3f}\",\n",
    "            #     row=2, col=1\n",
    "            # )\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error in cross-correlation calculation: {e}\")\n",
    "            # # Add empty trace to maintain plot structure\n",
    "            # fig.add_trace(\n",
    "            #     go.Scatter(\n",
    "            #         x=[0], y=[0],\n",
    "            #         mode='lines',\n",
    "            #         name=\"Cross-Correlation (Error)\",\n",
    "            #         line=dict(color='gray')\n",
    "            #     ),\n",
    "            #     row=2, col=1\n",
    "            # )\n",
    "\n",
    "    # # Update axes labels\n",
    "    # fig.update_xaxes(title_text=\"Time (seconds)\", row=1, col=1)\n",
    "    # fig.update_yaxes(title_text=\"Pupil Diameter\", row=1, col=1)\n",
    "    # fig.update_xaxes(title_text=\"Lag (seconds)\", row=2, col=1)\n",
    "    # fig.update_yaxes(title_text=\"Normalized Correlation\", row=2, col=1)\n",
    "\n",
    "    # fig.update_layout(\n",
    "    #     height=800,\n",
    "    #     width=1000,\n",
    "    #     title_text=f\"SLEAP Pupil Diameter Analysis: Comparison & Cross-Correlation\"\n",
    "    # )\n",
    "\n",
    "    # fig.show()\n",
    "\n",
    "    # Additional correlation statistics\n",
    "    if len(pupil1_clean) >= 2 and len(pupil2_clean) >= 2:\n",
    "        try:\n",
    "            # Calculate Pearson correlation coefficient on z-scored signals\n",
    "            # Note: For z-scored signals, Pearson correlation is equivalent to the normalized cross-correlation at zero lag\n",
    "            pearson_r, pearson_p = pearsonr(pupil1_z, pupil2_z)\n",
    "            pearson_r_display = pearson_r\n",
    "            pearson_p_display = pearson_p\n",
    "            \n",
    "            print(f\"\\n=== Additional Statistics ===\")\n",
    "            print(f\"Pearson correlation coefficient: {pearson_r:.2f}\")\n",
    "\n",
    "            # Handle extremely small p-values\n",
    "            if pearson_p < 1e-300:\n",
    "                print(f'Pearson p-value: < 1e-300 (extremely significant)')\n",
    "            else:\n",
    "                print(f'Pearson p-value: {pearson_p:.5e}')\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error in additional statistics: {e}\")\n",
    "            pearson_r_display = None\n",
    "            pearson_p_display = None\n",
    "    else:\n",
    "        print(\"‚ùå Cannot calculate additional statistics - insufficient data\")\n",
    "else:\n",
    "    print(\"Only one eye is present, no pupil diameter cross-correlation can be done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f386d901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if Second values match 1:1 between VideoData and SleapVideoData then merge them into VideoData\n",
    "############################################################################################################\n",
    "\n",
    "if VideoData1_Has_Sleap is True:\n",
    "    if VideoData1['Seconds'].equals(SleapVideoData1['Seconds']) is False:\n",
    "        print(f\"‚ùó {get_eye_label('VideoData1')}: The 'Seconds' columns DO NOT correspond 1:1 between the two DataFrames. This should not happen\")\n",
    "    else:\n",
    "        VideoData1 = VideoData1.merge(SleapVideoData1, on='Seconds', how='outer')\n",
    "        del SleapVideoData1\n",
    "\n",
    "if VideoData2_Has_Sleap is True:\n",
    "    if VideoData2['Seconds'].equals(SleapVideoData2['Seconds']) is False:\n",
    "        print(f\"‚ùó {get_eye_label('VideoData2')}: The 'Seconds' columns DO NOT correspond 1:1 between the two DataFrames. This should not happen\")\n",
    "    else:\n",
    "        VideoData2 = VideoData2.merge(SleapVideoData2, on='Seconds', how='outer')\n",
    "        del SleapVideoData2\n",
    "gc.collect()\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b328cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== VideoData1 Analysis ===\n",
      "VideoData1 (L: Left) - R^2 between center point and ellipse center X data: nan\n",
      "VideoData1 (L: Left) - R^2 between center point and ellipse center Y data: nan\n",
      "\n",
      "=== VideoData2 Analysis ===\n",
      "VideoData2 (R: Right) - R^2 between center point and ellipse center X data: nan\n",
      "VideoData2 (R: Right) - R^2 between center point and ellipse center Y data: nan\n",
      "\n",
      "=== Center of Mass Distance Analysis ===\n",
      "\n",
      "VideoData1 (L: Left):\n",
      "  Center of mass for center.x/y: (0.0226, -1.0638)\n",
      "  Center of mass for Ellipse.Center.X/Y: (-1.3348, -12.2054)\n",
      "  Absolute distance in X: 1.3573 pixels\n",
      "  Absolute distance in Y: 11.1416 pixels\n",
      "\n",
      "VideoData2 (R: Right):\n",
      "  Center of mass for center.x/y: (0.4042, 0.1208)\n",
      "  Center of mass for Ellipse.Center.X/Y: (-13.5476, -6.6801)\n",
      "  Absolute distance in X: 13.9518 pixels\n",
      "  Absolute distance in Y: 6.8010 pixels\n",
      "\n",
      "=== Re-centering Ellipse.Center coordinates ===\n",
      "VideoData1 (L: Left) - Re-centered Ellipse.Center using median: (-0.9060, -12.2703)\n",
      "VideoData2 (R: Right) - Re-centered Ellipse.Center using median: (-13.1409, -6.6927)\n"
     ]
    }
   ],
   "source": [
    "# Compare SLEAP center.x and .y with fitted ellipse centre distributions for both VideoData1 and VideoData2\n",
    "############################################################################################################\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1) Compute correlations for VideoData1\n",
    "# ------------------------------------------------------------------\n",
    "if VideoData1_Has_Sleap is True:\n",
    "    print(f\"=== VideoData1 Analysis ===\")\n",
    "    slope_x1, intercept_x1, r_value_x1, p_value_x1, std_err_x1 = linregress(\n",
    "        VideoData1[\"Ellipse.Center.X\"], \n",
    "        VideoData1[\"center.x\"]\n",
    "    )\n",
    "    r_squared_x1 = r_value_x1**2\n",
    "    print(f\"{get_eye_label('VideoData1')} - R^2 between center point and ellipse center X data: {r_squared_x1:.4f}\")\n",
    "\n",
    "    slope_y1, intercept_y1, r_value_y1, p_value_y1, std_err_y1 = linregress(\n",
    "        VideoData1[\"Ellipse.Center.Y\"], \n",
    "        VideoData1[\"center.y\"]\n",
    "    )\n",
    "    r_squared_y1 = r_value_y1**2\n",
    "    print(f\"{get_eye_label('VideoData1')} - R^2 between center point and ellipse center Y data: {r_squared_y1:.4f}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2) Compute correlations for VideoData2\n",
    "# ------------------------------------------------------------------\n",
    "if VideoData2_Has_Sleap is True:\n",
    "    print(f\"\\n=== VideoData2 Analysis ===\")\n",
    "    slope_x2, intercept_x2, r_value_x2, p_value_x2, std_err_x2 = linregress(\n",
    "        VideoData2[\"Ellipse.Center.X\"], \n",
    "        VideoData2[\"center.x\"]\n",
    "    )\n",
    "    r_squared_x2 = r_value_x2**2\n",
    "    print(f\"{get_eye_label('VideoData2')} - R^2 between center point and ellipse center X data: {r_squared_x2:.4f}\")\n",
    "\n",
    "    slope_y2, intercept_y2, r_value_y2, p_value_y2, std_err_y2 = linregress(\n",
    "        VideoData2[\"Ellipse.Center.Y\"], \n",
    "        VideoData2[\"center.y\"]\n",
    "    )\n",
    "    r_squared_y2 = r_value_y2**2\n",
    "    print(f\"{get_eye_label('VideoData2')} - R^2 between center point and ellipse center Y data: {r_squared_y2:.4f}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3) Center of Mass Analysis (if both VideoData1 and VideoData2 are available)\n",
    "# ------------------------------------------------------------------\n",
    "if VideoData1_Has_Sleap is True and VideoData2_Has_Sleap is True:\n",
    "    print(f\"\\n=== Center of Mass Distance Analysis ===\")\n",
    "    \n",
    "    # Calculate center of mass (mean) for VideoData1\n",
    "    com_center_x1 = VideoData1[\"center.x\"].mean()\n",
    "    com_center_y1 = VideoData1[\"center.y\"].mean()\n",
    "    com_ellipse_x1 = VideoData1[\"Ellipse.Center.X\"].mean()\n",
    "    com_ellipse_y1 = VideoData1[\"Ellipse.Center.Y\"].mean()\n",
    "    \n",
    "    # Calculate absolute distances for VideoData1\n",
    "    dist_x1 = abs(com_center_x1 - com_ellipse_x1)\n",
    "    dist_y1 = abs(com_center_y1 - com_ellipse_y1)\n",
    "    \n",
    "    print(f\"\\n{get_eye_label('VideoData1')}:\")\n",
    "    print(f\"  Center of mass for center.x/y: ({com_center_x1:.4f}, {com_center_y1:.4f})\")\n",
    "    print(f\"  Center of mass for Ellipse.Center.X/Y: ({com_ellipse_x1:.4f}, {com_ellipse_y1:.4f})\")\n",
    "    print(f\"  Absolute distance in X: {dist_x1:.4f} pixels\")\n",
    "    print(f\"  Absolute distance in Y: {dist_y1:.4f} pixels\")\n",
    "    \n",
    "    # Calculate center of mass (mean) for VideoData2\n",
    "    com_center_x2 = VideoData2[\"center.x\"].mean()\n",
    "    com_center_y2 = VideoData2[\"center.y\"].mean()\n",
    "    com_ellipse_x2 = VideoData2[\"Ellipse.Center.X\"].mean()\n",
    "    com_ellipse_y2 = VideoData2[\"Ellipse.Center.Y\"].mean()\n",
    "    \n",
    "    # Calculate absolute distances for VideoData2\n",
    "    dist_x2 = abs(com_center_x2 - com_ellipse_x2)\n",
    "    dist_y2 = abs(com_center_y2 - com_ellipse_y2)\n",
    "    \n",
    "    print(f\"\\n{get_eye_label('VideoData2')}:\")\n",
    "    print(f\"  Center of mass for center.x/y: ({com_center_x2:.4f}, {com_center_y2:.4f})\")\n",
    "    print(f\"  Center of mass for Ellipse.Center.X/Y: ({com_ellipse_x2:.4f}, {com_ellipse_y2:.4f})\")\n",
    "    print(f\"  Absolute distance in X: {dist_x2:.4f} pixels\")\n",
    "    print(f\"  Absolute distance in Y: {dist_y2:.4f} pixels\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4) Re-center Ellipse.Center.X and Ellipse.Center.Y using median\n",
    "# ------------------------------------------------------------------\n",
    "print(\"\\n=== Re-centering Ellipse.Center coordinates ===\")\n",
    "\n",
    "# Re-center VideoData1 Ellipse.Center coordinates\n",
    "if VideoData1_Has_Sleap is True:\n",
    "    # Calculate median\n",
    "    median_ellipse_x1 = VideoData1[\"Ellipse.Center.X\"].median()\n",
    "    median_ellipse_y1 = VideoData1[\"Ellipse.Center.Y\"].median()\n",
    "    \n",
    "    # Center the coordinates\n",
    "    VideoData1[\"Ellipse.Center.X\"] = VideoData1[\"Ellipse.Center.X\"] - median_ellipse_x1\n",
    "    VideoData1[\"Ellipse.Center.Y\"] = VideoData1[\"Ellipse.Center.Y\"] - median_ellipse_y1\n",
    "    \n",
    "    print(f\"{get_eye_label('VideoData1')} - Re-centered Ellipse.Center using median: ({median_ellipse_x1:.4f}, {median_ellipse_y1:.4f})\")\n",
    "\n",
    "# Re-center VideoData2 Ellipse.Center coordinates\n",
    "if VideoData2_Has_Sleap is True:\n",
    "    # Calculate median\n",
    "    median_ellipse_x2 = VideoData2[\"Ellipse.Center.X\"].median()\n",
    "    median_ellipse_y2 = VideoData2[\"Ellipse.Center.Y\"].median()\n",
    "    \n",
    "    # Center the coordinates\n",
    "    VideoData2[\"Ellipse.Center.X\"] = VideoData2[\"Ellipse.Center.X\"] - median_ellipse_x2\n",
    "    VideoData2[\"Ellipse.Center.Y\"] = VideoData2[\"Ellipse.Center.Y\"] - median_ellipse_y2\n",
    "    \n",
    "    print(f\"{get_eye_label('VideoData2')} - Re-centered Ellipse.Center using median: ({median_ellipse_x2:.4f}, {median_ellipse_y2:.4f})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "abb18805",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Make and save summary QC plot using matplotlib with scatter plots for 2D distributions\n",
    "\n",
    "# Initialize the statistics variables (these are calculated in Cell 11)\n",
    "try:\n",
    "    pearson_r_display\n",
    "except NameError:\n",
    "    pearson_r_display = None\n",
    "    pearson_p_display = None\n",
    "    peak_lag_time_display = None\n",
    "    print(\"‚ö†Ô∏è Note: Statistics not found. They should be calculated in Cell 11.\")\n",
    "\n",
    "# Visualization and statistics calculation (only if plot_QC_timeseries is True)\n",
    "if plot_QC_timeseries:\n",
    "    # Calculate correlation for Ellipse.Center.X between VideoData1 and VideoData2 (if both exist)\n",
    "    pearson_r_center = None\n",
    "    pearson_p_center = None\n",
    "    peak_lag_time_center = None\n",
    "\n",
    "    if VideoData1_Has_Sleap and VideoData2_Has_Sleap:\n",
    "        # Get the Center.X data\n",
    "        center_x1 = VideoData1['Ellipse.Center.X'].values\n",
    "        center_x2 = VideoData2['Ellipse.Center.X'].values\n",
    "        \n",
    "        min_length = min(len(center_x1), len(center_x2))\n",
    "        center_x1_truncated = center_x1[:min_length]\n",
    "        center_x2_truncated = center_x2[:min_length]\n",
    "        \n",
    "        valid_mask1 = ~np.isnan(center_x1_truncated)\n",
    "        valid_mask2 = ~np.isnan(center_x2_truncated)\n",
    "        valid_mask = valid_mask1 & valid_mask2\n",
    "        \n",
    "        center_x1_clean = center_x1_truncated[valid_mask]\n",
    "        center_x2_clean = center_x2_truncated[valid_mask]\n",
    "        \n",
    "        if len(center_x1_clean) >= 2 and len(center_x2_clean) >= 2:\n",
    "            try:\n",
    "                # Calculate Pearson correlation\n",
    "                pearson_r_center, pearson_p_center = pearsonr(center_x1_clean, center_x2_clean)\n",
    "                \n",
    "                # Calculate cross-correlation for peak lag\n",
    "                correlation = correlate(center_x1_clean, center_x2_clean, mode='full')\n",
    "                lags = np.arange(-len(center_x2_clean) + 1, len(center_x1_clean))\n",
    "                dt = np.median(np.diff(VideoData1['Seconds']))\n",
    "                lag_times = lags * dt\n",
    "                peak_idx = np.argmax(correlation)\n",
    "                peak_lag_time_center = lag_times[peak_idx]\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error calculating Ellipse.Center.X correlation stats: {e}\")\n",
    "\n",
    "    # Create the QC summary figure using matplotlib with custom grid layout\n",
    "    fig = plt.figure(figsize=(20, 18))\n",
    "    fig.suptitle(str(data_path), fontsize=16, y=0.995)\n",
    "\n",
    "    # Create a grid layout:\n",
    "    # - Top row (full width): VideoData1 Time Series\n",
    "    # - Second row (full width): VideoData2 Time Series  \n",
    "    # - Third row (two columns): 2D scatter plots (VideoData1 left, VideoData2 right)\n",
    "    # - Fourth row (two columns): Pupil diameter (left), Ellipse.Center.X correlation (right)\n",
    "\n",
    "    gs = fig.add_gridspec(4, 2, hspace=0.3, wspace=0.3)\n",
    "\n",
    "    # Panel 1: VideoData1 center coordinates - Time Series (full width)\n",
    "    if VideoData1_Has_Sleap:\n",
    "        ax1 = fig.add_subplot(gs[0, :])\n",
    "        ax1.plot(VideoData1_centered['Seconds'], VideoData1_centered['center.x'],\n",
    "                linewidth=0.5, c='blue', alpha=0.6, label='center.x original')\n",
    "        ax1.plot(VideoData1['Seconds'], VideoData1['Ellipse.Center.X'],\n",
    "                linewidth=0.5, c='red', alpha=0.6, label='Ellipse Center.X')\n",
    "        ax1.set_xlabel('Time (s)')\n",
    "        ax1.set_ylabel('Position (pixels)')\n",
    "        ax1.set_title(f\"{get_eye_label('VideoData1')} - center.X Time Series\")\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    # Panel 2: VideoData2 center coordinates - Time Series (full width)\n",
    "    if VideoData2_Has_Sleap:\n",
    "        ax2 = fig.add_subplot(gs[1, :])\n",
    "        ax2.plot(VideoData2_centered['Seconds'], VideoData2_centered['center.x'],\n",
    "                linewidth=0.5, c='blue', alpha=0.6, label='center.x original')\n",
    "        ax2.plot(VideoData2['Seconds'], VideoData2['Ellipse.Center.X'],\n",
    "                linewidth=0.5, c='red', alpha=0.6, label='Ellipse Center.X')\n",
    "        ax2.set_xlabel('Time (s)')\n",
    "        ax2.set_ylabel('Position (pixels)')\n",
    "        ax2.set_title(f\"{get_eye_label('VideoData2')} - center.X Time Series\")\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "\n",
    "    # Panel 3: VideoData1 center coordinates - Scatter plot (left half)\n",
    "    if VideoData1_Has_Sleap:\n",
    "        ax3 = fig.add_subplot(gs[2, 0])\n",
    "        \n",
    "        # Ellipse.Center (blue)\n",
    "        x_ellipse1 = VideoData1['Ellipse.Center.X'].to_numpy()\n",
    "        y_ellipse1 = VideoData1['Ellipse.Center.Y'].to_numpy()\n",
    "        mask1 = ~(np.isnan(x_ellipse1) | np.isnan(y_ellipse1))\n",
    "        \n",
    "        ax3.scatter(x_ellipse1[mask1], y_ellipse1[mask1],\n",
    "                   s=1, alpha=0.3, c='blue', label='Ellipse.Center')\n",
    "        \n",
    "        # Center (red) - from centered data\n",
    "        x_center1 = VideoData1_centered['center.x'].to_numpy()\n",
    "        y_center1 = VideoData1_centered['center.y'].to_numpy()\n",
    "        mask2 = ~(np.isnan(x_center1) | np.isnan(y_center1))\n",
    "        \n",
    "        ax3.scatter(x_center1[mask2], y_center1[mask2],\n",
    "                   s=1, alpha=0.3, c='red', label='center.x original')\n",
    "        \n",
    "        ax3.set_xlabel('Center X (pixels)')\n",
    "        ax3.set_ylabel('Center Y (pixels)')\n",
    "        ax3.set_title(f\"{get_eye_label('VideoData1')} - Center X-Y Distribution (center.X vs Ellipse)\")\n",
    "        ax3.legend()\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add R¬≤ statistics for VideoData1 (bottom left)\n",
    "        try:\n",
    "            if 'r_squared_x1' in globals() and 'r_squared_y1' in globals():\n",
    "                stats_text = f'R¬≤ X: {r_squared_x1:.2g}\\nR¬≤ Y: {r_squared_y1:.2g}'\n",
    "                ax3.text(0.02, 0.02, stats_text, transform=ax3.transAxes,\n",
    "                        verticalalignment='bottom', horizontalalignment='left',\n",
    "                        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8),\n",
    "                        fontsize=9, family='monospace')\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Add center of mass distance for VideoData1 (bottom right)\n",
    "        try:\n",
    "            if 'dist_x1' in globals() and 'dist_y1' in globals():\n",
    "                distance_text = f'COM Dist X: {dist_x1:.3g}\\nCOM Dist Y: {dist_y1:.3g}'\n",
    "                ax3.text(0.98, 0.02, distance_text, transform=ax3.transAxes,\n",
    "                        verticalalignment='bottom', horizontalalignment='right',\n",
    "                        bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8),\n",
    "                        fontsize=9, family='monospace')\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # Panel 4: VideoData2 center coordinates - Scatter plot (right half)\n",
    "    if VideoData2_Has_Sleap:\n",
    "        ax4 = fig.add_subplot(gs[2, 1])\n",
    "        \n",
    "        # Ellipse.Center (blue)\n",
    "        x_ellipse2 = VideoData2['Ellipse.Center.X'].to_numpy()\n",
    "        y_ellipse2 = VideoData2['Ellipse.Center.Y'].to_numpy()\n",
    "        mask3 = ~(np.isnan(x_ellipse2) | np.isnan(y_ellipse2))\n",
    "        \n",
    "        ax4.scatter(x_ellipse2[mask3], y_ellipse2[mask3],\n",
    "                   s=1, alpha=0.3, c='blue', label='Ellipse.Center')\n",
    "        \n",
    "        # Center (red) - from centered data\n",
    "        x_center2 = VideoData2_centered['center.x'].to_numpy()\n",
    "        y_center2 = VideoData2_centered['center.y'].to_numpy()\n",
    "        mask4 = ~(np.isnan(x_center2) | np.isnan(y_center2))\n",
    "        \n",
    "        ax4.scatter(x_center2[mask4], y_center2[mask4],\n",
    "                   s=1, alpha=0.3, c='red', label='center.X Center')\n",
    "        \n",
    "        ax4.set_xlabel('Center X (pixels)')\n",
    "        ax4.set_ylabel('Center Y (pixels)')\n",
    "        ax4.set_title(f\"{get_eye_label('VideoData2')} - Center X-Y Distribution (center.X vs Ellipse)\")\n",
    "        ax4.legend()\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add R¬≤ statistics for VideoData2 (bottom left)\n",
    "        try:\n",
    "            if 'r_squared_x2' in globals() and 'r_squared_y2' in globals():\n",
    "                stats_text = f'R¬≤ X: {r_squared_x2:.2g}\\nR¬≤ Y: {r_squared_y2:.2g}'\n",
    "                ax4.text(0.02, 0.02, stats_text, transform=ax4.transAxes,\n",
    "                        verticalalignment='bottom', horizontalalignment='left',\n",
    "                        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8),\n",
    "                        fontsize=9, family='monospace')\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Add center of mass distance for VideoData2 (bottom right)\n",
    "        try:\n",
    "            if 'dist_x2' in globals() and 'dist_y2' in globals():\n",
    "                distance_text = f'COM Dist X: {dist_x2:.3g}\\nCOM Dist Y: {dist_y2:.3g}'\n",
    "                ax4.text(0.98, 0.02, distance_text, transform=ax4.transAxes,\n",
    "                        verticalalignment='bottom', horizontalalignment='right',\n",
    "                        bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8),\n",
    "                        fontsize=9, family='monospace')\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # Panel 5: Pupil diameter comparison (bottom left)\n",
    "    ax5 = fig.add_subplot(gs[3, 0])\n",
    "    if VideoData1_Has_Sleap and VideoData2_Has_Sleap:\n",
    "        ax5.plot(VideoData1['Seconds'], VideoData1['Ellipse.Diameter.Filt'],\n",
    "                linewidth=0.5, c='#FF7F00', alpha=0.6, label='VideoData1 Diameter')\n",
    "        ax5.plot(VideoData2['Seconds'], VideoData2['Ellipse.Diameter.Filt'],\n",
    "                linewidth=0.5, c='#9370DB', alpha=0.6, label='VideoData2 Diameter')\n",
    "    elif VideoData1_Has_Sleap:\n",
    "        ax5.plot(VideoData1['Seconds'], VideoData1['Ellipse.Diameter.Filt'],\n",
    "                linewidth=0.5, c='#FF7F00', alpha=0.6, label='VideoData1 Diameter')\n",
    "    elif VideoData2_Has_Sleap:\n",
    "        ax5.plot(VideoData2['Seconds'], VideoData2['Ellipse.Diameter.Filt'],\n",
    "                linewidth=0.5, c='#9370DB', alpha=0.6, label='VideoData2 Diameter')\n",
    "\n",
    "    ax5.set_xlabel('Time (s)')\n",
    "    ax5.set_ylabel('Diameter (pixels)')\n",
    "    ax5.set_title('Pupil Diameter Comparison')\n",
    "    ax5.legend()\n",
    "    ax5.grid(True, alpha=0.3)\n",
    "\n",
    "    # Add statistics text to Panel 5\n",
    "    if pearson_r_display is not None and pearson_p_display is not None and peak_lag_time_display is not None:\n",
    "        stats_text = (f'Pearson r = {pearson_r_display:.4f}\\n'\n",
    "                      f'Pearson p = {pearson_p_display:.4e}\\n'\n",
    "                      f'Peak lag = {peak_lag_time_display:.4f} s')\n",
    "        ax5.text(0.98, 0.98, stats_text, transform=ax5.transAxes,\n",
    "                verticalalignment='top', horizontalalignment='right',\n",
    "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8),\n",
    "                fontsize=10, family='monospace')\n",
    "    else:\n",
    "        ax5.text(0.5, 0.5, 'Statistics not available\\n(See Cell 11 for correlation analysis)', \n",
    "                transform=ax5.transAxes, ha='center', va='center', fontsize=10)\n",
    "\n",
    "    # Panel 6: Ellipse.Center.X comparison (bottom right) with dual y-axis\n",
    "    ax6 = fig.add_subplot(gs[3, 1])\n",
    "    ax6_twin = ax6.twinx()  # Create a second y-axis\n",
    "\n",
    "    if VideoData1_Has_Sleap and VideoData2_Has_Sleap:\n",
    "        # Plot the individual traces\n",
    "        ax6.plot(VideoData1['Seconds'], VideoData1['Ellipse.Center.X'],\n",
    "                linewidth=0.5, c='#FF7F00', alpha=0.6, label='VideoData1 Ellipse.Center.X')\n",
    "        ax6.plot(VideoData2['Seconds'], VideoData2['Ellipse.Center.X'],\n",
    "                linewidth=0.5, c='#9370DB', alpha=0.6, label='VideoData2 Ellipse.Center.X')\n",
    "        \n",
    "        # Plot the difference on the right axis\n",
    "        # Align the data to the same length and normalize for fair comparison\n",
    "        min_length = min(len(VideoData1), len(VideoData2))\n",
    "        \n",
    "        # Normalize data (z-score) to account for different scales\n",
    "        center_x1_aligned = VideoData1['Ellipse.Center.X'].iloc[:min_length]\n",
    "        center_x2_aligned = VideoData2['Ellipse.Center.X'].iloc[:min_length]\n",
    "        \n",
    "        # Calculate mean and std for normalization\n",
    "        mean1 = center_x1_aligned.mean()\n",
    "        std1 = center_x1_aligned.std()\n",
    "        mean2 = center_x2_aligned.mean()\n",
    "        std2 = center_x2_aligned.std()\n",
    "        \n",
    "        # Normalize both datasets\n",
    "        center_x1_norm = (center_x1_aligned - mean1) / std1\n",
    "        center_x2_norm = (center_x2_aligned - mean2) / std2\n",
    "        \n",
    "        # Calculate difference of normalized data\n",
    "        center_x_diff = center_x1_norm - center_x2_norm\n",
    "        seconds_aligned = VideoData1['Seconds'].iloc[:min_length]\n",
    "        ax6_twin.plot(seconds_aligned, center_x_diff,\n",
    "                      linewidth=0.5, c='green', alpha=0.6, label='Difference (normalized)')\n",
    "        \n",
    "    elif VideoData1_Has_Sleap:\n",
    "        ax6.plot(VideoData1['Seconds'], VideoData1['Ellipse.Center.X'],\n",
    "                linewidth=0.5, c='#FF7F00', alpha=0.6, label='VideoData1 Ellipse.Center.X')\n",
    "    elif VideoData2_Has_Sleap:\n",
    "        ax6.plot(VideoData2['Seconds'], VideoData2['Ellipse.Center.X'],\n",
    "                linewidth=0.5, c='#9370DB', alpha=0.6, label='VideoData2 Ellipse.Center.X')\n",
    "\n",
    "    ax6.set_xlabel('Time (s)')\n",
    "    ax6.set_ylabel('Center X (pixels)', color='black')\n",
    "    ax6.set_title('Ellipse.Center.X Comparison')\n",
    "    ax6.tick_params(axis='y', labelcolor='black')\n",
    "    if VideoData1_Has_Sleap and VideoData2_Has_Sleap:\n",
    "        ax6_twin.set_ylabel('Normalized Difference (z-score)', color='green')\n",
    "        ax6_twin.tick_params(axis='y', labelcolor='green')\n",
    "\n",
    "    # Combine legends from both axes\n",
    "    lines1, labels1 = ax6.get_legend_handles_labels()\n",
    "    if VideoData1_Has_Sleap and VideoData2_Has_Sleap:\n",
    "        lines2, labels2 = ax6_twin.get_legend_handles_labels()\n",
    "        ax6.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "    else:\n",
    "        ax6.legend(loc='upper left')\n",
    "\n",
    "    ax6.grid(True, alpha=0.3)\n",
    "\n",
    "    # Add statistics text to Panel 6\n",
    "    if pearson_r_center is not None and pearson_p_center is not None and peak_lag_time_center is not None:\n",
    "        stats_text = (f'Pearson r = {pearson_r_center:.4f}\\n'\n",
    "                      f'Pearson p = {pearson_p_center:.4e}\\n'\n",
    "                      f'Peak lag = {peak_lag_time_center:.4f} s')\n",
    "        ax6.text(0.98, 0.98, stats_text, transform=ax6.transAxes,\n",
    "                verticalalignment='top', horizontalalignment='right',\n",
    "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8),\n",
    "                fontsize=10, family='monospace')\n",
    "    else:\n",
    "        ax6.text(0.5, 0.5, 'Statistics not available\\n(both eyes required)', \n",
    "                transform=ax6.transAxes, ha='center', va='center', fontsize=10)\n",
    "\n",
    "    # Save as PDF (editable vector format)\n",
    "    save_path.mkdir(parents=True, exist_ok=True)\n",
    "    pdf_path = save_path / \"Eye_data_QC.pdf\"\n",
    "    plt.savefig(pdf_path, dpi=300, bbox_inches='tight', format='pdf')\n",
    "    print(f\"‚úÖ QC figure saved as PDF (editable): {pdf_path}\")\n",
    "\n",
    "    # Also save as 600 dpi PNG (high-resolution for printing)\n",
    "    png_path = save_path / \"Eye_data_QC.png\"\n",
    "    plt.savefig(png_path, dpi=600, bbox_inches='tight', format='png')\n",
    "    print(f\"‚úÖ QC figure saved as PNG (600 dpi for printing): {png_path}\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e8bd88af",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Create interactive time series plots using plotly for browser viewing\n",
    "if plot_QC_timeseries:\n",
    "    # Create subplots for the time series (3 rows now instead of 2)\n",
    "    # Need to enable secondary_y for the third panel\n",
    "    fig = make_subplots(\n",
    "        rows=3, cols=1,\n",
    "        shared_xaxes=True,\n",
    "        vertical_spacing=0.08,\n",
    "        subplot_titles=(\n",
    "            f\"{get_eye_label('VideoData1')} - center.X Time Series\",\n",
    "            f\"{get_eye_label('VideoData2')} - center.X Time Series\",\n",
    "            \"Ellipse.Center.X Comparison with Difference\"\n",
    "        ),\n",
    "        specs=[[{}], [{}], [{\"secondary_y\": True}]]  # Enable secondary_y for row 3\n",
    "    )\n",
    "\n",
    "    # Panel 1: VideoData1 center coordinates - Time Series\n",
    "    if VideoData1_Has_Sleap:\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=VideoData1_centered['Seconds'],\n",
    "            y=VideoData1_centered['center.x'],\n",
    "            mode='lines',\n",
    "            name='center.x original',\n",
    "            line=dict(color='blue', width=0.5),\n",
    "            opacity=0.6\n",
    "        ), row=1, col=1)\n",
    "        \n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=VideoData1['Seconds'],\n",
    "            y=VideoData1['Ellipse.Center.X'],\n",
    "            mode='lines',\n",
    "            name='Ellipse Center.X',\n",
    "            line=dict(color='red', width=0.5),\n",
    "            opacity=0.6\n",
    "        ), row=1, col=1)\n",
    "\n",
    "    # Panel 2: VideoData2 center coordinates - Time Series\n",
    "    if VideoData2_Has_Sleap:\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=VideoData2_centered['Seconds'],\n",
    "            y=VideoData2_centered['center.x'],\n",
    "            mode='lines',\n",
    "            name='center.x original',\n",
    "            line=dict(color='blue', width=0.5),\n",
    "            opacity=0.6\n",
    "        ), row=2, col=1)\n",
    "        \n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=VideoData2['Seconds'],\n",
    "            y=VideoData2['Ellipse.Center.X'],\n",
    "            mode='lines',\n",
    "            name='Ellipse Center.X',\n",
    "            line=dict(color='red', width=0.5),\n",
    "            opacity=0.6\n",
    "        ), row=2, col=1)\n",
    "\n",
    "    # Panel 3: Ellipse.Center.X Comparison with difference\n",
    "    if VideoData1_Has_Sleap and VideoData2_Has_Sleap:\n",
    "        # Plot the individual traces\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=VideoData1['Seconds'],\n",
    "            y=VideoData1['Ellipse.Center.X'],\n",
    "            mode='lines',\n",
    "            name='VideoData1 Ellipse.Center.X',\n",
    "            line=dict(color='#FF7F00', width=0.5),  # Orange\n",
    "            opacity=0.6\n",
    "        ), row=3, col=1)\n",
    "        \n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=VideoData2['Seconds'],\n",
    "            y=VideoData2['Ellipse.Center.X'],\n",
    "            mode='lines',\n",
    "            name='VideoData2 Ellipse.Center.X',\n",
    "            line=dict(color='#9370DB', width=0.5),  # Purple\n",
    "            opacity=0.6\n",
    "        ), row=3, col=1)\n",
    "        \n",
    "        # Plot the difference on secondary y-axis\n",
    "        # Align the data to the same length and normalize for fair comparison\n",
    "        min_length = min(len(VideoData1), len(VideoData2))\n",
    "        \n",
    "        # Normalize data (z-score) to account for different scales\n",
    "        center_x1_aligned = VideoData1['Ellipse.Center.X'].iloc[:min_length]\n",
    "        center_x2_aligned = VideoData2['Ellipse.Center.X'].iloc[:min_length]\n",
    "        \n",
    "        # Calculate mean and std for normalization\n",
    "        mean1 = center_x1_aligned.mean()\n",
    "        std1 = center_x1_aligned.std()\n",
    "        mean2 = center_x2_aligned.mean()\n",
    "        std2 = center_x2_aligned.std()\n",
    "        \n",
    "        # Normalize both datasets\n",
    "        center_x1_norm = (center_x1_aligned - mean1) / std1\n",
    "        center_x2_norm = (center_x2_aligned - mean2) / std2\n",
    "        \n",
    "        # Calculate difference of normalized data\n",
    "        center_x_diff = center_x1_norm - center_x2_norm\n",
    "        seconds_aligned = VideoData1['Seconds'].iloc[:min_length]\n",
    "        \n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=seconds_aligned,\n",
    "            y=center_x_diff,\n",
    "            mode='lines',\n",
    "            name='Difference (normalized)',\n",
    "            line=dict(color='green', width=0.5),\n",
    "            opacity=0.6\n",
    "        ), row=3, col=1, secondary_y=True)\n",
    "        \n",
    "    elif VideoData1_Has_Sleap:\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=VideoData1['Seconds'],\n",
    "            y=VideoData1['Ellipse.Center.X'],\n",
    "            mode='lines',\n",
    "            name='VideoData1 Ellipse.Center.X',\n",
    "            line=dict(color='#FF7F00', width=0.5),\n",
    "            opacity=0.6\n",
    "        ), row=3, col=1)\n",
    "    elif VideoData2_Has_Sleap:\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=VideoData2['Seconds'],\n",
    "            y=VideoData2['Ellipse.Center.X'],\n",
    "            mode='lines',\n",
    "            name='VideoData2 Ellipse.Center.X',\n",
    "            line=dict(color='#9370DB', width=0.5),\n",
    "            opacity=0.6\n",
    "        ), row=3, col=1)\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        height=1200,  # Increased height for 3 panels\n",
    "        title_text=f'{data_path} - Eye Tracking Time Series QC',\n",
    "        showlegend=True,\n",
    "        hovermode='x unified'\n",
    "    )\n",
    "\n",
    "    # Update axes\n",
    "    fig.update_xaxes(title_text=\"Time (s)\", row=3, col=1)\n",
    "    fig.update_yaxes(title_text=\"Position (pixels)\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Position (pixels)\", row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"Center X (pixels)\", row=3, col=1)\n",
    "\n",
    "    # Update secondary y-axis for difference plot\n",
    "    if VideoData1_Has_Sleap and VideoData2_Has_Sleap:\n",
    "        fig.update_yaxes(title_text=\"Normalized Difference (z-score)\", row=3, col=1, secondary_y=True)\n",
    "\n",
    "    # Show in browser\n",
    "    fig.show(renderer='browser')\n",
    "\n",
    "    # Also save as HTML\n",
    "    save_path.mkdir(parents=True, exist_ok=True)\n",
    "    html_path = save_path / \"Eye_data_QC_time_series.html\"\n",
    "    fig.write_html(html_path)\n",
    "    print(f\"‚úÖ Interactive time series plot saved to: {html_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "086008a7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# save as df to csv to be loaded in the photometry/harp/etc. analysis notebook \n",
    "############################################################################################################\n",
    "# reindex to aeon datetime to be done in the other notebook\n",
    " \n",
    "if VideoData1_Has_Sleap:\n",
    "    # Save  DataFrame as CSV to proper path and filename\n",
    "    save_path1 = save_path / \"Video_Sleap_Data1\" / \"Video_Sleap_Data1_1904-01-01T00-00-00.csv\"\n",
    "    save_path1.parent.mkdir(parents=True, exist_ok=True)\n",
    "    #save_path1.parent.mkdir(parents=True, exist_ok=True)\n",
    "    VideoData1.to_csv(save_path1)\n",
    "\n",
    "if VideoData2_Has_Sleap:\n",
    "    # Save  DataFrame as CSV to proper path and filename\n",
    "    save_path2 = save_path / \"Video_Sleap_Data2\" / \"Video_Sleap_Data2_1904-01-01T00-00-00.csv\"\n",
    "    save_path2.parent.mkdir(parents=True, exist_ok=True)\n",
    "    #save_path2.parent.mkdir(parents=True, exist_ok=True)\n",
    "    VideoData2.to_csv(save_path2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414c4a5f",
   "metadata": {},
   "source": [
    "# Saccade detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "261f230c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîé === Source: (VideoData1 (L: Left)) ===\n",
      "\n",
      "Adaptive velocity threshold: 135.71 px/s\n",
      "  (mean: 9.14 px/s, std: 30.14 px/s, k=4.2)\n",
      "Saccade duration parameter: 0.2 (saccade ends when velocity < 27.14 px/s)\n",
      "\n",
      "‚úÖ Detected 99 upward (NT) saccades\n",
      "‚úÖ Detected 314 downward (TN) saccades\n",
      "Upward saccades - mean velocity: 503.36 px/s, mean duration: 0.067 s, mean amplitude: 12.33 px, std amplitude: 6.23 px\n",
      "Downward saccades - mean velocity: -298.66 px/s, mean duration: 0.067 s, mean amplitude: 9.61 px, std amplitude: 4.76 px\n",
      "\n",
      "üìä Adaptive Thresholds Calculated (from feature distributions):\n",
      "  Pre-saccade velocity threshold (75th percentile): 21.78 px/s\n",
      "  Pre-saccade drift threshold (75th percentile): 4.77 px\n",
      "  Post-saccade variance threshold (25th percentile): 1.15 px¬≤\n",
      "\n",
      "üìä Saccade Classification Results:\n",
      "  Total saccades: 413\n",
      "  Orienting saccades: 66 (16.0%)\n",
      "  Compensatory saccades: 347 (84.0%)\n",
      "  Detected bouts: 154\n",
      "  Isolated saccades: 78\n",
      "\n",
      "  Classification Confidence:\n",
      "    Mean: 0.694 ¬± 0.266\n",
      "    Range: [0.300, 1.000]\n",
      "    Orienting: 0.989 ¬± 0.025\n",
      "    Compensatory: 0.638 ¬± 0.254\n",
      "    High confidence (‚â•0.7): 176 (42.6%)\n",
      "    Medium confidence (0.4-0.7): 165 (40.0%)\n",
      "    Low confidence (<0.4): 72 (17.4%)\n",
      "\n",
      "  Orienting saccades stats:\n",
      "    Mean amplitude: 12.09 px\n",
      "    Mean duration: 0.069 s\n",
      "    Mean pre-saccade velocity: 8.40 px/s\n",
      "\n",
      "  Compensatory saccades stats:\n",
      "    Mean amplitude: 9.77 px\n",
      "    Mean duration: 0.067 s\n",
      "    Mean pre-saccade velocity: 29.81 px/s\n",
      "    Mean bout size: 5.89 saccades\n",
      "‚úÖ All 413 saccades successfully classified (66 orienting, 347 compensatory)\n",
      "\n",
      "üîé === Source: (VideoData2 (R: Right)) ===\n",
      "\n",
      "Adaptive velocity threshold: 234.57 px/s\n",
      "  (mean: 12.67 px/s, std: 49.31 px/s, k=4.5)\n",
      "Saccade duration parameter: 0.2 (saccade ends when velocity < 46.91 px/s)\n",
      "\n",
      "‚úÖ Detected 95 upward (TN) saccades\n",
      "‚úÖ Detected 371 downward (NT) saccades\n",
      "Upward saccades - mean velocity: 480.97 px/s, mean duration: 0.066 s, mean amplitude: 14.19 px, std amplitude: 5.94 px\n",
      "Downward saccades - mean velocity: -642.97 px/s, mean duration: 0.063 s, mean amplitude: 15.78 px, std amplitude: 6.80 px\n",
      "\n",
      "üìä Adaptive Thresholds Calculated (from feature distributions):\n",
      "  Pre-saccade velocity threshold (75th percentile): 31.43 px/s\n",
      "  Pre-saccade drift threshold (75th percentile): 7.41 px\n",
      "  Post-saccade variance threshold (25th percentile): 2.73 px¬≤\n",
      "\n",
      "üìä Saccade Classification Results:\n",
      "  Total saccades: 466\n",
      "  Orienting saccades: 77 (16.5%)\n",
      "  Compensatory saccades: 389 (83.5%)\n",
      "  Detected bouts: 143\n",
      "  Isolated saccades: 66\n",
      "\n",
      "  Classification Confidence:\n",
      "    Mean: 0.707 ¬± 0.259\n",
      "    Range: [0.300, 1.000]\n",
      "    Orienting: 0.994 ¬± 0.018\n",
      "    Compensatory: 0.650 ¬± 0.246\n",
      "    High confidence (‚â•0.7): 203 (43.6%)\n",
      "    Medium confidence (0.4-0.7): 198 (42.5%)\n",
      "    Low confidence (<0.4): 65 (13.9%)\n",
      "\n",
      "  Orienting saccades stats:\n",
      "    Mean amplitude: 17.26 px\n",
      "    Mean duration: 0.066 s\n",
      "    Mean pre-saccade velocity: 10.42 px/s\n",
      "\n",
      "  Compensatory saccades stats:\n",
      "    Mean amplitude: 14.96 px\n",
      "    Mean duration: 0.063 s\n",
      "    Mean pre-saccade velocity: 39.54 px/s\n",
      "    Mean bout size: 7.85 saccades\n",
      "‚úÖ All 466 saccades successfully classified (77 orienting, 389 compensatory)\n"
     ]
    }
   ],
   "source": [
    "saccade_results = {}\n",
    "\n",
    "# Helper: map detected directions (upward/downward) to NT/TN based on eye assignment\n",
    "# Left eye: upward‚ÜíNT, downward‚ÜíTN; Right eye: upward‚ÜíTN, downward‚ÜíNT\n",
    "def get_direction_map_for_video(video_key):\n",
    "    eye = video1_eye if video_key == 'VideoData1' else video2_eye\n",
    "    if eye == 'L':\n",
    "        return {'upward': 'NT', 'downward': 'TN'}\n",
    "    else:\n",
    "        return {'upward': 'TN', 'downward': 'NT'}\n",
    "\n",
    "if VideoData1_Has_Sleap:\n",
    "    print(f\"\\nüîé === Source: ({get_eye_label('VideoData1')}) ===\\n\")\n",
    "    df1 = VideoData1[['Ellipse.Center.X', 'Seconds']].copy()\n",
    "    dir_map_v1 = get_direction_map_for_video('VideoData1')\n",
    "    saccade_results['VideoData1'] = analyze_eye_video_saccades(\n",
    "        df1, FPS_1, get_eye_label('VideoData1'),\n",
    "        k=k1, refractory_period=refractory_period,\n",
    "        onset_offset_fraction=onset_offset_fraction,\n",
    "        pre_saccade_window_time=pre_saccade_window_time,\n",
    "        post_saccade_window_time=post_saccade_window_time,\n",
    "        baseline_window_start_time=baseline_window_start_time,\n",
    "        baseline_window_end_time=baseline_window_end_time,\n",
    "        smoothing_window_time=smoothing_window_time,\n",
    "        peak_width_time=peak_width_time,\n",
    "        min_saccade_duration=min_saccade_duration,\n",
    "        upward_label=dir_map_v1['upward'],\n",
    "        downward_label=dir_map_v1['downward'],\n",
    "        classify_orienting_compensatory=classify_orienting_compensatory,\n",
    "        bout_window=bout_window,\n",
    "        pre_saccade_window=pre_saccade_window,\n",
    "        max_intersaccade_interval_for_classification=max_intersaccade_interval_for_classification,\n",
    "        pre_saccade_velocity_threshold=pre_saccade_velocity_threshold,\n",
    "        pre_saccade_drift_threshold=pre_saccade_drift_threshold,\n",
    "        post_saccade_variance_threshold=post_saccade_variance_threshold,\n",
    "        post_saccade_position_change_threshold_percent=post_saccade_position_change_threshold_percent,\n",
    "        use_adaptive_thresholds=use_adaptive_thresholds,\n",
    "        adaptive_percentile_pre_velocity=adaptive_percentile_pre_velocity,\n",
    "        adaptive_percentile_pre_drift=adaptive_percentile_pre_drift,\n",
    "        adaptive_percentile_post_variance=adaptive_percentile_post_variance\n",
    "    )\n",
    "\n",
    "\n",
    "if VideoData2_Has_Sleap:\n",
    "    print(f\"\\nüîé === Source: ({get_eye_label('VideoData2')}) ===\\n\")\n",
    "    df2 = VideoData2[['Ellipse.Center.X', 'Seconds']].copy()\n",
    "    dir_map_v2 = get_direction_map_for_video('VideoData2')\n",
    "    saccade_results['VideoData2'] = analyze_eye_video_saccades(\n",
    "        df2, FPS_2, get_eye_label('VideoData2'),\n",
    "        k=k2, refractory_period=refractory_period,\n",
    "        onset_offset_fraction=onset_offset_fraction,\n",
    "        pre_saccade_window_time=pre_saccade_window_time,\n",
    "        post_saccade_window_time=post_saccade_window_time,\n",
    "        baseline_window_start_time=baseline_window_start_time,\n",
    "        baseline_window_end_time=baseline_window_end_time,\n",
    "        smoothing_window_time=smoothing_window_time,\n",
    "        peak_width_time=peak_width_time,\n",
    "        min_saccade_duration=min_saccade_duration,\n",
    "        upward_label=dir_map_v2['upward'],\n",
    "        downward_label=dir_map_v2['downward'],\n",
    "        classify_orienting_compensatory=classify_orienting_compensatory,\n",
    "        bout_window=bout_window,\n",
    "        pre_saccade_window=pre_saccade_window,\n",
    "        max_intersaccade_interval_for_classification=max_intersaccade_interval_for_classification,\n",
    "        pre_saccade_velocity_threshold=pre_saccade_velocity_threshold,\n",
    "        pre_saccade_drift_threshold=pre_saccade_drift_threshold,\n",
    "        post_saccade_variance_threshold=post_saccade_variance_threshold,\n",
    "        post_saccade_position_change_threshold_percent=post_saccade_position_change_threshold_percent,\n",
    "        use_adaptive_thresholds=use_adaptive_thresholds,\n",
    "        adaptive_percentile_pre_velocity=adaptive_percentile_pre_velocity,\n",
    "        adaptive_percentile_pre_drift=adaptive_percentile_pre_drift,\n",
    "        adaptive_percentile_post_variance=adaptive_percentile_post_variance\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "851033c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZE ALL SACCADES - SIDE BY SIDE\n",
    "#-------------------------------------------------------------------------------\n",
    "# Plot all upward and downward saccades aligned by time with position and velocity traces\n",
    "# Using extracted visualization function from sleap.visualization\n",
    "\n",
    "if plot_saccade_detection_QC:\n",
    "    plot_all_saccades_overlay(\n",
    "        saccade_results=saccade_results,\n",
    "        video_labels=VIDEO_LABELS,\n",
    "        video1_eye=video1_eye,\n",
    "        video2_eye=video2_eye,\n",
    "        pre_saccade_window_time=pre_saccade_window_time,\n",
    "        post_saccade_window_time=post_saccade_window_time,\n",
    "        debug=debug,\n",
    "        show_plot=True\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d1392c11",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# SACCADE AMPLITUDE QC VISUALIZATION\n",
    "#-------------------------------------------------------------------------------\n",
    "# 1. Distribution of saccade amplitudes\n",
    "# 2. Correlation between saccade amplitude and duration\n",
    "# 3. Peri-saccade segments colored by amplitude (outlier detection)\n",
    "# Using extracted visualization function from sleap.visualization\n",
    "\n",
    "if plot_saccade_detection_QC:\n",
    "    plot_saccade_amplitude_qc(\n",
    "        saccade_results=saccade_results,\n",
    "        video_labels=VIDEO_LABELS,\n",
    "        video1_eye=video1_eye,\n",
    "        video2_eye=video2_eye,\n",
    "        debug=debug,\n",
    "        show_plot=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9d0e35b1",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# DEBUG: BASELINING DIAGNOSTICS\n",
    "#-------------------------------------------------------------------------------\n",
    "# Plot distribution of baseline window values before and after baselining\n",
    "# This helps diagnose why some segments might not be baselined correctly\n",
    "\n",
    "if debug:\n",
    "    for video_key, res in saccade_results.items():\n",
    "        dir_map = get_direction_map_for_video(video_key)\n",
    "        label_up = dir_map['upward']\n",
    "        label_down = dir_map['downward']\n",
    "        \n",
    "        peri_saccades = res['peri_saccades']\n",
    "        \n",
    "        if len(peri_saccades) == 0:\n",
    "            print(f\"\\n‚ö†Ô∏è  No saccades found for {get_eye_label(video_key)}, skipping baselining diagnostics\")\n",
    "            continue\n",
    "        \n",
    "        # Extract baseline window statistics for each segment\n",
    "        baseline_values = []  # What was subtracted\n",
    "        baseline_window_means_before = []  # Mean position in baseline window BEFORE baselining\n",
    "        baseline_window_means_after = []  # Mean position in baseline window AFTER baselining\n",
    "        baseline_window_counts = []  # Number of points in baseline window\n",
    "        segment_directions = []\n",
    "        segment_ids = []\n",
    "        \n",
    "        # Get baseline window parameters from the function call (use defaults if not available)\n",
    "        baseline_window_start_time = -0.1 if 'baseline_window_start_time' not in globals() else baseline_window_start_time\n",
    "        baseline_window_end_time = -0.02 if 'baseline_window_end_time' not in globals() else baseline_window_end_time\n",
    "        \n",
    "        for seg in peri_saccades:\n",
    "            seg_id = seg['saccade_id'].iloc[0] if 'saccade_id' in seg.columns else len(baseline_values)\n",
    "            direction = seg['saccade_direction'].iloc[0] if 'saccade_direction' in seg.columns else 'unknown'\n",
    "            \n",
    "            # Get baseline value that was used\n",
    "            if 'baseline_value' in seg.columns:\n",
    "                baseline_val = seg['baseline_value'].iloc[0]\n",
    "            else:\n",
    "                baseline_val = np.nan\n",
    "            \n",
    "            # Find baseline window points (before threshold crossing)\n",
    "            baseline_mask = (\n",
    "                (seg['Time_rel_threshold'] >= baseline_window_start_time) & \n",
    "                (seg['Time_rel_threshold'] <= baseline_window_end_time) &\n",
    "                (seg['Time_rel_threshold'] < 0)  # Pre-threshold only\n",
    "            )\n",
    "            \n",
    "            # Get original position values (reconstruct if needed)\n",
    "            if 'X_raw' in seg.columns:\n",
    "                original_pos_col = 'X_raw'\n",
    "            elif 'X_smooth' in seg.columns:\n",
    "                original_pos_col = 'X_smooth'\n",
    "            else:\n",
    "                original_pos_col = None\n",
    "            \n",
    "            # Calculate mean in baseline window BEFORE baselining\n",
    "            if original_pos_col is not None:\n",
    "                baseline_window_original = seg.loc[baseline_mask, original_pos_col].dropna()\n",
    "                if len(baseline_window_original) > 0:\n",
    "                    mean_before = baseline_window_original.mean()\n",
    "                else:\n",
    "                    mean_before = np.nan\n",
    "            else:\n",
    "                # Reconstruct: original = baselined + baseline_value\n",
    "                if 'X_smooth_baselined' in seg.columns and not pd.isna(baseline_val):\n",
    "                    baseline_window_baselined = seg.loc[baseline_mask, 'X_smooth_baselined'].dropna()\n",
    "                    if len(baseline_window_baselined) > 0:\n",
    "                        mean_before = baseline_window_baselined.mean() + baseline_val\n",
    "                    else:\n",
    "                        mean_before = np.nan\n",
    "                else:\n",
    "                    mean_before = np.nan\n",
    "            \n",
    "            # Calculate mean in baseline window AFTER baselining\n",
    "            if 'X_smooth_baselined' in seg.columns:\n",
    "                baseline_window_baselined = seg.loc[baseline_mask, 'X_smooth_baselined'].dropna()\n",
    "                if len(baseline_window_baselined) > 0:\n",
    "                    mean_after = baseline_window_baselined.mean()\n",
    "                    n_points = len(baseline_window_baselined)\n",
    "                else:\n",
    "                    mean_after = np.nan\n",
    "                    n_points = 0\n",
    "            else:\n",
    "                mean_after = np.nan\n",
    "                n_points = 0\n",
    "            \n",
    "            baseline_values.append(baseline_val)\n",
    "            baseline_window_means_before.append(mean_before)\n",
    "            baseline_window_means_after.append(mean_after)\n",
    "            baseline_window_counts.append(n_points)\n",
    "            segment_directions.append(direction)\n",
    "            segment_ids.append(seg_id)\n",
    "        \n",
    "        # Convert to arrays for easier manipulation\n",
    "        baseline_values = np.array(baseline_values)\n",
    "        baseline_window_means_before = np.array(baseline_window_means_before)\n",
    "        baseline_window_means_after = np.array(baseline_window_means_after)\n",
    "        baseline_window_counts = np.array(baseline_window_counts)\n",
    "        segment_directions = np.array(segment_directions)\n",
    "        \n",
    "        # Create diagnostic figure\n",
    "        fig_baseline = make_subplots(\n",
    "            rows=2, cols=2,\n",
    "            subplot_titles=(\n",
    "                'Baseline Values (What Was Subtracted)',\n",
    "                'Baseline Window Mean - BEFORE Baselining',\n",
    "                'Baseline Window Mean - AFTER Baselining',\n",
    "                'Baseline Window Point Counts'\n",
    "            ),\n",
    "            vertical_spacing=0.15,\n",
    "            horizontal_spacing=0.12\n",
    "        )\n",
    "        \n",
    "        # Plot 1: Baseline values distribution\n",
    "        for direction in ['upward', 'downward']:\n",
    "            mask = segment_directions == direction\n",
    "            if mask.sum() > 0:\n",
    "                label = label_up if direction == 'upward' else label_down\n",
    "                color = 'green' if direction == 'upward' else 'purple'\n",
    "                fig_baseline.add_trace(\n",
    "                    go.Histogram(\n",
    "                        x=baseline_values[mask],\n",
    "                        nbinsx=50,\n",
    "                        name=f'{label}',\n",
    "                        marker_color=color,\n",
    "                        opacity=0.6\n",
    "                    ),\n",
    "                    row=1, col=1\n",
    "                )\n",
    "        \n",
    "        # Plot 2: Baseline window mean BEFORE baselining\n",
    "        for direction in ['upward', 'downward']:\n",
    "            mask = segment_directions == direction\n",
    "            if mask.sum() > 0:\n",
    "                label = label_up if direction == 'upward' else label_down\n",
    "                color = 'green' if direction == 'upward' else 'purple'\n",
    "                valid_mask = mask & ~np.isnan(baseline_window_means_before)\n",
    "                if valid_mask.sum() > 0:\n",
    "                    fig_baseline.add_trace(\n",
    "                        go.Histogram(\n",
    "                            x=baseline_window_means_before[valid_mask],\n",
    "                            nbinsx=50,\n",
    "                            name=f'{label}',\n",
    "                            marker_color=color,\n",
    "                            opacity=0.6,\n",
    "                            showlegend=False\n",
    "                        ),\n",
    "                        row=1, col=2\n",
    "                    )\n",
    "        \n",
    "        # Plot 3: Baseline window mean AFTER baselining (should be ~0)\n",
    "        for direction in ['upward', 'downward']:\n",
    "            mask = segment_directions == direction\n",
    "            if mask.sum() > 0:\n",
    "                label = label_up if direction == 'upward' else label_down\n",
    "                color = 'green' if direction == 'upward' else 'purple'\n",
    "                valid_mask = mask & ~np.isnan(baseline_window_means_after)\n",
    "                if valid_mask.sum() > 0:\n",
    "                    fig_baseline.add_trace(\n",
    "                        go.Histogram(\n",
    "                            x=baseline_window_means_after[valid_mask],\n",
    "                            nbinsx=50,\n",
    "                            name=f'{label}',\n",
    "                            marker_color=color,\n",
    "                            opacity=0.6,\n",
    "                            showlegend=False\n",
    "                        ),\n",
    "                        row=2, col=1\n",
    "                    )\n",
    "        \n",
    "        # Plot 4: Baseline window point counts\n",
    "        for direction in ['upward', 'downward']:\n",
    "            mask = segment_directions == direction\n",
    "            if mask.sum() > 0:\n",
    "                label = label_up if direction == 'upward' else label_down\n",
    "                color = 'green' if direction == 'upward' else 'purple'\n",
    "                fig_baseline.add_trace(\n",
    "                    go.Histogram(\n",
    "                        x=baseline_window_counts[mask],\n",
    "                        nbinsx=20,\n",
    "                        name=f'{label}',\n",
    "                        marker_color=color,\n",
    "                        opacity=0.6,\n",
    "                        showlegend=False\n",
    "                    ),\n",
    "                    row=2, col=2\n",
    "                )\n",
    "        \n",
    "        # Add vertical line at 0 for \"AFTER baselining\" plot\n",
    "        fig_baseline.add_vline(\n",
    "            x=0,\n",
    "            line_dash=\"dash\",\n",
    "            line_color=\"red\",\n",
    "            line_width=2,\n",
    "            opacity=0.7,\n",
    "            annotation_text=\"Expected: 0\",\n",
    "            row=2, col=1\n",
    "        )\n",
    "        \n",
    "        # Update layout\n",
    "        fig_baseline.update_layout(\n",
    "            title_text=f'Baselining Diagnostics: {get_eye_label(video_key)}<br><sub>After baselining, baseline window mean should be ~0</sub>',\n",
    "            height=800,\n",
    "            showlegend=True,\n",
    "            legend=dict(x=0.02, y=0.98)\n",
    "        )\n",
    "        \n",
    "        # Update axes labels\n",
    "        fig_baseline.update_xaxes(title_text=\"Baseline Value (px)\", row=1, col=1)\n",
    "        fig_baseline.update_xaxes(title_text=\"Mean Position in Baseline Window (px)\", row=1, col=2)\n",
    "        fig_baseline.update_xaxes(title_text=\"Mean Position in Baseline Window (px)\", row=2, col=1)\n",
    "        fig_baseline.update_xaxes(title_text=\"Number of Points\", row=2, col=2)\n",
    "        fig_baseline.update_yaxes(title_text=\"Count\", row=1, col=1)\n",
    "        fig_baseline.update_yaxes(title_text=\"Count\", row=1, col=2)\n",
    "        fig_baseline.update_yaxes(title_text=\"Count\", row=2, col=1)\n",
    "        fig_baseline.update_yaxes(title_text=\"Count\", row=2, col=2)\n",
    "        \n",
    "        fig_baseline.show()\n",
    "        \n",
    "        # Print statistics\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"BASELINING DIAGNOSTICS: {get_eye_label(video_key)}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        for direction in ['upward', 'downward']:\n",
    "            mask = segment_directions == direction\n",
    "            label = label_up if direction == 'upward' else label_down\n",
    "            \n",
    "            if mask.sum() > 0:\n",
    "                print(f\"\\n{label} saccades (n={mask.sum()}):\")\n",
    "                \n",
    "                # Baseline values\n",
    "                valid_baseline = baseline_values[mask & ~np.isnan(baseline_values)]\n",
    "                if len(valid_baseline) > 0:\n",
    "                    print(f\"  Baseline values (subtracted):\")\n",
    "                    print(f\"    Mean: {valid_baseline.mean():.2f} px\")\n",
    "                    print(f\"    Std: {valid_baseline.std():.2f} px\")\n",
    "                    print(f\"    Range: [{valid_baseline.min():.2f}, {valid_baseline.max():.2f}] px\")\n",
    "                    print(f\"    NaN count: {np.isnan(baseline_values[mask]).sum()}\")\n",
    "                \n",
    "                # Baseline window means BEFORE\n",
    "                valid_before = baseline_window_means_before[mask & ~np.isnan(baseline_window_means_before)]\n",
    "                if len(valid_before) > 0:\n",
    "                    print(f\"\\n  Baseline window mean BEFORE baselining:\")\n",
    "                    print(f\"    Mean: {valid_before.mean():.2f} px\")\n",
    "                    print(f\"    Std: {valid_before.std():.2f} px\")\n",
    "                    print(f\"    Range: [{valid_before.min():.2f}, {valid_before.max():.2f}] px\")\n",
    "                    print(f\"    NaN count: {np.isnan(baseline_window_means_before[mask]).sum()}\")\n",
    "                \n",
    "                # Baseline window means AFTER (should be ~0)\n",
    "                valid_after = baseline_window_means_after[mask & ~np.isnan(baseline_window_means_after)]\n",
    "                if len(valid_after) > 0:\n",
    "                    print(f\"\\n  Baseline window mean AFTER baselining (should be ~0):\")\n",
    "                    print(f\"    Mean: {valid_after.mean():.2f} px\")\n",
    "                    print(f\"    Std: {valid_after.std():.2f} px\")\n",
    "                    print(f\"    Range: [{valid_after.min():.2f}, {valid_after.max():.2f}] px\")\n",
    "                    print(f\"    NaN count: {np.isnan(baseline_window_means_after[mask]).sum()}\")\n",
    "                    \n",
    "                    # Count segments that are NOT properly baselined (mean > 1px away from 0)\n",
    "                    not_baselined = np.abs(valid_after) > 1.0\n",
    "                    if not_baselined.sum() > 0:\n",
    "                        print(f\"\\n  ‚ö†Ô∏è  WARNING: {not_baselined.sum()} segments NOT properly baselined (|mean| > 1px)\")\n",
    "                        print(f\"    Mean values for non-baselined segments: {valid_after[not_baselined]}\")\n",
    "                    \n",
    "                    # Show distribution of baseline window means (should all be ~0)\n",
    "                    print(f\"\\n  Distribution of baseline window means AFTER baselining:\")\n",
    "                    print(f\"    Segments with |mean| < 0.1 px: {(np.abs(valid_after) < 0.1).sum()}\")\n",
    "                    print(f\"    Segments with 0.1 <= |mean| < 0.5 px: {((np.abs(valid_after) >= 0.1) & (np.abs(valid_after) < 0.5)).sum()}\")\n",
    "                    print(f\"    Segments with 0.5 <= |mean| < 1.0 px: {((np.abs(valid_after) >= 0.5) & (np.abs(valid_after) < 1.0)).sum()}\")\n",
    "                    print(f\"    Segments with |mean| >= 1.0 px: {(np.abs(valid_after) >= 1.0).sum()}\")\n",
    "                    \n",
    "                    # Show worst offenders\n",
    "                    worst_indices = np.argsort(np.abs(valid_after))[-10:]  # Top 10 worst\n",
    "                    if len(worst_indices) > 0:\n",
    "                        print(f\"\\n  Top 10 segments with largest |baseline window mean|:\")\n",
    "                        for i, idx in enumerate(worst_indices[::-1]):  # Reverse to show worst first\n",
    "                            seg_idx = np.where(mask)[0][idx]\n",
    "                            seg_id = segment_ids[seg_idx]\n",
    "                            mean_val = valid_after[idx]\n",
    "                            baseline_val = baseline_values[mask][idx]\n",
    "                            print(f\"    Segment {seg_id}: baseline_window_mean={mean_val:.3f} px, baseline_value={baseline_val:.3f} px\")\n",
    "                    \n",
    "                    # CRITICAL CHECK: Verify that baseline_value actually equals baseline_window_mean_before\n",
    "                    # This checks if baselining logic is correct\n",
    "                    valid_before_check = baseline_window_means_before[mask & ~np.isnan(baseline_window_means_before)]\n",
    "                    if len(valid_before_check) > 0 and len(valid_baseline) > 0:\n",
    "                        # Check if baseline_value matches baseline_window_mean_before (should be identical)\n",
    "                        mismatches = np.abs(valid_baseline - valid_before_check) > 0.01  # Allow small floating point differences\n",
    "                        if mismatches.sum() > 0:\n",
    "                            print(f\"\\n  ‚ö†Ô∏è  CRITICAL: {mismatches.sum()} segments where baseline_value != baseline_window_mean_before\")\n",
    "                            print(f\"    This indicates a bug in baselining logic!\")\n",
    "                            for i in np.where(mismatches)[0][:5]:  # Show first 5\n",
    "                                print(f\"      Segment {i}: baseline_value={valid_baseline[i]:.3f}, baseline_window_mean_before={valid_before_check[i]:.3f}\")\n",
    "                        else:\n",
    "                            print(f\"\\n  ‚úÖ Baselining logic check: baseline_value matches baseline_window_mean_before for all segments\")\n",
    "                \n",
    "                # Baseline window point counts\n",
    "                valid_counts = baseline_window_counts[mask & (baseline_window_counts > 0)]\n",
    "                if len(valid_counts) > 0:\n",
    "                    print(f\"\\n  Baseline window point counts:\")\n",
    "                    print(f\"    Mean: {valid_counts.mean():.1f} points\")\n",
    "                    print(f\"    Range: [{valid_counts.min()}, {valid_counts.max()}] points\")\n",
    "                    print(f\"    Segments with 0 points in baseline window: {(baseline_window_counts[mask] == 0).sum()}\")\n",
    "        \n",
    "        print(f\"\\n{'='*80}\\n\")\n",
    "        \n",
    "        # Additional diagnostic: Check if baselining is actually applied to segments\n",
    "        # Compare original vs baselined values at the start of segments (before threshold)\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"ADDITIONAL BASELINING CHECK: Comparing segment start positions\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        for direction in ['upward', 'downward']:\n",
    "            mask = segment_directions == direction\n",
    "            label = label_up if direction == 'upward' else label_down\n",
    "            \n",
    "            if mask.sum() > 0:\n",
    "                print(f\"\\n{label} saccades:\")\n",
    "                \n",
    "                # Check first few points of each segment (should be ~0 after baselining)\n",
    "                segment_start_means_original = []\n",
    "                segment_start_means_baselined = []\n",
    "                \n",
    "                for i, seg in enumerate(peri_saccades):\n",
    "                    if segment_directions[i] != direction:\n",
    "                        continue\n",
    "                    \n",
    "                    # Get first 5 points before threshold (if available)\n",
    "                    pre_threshold_mask = seg['Time_rel_threshold'] < 0\n",
    "                    pre_threshold_seg = seg.loc[pre_threshold_mask].head(5)\n",
    "                    \n",
    "                    if len(pre_threshold_seg) > 0:\n",
    "                        # Get original position\n",
    "                        if 'X_raw' in seg.columns:\n",
    "                            orig_col = 'X_raw'\n",
    "                        elif 'X_smooth' in seg.columns:\n",
    "                            orig_col = 'X_smooth'\n",
    "                        else:\n",
    "                            orig_col = None\n",
    "                        \n",
    "                        if orig_col is not None:\n",
    "                            orig_mean = pre_threshold_seg[orig_col].mean()\n",
    "                            segment_start_means_original.append(orig_mean)\n",
    "                        \n",
    "                        # Get baselined position\n",
    "                        if 'X_smooth_baselined' in seg.columns:\n",
    "                            baselined_mean = pre_threshold_seg['X_smooth_baselined'].mean()\n",
    "                            segment_start_means_baselined.append(baselined_mean)\n",
    "                \n",
    "                if len(segment_start_means_baselined) > 0:\n",
    "                    segment_start_means_baselined = np.array(segment_start_means_baselined)\n",
    "                    print(f\"  Mean position at segment start (first 5 pre-threshold points) AFTER baselining:\")\n",
    "                    print(f\"    Mean: {segment_start_means_baselined.mean():.3f} px\")\n",
    "                    print(f\"    Std: {segment_start_means_baselined.std():.3f} px\")\n",
    "                    print(f\"    Range: [{segment_start_means_baselined.min():.3f}, {segment_start_means_baselined.max():.3f}] px\")\n",
    "                    print(f\"    Segments with |mean| > 1 px: {(np.abs(segment_start_means_baselined) > 1.0).sum()}\")\n",
    "                    print(f\"    Segments with |mean| > 5 px: {(np.abs(segment_start_means_baselined) > 5.0).sum()}\")\n",
    "                    \n",
    "                    # Show worst offenders\n",
    "                    worst_indices = np.argsort(np.abs(segment_start_means_baselined))[-10:]\n",
    "                    if len(worst_indices) > 0:\n",
    "                        print(f\"\\n  Top 10 segments with largest |start position mean| AFTER baselining:\")\n",
    "                        for idx in worst_indices[::-1]:\n",
    "                            print(f\"    Segment {idx}: start_mean={segment_start_means_baselined[idx]:.3f} px\")\n",
    "        \n",
    "        print(f\"\\n{'='*80}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "29bbd4de",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# VISUALIZE DETECTED SACCADES (Adaptive Method)\n",
    "#-------------------------------------------------------------------------------\n",
    "# Create overlay plot showing detected saccades with duration lines and peak arrows\n",
    "if plot_saccade_detection_QC:\n",
    "    for video_key, res in saccade_results.items():\n",
    "        dir_map = get_direction_map_for_video(video_key)\n",
    "        label_up = dir_map['upward']\n",
    "        label_down = dir_map['downward']\n",
    "\n",
    "        upward_saccades_df = res['upward_saccades_df']\n",
    "        downward_saccades_df = res['downward_saccades_df']\n",
    "        peri_saccades = res['peri_saccades']   \n",
    "        upward_segments = res['upward_segments']\n",
    "        downward_segments = res['downward_segments']\n",
    "        # Any other variables you need...\n",
    "\n",
    "        # Optional: Find 5-minute window with highest saccade density\n",
    "        plot_5min_window = True  # Set to True to plot only highest density 5-minute window\n",
    "        window_duration = 300  # 5 minutes in seconds\n",
    "        \n",
    "        # Initialize variables for windowing\n",
    "        best_window_start = None\n",
    "        best_window_end = None\n",
    "        best_window_count = 0\n",
    "        \n",
    "        if plot_5min_window and (len(upward_saccades_df) > 0 or len(downward_saccades_df) > 0):\n",
    "            # Combine all saccades and find time window with highest density\n",
    "            all_saccade_times = []\n",
    "            if len(upward_saccades_df) > 0:\n",
    "                all_saccade_times.extend(upward_saccades_df['time'].values)\n",
    "            if len(downward_saccades_df) > 0:\n",
    "                all_saccade_times.extend(downward_saccades_df['time'].values)\n",
    "            \n",
    "            if len(all_saccade_times) > 0:\n",
    "                all_saccade_times = np.array(all_saccade_times)\n",
    "                time_min = all_saccade_times.min()\n",
    "                time_max = all_saccade_times.max()\n",
    "                \n",
    "                # Slide window and count saccades in each window\n",
    "                best_window_start = time_min\n",
    "                best_window_count = 0\n",
    "                step_size = 10  # Check every 10 seconds\n",
    "                \n",
    "                for window_start in np.arange(time_min, time_max - window_duration + step_size, step_size):\n",
    "                    window_end = window_start + window_duration\n",
    "                    count = np.sum((all_saccade_times >= window_start) & (all_saccade_times <= window_end))\n",
    "                    if count > best_window_count:\n",
    "                        best_window_count = count\n",
    "                        best_window_start = window_start\n",
    "                \n",
    "                best_window_end = best_window_start + window_duration\n",
    "                \n",
    "                # Filter data to this window\n",
    "                df_window = res['df'][(res['df']['Seconds'] >= best_window_start) & \n",
    "                                     (res['df']['Seconds'] <= best_window_end)].copy()\n",
    "                upward_saccades_df_window = upward_saccades_df[\n",
    "                    (upward_saccades_df['time'] >= best_window_start) & \n",
    "                    (upward_saccades_df['time'] <= best_window_end)\n",
    "                ].copy()\n",
    "                downward_saccades_df_window = downward_saccades_df[\n",
    "                    (downward_saccades_df['time'] >= best_window_start) & \n",
    "                    (downward_saccades_df['time'] <= best_window_end)\n",
    "                ].copy()\n",
    "                \n",
    "                if debug:\n",
    "                    print(f\"\\nüìä Highest saccade density window: {best_window_start:.1f}s to {best_window_end:.1f}s\")\n",
    "                    print(f\"   ({best_window_count} saccades in {window_duration/60:.1f} minutes)\")\n",
    "                    print(f\"   Density: {best_window_count / (window_duration/60):.1f} saccades/min\")\n",
    "            else:\n",
    "                plot_5min_window = False\n",
    "        else:\n",
    "            plot_5min_window = False\n",
    "        \n",
    "        # Use windowed data if requested, otherwise use full data\n",
    "        if plot_5min_window:\n",
    "            df_plot = df_window\n",
    "            upward_saccades_df_plot = upward_saccades_df_window\n",
    "            downward_saccades_df_plot = downward_saccades_df_window\n",
    "            time_range_text = f\" (5-min window: {best_window_start:.1f}-{best_window_end:.1f}s, {best_window_count} saccades)\"\n",
    "        else:\n",
    "            df_plot = res['df']\n",
    "            upward_saccades_df_plot = upward_saccades_df\n",
    "            downward_saccades_df_plot = downward_saccades_df\n",
    "            time_range_text = \"\"\n",
    "\n",
    "        fig = make_subplots(\n",
    "            rows=2, cols=1,\n",
    "            shared_xaxes=True,\n",
    "            vertical_spacing=0.1,\n",
    "            subplot_titles=('X Position (px)', 'Velocity (px/s) with Detected Saccades')\n",
    "        )\n",
    "\n",
    "        # Add smoothed X position to the first subplot\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=df_plot['Seconds'],\n",
    "                y=df_plot['X_smooth'],\n",
    "                mode='lines',\n",
    "                name='Smoothed X',\n",
    "                line=dict(color='blue', width=2)\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "\n",
    "        # Add smoothed velocity to the second subplot\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=df_plot['Seconds'],\n",
    "                y=df_plot['vel_x_smooth'],\n",
    "                mode='lines',\n",
    "                name='Smoothed Velocity',\n",
    "                line=dict(color='red', width=2)\n",
    "            ),\n",
    "            row=2, col=1\n",
    "        )\n",
    "\n",
    "        # Add adaptive threshold lines for reference\n",
    "        fig.add_hline(\n",
    "            y=res['vel_thresh'],\n",
    "            line_dash=\"dash\",\n",
    "            line_color=\"green\",\n",
    "            opacity=0.5,\n",
    "            annotation_text=f\"Adaptive threshold (¬±{res['vel_thresh']:.0f} px/s)\",\n",
    "            row=2, col=1\n",
    "        )\n",
    "\n",
    "        fig.add_hline(\n",
    "            y=-res['vel_thresh'],\n",
    "            line_dash=\"dash\",\n",
    "            line_color=\"green\",\n",
    "            opacity=0.5,\n",
    "            row=2, col=1\n",
    "        )\n",
    "\n",
    "        # Calculate position y-axis range for vertical lines\n",
    "        pos_max = df_plot['X_smooth'].max()\n",
    "        pos_min = df_plot['X_smooth'].min()\n",
    "        pos_range = pos_max - pos_min\n",
    "        # Add small padding to ensure lines span full visible range\n",
    "        pos_padding = pos_range * 0.05\n",
    "        pos_y_min = pos_min - pos_padding\n",
    "        pos_y_max = pos_max + pos_padding\n",
    "\n",
    "        # Plot upward saccades (TN) as vertical lines on position trace\n",
    "        if len(upward_saccades_df_plot) > 0:\n",
    "            for idx, row in upward_saccades_df_plot.iterrows():\n",
    "                start_time = row['start_time']\n",
    "                end_time = row['end_time']\n",
    "                \n",
    "                # Use rectangle with thin border to show duration span more efficiently\n",
    "                # Opacity must be set at shape level or via rgba color, not in line dict\n",
    "                fig.add_shape(\n",
    "                    type=\"rect\",\n",
    "                    x0=start_time, y0=pos_y_min,\n",
    "                    x1=end_time, y1=pos_y_max,\n",
    "                    fillcolor='rgba(0,128,0,0.1)',  # Light green fill with opacity\n",
    "                    line=dict(color='rgba(0,128,0,0.3)', width=2),  # Green border with opacity via rgba\n",
    "                    row=1, col=1\n",
    "                )\n",
    "            \n",
    "            # Add legend entry for upward saccades\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=[None],\n",
    "                    y=[None],\n",
    "                    mode='lines',\n",
    "                    name=f'{label_up} Saccades',\n",
    "                    line=dict(color='rgba(0,128,0,0.3)', width=3)  # Green with opacity via rgba\n",
    "                ),\n",
    "                row=1, col=1\n",
    "            )\n",
    "\n",
    "        # Plot downward saccades (NT) as vertical lines on position trace\n",
    "        if len(downward_saccades_df_plot) > 0:\n",
    "            for idx, row in downward_saccades_df_plot.iterrows():\n",
    "                start_time = row['start_time']\n",
    "                end_time = row['end_time']\n",
    "                \n",
    "                # Use rectangle with thin border to show duration span more efficiently\n",
    "                # Opacity must be set at shape level or via rgba color, not in line dict\n",
    "                fig.add_shape(\n",
    "                    type=\"rect\",\n",
    "                    x0=start_time, y0=pos_y_min,\n",
    "                    x1=end_time, y1=pos_y_max,\n",
    "                    fillcolor='rgba(128,0,128,0.1)',  # Light purple fill with opacity\n",
    "                    line=dict(color='rgba(128,0,128,0.3)', width=2),  # Purple border with opacity via rgba\n",
    "                    row=1, col=1\n",
    "                )\n",
    "            \n",
    "            # Add legend entry for downward saccades\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=[None],\n",
    "                    y=[None],\n",
    "                    mode='lines',\n",
    "                    name=f'{label_down} Saccades',\n",
    "                    line=dict(color='rgba(128,0,128,0.3)', width=3)  # Purple with opacity via rgba\n",
    "                ),\n",
    "                row=1, col=1\n",
    "            )\n",
    "\n",
    "        # Update layout\n",
    "        fig.update_layout(\n",
    "            title=f'Detected Saccades ({get_eye_label(video_key)}): Vertical Lines on Position Trace (QC Visualization){time_range_text}',\n",
    "            height=600,\n",
    "            showlegend=True,\n",
    "            legend=dict(x=0.01, y=0.99)\n",
    "        )\n",
    "\n",
    "        # Update axes\n",
    "        fig.update_xaxes(title_text=\"Time (s)\", row=2, col=1)\n",
    "        fig.update_yaxes(title_text=\"X Position (px)\", row=1, col=1)\n",
    "        fig.update_yaxes(title_text=\"Velocity (px/s)\", row=2, col=1)\n",
    "\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a17b0d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADAPTIVE THRESHOLD DIAGNOSTIC PLOTS (only if debug=True)\n",
    "#-------------------------------------------------------------------------------\n",
    "# Plot distributions of classification features to help determine meaningful adaptive thresholds\n",
    "if debug and len(saccade_results) > 0:\n",
    "    print(\"\\nüìä Generating adaptive threshold diagnostic plots...\")\n",
    "    \n",
    "    for video_key, res in saccade_results.items():\n",
    "        all_saccades_df = res.get('all_saccades_df', pd.DataFrame())\n",
    "        \n",
    "        if len(all_saccades_df) == 0:\n",
    "            print(f\"‚ö†Ô∏è  No saccades found for {get_eye_label(video_key)}, skipping diagnostic plots\")\n",
    "            continue\n",
    "        \n",
    "        # Filter out NaN values for plotting\n",
    "        pre_vel = all_saccades_df['pre_saccade_mean_velocity'].dropna()\n",
    "        pre_drift = all_saccades_df['pre_saccade_position_drift'].dropna()\n",
    "        post_var = all_saccades_df['post_saccade_position_variance'].dropna()\n",
    "        post_change = all_saccades_df['post_saccade_position_change'].dropna()\n",
    "        amplitude = all_saccades_df['amplitude'].dropna()\n",
    "        \n",
    "        # Calculate post_change / amplitude ratio (for percentage threshold visualization)\n",
    "        # Align by index to ensure matching\n",
    "        aligned_indices = post_change.index.intersection(amplitude.index)\n",
    "        post_change_aligned = post_change.loc[aligned_indices]\n",
    "        amplitude_aligned = amplitude.loc[aligned_indices]\n",
    "        post_change_ratio = (post_change_aligned / amplitude_aligned) * 100  # Convert to percentage\n",
    "        \n",
    "        # Calculate current thresholds for visualization\n",
    "        if use_adaptive_thresholds:\n",
    "            # Calculate adaptive thresholds from current data\n",
    "            if len(pre_vel) >= 3:\n",
    "                current_pre_vel_threshold = np.percentile(pre_vel, adaptive_percentile_pre_velocity)\n",
    "            else:\n",
    "                current_pre_vel_threshold = pre_saccade_velocity_threshold\n",
    "            \n",
    "            if len(pre_drift) >= 3:\n",
    "                current_pre_drift_threshold = np.percentile(pre_drift, adaptive_percentile_pre_drift)\n",
    "            else:\n",
    "                current_pre_drift_threshold = pre_saccade_drift_threshold\n",
    "            \n",
    "            if len(post_var) >= 3:\n",
    "                current_post_var_threshold = np.percentile(post_var, adaptive_percentile_post_variance)\n",
    "            else:\n",
    "                current_post_var_threshold = post_saccade_variance_threshold\n",
    "        else:\n",
    "            # Use fixed thresholds\n",
    "            current_pre_vel_threshold = pre_saccade_velocity_threshold\n",
    "            current_pre_drift_threshold = pre_saccade_drift_threshold\n",
    "            current_post_var_threshold = post_saccade_variance_threshold\n",
    "        \n",
    "        # Create figure with 2x2 subplots\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "        fig.suptitle(f'Adaptive Threshold Diagnostic Plots: {get_eye_label(video_key)}\\n'\n",
    "                    f'(n={len(all_saccades_df)} saccades)', fontsize=14, fontweight='bold')\n",
    "        \n",
    "        # Plot 1: Pre-saccade mean velocity\n",
    "        ax = axes[0, 0]\n",
    "        if len(pre_vel) > 0:\n",
    "            ax.hist(pre_vel, bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "            ax.axvline(current_pre_vel_threshold, color='red', linestyle='--', linewidth=2, \n",
    "                      label=f'Threshold: {current_pre_vel_threshold:.2f} px/s')\n",
    "            if use_adaptive_thresholds:\n",
    "                ax.axvline(np.percentile(pre_vel, 50), color='gray', linestyle=':', linewidth=1, \n",
    "                          label=f'Median: {np.percentile(pre_vel, 50):.2f} px/s')\n",
    "                ax.axvline(np.percentile(pre_vel, 75), color='orange', linestyle=':', linewidth=1, \n",
    "                          label=f'75th: {np.percentile(pre_vel, 75):.2f} px/s')\n",
    "            ax.set_xlabel('Pre-saccade Mean Velocity (px/s)')\n",
    "            ax.set_ylabel('Count')\n",
    "            ax.set_title(f'Pre-saccade Velocity Distribution\\n'\n",
    "                        f'{\"Adaptive\" if use_adaptive_thresholds else \"Fixed\"} threshold at '\n",
    "                        f'{adaptive_percentile_pre_velocity if use_adaptive_thresholds else \"fixed\"}th percentile')\n",
    "            ax.legend()\n",
    "            ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 2: Pre-saccade position drift\n",
    "        ax = axes[0, 1]\n",
    "        if len(pre_drift) > 0:\n",
    "            ax.hist(pre_drift, bins=50, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "            ax.axvline(current_pre_drift_threshold, color='red', linestyle='--', linewidth=2,\n",
    "                      label=f'Threshold: {current_pre_drift_threshold:.2f} px')\n",
    "            if use_adaptive_thresholds:\n",
    "                ax.axvline(np.percentile(pre_drift, 50), color='gray', linestyle=':', linewidth=1,\n",
    "                          label=f'Median: {np.percentile(pre_drift, 50):.2f} px')\n",
    "                ax.axvline(np.percentile(pre_drift, 75), color='orange', linestyle=':', linewidth=1,\n",
    "                          label=f'75th: {np.percentile(pre_drift, 75):.2f} px')\n",
    "            ax.set_xlabel('Pre-saccade Position Drift (px)')\n",
    "            ax.set_ylabel('Count')\n",
    "            ax.set_title(f'Pre-saccade Drift Distribution\\n'\n",
    "                        f'{\"Adaptive\" if use_adaptive_thresholds else \"Fixed\"} threshold at '\n",
    "                        f'{adaptive_percentile_pre_drift if use_adaptive_thresholds else \"fixed\"}th percentile')\n",
    "            ax.legend()\n",
    "            ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 3: Post-saccade position variance\n",
    "        ax = axes[1, 0]\n",
    "        if len(post_var) > 0:\n",
    "            ax.hist(post_var, bins=50, alpha=0.7, color='plum', edgecolor='black')\n",
    "            ax.axvline(current_post_var_threshold, color='red', linestyle='--', linewidth=2,\n",
    "                      label=f'Threshold: {current_post_var_threshold:.2f} px¬≤')\n",
    "            if use_adaptive_thresholds:\n",
    "                ax.axvline(np.percentile(post_var, 25), color='orange', linestyle=':', linewidth=1,\n",
    "                          label=f'25th: {np.percentile(post_var, 25):.2f} px¬≤')\n",
    "                ax.axvline(np.percentile(post_var, 50), color='gray', linestyle=':', linewidth=1,\n",
    "                          label=f'Median: {np.percentile(post_var, 50):.2f} px¬≤')\n",
    "            ax.set_xlabel('Post-saccade Position Variance (px¬≤)')\n",
    "            ax.set_ylabel('Count')\n",
    "            ax.set_title(f'Post-saccade Variance Distribution\\n'\n",
    "                        f'{\"Adaptive\" if use_adaptive_thresholds else \"Fixed\"} threshold at '\n",
    "                        f'{adaptive_percentile_post_variance if use_adaptive_thresholds else \"fixed\"}th percentile')\n",
    "            ax.legend()\n",
    "            ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 4: Post-saccade position change (as percentage of amplitude)\n",
    "        ax = axes[1, 1]\n",
    "        if len(post_change_ratio) > 0:\n",
    "            ax.hist(post_change_ratio, bins=50, alpha=0.7, color='salmon', edgecolor='black')\n",
    "            ax.axvline(post_saccade_position_change_threshold_percent, color='red', linestyle='--', \n",
    "                      linewidth=2, label=f'Threshold: {post_saccade_position_change_threshold_percent:.1f}%')\n",
    "            ax.axvline(np.percentile(post_change_ratio, 50), color='gray', linestyle=':', linewidth=1,\n",
    "                      label=f'Median: {np.percentile(post_change_ratio, 50):.1f}%')\n",
    "            ax.axvline(np.percentile(post_change_ratio, 75), color='orange', linestyle=':', linewidth=1,\n",
    "                      label=f'75th: {np.percentile(post_change_ratio, 75):.1f}%')\n",
    "            ax.set_xlabel('Post-saccade Position Change / Amplitude (%)')\n",
    "            ax.set_ylabel('Count')\n",
    "            ax.set_title(f'Post-saccade Position Change Ratio\\n'\n",
    "                        f'Fixed threshold: {post_saccade_position_change_threshold_percent:.1f}% of amplitude')\n",
    "            ax.legend()\n",
    "            ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print summary statistics\n",
    "        print(f\"\\nüìà Summary Statistics for {get_eye_label(video_key)}:\")\n",
    "        if len(pre_vel) > 0:\n",
    "            print(f\"  Pre-saccade velocity: mean={pre_vel.mean():.2f}, median={pre_vel.median():.2f}, \"\n",
    "                  f\"std={pre_vel.std():.2f} px/s\")\n",
    "        if len(pre_drift) > 0:\n",
    "            print(f\"  Pre-saccade drift: mean={pre_drift.mean():.2f}, median={pre_drift.median():.2f}, \"\n",
    "                  f\"std={pre_drift.std():.2f} px\")\n",
    "        if len(post_var) > 0:\n",
    "            print(f\"  Post-saccade variance: mean={post_var.mean():.2f}, median={post_var.median():.2f}, \"\n",
    "                  f\"std={post_var.std():.2f} px¬≤\")\n",
    "        if len(post_change_ratio) > 0:\n",
    "            print(f\"  Post-saccade change ratio: mean={post_change_ratio.mean():.1f}%, \"\n",
    "                  f\"median={post_change_ratio.median():.1f}%, std={post_change_ratio.std():.1f}%\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7f0d6d3b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# VISUALIZE AND ANALYZE SACCADE CLASSIFICATION (Orienting vs Compensatory)\n",
    "#-------------------------------------------------------------------------------\n",
    "# Create validation plots and statistical comparisons for saccade classification\n",
    "\n",
    "if plot_saccade_detection_QC:\n",
    "    for video_key, res in saccade_results.items():\n",
    "        dir_map = get_direction_map_for_video(video_key)\n",
    "        label_up = dir_map['upward']\n",
    "        label_down = dir_map['downward']\n",
    "        \n",
    "        all_saccades_df = res.get('all_saccades_df', pd.DataFrame())\n",
    "        \n",
    "        if len(all_saccades_df) == 0:\n",
    "            print(f\"\\n‚ö†Ô∏è No saccades found for {get_eye_label(video_key)}\")\n",
    "            continue\n",
    "        \n",
    "        # Check if classification was performed\n",
    "        if 'saccade_type' not in all_saccades_df.columns:\n",
    "            print(f\"\\n‚ö†Ô∏è Classification not performed for {get_eye_label(video_key)}\")\n",
    "            continue\n",
    "        \n",
    "        orienting_saccades = all_saccades_df[all_saccades_df['saccade_type'] == 'orienting']\n",
    "        compensatory_saccades = all_saccades_df[all_saccades_df['saccade_type'] == 'compensatory']\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"CLASSIFICATION ANALYSIS: {get_eye_label(video_key)}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Statistical comparisons\n",
    "        from scipy import stats\n",
    "        \n",
    "        print(f\"\\nüìä Statistical Comparisons:\")\n",
    "        print(f\"  Orienting saccades: {len(orienting_saccades)}\")\n",
    "        print(f\"  Compensatory saccades: {len(compensatory_saccades)}\")\n",
    "        \n",
    "        if len(orienting_saccades) > 0 and len(compensatory_saccades) > 0:\n",
    "            # Amplitude comparison\n",
    "            orienting_amps = orienting_saccades['amplitude'].values\n",
    "            compensatory_amps = compensatory_saccades['amplitude'].values\n",
    "            amp_stat, amp_p = stats.mannwhitneyu(orienting_amps, compensatory_amps, alternative='two-sided')\n",
    "            print(f\"\\n  Amplitude (px):\")\n",
    "            print(f\"    Orienting: {orienting_amps.mean():.2f} ¬± {orienting_amps.std():.2f} (median: {np.median(orienting_amps):.2f})\")\n",
    "            print(f\"    Compensatory: {compensatory_amps.mean():.2f} ¬± {compensatory_amps.std():.2f} (median: {np.median(compensatory_amps):.2f})\")\n",
    "            print(f\"    Mann-Whitney U test: U={amp_stat:.1f}, p={amp_p:.4f}\")\n",
    "            \n",
    "            # Duration comparison\n",
    "            orienting_durs = orienting_saccades['duration'].values\n",
    "            compensatory_durs = compensatory_saccades['duration'].values\n",
    "            dur_stat, dur_p = stats.mannwhitneyu(orienting_durs, compensatory_durs, alternative='two-sided')\n",
    "            print(f\"\\n  Duration (s):\")\n",
    "            print(f\"    Orienting: {orienting_durs.mean():.3f} ¬± {orienting_durs.std():.3f} (median: {np.median(orienting_durs):.3f})\")\n",
    "            print(f\"    Compensatory: {compensatory_durs.mean():.3f} ¬± {compensatory_durs.std():.3f} (median: {np.median(compensatory_durs):.3f})\")\n",
    "            print(f\"    Mann-Whitney U test: U={dur_stat:.1f}, p={dur_p:.4f}\")\n",
    "            \n",
    "            # Pre-saccade velocity comparison\n",
    "            orienting_pre_vel = orienting_saccades['pre_saccade_mean_velocity'].values\n",
    "            compensatory_pre_vel = compensatory_saccades['pre_saccade_mean_velocity'].values\n",
    "            pre_vel_stat, pre_vel_p = stats.mannwhitneyu(orienting_pre_vel, compensatory_pre_vel, alternative='two-sided')\n",
    "            print(f\"\\n  Pre-saccade velocity (px/s):\")\n",
    "            print(f\"    Orienting: {orienting_pre_vel.mean():.2f} ¬± {orienting_pre_vel.std():.2f} (median: {np.median(orienting_pre_vel):.2f})\")\n",
    "            print(f\"    Compensatory: {compensatory_pre_vel.mean():.2f} ¬± {compensatory_pre_vel.std():.2f} (median: {np.median(compensatory_pre_vel):.2f})\")\n",
    "            print(f\"    Mann-Whitney U test: U={pre_vel_stat:.1f}, p={pre_vel_p:.4f}\")\n",
    "            \n",
    "            # Pre-saccade drift comparison\n",
    "            orienting_pre_drift = orienting_saccades['pre_saccade_position_drift'].values\n",
    "            compensatory_pre_drift = compensatory_saccades['pre_saccade_position_drift'].values\n",
    "            pre_drift_stat, pre_drift_p = stats.mannwhitneyu(orienting_pre_drift, compensatory_pre_drift, alternative='two-sided')\n",
    "            print(f\"\\n  Pre-saccade position drift (px):\")\n",
    "            print(f\"    Orienting: {orienting_pre_drift.mean():.2f} ¬± {orienting_pre_drift.std():.2f} (median: {np.median(orienting_pre_drift):.2f})\")\n",
    "            print(f\"    Compensatory: {compensatory_pre_drift.mean():.2f} ¬± {compensatory_pre_drift.std():.2f} (median: {np.median(compensatory_pre_drift):.2f})\")\n",
    "            print(f\"    Mann-Whitney U test: U={pre_drift_stat:.1f}, p={pre_drift_p:.4f}\")\n",
    "            \n",
    "            # Post-saccade variance comparison\n",
    "            orienting_post_var = orienting_saccades['post_saccade_position_variance'].values\n",
    "            compensatory_post_var = compensatory_saccades['post_saccade_position_variance'].values\n",
    "            post_var_stat, post_var_p = stats.mannwhitneyu(orienting_post_var, compensatory_post_var, alternative='two-sided')\n",
    "            print(f\"\\n  Post-saccade position variance (px¬≤):\")\n",
    "            print(f\"    Orienting: {orienting_post_var.mean():.2f} ¬± {orienting_post_var.std():.2f} (median: {np.median(orienting_post_var):.2f})\")\n",
    "            print(f\"    Compensatory: {compensatory_post_var.mean():.2f} ¬± {compensatory_post_var.std():.2f} (median: {np.median(compensatory_post_var):.2f})\")\n",
    "            print(f\"    Mann-Whitney U test: U={post_var_stat:.1f}, p={post_var_p:.4f}\")\n",
    "            \n",
    "            # Bout size for compensatory saccades\n",
    "            if len(compensatory_saccades) > 0:\n",
    "                bout_sizes = compensatory_saccades['bout_size'].values\n",
    "                print(f\"\\n  Bout size (compensatory saccades only):\")\n",
    "                print(f\"    Mean: {bout_sizes.mean():.2f} ¬± {bout_sizes.std():.2f} saccades\")\n",
    "                print(f\"    Range: {bout_sizes.min():.0f} - {bout_sizes.max():.0f} saccades\")\n",
    "                print(f\"    Median: {np.median(bout_sizes):.0f} saccades\")\n",
    "            \n",
    "            # Classification confidence comparison\n",
    "            if 'classification_confidence' in all_saccades_df.columns:\n",
    "                orienting_conf = orienting_saccades['classification_confidence'].values\n",
    "                compensatory_conf = compensatory_saccades['classification_confidence'].values\n",
    "                conf_stat, conf_p = stats.mannwhitneyu(orienting_conf, compensatory_conf, alternative='two-sided')\n",
    "                print(f\"\\n  Classification Confidence:\")\n",
    "                print(f\"    Orienting: {orienting_conf.mean():.3f} ¬± {orienting_conf.std():.3f} (median: {np.median(orienting_conf):.3f})\")\n",
    "                print(f\"    Compensatory: {compensatory_conf.mean():.3f} ¬± {compensatory_conf.std():.3f} (median: {np.median(compensatory_conf):.3f})\")\n",
    "                print(f\"    Mann-Whitney U test: U={conf_stat:.1f}, p={conf_p:.4f}\")\n",
    "        else:\n",
    "            print(f\"  ‚ö†Ô∏è Cannot perform statistical comparisons - need both types present\")\n",
    "        \n",
    "        # Visualization\n",
    "        # Create visualization figure\n",
    "        fig_class = make_subplots(\n",
    "            rows=2, cols=3,\n",
    "            subplot_titles=(\n",
    "                'Amplitude Distribution',\n",
    "                'Duration Distribution',\n",
    "                'Pre-saccade Velocity Distribution',\n",
    "                'Pre-saccade Position Drift',\n",
    "                'Post-saccade Position Variance',\n",
    "                'Bout Size Distribution (Compensatory)'\n",
    "            ),\n",
    "            vertical_spacing=0.12,\n",
    "            horizontal_spacing=0.1\n",
    "        )\n",
    "        \n",
    "        # Row 1, Col 1: Amplitude distributions\n",
    "        if len(orienting_saccades) > 0:\n",
    "            fig_class.add_trace(\n",
    "                go.Histogram(\n",
    "                    x=orienting_saccades['amplitude'],\n",
    "                    nbinsx=30,\n",
    "                    name='Orienting',\n",
    "                    marker_color='blue',\n",
    "                    opacity=0.6,\n",
    "                    histnorm='probability'\n",
    "                ),\n",
    "                row=1, col=1\n",
    "            )\n",
    "        if len(compensatory_saccades) > 0:\n",
    "            fig_class.add_trace(\n",
    "                go.Histogram(\n",
    "                    x=compensatory_saccades['amplitude'],\n",
    "                    nbinsx=30,\n",
    "                    name='Compensatory',\n",
    "                    marker_color='orange',\n",
    "                    opacity=0.6,\n",
    "                    histnorm='probability'\n",
    "                ),\n",
    "                row=1, col=1\n",
    "            )\n",
    "        \n",
    "        # Row 1, Col 2: Duration distributions\n",
    "        if len(orienting_saccades) > 0:\n",
    "            fig_class.add_trace(\n",
    "                go.Histogram(\n",
    "                    x=orienting_saccades['duration'],\n",
    "                    nbinsx=30,\n",
    "                    name='Orienting',\n",
    "                    marker_color='blue',\n",
    "                    opacity=0.6,\n",
    "                    showlegend=False,\n",
    "                    histnorm='probability'\n",
    "                ),\n",
    "                row=1, col=2\n",
    "            )\n",
    "        if len(compensatory_saccades) > 0:\n",
    "            fig_class.add_trace(\n",
    "                go.Histogram(\n",
    "                    x=compensatory_saccades['duration'],\n",
    "                    nbinsx=30,\n",
    "                    name='Compensatory',\n",
    "                    marker_color='orange',\n",
    "                    opacity=0.6,\n",
    "                    showlegend=False,\n",
    "                    histnorm='probability'\n",
    "                ),\n",
    "                row=1, col=2\n",
    "            )\n",
    "        \n",
    "        # Row 1, Col 3: Pre-saccade velocity distributions\n",
    "        if len(orienting_saccades) > 0:\n",
    "            fig_class.add_trace(\n",
    "                go.Histogram(\n",
    "                    x=orienting_saccades['pre_saccade_mean_velocity'],\n",
    "                    nbinsx=30,\n",
    "                    name='Orienting',\n",
    "                    marker_color='blue',\n",
    "                    opacity=0.6,\n",
    "                    showlegend=False,\n",
    "                    histnorm='probability'\n",
    "                ),\n",
    "                row=1, col=3\n",
    "            )\n",
    "        if len(compensatory_saccades) > 0:\n",
    "            fig_class.add_trace(\n",
    "                go.Histogram(\n",
    "                    x=compensatory_saccades['pre_saccade_mean_velocity'],\n",
    "                    nbinsx=30,\n",
    "                    name='Compensatory',\n",
    "                    marker_color='orange',\n",
    "                    opacity=0.6,\n",
    "                    showlegend=False,\n",
    "                    histnorm='probability'\n",
    "                ),\n",
    "                row=1, col=3\n",
    "            )\n",
    "        \n",
    "        # Row 2, Col 1: Pre-saccade drift distributions\n",
    "        if len(orienting_saccades) > 0:\n",
    "            fig_class.add_trace(\n",
    "                go.Histogram(\n",
    "                    x=orienting_saccades['pre_saccade_position_drift'],\n",
    "                    nbinsx=30,\n",
    "                    name='Orienting',\n",
    "                    marker_color='blue',\n",
    "                    opacity=0.6,\n",
    "                    showlegend=False,\n",
    "                    histnorm='probability'\n",
    "                ),\n",
    "                row=2, col=1\n",
    "            )\n",
    "        if len(compensatory_saccades) > 0:\n",
    "            fig_class.add_trace(\n",
    "                go.Histogram(\n",
    "                    x=compensatory_saccades['pre_saccade_position_drift'],\n",
    "                    nbinsx=30,\n",
    "                    name='Compensatory',\n",
    "                    marker_color='orange',\n",
    "                    opacity=0.6,\n",
    "                    showlegend=False,\n",
    "                    histnorm='probability'\n",
    "                ),\n",
    "                row=2, col=1\n",
    "            )\n",
    "        \n",
    "        # Row 2, Col 2: Post-saccade variance distributions\n",
    "        if len(orienting_saccades) > 0:\n",
    "            fig_class.add_trace(\n",
    "                go.Histogram(\n",
    "                    x=orienting_saccades['post_saccade_position_variance'],\n",
    "                    nbinsx=30,\n",
    "                    name='Orienting',\n",
    "                    marker_color='blue',\n",
    "                    opacity=0.6,\n",
    "                    showlegend=False,\n",
    "                    histnorm='probability'\n",
    "                ),\n",
    "                row=2, col=2\n",
    "            )\n",
    "        if len(compensatory_saccades) > 0:\n",
    "            fig_class.add_trace(\n",
    "                go.Histogram(\n",
    "                    x=compensatory_saccades['post_saccade_position_variance'],\n",
    "                    nbinsx=30,\n",
    "                    name='Compensatory',\n",
    "                    marker_color='orange',\n",
    "                    opacity=0.6,\n",
    "                    showlegend=False,\n",
    "                    histnorm='probability'\n",
    "                ),\n",
    "                row=2, col=2\n",
    "            )\n",
    "        \n",
    "        # Row 2, Col 3: Bout size distribution (compensatory only)\n",
    "        if len(compensatory_saccades) > 0:\n",
    "            fig_class.add_trace(\n",
    "                go.Histogram(\n",
    "                    x=compensatory_saccades['bout_size'],\n",
    "                    nbinsx=20,\n",
    "                    name='Compensatory Bout Size',\n",
    "                    marker_color='orange',\n",
    "                    opacity=0.6,\n",
    "                    showlegend=False,\n",
    "                    histnorm='probability'\n",
    "                ),\n",
    "                row=2, col=3\n",
    "            )\n",
    "        else:\n",
    "            # Add empty trace to maintain layout\n",
    "            fig_class.add_trace(\n",
    "                go.Histogram(x=[], name='No compensatory saccades'),\n",
    "                row=2, col=3\n",
    "            )\n",
    "        \n",
    "        # Update layout\n",
    "        fig_class.update_layout(\n",
    "            title_text=f'Saccade Classification Analysis: Orienting vs Compensatory ({get_eye_label(video_key)})',\n",
    "            height=800,\n",
    "            showlegend=True,\n",
    "            legend=dict(x=0.02, y=0.98)\n",
    "        )\n",
    "        \n",
    "        # Update axes labels\n",
    "        fig_class.update_xaxes(title_text=\"Amplitude (px)\", row=1, col=1)\n",
    "        fig_class.update_xaxes(title_text=\"Duration (s)\", row=1, col=2)\n",
    "        fig_class.update_xaxes(title_text=\"Velocity (px/s)\", row=1, col=3)\n",
    "        fig_class.update_xaxes(title_text=\"Drift (px)\", row=2, col=1)\n",
    "        fig_class.update_xaxes(title_text=\"Variance (px¬≤)\", row=2, col=2)\n",
    "        fig_class.update_xaxes(title_text=\"Bout Size (saccades)\", row=2, col=3)\n",
    "        \n",
    "        fig_class.update_yaxes(title_text=\"Probability\", row=1, col=1)\n",
    "        fig_class.update_yaxes(title_text=\"Probability\", row=1, col=2)\n",
    "        fig_class.update_yaxes(title_text=\"Probability\", row=1, col=3)\n",
    "        fig_class.update_yaxes(title_text=\"Probability\", row=2, col=1)\n",
    "        fig_class.update_yaxes(title_text=\"Probability\", row=2, col=2)\n",
    "        fig_class.update_yaxes(title_text=\"Probability\", row=2, col=3)\n",
    "        \n",
    "        fig_class.show()\n",
    "        \n",
    "        # Confidence distribution visualization\n",
    "        if 'classification_confidence' in all_saccades_df.columns:\n",
    "            fig_conf = make_subplots(\n",
    "                rows=1, cols=2,\n",
    "                subplot_titles=(\n",
    "                    'Classification Confidence Distribution',\n",
    "                    'Confidence by Saccade Type'\n",
    "                ),\n",
    "                horizontal_spacing=0.15\n",
    "            )\n",
    "            \n",
    "            # Overall confidence distribution\n",
    "            fig_conf.add_trace(\n",
    "                go.Histogram(\n",
    "                    x=all_saccades_df['classification_confidence'],\n",
    "                    nbinsx=30,\n",
    "                    name='All Saccades',\n",
    "                    marker_color='gray',\n",
    "                    opacity=0.7,\n",
    "                    histnorm='probability'\n",
    "                ),\n",
    "                row=1, col=1\n",
    "            )\n",
    "            \n",
    "            # Add vertical lines for confidence thresholds\n",
    "            fig_conf.add_vline(\n",
    "                x=0.7, line_dash=\"dash\", line_color=\"green\", \n",
    "                annotation_text=\"High (‚â•0.7)\", row=1, col=1\n",
    "            )\n",
    "            fig_conf.add_vline(\n",
    "                x=0.4, line_dash=\"dash\", line_color=\"orange\", \n",
    "                annotation_text=\"Medium (0.4-0.7)\", row=1, col=1\n",
    "            )\n",
    "            \n",
    "            # Confidence by type\n",
    "            if len(orienting_saccades) > 0:\n",
    "                fig_conf.add_trace(\n",
    "                    go.Histogram(\n",
    "                        x=orienting_saccades['classification_confidence'],\n",
    "                        nbinsx=30,\n",
    "                        name='Orienting',\n",
    "                        marker_color='blue',\n",
    "                        opacity=0.6,\n",
    "                        histnorm='probability'\n",
    "                    ),\n",
    "                    row=1, col=2\n",
    "                )\n",
    "            if len(compensatory_saccades) > 0:\n",
    "                fig_conf.add_trace(\n",
    "                    go.Histogram(\n",
    "                        x=compensatory_saccades['classification_confidence'],\n",
    "                        nbinsx=30,\n",
    "                        name='Compensatory',\n",
    "                        marker_color='orange',\n",
    "                        opacity=0.6,\n",
    "                        histnorm='probability'\n",
    "                    ),\n",
    "                    row=1, col=2\n",
    "                )\n",
    "            \n",
    "            fig_conf.update_layout(\n",
    "                title_text=f'Classification Confidence Analysis ({get_eye_label(video_key)})',\n",
    "                height=400,\n",
    "                showlegend=True,\n",
    "                legend=dict(x=0.02, y=0.98)\n",
    "            )\n",
    "            \n",
    "            fig_conf.update_xaxes(title_text=\"Confidence Score\", row=1, col=1)\n",
    "            fig_conf.update_xaxes(title_text=\"Confidence Score\", row=1, col=2)\n",
    "            fig_conf.update_yaxes(title_text=\"Probability\", row=1, col=1)\n",
    "            fig_conf.update_yaxes(title_text=\"Probability\", row=1, col=2)\n",
    "            \n",
    "            fig_conf.show()\n",
    "        \n",
    "        # Time series visualization with classification\n",
    "        fig_ts = make_subplots(\n",
    "            rows=2, cols=1,\n",
    "            shared_xaxes=True,\n",
    "            vertical_spacing=0.1,\n",
    "            subplot_titles=('X Position (px)', 'Velocity (px/s) with Classified Saccades')\n",
    "        )\n",
    "        \n",
    "        # Add position trace\n",
    "        fig_ts.add_trace(\n",
    "            go.Scatter(\n",
    "                x=res['df']['Seconds'],\n",
    "                y=res['df']['X_smooth'],\n",
    "                mode='lines',\n",
    "                name='Smoothed X',\n",
    "                line=dict(color='blue', width=2)\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        # Add velocity trace\n",
    "        fig_ts.add_trace(\n",
    "            go.Scatter(\n",
    "                x=res['df']['Seconds'],\n",
    "                y=res['df']['vel_x_smooth'],\n",
    "                mode='lines',\n",
    "                name='Smoothed Velocity',\n",
    "                line=dict(color='red', width=2)\n",
    "            ),\n",
    "            row=2, col=1\n",
    "        )\n",
    "        \n",
    "        # Add adaptive threshold lines\n",
    "        fig_ts.add_hline(\n",
    "            y=res['vel_thresh'],\n",
    "            line_dash=\"dash\",\n",
    "            line_color=\"green\",\n",
    "            opacity=0.5,\n",
    "            annotation_text=f\"Adaptive threshold (¬±{res['vel_thresh']:.0f} px/s)\",\n",
    "            row=2, col=1\n",
    "        )\n",
    "        fig_ts.add_hline(\n",
    "            y=-res['vel_thresh'],\n",
    "            line_dash=\"dash\",\n",
    "            line_color=\"green\",\n",
    "            opacity=0.5,\n",
    "            row=2, col=1\n",
    "        )\n",
    "        \n",
    "        # Calculate position y-axis range for vertical lines\n",
    "        pos_max = res['df']['X_smooth'].max()\n",
    "        pos_min = res['df']['X_smooth'].min()\n",
    "        pos_range = pos_max - pos_min\n",
    "        # Add small padding to ensure lines span full visible range\n",
    "        pos_padding = pos_range * 0.05\n",
    "        pos_y_min = pos_min - pos_padding\n",
    "        pos_y_max = pos_max + pos_padding\n",
    "        \n",
    "        # Plot orienting saccades (blue) as rectangles on position trace\n",
    "        orienting_in_df = all_saccades_df[all_saccades_df['saccade_type'] == 'orienting']\n",
    "        if len(orienting_in_df) > 0:\n",
    "            for idx, row in orienting_in_df.iterrows():\n",
    "                start_time = row['start_time']\n",
    "                end_time = row['end_time']\n",
    "                \n",
    "                # Use rectangle with thin border to show duration span more efficiently\n",
    "                # Opacity must be set via rgba color, not in line dict\n",
    "                fig_ts.add_shape(\n",
    "                    type=\"rect\",\n",
    "                    x0=start_time, y0=pos_y_min,\n",
    "                    x1=end_time, y1=pos_y_max,\n",
    "                    fillcolor='rgba(0,0,255,0.1)',  # Light blue fill with opacity\n",
    "                    line=dict(color='rgba(0,0,255,0.3)', width=2),  # Blue border with opacity via rgba\n",
    "                    row=1, col=1\n",
    "                )\n",
    "            \n",
    "            # Legend entry\n",
    "            fig_ts.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=[None], y=[None],\n",
    "                    mode='lines',\n",
    "                    name='Orienting Saccades',\n",
    "                    line=dict(color='rgba(0,0,255,0.3)', width=3)  # Blue with opacity via rgba\n",
    "                ),\n",
    "                row=1, col=1\n",
    "            )\n",
    "        \n",
    "        # Plot compensatory saccades (orange) as rectangles on position trace\n",
    "        compensatory_in_df = all_saccades_df[all_saccades_df['saccade_type'] == 'compensatory']\n",
    "        if len(compensatory_in_df) > 0:\n",
    "            for idx, row in compensatory_in_df.iterrows():\n",
    "                start_time = row['start_time']\n",
    "                end_time = row['end_time']\n",
    "                \n",
    "                # Use rectangle with thin border to show duration span more efficiently\n",
    "                # Opacity must be set via rgba color, not in line dict\n",
    "                fig_ts.add_shape(\n",
    "                    type=\"rect\",\n",
    "                    x0=start_time, y0=pos_y_min,\n",
    "                    x1=end_time, y1=pos_y_max,\n",
    "                    fillcolor='rgba(255,165,0,0.1)',  # Light orange fill with opacity\n",
    "                    line=dict(color='rgba(255,165,0,0.3)', width=2),  # Orange border with opacity via rgba\n",
    "                    row=1, col=1\n",
    "                )\n",
    "            \n",
    "            # Legend entry\n",
    "            fig_ts.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=[None], y=[None],\n",
    "                    mode='lines',\n",
    "                    name='Compensatory Saccades',\n",
    "                    line=dict(color='rgba(255,165,0,0.3)', width=3)  # Orange with opacity via rgba\n",
    "                ),\n",
    "                row=1, col=1\n",
    "            )\n",
    "        \n",
    "        # Update layout\n",
    "        fig_ts.update_layout(\n",
    "            title=f'Time Series with Saccade Classification ({get_eye_label(video_key)})<br><sub>Blue: Orienting, Orange: Compensatory</sub>',\n",
    "            height=600,\n",
    "            showlegend=True,\n",
    "            legend=dict(x=0.01, y=0.99)\n",
    "        )\n",
    "        \n",
    "        # Update axes\n",
    "        fig_ts.update_xaxes(title_text=\"Time (s)\", row=2, col=1)\n",
    "        fig_ts.update_yaxes(title_text=\"X Position (px)\", row=1, col=1)\n",
    "        fig_ts.update_yaxes(title_text=\"Velocity (px/s)\", row=2, col=1)\n",
    "        \n",
    "        fig_ts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "75910404",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Extracting ML features for VideoData1...\n",
      "================================================================================\n",
      "\n",
      "üìä Extracting ML features for 413 saccades...\n",
      "   Experiment ID: Cohort3_20250409_Cohort3_rotation_B6J2782_2025-04-28\n",
      "   Video Label: VideoData1 (L: Left)\n",
      "   Category A: Basic properties...\n",
      "   Category B: Pre-saccade features...\n",
      "   Category C: Post-saccade features...\n",
      "   Category D: Temporal context features...\n",
      "   Category G: Amplitude & direction consistency...\n",
      "   Category H: Rule-based classification features...\n",
      "      Found saccade_type values: ['compensatory' 'orienting']\n",
      "      Mapped rule_based_class values: [0 1]\n",
      "      Class counts: {0: 347, 1: 66}\n",
      "‚úÖ Feature extraction complete: 413 saccades, 36 features\n",
      "   Feature columns: ['experiment_id', 'saccade_id', 'video_label', 'amplitude', 'duration', 'peak_velocity', 'direction', 'start_time', 'end_time', 'time', 'pre_saccade_mean_velocity', 'pre_saccade_position_drift', 'pre_saccade_position_variance', 'pre_saccade_drift_rate', 'pre_saccade_window_duration', 'post_saccade_position_variance', 'post_saccade_position_change', 'post_saccade_position_change_pct', 'post_saccade_mean_velocity', 'post_saccade_drift_rate', 'post_saccade_window_duration', 'bout_size', 'time_since_previous_saccade', 'time_until_next_saccade', 'position_in_bout', 'is_first_in_bout', 'is_last_in_bout', 'is_isolated', 'bout_duration', 'inter_saccade_interval_mean', 'inter_saccade_interval_std', 'amplitude_relative_to_bout_mean', 'amplitude_consistency_in_bout', 'direction_relative_to_previous', 'rule_based_class', 'rule_based_confidence']\n",
      "‚úÖ Extracted 413 saccades with 37 features\n",
      "\n",
      "================================================================================\n",
      "Extracting ML features for VideoData2...\n",
      "================================================================================\n",
      "\n",
      "üìä Extracting ML features for 466 saccades...\n",
      "   Experiment ID: Cohort3_20250409_Cohort3_rotation_B6J2782_2025-04-28\n",
      "   Video Label: VideoData2 (R: Right)\n",
      "   Category A: Basic properties...\n",
      "   Category B: Pre-saccade features...\n",
      "   Category C: Post-saccade features...\n",
      "   Category D: Temporal context features...\n",
      "   Category G: Amplitude & direction consistency...\n",
      "   Category H: Rule-based classification features...\n",
      "      Found saccade_type values: ['compensatory' 'orienting']\n",
      "      Mapped rule_based_class values: [0 1]\n",
      "      Class counts: {0: 389, 1: 77}\n",
      "‚úÖ Feature extraction complete: 466 saccades, 36 features\n",
      "   Feature columns: ['experiment_id', 'saccade_id', 'video_label', 'amplitude', 'duration', 'peak_velocity', 'direction', 'start_time', 'end_time', 'time', 'pre_saccade_mean_velocity', 'pre_saccade_position_drift', 'pre_saccade_position_variance', 'pre_saccade_drift_rate', 'pre_saccade_window_duration', 'post_saccade_position_variance', 'post_saccade_position_change', 'post_saccade_position_change_pct', 'post_saccade_mean_velocity', 'post_saccade_drift_rate', 'post_saccade_window_duration', 'bout_size', 'time_since_previous_saccade', 'time_until_next_saccade', 'position_in_bout', 'is_first_in_bout', 'is_last_in_bout', 'is_isolated', 'bout_duration', 'inter_saccade_interval_mean', 'inter_saccade_interval_std', 'amplitude_relative_to_bout_mean', 'amplitude_consistency_in_bout', 'direction_relative_to_previous', 'rule_based_class', 'rule_based_confidence']\n",
      "‚úÖ Extracted 466 saccades with 37 features\n",
      "\n",
      "‚úÖ Combined features: 879 total saccades (413 left, 466 right)\n"
     ]
    }
   ],
   "source": [
    "# ML Feature Extraction and Visualization\n",
    "# Extract features for ML classification and visualize distributions\n",
    "############################################################################################################\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sleap.ml_feature_extraction import extract_ml_features\n",
    "from sleap.visualization import visualize_ml_features\n",
    "\n",
    "# Extract features for VideoData1 (if available)\n",
    "features_v1 = None\n",
    "if VideoData1_Has_Sleap and 'VideoData1' in saccade_results:\n",
    "    print(\"=\"*80)\n",
    "    print(\"Extracting ML features for VideoData1...\")\n",
    "    print(\"=\"*80)\n",
    "    # Use the processed df from saccade_results (has X_smooth, vel_x_smooth columns)\n",
    "    df1_processed = saccade_results['VideoData1']['df']\n",
    "    features_v1 = extract_ml_features(\n",
    "        saccade_results=saccade_results['VideoData1'],\n",
    "        df=df1_processed,  # Use processed df with X_smooth, vel_x_smooth\n",
    "        fps=FPS_1,\n",
    "        data_path=data_path,\n",
    "        verbose=True\n",
    "    )\n",
    "    if len(features_v1) > 0:\n",
    "        features_v1['eye'] = 'Left' if video1_eye == 'L' else 'Right'\n",
    "        print(f\"‚úÖ Extracted {len(features_v1)} saccades with {len(features_v1.columns)} features\")\n",
    "\n",
    "# Extract features for VideoData2 (if available)\n",
    "features_v2 = None\n",
    "if VideoData2_Has_Sleap and 'VideoData2' in saccade_results:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Extracting ML features for VideoData2...\")\n",
    "    print(\"=\"*80)\n",
    "    # Use the processed df from saccade_results (has X_smooth, vel_x_smooth columns)\n",
    "    df2_processed = saccade_results['VideoData2']['df']\n",
    "    features_v2 = extract_ml_features(\n",
    "        saccade_results=saccade_results['VideoData2'],\n",
    "        df=df2_processed,  # Use processed df with X_smooth, vel_x_smooth\n",
    "        fps=FPS_2,\n",
    "        data_path=data_path,\n",
    "        verbose=True\n",
    "    )\n",
    "    if len(features_v2) > 0:\n",
    "        features_v2['eye'] = 'Right' if video1_eye == 'L' else 'Left'\n",
    "        print(f\"‚úÖ Extracted {len(features_v2)} saccades with {len(features_v2.columns)} features\")\n",
    "\n",
    "# Combine features from both eyes\n",
    "if features_v1 is not None and features_v2 is not None:\n",
    "    features_combined = pd.concat([features_v1, features_v2], ignore_index=True)\n",
    "    print(f\"\\n‚úÖ Combined features: {len(features_combined)} total saccades ({len(features_v1)} left, {len(features_v2)} right)\")\n",
    "elif features_v1 is not None:\n",
    "    features_combined = features_v1.copy()\n",
    "    print(f\"\\n‚úÖ Using VideoData1 only: {len(features_combined)} saccades\")\n",
    "elif features_v2 is not None:\n",
    "    features_combined = features_v2.copy()\n",
    "    print(f\"\\n‚úÖ Using VideoData2 only: {len(features_combined)} saccades\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No features extracted - check if saccades were detected\")\n",
    "    features_combined = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b7f63389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Preparing visualizations for 33 numeric features...\n",
      "   Total saccades: 879\n",
      "   By eye: {'Right': 466, 'Left': 413}\n",
      "   By rule-based class: {0: 736, 1: 143}\n",
      "\n",
      "================================================================================\n",
      "üîç DIAGNOSTIC: Checking data before visualization\n",
      "================================================================================\n",
      "‚úÖ rule_based_class column found\n",
      "   Unique rule_based_class values: [0, 1]\n",
      "   Unique rule_based_class_label values: ['Compensatory' 'Orienting']\n",
      "   Counts by label:\n",
      "     - Compensatory: 736\n",
      "     - Orienting: 143\n",
      "\n",
      "‚úÖ eye column found\n",
      "   Unique eye values: ['Left' 'Right']\n",
      "   Counts by eye:\n",
      "     - Right: 466\n",
      "     - Left: 413\n",
      "\n",
      "‚úÖ Numeric features check\n",
      "   Total numeric features: 33\n",
      "   Features normalized: 32\n",
      "   Features excluded from normalization: 1\n",
      "   Total rows in features_normalized: 879\n",
      "   First 10 numeric features: ['amplitude', 'duration', 'peak_velocity', 'direction', 'start_time', 'end_time', 'time', 'pre_saccade_mean_velocity', 'pre_saccade_position_drift', 'pre_saccade_position_variance']\n",
      "\n",
      "================================================================================\n",
      "PANEL 1: Feature Distributions by Category (Violin Plots)\n",
      "================================================================================\n",
      "\n",
      "üîç DIAGNOSTIC: Preparing Panel 1 data\n",
      "   Iterating over 879 rows\n",
      "   Processing 32 normalized features (excluding rule_based_class)\n",
      "   ‚úÖ Processed 879 rows\n",
      "   ‚úÖ Created 27807 data points\n",
      "   ‚úÖ Features with non-NaN values: 27807\n",
      "   ‚úÖ plot_df shape: (27807, 5)\n",
      "   ‚úÖ Columns in plot_df: ['Feature', 'Category', 'Value', 'Eye', 'Rule_Based_Class']\n",
      "   ‚úÖ Unique Eye values: ['Left' 'Right']\n",
      "   ‚úÖ Unique Rule_Based_Class values: ['Compensatory' 'Orienting']\n",
      "   ‚úÖ Unique Categories: ['Category A: Basic Properties' 'Category B: Pre-Saccade'\n",
      " 'Category C: Post-Saccade' 'Category D: Temporal Context'\n",
      " 'Category G: Amplitude/Direction Consistency'\n",
      " 'Category H: Rule-Based Classification']\n",
      "   ‚úÖ Sample data (first 3 rows):\n",
      "         Feature                      Category     Value   Eye Rule_Based_Class\n",
      "0      amplitude  Category A: Basic Properties -0.876991  Left     Compensatory\n",
      "1       duration  Category A: Basic Properties  0.095427  Left     Compensatory\n",
      "2  peak_velocity  Category A: Basic Properties  1.151539  Left     Compensatory\n",
      "\n",
      "üîç DIAGNOSTIC: Panel 1 categories\n",
      "   Total categories defined: 7\n",
      "   Categories with data: 6\n",
      "   ‚úÖ Category A: Basic Properties: 6153 data points, 7 unique features\n",
      "   ‚úÖ Category B: Pre-Saccade: 4392 data points, 5 unique features\n",
      "      Features: ['pre_saccade_mean_velocity', 'pre_saccade_position_drift', 'pre_saccade_position_variance', 'pre_saccade_drift_rate', 'pre_saccade_window_duration']\n",
      "   ‚úÖ Category C: Post-Saccade: 5248 data points, 6 unique features\n",
      "   ‚úÖ Category D: Temporal Context: 8498 data points, 10 unique features\n",
      "   ‚úÖ Category G: Amplitude/Direction Consistency: 2637 data points, 3 unique features\n",
      "      Features: ['amplitude_relative_to_bout_mean', 'amplitude_consistency_in_bout', 'direction_relative_to_previous']\n",
      "   ‚úÖ Category H: Rule-Based Classification: 879 data points, 1 unique features\n",
      "      Features: ['rule_based_confidence']\n",
      "   ‚ùå Other: NO DATA\n",
      "\n",
      "   Creating Panel 1 figure: 3 rows x 2 cols = 6 subplots\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rancze/Documents/GitHub/GitHub - CURSOR/vestibular_vr_pipeline/sleap/visualization.py:132: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[ 1.87907632 -0.53217636 -0.53217636 -0.53217636 -0.53217636  1.87907632\n",
      " -0.53217636 -0.53217636  1.87907632 -0.53217636 -0.53217636 -0.53217636\n",
      " -0.53217636  1.87907632 -0.53217636 -0.53217636  1.87907632  1.87907632\n",
      " -0.53217636 -0.53217636 -0.53217636  1.87907632 -0.53217636 -0.53217636\n",
      " -0.53217636 -0.53217636 -0.53217636  1.87907632  1.87907632 -0.53217636\n",
      " -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636\n",
      "  1.87907632 -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636\n",
      "  1.87907632 -0.53217636 -0.53217636 -0.53217636 -0.53217636  1.87907632\n",
      " -0.53217636 -0.53217636  1.87907632 -0.53217636 -0.53217636 -0.53217636\n",
      " -0.53217636 -0.53217636 -0.53217636 -0.53217636  1.87907632 -0.53217636\n",
      "  1.87907632  1.87907632 -0.53217636 -0.53217636 -0.53217636 -0.53217636\n",
      " -0.53217636  1.87907632  1.87907632 -0.53217636 -0.53217636 -0.53217636\n",
      "  1.87907632  1.87907632  1.87907632 -0.53217636 -0.53217636 -0.53217636\n",
      " -0.53217636  1.87907632  1.87907632  1.87907632  1.87907632 -0.53217636\n",
      "  1.87907632  1.87907632  1.87907632  1.87907632  1.87907632  1.87907632\n",
      "  1.87907632 -0.53217636  1.87907632  1.87907632  1.87907632 -0.53217636\n",
      "  1.87907632  1.87907632 -0.53217636 -0.53217636  1.87907632 -0.53217636\n",
      " -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636\n",
      "  1.87907632 -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636\n",
      "  1.87907632  1.87907632 -0.53217636 -0.53217636  1.87907632 -0.53217636\n",
      " -0.53217636  1.87907632 -0.53217636 -0.53217636  1.87907632 -0.53217636\n",
      " -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636\n",
      " -0.53217636 -0.53217636 -0.53217636 -0.53217636  1.87907632 -0.53217636\n",
      " -0.53217636 -0.53217636 -0.53217636  1.87907632 -0.53217636 -0.53217636\n",
      " -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636\n",
      " -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636\n",
      "  1.87907632 -0.53217636  1.87907632 -0.53217636 -0.53217636 -0.53217636\n",
      " -0.53217636 -0.53217636 -0.53217636  1.87907632  1.87907632 -0.53217636\n",
      " -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636  1.87907632\n",
      " -0.53217636 -0.53217636  1.87907632 -0.53217636  1.87907632 -0.53217636\n",
      " -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636\n",
      " -0.53217636  1.87907632 -0.53217636 -0.53217636 -0.53217636 -0.53217636\n",
      " -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636\n",
      "  1.87907632 -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636\n",
      " -0.53217636  1.87907632 -0.53217636 -0.53217636 -0.53217636 -0.53217636\n",
      " -0.53217636 -0.53217636 -0.53217636 -0.53217636  1.87907632 -0.53217636\n",
      " -0.53217636 -0.53217636 -0.53217636 -0.53217636  1.87907632 -0.53217636\n",
      " -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636\n",
      " -0.53217636  1.87907632 -0.53217636  1.87907632 -0.53217636 -0.53217636\n",
      " -0.53217636  1.87907632 -0.53217636 -0.53217636 -0.53217636 -0.53217636\n",
      " -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636\n",
      " -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636\n",
      " -0.53217636  1.87907632  1.87907632 -0.53217636  1.87907632 -0.53217636\n",
      " -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636\n",
      "  1.87907632 -0.53217636 -0.53217636 -0.53217636  1.87907632 -0.53217636\n",
      " -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636\n",
      "  1.87907632 -0.53217636  1.87907632 -0.53217636 -0.53217636  1.87907632\n",
      " -0.53217636  1.87907632  1.87907632  1.87907632 -0.53217636 -0.53217636\n",
      " -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636\n",
      "  1.87907632 -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636\n",
      " -0.53217636 -0.53217636 -0.53217636 -0.53217636  1.87907632 -0.53217636\n",
      " -0.53217636 -0.53217636  1.87907632 -0.53217636  1.87907632 -0.53217636\n",
      " -0.53217636 -0.53217636  1.87907632 -0.53217636  1.87907632 -0.53217636\n",
      " -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636\n",
      " -0.53217636 -0.53217636 -0.53217636 -0.53217636  1.87907632  1.87907632\n",
      " -0.53217636  1.87907632 -0.53217636  1.87907632 -0.53217636 -0.53217636\n",
      " -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636\n",
      "  1.87907632 -0.53217636 -0.53217636 -0.53217636  1.87907632 -0.53217636\n",
      "  1.87907632 -0.53217636  1.87907632 -0.53217636 -0.53217636 -0.53217636\n",
      " -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636\n",
      " -0.53217636 -0.53217636 -0.53217636  1.87907632  1.87907632  1.87907632\n",
      " -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636\n",
      "  1.87907632  1.87907632 -0.53217636 -0.53217636 -0.53217636 -0.53217636\n",
      " -0.53217636 -0.53217636 -0.53217636 -0.53217636  1.87907632 -0.53217636\n",
      " -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636  1.87907632\n",
      "  1.87907632 -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636\n",
      " -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636  1.87907632\n",
      " -0.53217636 -0.53217636  1.87907632 -0.53217636  1.87907632 -0.53217636\n",
      "  1.87907632 -0.53217636  1.87907632 -0.53217636 -0.53217636 -0.53217636\n",
      " -0.53217636  1.87907632 -0.53217636 -0.53217636 -0.53217636  1.87907632\n",
      " -0.53217636 -0.53217636  1.87907632 -0.53217636 -0.53217636 -0.53217636\n",
      " -0.53217636 -0.53217636 -0.53217636  1.87907632 -0.53217636 -0.53217636\n",
      "  1.87907632 -0.53217636  1.87907632 -0.53217636 -0.53217636 -0.53217636\n",
      " -0.53217636 -0.53217636  1.87907632 -0.53217636 -0.53217636 -0.53217636\n",
      "  1.87907632  1.87907632 -0.53217636 -0.53217636 -0.53217636 -0.53217636\n",
      " -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636  1.87907632\n",
      " -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636\n",
      "  1.87907632 -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636\n",
      "  1.87907632 -0.53217636 -0.53217636  1.87907632 -0.53217636 -0.53217636\n",
      " -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636  1.87907632\n",
      " -0.53217636  1.87907632 -0.53217636 -0.53217636 -0.53217636 -0.53217636\n",
      " -0.53217636  1.87907632  1.87907632 -0.53217636 -0.53217636 -0.53217636\n",
      " -0.53217636 -0.53217636  1.87907632 -0.53217636 -0.53217636 -0.53217636\n",
      " -0.53217636 -0.53217636 -0.53217636 -0.53217636  1.87907632  1.87907632\n",
      "  1.87907632  1.87907632 -0.53217636  1.87907632  1.87907632  1.87907632\n",
      "  1.87907632  1.87907632  1.87907632  1.87907632  1.87907632  1.87907632\n",
      "  1.87907632  1.87907632 -0.53217636 -0.53217636  1.87907632 -0.53217636\n",
      " -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636\n",
      " -0.53217636  1.87907632 -0.53217636 -0.53217636 -0.53217636 -0.53217636\n",
      " -0.53217636  1.87907632 -0.53217636  1.87907632  1.87907632  1.87907632\n",
      " -0.53217636  1.87907632 -0.53217636 -0.53217636 -0.53217636 -0.53217636\n",
      "  1.87907632 -0.53217636  1.87907632 -0.53217636  1.87907632 -0.53217636\n",
      "  1.87907632 -0.53217636 -0.53217636  1.87907632 -0.53217636 -0.53217636\n",
      "  1.87907632 -0.53217636  1.87907632 -0.53217636 -0.53217636  1.87907632\n",
      "  1.87907632 -0.53217636 -0.53217636 -0.53217636 -0.53217636  1.87907632\n",
      " -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636\n",
      " -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636\n",
      " -0.53217636 -0.53217636  1.87907632 -0.53217636 -0.53217636 -0.53217636\n",
      " -0.53217636  1.87907632 -0.53217636 -0.53217636 -0.53217636 -0.53217636\n",
      " -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636\n",
      " -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636\n",
      "  1.87907632 -0.53217636 -0.53217636  1.87907632 -0.53217636  1.87907632\n",
      " -0.53217636 -0.53217636 -0.53217636 -0.53217636  1.87907632 -0.53217636\n",
      " -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636\n",
      " -0.53217636 -0.53217636  1.87907632 -0.53217636  1.87907632 -0.53217636\n",
      " -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636\n",
      " -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636\n",
      " -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636\n",
      " -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636\n",
      " -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636\n",
      " -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636\n",
      " -0.53217636 -0.53217636 -0.53217636 -0.53217636  1.87907632 -0.53217636\n",
      " -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636\n",
      " -0.53217636 -0.53217636  1.87907632 -0.53217636 -0.53217636 -0.53217636\n",
      " -0.53217636 -0.53217636  1.87907632 -0.53217636 -0.53217636 -0.53217636\n",
      " -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636\n",
      " -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636\n",
      " -0.53217636  1.87907632  1.87907632  1.87907632 -0.53217636  1.87907632\n",
      " -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636\n",
      " -0.53217636 -0.53217636 -0.53217636  1.87907632 -0.53217636 -0.53217636\n",
      " -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636\n",
      " -0.53217636 -0.53217636  1.87907632 -0.53217636  1.87907632 -0.53217636\n",
      " -0.53217636  1.87907632 -0.53217636 -0.53217636 -0.53217636  1.87907632\n",
      " -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636\n",
      " -0.53217636 -0.53217636 -0.53217636  1.87907632 -0.53217636 -0.53217636\n",
      " -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636\n",
      " -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636\n",
      " -0.53217636  1.87907632  1.87907632 -0.53217636 -0.53217636  1.87907632\n",
      " -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636  1.87907632\n",
      " -0.53217636 -0.53217636  1.87907632 -0.53217636 -0.53217636  1.87907632\n",
      " -0.53217636 -0.53217636  1.87907632 -0.53217636  1.87907632 -0.53217636\n",
      " -0.53217636  1.87907632 -0.53217636  1.87907632 -0.53217636 -0.53217636\n",
      " -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636\n",
      " -0.53217636 -0.53217636  1.87907632  1.87907632 -0.53217636  1.87907632\n",
      " -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636\n",
      "  1.87907632 -0.53217636 -0.53217636  1.87907632 -0.53217636  1.87907632\n",
      " -0.53217636 -0.53217636  1.87907632 -0.53217636  1.87907632 -0.53217636\n",
      " -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636\n",
      " -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636\n",
      "  1.87907632 -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636\n",
      " -0.53217636  1.87907632 -0.53217636 -0.53217636 -0.53217636 -0.53217636\n",
      " -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636\n",
      " -0.53217636 -0.53217636 -0.53217636  1.87907632  1.87907632 -0.53217636\n",
      " -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636\n",
      " -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636 -0.53217636\n",
      " -0.53217636  1.87907632 -0.53217636 -0.53217636  1.87907632 -0.53217636\n",
      " -0.53217636 -0.53217636 -0.53217636]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  features_normalized.loc[valid_mask, col] = normalized_data\n",
      "/Users/rancze/Documents/GitHub/GitHub - CURSOR/vestibular_vr_pipeline/sleap/visualization.py:132: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[-0.9539549  -0.9539549  -0.9539549  -0.9539549  -0.9539549  -0.9539549\n",
      " -0.9539549  -0.9539549  -0.77991456 -0.77991456 -0.77991456 -0.77991456\n",
      " -0.9539549  -0.9539549   1.13452918  1.13452918  1.13452918  1.13452918\n",
      "  1.13452918  1.13452918  1.13452918  1.13452918  1.13452918  1.13452918\n",
      "  1.13452918  1.13452918  1.13452918 -0.9539549  -0.9539549  -0.77991456\n",
      " -0.77991456 -0.60587422 -0.60587422 -0.60587422 -0.77991456 -0.77991456\n",
      " -0.9539549  -0.25779354 -0.25779354 -0.25779354 -0.25779354 -0.25779354\n",
      " -0.9539549  -0.43183388 -0.43183388 -0.43183388 -0.43183388 -0.77991456\n",
      " -0.77991456 -0.9539549  -0.43183388 -0.43183388 -0.43183388 -0.43183388\n",
      " -0.43183388 -0.43183388 -0.43183388 -0.43183388 -0.77991456 -0.77991456\n",
      " -0.9539549  -0.25779354 -0.25779354 -0.25779354 -0.25779354 -0.25779354\n",
      " -0.77991456 -0.77991456 -0.60587422 -0.60587422 -0.60587422 -0.77991456\n",
      " -0.77991456 -0.0837532  -0.0837532  -0.0837532  -0.0837532  -0.0837532\n",
      " -0.0837532  -0.9539549   1.65665019  1.65665019  1.65665019  1.65665019\n",
      "  1.65665019  1.65665019  1.65665019  1.65665019  1.65665019  1.65665019\n",
      "  1.65665019  1.65665019  1.65665019  1.65665019  1.65665019  1.65665019\n",
      " -0.9539549  -0.60587422 -0.60587422 -0.60587422  0.26432748  0.26432748\n",
      "  0.26432748  0.26432748  0.26432748  0.26432748  0.26432748  0.26432748\n",
      " -0.77991456 -0.77991456 -0.43183388 -0.43183388 -0.43183388 -0.43183388\n",
      " -0.9539549  -0.77991456 -0.77991456 -0.60587422 -0.60587422 -0.60587422\n",
      " -0.0837532  -0.0837532  -0.0837532  -0.0837532  -0.0837532  -0.0837532\n",
      " -0.9539549  -0.60587422 -0.60587422 -0.60587422 -0.9539549  -0.9539549\n",
      " -0.9539549  -0.9539549  -0.9539549  -0.9539549  -0.9539549  -0.43183388\n",
      " -0.43183388 -0.43183388 -0.43183388 -0.0837532  -0.0837532  -0.0837532\n",
      " -0.0837532  -0.0837532  -0.0837532  -0.60587422 -0.60587422 -0.60587422\n",
      " -0.9539549  -0.9539549  -0.77991456 -0.77991456 -0.9539549  -0.9539549\n",
      " -0.9539549  -0.9539549  -0.60587422 -0.60587422 -0.60587422 -0.43183388\n",
      " -0.43183388 -0.43183388 -0.43183388 -0.9539549  -0.9539549   0.26432748\n",
      "  0.26432748  0.26432748  0.26432748  0.26432748  0.26432748  0.26432748\n",
      "  0.26432748 -0.9539549  -0.9539549  -0.9539549  -0.77991456 -0.77991456\n",
      " -0.9539549  -0.0837532  -0.0837532  -0.0837532  -0.0837532  -0.0837532\n",
      " -0.0837532  -0.9539549   0.09028714  0.09028714  0.09028714  0.09028714\n",
      "  0.09028714  0.09028714  0.09028714 -0.60587422 -0.60587422 -0.60587422\n",
      " -0.9539549  -0.9539549   0.26432748  0.26432748  0.26432748  0.26432748\n",
      "  0.26432748  0.26432748  0.26432748  0.26432748 -0.9539549  -0.77991456\n",
      " -0.77991456 -0.60587422 -0.60587422 -0.60587422 -0.9539549  -0.25779354\n",
      " -0.25779354 -0.25779354 -0.25779354 -0.25779354 -0.9539549   0.26432748\n",
      "  0.26432748  0.26432748  0.26432748  0.26432748  0.26432748  0.26432748\n",
      "  0.26432748 -0.9539549  -0.25779354 -0.25779354 -0.25779354 -0.25779354\n",
      " -0.25779354 -0.9539549  -0.43183388 -0.43183388 -0.43183388 -0.43183388\n",
      " -0.9539549  -0.77991456 -0.77991456 -0.9539549  -0.60587422 -0.60587422\n",
      " -0.60587422 -0.77991456 -0.77991456 -0.60587422 -0.60587422 -0.60587422\n",
      " -0.9539549  -0.77991456 -0.77991456 -0.9539549  -0.9539549   0.09028714\n",
      "  0.09028714  0.09028714  0.09028714  0.09028714  0.09028714  0.09028714\n",
      " -0.9539549   1.13452918  1.13452918  1.13452918  1.13452918  1.13452918\n",
      "  1.13452918  1.13452918  1.13452918  1.13452918  1.13452918  1.13452918\n",
      "  1.13452918  1.13452918 -0.25779354 -0.25779354 -0.25779354 -0.25779354\n",
      " -0.25779354 -0.9539549  -0.9539549  -0.9539549  -0.9539549  -0.77991456\n",
      " -0.77991456 -0.77991456 -0.77991456 -0.60587422 -0.60587422 -0.60587422\n",
      " -0.77991456 -0.77991456 -0.25779354 -0.25779354 -0.25779354 -0.25779354\n",
      " -0.25779354 -0.9539549  -0.77991456 -0.77991456 -0.9539549  -0.77991456\n",
      " -0.77991456 -0.9539549  -0.77991456 -0.77991456 -0.0837532  -0.0837532\n",
      " -0.0837532  -0.0837532  -0.0837532  -0.0837532  -0.77991456 -0.77991456\n",
      " -0.43183388 -0.43183388 -0.43183388 -0.43183388 -0.0837532  -0.0837532\n",
      " -0.0837532  -0.0837532  -0.0837532  -0.0837532  -0.43183388 -0.43183388\n",
      " -0.43183388 -0.43183388  0.61240816  0.61240816  0.61240816  0.61240816\n",
      "  0.61240816  0.61240816  0.61240816  0.61240816  0.61240816  0.61240816\n",
      " -0.9539549  -0.9539549  -0.9539549  -0.9539549  -0.9539549   1.65665019\n",
      "  1.65665019  1.65665019  1.65665019  1.65665019  1.65665019  1.65665019\n",
      "  1.65665019  1.65665019  1.65665019  1.65665019  1.65665019  1.65665019\n",
      "  1.65665019  1.65665019  1.65665019 -0.9539549  -0.77991456 -0.77991456\n",
      " -0.9539549  -0.25779354 -0.25779354 -0.25779354 -0.25779354 -0.25779354\n",
      " -0.9539549  -0.25779354 -0.25779354 -0.25779354 -0.25779354 -0.25779354\n",
      " -0.60587422 -0.60587422 -0.60587422  0.26432748  0.26432748  0.26432748\n",
      "  0.26432748  0.26432748  0.26432748  0.26432748  0.26432748 -0.9539549\n",
      " -0.9539549   0.09028714  0.09028714  0.09028714  0.09028714  0.09028714\n",
      "  0.09028714  0.09028714 -0.9539549  -0.9539549  -0.9539549  -0.9539549\n",
      " -0.60587422 -0.60587422 -0.60587422 -0.9539549  -0.77991456 -0.77991456\n",
      " -0.9539549  -0.9539549  -0.77991456 -0.77991456 -0.9539549  -0.9539549\n",
      " -0.9539549  -0.9539549  -0.9539549  -0.9539549  -0.9539549  -0.9539549\n",
      " -0.9539549  -0.9539549  -0.9539549  -0.60587422 -0.60587422 -0.60587422\n",
      " -0.77991456 -0.77991456 -0.9539549  -0.9539549   1.48260985  1.48260985\n",
      "  1.48260985  1.48260985  1.48260985  1.48260985  1.48260985  1.48260985\n",
      "  1.48260985  1.48260985  1.48260985  1.48260985  1.48260985  1.48260985\n",
      "  1.48260985 -0.9539549  -0.9539549  -0.77991456 -0.77991456 -0.43183388\n",
      " -0.43183388 -0.43183388 -0.43183388 -0.77991456 -0.77991456 -0.9539549\n",
      " -0.0837532  -0.0837532  -0.0837532  -0.0837532  -0.0837532  -0.0837532\n",
      " -0.9539549  -0.25779354 -0.25779354 -0.25779354 -0.25779354 -0.25779354\n",
      " -0.9539549  -0.77991456 -0.77991456 -0.43183388 -0.43183388 -0.43183388\n",
      " -0.43183388 -0.43183388 -0.43183388 -0.43183388 -0.43183388 -0.77991456\n",
      " -0.77991456 -0.25779354 -0.25779354 -0.25779354 -0.25779354 -0.25779354\n",
      " -0.77991456 -0.77991456 -0.25779354 -0.25779354 -0.25779354 -0.25779354\n",
      " -0.25779354 -0.9539549   0.26432748  0.26432748  0.26432748  0.26432748\n",
      "  0.26432748  0.26432748  0.26432748  0.26432748 -0.9539549  -0.25779354\n",
      " -0.25779354 -0.25779354 -0.25779354 -0.25779354  0.26432748  0.26432748\n",
      "  0.26432748  0.26432748  0.26432748  0.26432748  0.26432748  0.26432748\n",
      " -0.9539549  -0.60587422 -0.60587422 -0.60587422  0.43836782  0.43836782\n",
      "  0.43836782  0.43836782  0.43836782  0.43836782  0.43836782  0.43836782\n",
      "  0.43836782 -0.9539549  -0.25779354 -0.25779354 -0.25779354 -0.25779354\n",
      " -0.25779354 -0.9539549   3.74513427  3.74513427  3.74513427  3.74513427\n",
      "  3.74513427  3.74513427  3.74513427  3.74513427  3.74513427  3.74513427\n",
      "  3.74513427  3.74513427  3.74513427  3.74513427  3.74513427  3.74513427\n",
      "  3.74513427  3.74513427  3.74513427  3.74513427  3.74513427  3.74513427\n",
      "  3.74513427  3.74513427  3.74513427  3.74513427  3.74513427  3.74513427\n",
      " -0.0837532  -0.0837532  -0.0837532  -0.0837532  -0.0837532  -0.0837532\n",
      " -0.25779354 -0.25779354 -0.25779354 -0.25779354 -0.25779354 -0.60587422\n",
      " -0.60587422 -0.60587422 -0.9539549  -0.60587422 -0.60587422 -0.60587422\n",
      " -0.9539549  -0.9539549  -0.9539549  -0.43183388 -0.43183388 -0.43183388\n",
      " -0.43183388 -0.0837532  -0.0837532  -0.0837532  -0.0837532  -0.0837532\n",
      " -0.0837532  -0.60587422 -0.60587422 -0.60587422 -0.9539549  -0.9539549\n",
      " -0.43183388 -0.43183388 -0.43183388 -0.43183388 -0.9539549  -0.9539549\n",
      " -0.9539549  -0.60587422 -0.60587422 -0.60587422 -0.0837532  -0.0837532\n",
      " -0.0837532  -0.0837532  -0.0837532  -0.0837532  -0.9539549   0.26432748\n",
      "  0.26432748  0.26432748  0.26432748  0.26432748  0.26432748  0.26432748\n",
      "  0.26432748 -0.9539549  -0.9539549  -0.9539549  -0.9539549  -0.9539549\n",
      " -0.0837532  -0.0837532  -0.0837532  -0.0837532  -0.0837532  -0.0837532\n",
      " -0.0837532  -0.0837532  -0.0837532  -0.0837532  -0.0837532  -0.0837532\n",
      " -0.60587422 -0.60587422 -0.60587422  0.26432748  0.26432748  0.26432748\n",
      "  0.26432748  0.26432748  0.26432748  0.26432748  0.26432748 -0.25779354\n",
      " -0.25779354 -0.25779354 -0.25779354 -0.25779354 -0.43183388 -0.43183388\n",
      " -0.43183388 -0.43183388  0.26432748  0.26432748  0.26432748  0.26432748\n",
      "  0.26432748  0.26432748  0.26432748  0.26432748 -0.9539549   0.43836782\n",
      "  0.43836782  0.43836782  0.43836782  0.43836782  0.43836782  0.43836782\n",
      "  0.43836782  0.43836782 -0.77991456 -0.77991456 -0.43183388 -0.43183388\n",
      " -0.43183388 -0.43183388 -0.9539549  -0.43183388 -0.43183388 -0.43183388\n",
      " -0.43183388 -0.9539549  -0.77991456 -0.77991456 -0.9539549  -0.77991456\n",
      " -0.77991456 -0.77991456 -0.77991456 -0.60587422 -0.60587422 -0.60587422\n",
      " -0.77991456 -0.77991456 -0.77991456 -0.77991456 -0.9539549  -0.9539549\n",
      "  0.43836782  0.43836782  0.43836782  0.43836782  0.43836782  0.43836782\n",
      "  0.43836782  0.43836782  0.43836782 -0.9539549   0.96048884  0.96048884\n",
      "  0.96048884  0.96048884  0.96048884  0.96048884  0.96048884  0.96048884\n",
      "  0.96048884  0.96048884  0.96048884  0.96048884  0.09028714  0.09028714\n",
      "  0.09028714  0.09028714  0.09028714  0.09028714  0.09028714 -0.9539549\n",
      " -0.43183388 -0.43183388 -0.43183388 -0.43183388 -0.0837532  -0.0837532\n",
      " -0.0837532  -0.0837532  -0.0837532  -0.0837532  -0.60587422 -0.60587422\n",
      " -0.60587422 -0.25779354 -0.25779354 -0.25779354 -0.25779354 -0.25779354\n",
      " -0.77991456 -0.77991456 -0.77991456 -0.77991456 -0.77991456 -0.77991456\n",
      " -0.9539549  -0.77991456 -0.77991456  1.48260985  1.48260985  1.48260985\n",
      "  1.48260985  1.48260985  1.48260985  1.48260985  1.48260985  1.48260985\n",
      "  1.48260985  1.48260985  1.48260985  1.48260985  1.48260985  1.48260985\n",
      "  0.43836782  0.43836782  0.43836782  0.43836782  0.43836782  0.43836782\n",
      "  0.43836782  0.43836782  0.43836782 -0.9539549  -0.43183388 -0.43183388\n",
      " -0.43183388 -0.43183388 -0.0837532  -0.0837532  -0.0837532  -0.0837532\n",
      " -0.0837532  -0.0837532  -0.77991456 -0.77991456 -0.77991456 -0.77991456\n",
      "  0.43836782  0.43836782  0.43836782  0.43836782  0.43836782  0.43836782\n",
      "  0.43836782  0.43836782  0.43836782 -0.9539549  -0.77991456 -0.77991456\n",
      " -0.9539549  -0.9539549  -0.9539549   1.48260985  1.48260985  1.48260985\n",
      "  1.48260985  1.48260985  1.48260985  1.48260985  1.48260985  1.48260985\n",
      "  1.48260985  1.48260985  1.48260985  1.48260985  1.48260985  1.48260985\n",
      " -0.9539549  -0.9539549  -0.25779354 -0.25779354 -0.25779354 -0.25779354\n",
      " -0.25779354 -0.9539549  -0.60587422 -0.60587422 -0.60587422 -0.60587422\n",
      " -0.60587422 -0.60587422  0.09028714  0.09028714  0.09028714  0.09028714\n",
      "  0.09028714  0.09028714  0.09028714 -0.9539549  -0.9539549   0.26432748\n",
      "  0.26432748  0.26432748  0.26432748  0.26432748  0.26432748  0.26432748\n",
      "  0.26432748 -0.9539549  -0.9539549  -0.9539549  -0.9539549  -0.77991456\n",
      " -0.77991456 -0.77991456 -0.77991456 -0.9539549  -0.9539549  -0.9539549\n",
      " -0.9539549  -0.9539549  -0.9539549 ]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  features_normalized.loc[valid_mask, col] = normalized_data\n",
      "/Users/rancze/Documents/GitHub/GitHub - CURSOR/vestibular_vr_pipeline/sleap/visualization.py:132: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[-0.72166645 -0.72166645 -0.72166645 -0.72166645 -0.72166645 -0.72166645\n",
      " -0.72166645 -0.72166645 -0.72166645 -0.45834357 -0.72166645 -0.45834357\n",
      " -0.72166645 -0.72166645 -0.72166645 -0.45834357 -0.1950207   0.06830218\n",
      "  0.33162506  0.59494793  0.85827081  1.12159368  1.38491656  1.64823943\n",
      "  1.91156231  2.17488519  2.43820806 -0.72166645 -0.72166645 -0.72166645\n",
      " -0.45834357 -0.72166645 -0.45834357 -0.1950207  -0.72166645 -0.45834357\n",
      " -0.72166645 -0.72166645 -0.45834357 -0.1950207   0.06830218  0.33162506\n",
      " -0.72166645 -0.72166645 -0.45834357 -0.1950207   0.06830218 -0.72166645\n",
      " -0.45834357 -0.72166645 -0.72166645 -0.45834357 -0.1950207   0.06830218\n",
      " -0.72166645 -0.45834357 -0.1950207   0.06830218 -0.72166645 -0.45834357\n",
      " -0.72166645 -0.72166645 -0.45834357 -0.1950207   0.06830218  0.33162506\n",
      " -0.72166645 -0.45834357 -0.72166645 -0.45834357 -0.1950207  -0.72166645\n",
      " -0.45834357 -0.72166645 -0.45834357 -0.1950207   0.06830218  0.33162506\n",
      "  0.59494793 -0.72166645 -0.72166645 -0.45834357 -0.1950207   0.06830218\n",
      "  0.33162506  0.59494793  0.85827081  1.12159368  1.38491656  1.64823943\n",
      "  1.91156231  2.17488519  2.43820806  2.70153094  2.96485381  3.22817669\n",
      " -0.72166645 -0.72166645 -0.45834357 -0.1950207  -0.72166645 -0.45834357\n",
      " -0.1950207   0.06830218  0.33162506  0.59494793  0.85827081  1.12159368\n",
      " -0.72166645 -0.45834357 -0.72166645 -0.45834357 -0.1950207   0.06830218\n",
      " -0.72166645 -0.72166645 -0.45834357 -0.72166645 -0.45834357 -0.1950207\n",
      " -0.72166645 -0.45834357 -0.1950207   0.06830218  0.33162506  0.59494793\n",
      " -0.72166645 -0.72166645 -0.45834357 -0.1950207  -0.72166645 -0.72166645\n",
      " -0.72166645 -0.72166645 -0.72166645 -0.72166645 -0.72166645 -0.72166645\n",
      " -0.45834357 -0.1950207   0.06830218 -0.72166645 -0.45834357 -0.1950207\n",
      "  0.06830218  0.33162506  0.59494793 -0.72166645 -0.45834357 -0.1950207\n",
      " -0.72166645 -0.72166645 -0.72166645 -0.45834357 -0.72166645 -0.72166645\n",
      " -0.72166645 -0.72166645 -0.72166645 -0.45834357 -0.1950207  -0.72166645\n",
      " -0.45834357 -0.1950207   0.06830218 -0.72166645 -0.72166645 -0.72166645\n",
      " -0.45834357 -0.1950207   0.06830218  0.33162506  0.59494793  0.85827081\n",
      "  1.12159368 -0.72166645 -0.72166645 -0.72166645 -0.72166645 -0.45834357\n",
      " -0.72166645 -0.72166645 -0.45834357 -0.1950207   0.06830218  0.33162506\n",
      "  0.59494793 -0.72166645 -0.72166645 -0.45834357 -0.1950207   0.06830218\n",
      "  0.33162506  0.59494793  0.85827081 -0.72166645 -0.45834357 -0.1950207\n",
      " -0.72166645 -0.72166645 -0.72166645 -0.45834357 -0.1950207   0.06830218\n",
      "  0.33162506  0.59494793  0.85827081  1.12159368 -0.72166645 -0.72166645\n",
      " -0.45834357 -0.72166645 -0.45834357 -0.1950207  -0.72166645 -0.72166645\n",
      " -0.45834357 -0.1950207   0.06830218  0.33162506 -0.72166645 -0.72166645\n",
      " -0.45834357 -0.1950207   0.06830218  0.33162506  0.59494793  0.85827081\n",
      "  1.12159368 -0.72166645 -0.72166645 -0.45834357 -0.1950207   0.06830218\n",
      "  0.33162506 -0.72166645 -0.72166645 -0.45834357 -0.1950207   0.06830218\n",
      " -0.72166645 -0.72166645 -0.45834357 -0.72166645 -0.72166645 -0.45834357\n",
      " -0.1950207  -0.72166645 -0.45834357 -0.72166645 -0.45834357 -0.1950207\n",
      " -0.72166645 -0.72166645 -0.45834357 -0.72166645 -0.72166645 -0.72166645\n",
      " -0.45834357 -0.1950207   0.06830218  0.33162506  0.59494793  0.85827081\n",
      " -0.72166645 -0.72166645 -0.45834357 -0.1950207   0.06830218  0.33162506\n",
      "  0.59494793  0.85827081  1.12159368  1.38491656  1.64823943  1.91156231\n",
      "  2.17488519  2.43820806 -0.72166645 -0.45834357 -0.1950207   0.06830218\n",
      "  0.33162506 -0.72166645 -0.72166645 -0.72166645 -0.72166645 -0.72166645\n",
      " -0.45834357 -0.72166645 -0.45834357 -0.72166645 -0.45834357 -0.1950207\n",
      " -0.72166645 -0.45834357 -0.72166645 -0.45834357 -0.1950207   0.06830218\n",
      "  0.33162506 -0.72166645 -0.72166645 -0.45834357 -0.72166645 -0.72166645\n",
      " -0.45834357 -0.72166645 -0.72166645 -0.45834357 -0.72166645 -0.45834357\n",
      " -0.1950207   0.06830218  0.33162506  0.59494793 -0.72166645 -0.45834357\n",
      " -0.72166645 -0.45834357 -0.1950207   0.06830218 -0.72166645 -0.45834357\n",
      " -0.1950207   0.06830218  0.33162506  0.59494793 -0.72166645 -0.45834357\n",
      " -0.1950207   0.06830218 -0.72166645 -0.45834357 -0.1950207   0.06830218\n",
      "  0.33162506  0.59494793  0.85827081  1.12159368  1.38491656  1.64823943\n",
      " -0.72166645 -0.72166645 -0.72166645 -0.72166645 -0.72166645 -0.72166645\n",
      " -0.45834357 -0.1950207   0.06830218  0.33162506  0.59494793  0.85827081\n",
      "  1.12159368  1.38491656  1.64823943  1.91156231  2.17488519  2.43820806\n",
      "  2.70153094  2.96485381  3.22817669 -0.72166645 -0.72166645 -0.45834357\n",
      " -0.72166645 -0.72166645 -0.45834357 -0.1950207   0.06830218  0.33162506\n",
      " -0.72166645 -0.72166645 -0.45834357 -0.1950207   0.06830218  0.33162506\n",
      " -0.72166645 -0.45834357 -0.1950207  -0.72166645 -0.45834357 -0.1950207\n",
      "  0.06830218  0.33162506  0.59494793  0.85827081  1.12159368 -0.72166645\n",
      " -0.72166645 -0.72166645 -0.45834357 -0.1950207   0.06830218  0.33162506\n",
      "  0.59494793  0.85827081 -0.72166645 -0.72166645 -0.72166645 -0.72166645\n",
      " -0.72166645 -0.45834357 -0.1950207  -0.72166645 -0.72166645 -0.45834357\n",
      " -0.72166645 -0.72166645 -0.72166645 -0.45834357 -0.72166645 -0.72166645\n",
      " -0.72166645 -0.72166645 -0.72166645 -0.72166645 -0.72166645 -0.72166645\n",
      " -0.72166645 -0.72166645 -0.72166645 -0.72166645 -0.45834357 -0.1950207\n",
      " -0.72166645 -0.45834357 -0.72166645 -0.72166645 -0.72166645 -0.45834357\n",
      " -0.1950207   0.06830218  0.33162506  0.59494793  0.85827081  1.12159368\n",
      "  1.38491656  1.64823943  1.91156231  2.17488519  2.43820806  2.70153094\n",
      "  2.96485381 -0.72166645 -0.72166645 -0.72166645 -0.45834357 -0.72166645\n",
      " -0.45834357 -0.1950207   0.06830218 -0.72166645 -0.45834357 -0.72166645\n",
      " -0.72166645 -0.45834357 -0.1950207   0.06830218  0.33162506  0.59494793\n",
      " -0.72166645 -0.72166645 -0.45834357 -0.1950207   0.06830218  0.33162506\n",
      " -0.72166645 -0.72166645 -0.45834357 -0.72166645 -0.45834357 -0.1950207\n",
      "  0.06830218 -0.72166645 -0.45834357 -0.1950207   0.06830218 -0.72166645\n",
      " -0.45834357 -0.72166645 -0.45834357 -0.1950207   0.06830218  0.33162506\n",
      " -0.72166645 -0.45834357 -0.72166645 -0.45834357 -0.1950207   0.06830218\n",
      "  0.33162506 -0.72166645 -0.72166645 -0.45834357 -0.1950207   0.06830218\n",
      "  0.33162506  0.59494793  0.85827081  1.12159368 -0.72166645 -0.72166645\n",
      " -0.45834357 -0.1950207   0.06830218  0.33162506 -0.72166645 -0.45834357\n",
      " -0.1950207   0.06830218  0.33162506  0.59494793  0.85827081  1.12159368\n",
      " -0.72166645 -0.72166645 -0.45834357 -0.1950207  -0.72166645 -0.45834357\n",
      " -0.1950207   0.06830218  0.33162506  0.59494793  0.85827081  1.12159368\n",
      "  1.38491656 -0.72166645 -0.72166645 -0.45834357 -0.1950207   0.06830218\n",
      "  0.33162506 -0.72166645 -0.72166645 -0.45834357 -0.1950207   0.06830218\n",
      "  0.33162506  0.59494793  0.85827081  1.12159368  1.38491656  1.64823943\n",
      "  1.91156231  2.17488519  2.43820806  2.70153094  2.96485381  3.22817669\n",
      "  3.49149956  3.75482244  4.01814531  4.28146819  4.54479107  4.80811394\n",
      "  5.07143682  5.33475969  5.59808257  5.86140544  6.12472832  6.3880512\n",
      " -0.72166645 -0.45834357 -0.1950207   0.06830218  0.33162506  0.59494793\n",
      " -0.72166645 -0.45834357 -0.1950207   0.06830218  0.33162506 -0.72166645\n",
      " -0.45834357 -0.1950207  -0.72166645 -0.72166645 -0.45834357 -0.1950207\n",
      " -0.72166645 -0.72166645 -0.72166645 -0.72166645 -0.45834357 -0.1950207\n",
      "  0.06830218 -0.72166645 -0.45834357 -0.1950207   0.06830218  0.33162506\n",
      "  0.59494793 -0.72166645 -0.45834357 -0.1950207  -0.72166645 -0.72166645\n",
      " -0.72166645 -0.45834357 -0.1950207   0.06830218 -0.72166645 -0.72166645\n",
      " -0.72166645 -0.72166645 -0.45834357 -0.1950207  -0.72166645 -0.45834357\n",
      " -0.1950207   0.06830218  0.33162506  0.59494793 -0.72166645 -0.72166645\n",
      " -0.45834357 -0.1950207   0.06830218  0.33162506  0.59494793  0.85827081\n",
      "  1.12159368 -0.72166645 -0.72166645 -0.72166645 -0.72166645 -0.72166645\n",
      " -0.72166645 -0.45834357 -0.1950207   0.06830218  0.33162506  0.59494793\n",
      " -0.72166645 -0.45834357 -0.1950207   0.06830218  0.33162506  0.59494793\n",
      " -0.72166645 -0.45834357 -0.1950207  -0.72166645 -0.45834357 -0.1950207\n",
      "  0.06830218  0.33162506  0.59494793  0.85827081  1.12159368 -0.72166645\n",
      " -0.45834357 -0.1950207   0.06830218  0.33162506 -0.72166645 -0.45834357\n",
      " -0.1950207   0.06830218 -0.72166645 -0.45834357 -0.1950207   0.06830218\n",
      "  0.33162506  0.59494793  0.85827081  1.12159368 -0.72166645 -0.72166645\n",
      " -0.45834357 -0.1950207   0.06830218  0.33162506  0.59494793  0.85827081\n",
      "  1.12159368  1.38491656 -0.72166645 -0.45834357 -0.72166645 -0.45834357\n",
      " -0.1950207   0.06830218 -0.72166645 -0.72166645 -0.45834357 -0.1950207\n",
      "  0.06830218 -0.72166645 -0.72166645 -0.45834357 -0.72166645 -0.72166645\n",
      " -0.45834357 -0.72166645 -0.45834357 -0.72166645 -0.45834357 -0.1950207\n",
      " -0.72166645 -0.45834357 -0.72166645 -0.45834357 -0.72166645 -0.72166645\n",
      " -0.72166645 -0.45834357 -0.1950207   0.06830218  0.33162506  0.59494793\n",
      "  0.85827081  1.12159368  1.38491656 -0.72166645 -0.72166645 -0.45834357\n",
      " -0.1950207   0.06830218  0.33162506  0.59494793  0.85827081  1.12159368\n",
      "  1.38491656  1.64823943  1.91156231  2.17488519 -0.72166645 -0.45834357\n",
      " -0.1950207   0.06830218  0.33162506  0.59494793  0.85827081 -0.72166645\n",
      " -0.72166645 -0.45834357 -0.1950207   0.06830218 -0.72166645 -0.45834357\n",
      " -0.1950207   0.06830218  0.33162506  0.59494793 -0.72166645 -0.45834357\n",
      " -0.1950207  -0.72166645 -0.45834357 -0.1950207   0.06830218  0.33162506\n",
      " -0.72166645 -0.45834357 -0.72166645 -0.45834357 -0.72166645 -0.45834357\n",
      " -0.72166645 -0.72166645 -0.45834357 -0.72166645 -0.45834357 -0.1950207\n",
      "  0.06830218  0.33162506  0.59494793  0.85827081  1.12159368  1.38491656\n",
      "  1.64823943  1.91156231  2.17488519  2.43820806  2.70153094  2.96485381\n",
      " -0.72166645 -0.45834357 -0.1950207   0.06830218  0.33162506  0.59494793\n",
      "  0.85827081  1.12159368  1.38491656 -0.72166645 -0.72166645 -0.45834357\n",
      " -0.1950207   0.06830218 -0.72166645 -0.45834357 -0.1950207   0.06830218\n",
      "  0.33162506  0.59494793 -0.72166645 -0.45834357 -0.72166645 -0.45834357\n",
      " -0.72166645 -0.45834357 -0.1950207   0.06830218  0.33162506  0.59494793\n",
      "  0.85827081  1.12159368  1.38491656 -0.72166645 -0.72166645 -0.45834357\n",
      " -0.72166645 -0.72166645 -0.72166645 -0.72166645 -0.45834357 -0.1950207\n",
      "  0.06830218  0.33162506  0.59494793  0.85827081  1.12159368  1.38491656\n",
      "  1.64823943  1.91156231  2.17488519  2.43820806  2.70153094  2.96485381\n",
      " -0.72166645 -0.72166645 -0.72166645 -0.45834357 -0.1950207   0.06830218\n",
      "  0.33162506 -0.72166645 -0.72166645 -0.45834357 -0.1950207  -0.72166645\n",
      " -0.45834357 -0.1950207  -0.72166645 -0.45834357 -0.1950207   0.06830218\n",
      "  0.33162506  0.59494793  0.85827081 -0.72166645 -0.72166645 -0.72166645\n",
      " -0.45834357 -0.1950207   0.06830218  0.33162506  0.59494793  0.85827081\n",
      "  1.12159368 -0.72166645 -0.72166645 -0.72166645 -0.72166645 -0.72166645\n",
      " -0.45834357 -0.72166645 -0.45834357 -0.72166645 -0.72166645 -0.72166645\n",
      " -0.72166645 -0.72166645 -0.72166645]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  features_normalized.loc[valid_mask, col] = normalized_data\n",
      "/Users/rancze/Documents/GitHub/GitHub - CURSOR/vestibular_vr_pipeline/sleap/visualization.py:132: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[ 1.39985569  1.39985569  1.39985569  1.39985569  1.39985569  1.39985569\n",
      "  1.39985569  1.39985569  1.39985569 -0.71435935  1.39985569 -0.71435935\n",
      "  1.39985569  1.39985569  1.39985569 -0.71435935 -0.71435935 -0.71435935\n",
      " -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935\n",
      " -0.71435935 -0.71435935 -0.71435935  1.39985569  1.39985569  1.39985569\n",
      " -0.71435935  1.39985569 -0.71435935 -0.71435935  1.39985569 -0.71435935\n",
      "  1.39985569  1.39985569 -0.71435935 -0.71435935 -0.71435935 -0.71435935\n",
      "  1.39985569  1.39985569 -0.71435935 -0.71435935 -0.71435935  1.39985569\n",
      " -0.71435935  1.39985569  1.39985569 -0.71435935 -0.71435935 -0.71435935\n",
      "  1.39985569 -0.71435935 -0.71435935 -0.71435935  1.39985569 -0.71435935\n",
      "  1.39985569  1.39985569 -0.71435935 -0.71435935 -0.71435935 -0.71435935\n",
      "  1.39985569 -0.71435935  1.39985569 -0.71435935 -0.71435935  1.39985569\n",
      " -0.71435935  1.39985569 -0.71435935 -0.71435935 -0.71435935 -0.71435935\n",
      " -0.71435935  1.39985569  1.39985569 -0.71435935 -0.71435935 -0.71435935\n",
      " -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935\n",
      " -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935\n",
      "  1.39985569  1.39985569 -0.71435935 -0.71435935  1.39985569 -0.71435935\n",
      " -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935\n",
      "  1.39985569 -0.71435935  1.39985569 -0.71435935 -0.71435935 -0.71435935\n",
      "  1.39985569  1.39985569 -0.71435935  1.39985569 -0.71435935 -0.71435935\n",
      "  1.39985569 -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935\n",
      "  1.39985569  1.39985569 -0.71435935 -0.71435935  1.39985569  1.39985569\n",
      "  1.39985569  1.39985569  1.39985569  1.39985569  1.39985569  1.39985569\n",
      " -0.71435935 -0.71435935 -0.71435935  1.39985569 -0.71435935 -0.71435935\n",
      " -0.71435935 -0.71435935 -0.71435935  1.39985569 -0.71435935 -0.71435935\n",
      "  1.39985569  1.39985569  1.39985569 -0.71435935  1.39985569  1.39985569\n",
      "  1.39985569  1.39985569  1.39985569 -0.71435935 -0.71435935  1.39985569\n",
      " -0.71435935 -0.71435935 -0.71435935  1.39985569  1.39985569  1.39985569\n",
      " -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935\n",
      " -0.71435935  1.39985569  1.39985569  1.39985569  1.39985569 -0.71435935\n",
      "  1.39985569  1.39985569 -0.71435935 -0.71435935 -0.71435935 -0.71435935\n",
      " -0.71435935  1.39985569  1.39985569 -0.71435935 -0.71435935 -0.71435935\n",
      " -0.71435935 -0.71435935 -0.71435935  1.39985569 -0.71435935 -0.71435935\n",
      "  1.39985569  1.39985569  1.39985569 -0.71435935 -0.71435935 -0.71435935\n",
      " -0.71435935 -0.71435935 -0.71435935 -0.71435935  1.39985569  1.39985569\n",
      " -0.71435935  1.39985569 -0.71435935 -0.71435935  1.39985569  1.39985569\n",
      " -0.71435935 -0.71435935 -0.71435935 -0.71435935  1.39985569  1.39985569\n",
      " -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935\n",
      " -0.71435935  1.39985569  1.39985569 -0.71435935 -0.71435935 -0.71435935\n",
      " -0.71435935  1.39985569  1.39985569 -0.71435935 -0.71435935 -0.71435935\n",
      "  1.39985569  1.39985569 -0.71435935  1.39985569  1.39985569 -0.71435935\n",
      " -0.71435935  1.39985569 -0.71435935  1.39985569 -0.71435935 -0.71435935\n",
      "  1.39985569  1.39985569 -0.71435935  1.39985569  1.39985569  1.39985569\n",
      " -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935\n",
      "  1.39985569  1.39985569 -0.71435935 -0.71435935 -0.71435935 -0.71435935\n",
      " -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935\n",
      " -0.71435935 -0.71435935  1.39985569 -0.71435935 -0.71435935 -0.71435935\n",
      " -0.71435935  1.39985569  1.39985569  1.39985569  1.39985569  1.39985569\n",
      " -0.71435935  1.39985569 -0.71435935  1.39985569 -0.71435935 -0.71435935\n",
      "  1.39985569 -0.71435935  1.39985569 -0.71435935 -0.71435935 -0.71435935\n",
      " -0.71435935  1.39985569  1.39985569 -0.71435935  1.39985569  1.39985569\n",
      " -0.71435935  1.39985569  1.39985569 -0.71435935  1.39985569 -0.71435935\n",
      " -0.71435935 -0.71435935 -0.71435935 -0.71435935  1.39985569 -0.71435935\n",
      "  1.39985569 -0.71435935 -0.71435935 -0.71435935  1.39985569 -0.71435935\n",
      " -0.71435935 -0.71435935 -0.71435935 -0.71435935  1.39985569 -0.71435935\n",
      " -0.71435935 -0.71435935  1.39985569 -0.71435935 -0.71435935 -0.71435935\n",
      " -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935\n",
      "  1.39985569  1.39985569  1.39985569  1.39985569  1.39985569  1.39985569\n",
      " -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935\n",
      " -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935\n",
      " -0.71435935 -0.71435935 -0.71435935  1.39985569  1.39985569 -0.71435935\n",
      "  1.39985569  1.39985569 -0.71435935 -0.71435935 -0.71435935 -0.71435935\n",
      "  1.39985569  1.39985569 -0.71435935 -0.71435935 -0.71435935 -0.71435935\n",
      "  1.39985569 -0.71435935 -0.71435935  1.39985569 -0.71435935 -0.71435935\n",
      " -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935  1.39985569\n",
      "  1.39985569  1.39985569 -0.71435935 -0.71435935 -0.71435935 -0.71435935\n",
      " -0.71435935 -0.71435935  1.39985569  1.39985569  1.39985569  1.39985569\n",
      "  1.39985569 -0.71435935 -0.71435935  1.39985569  1.39985569 -0.71435935\n",
      "  1.39985569  1.39985569  1.39985569 -0.71435935  1.39985569  1.39985569\n",
      "  1.39985569  1.39985569  1.39985569  1.39985569  1.39985569  1.39985569\n",
      "  1.39985569  1.39985569  1.39985569  1.39985569 -0.71435935 -0.71435935\n",
      "  1.39985569 -0.71435935  1.39985569  1.39985569  1.39985569 -0.71435935\n",
      " -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935\n",
      " -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935\n",
      " -0.71435935  1.39985569  1.39985569  1.39985569 -0.71435935  1.39985569\n",
      " -0.71435935 -0.71435935 -0.71435935  1.39985569 -0.71435935  1.39985569\n",
      "  1.39985569 -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935\n",
      "  1.39985569  1.39985569 -0.71435935 -0.71435935 -0.71435935 -0.71435935\n",
      "  1.39985569  1.39985569 -0.71435935  1.39985569 -0.71435935 -0.71435935\n",
      " -0.71435935  1.39985569 -0.71435935 -0.71435935 -0.71435935  1.39985569\n",
      " -0.71435935  1.39985569 -0.71435935 -0.71435935 -0.71435935 -0.71435935\n",
      "  1.39985569 -0.71435935  1.39985569 -0.71435935 -0.71435935 -0.71435935\n",
      " -0.71435935  1.39985569  1.39985569 -0.71435935 -0.71435935 -0.71435935\n",
      " -0.71435935 -0.71435935 -0.71435935 -0.71435935  1.39985569  1.39985569\n",
      " -0.71435935 -0.71435935 -0.71435935 -0.71435935  1.39985569 -0.71435935\n",
      " -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935\n",
      "  1.39985569  1.39985569 -0.71435935 -0.71435935  1.39985569 -0.71435935\n",
      " -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935\n",
      " -0.71435935  1.39985569  1.39985569 -0.71435935 -0.71435935 -0.71435935\n",
      " -0.71435935  1.39985569  1.39985569 -0.71435935 -0.71435935 -0.71435935\n",
      " -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935\n",
      " -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935\n",
      " -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935\n",
      " -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935\n",
      "  1.39985569 -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935\n",
      "  1.39985569 -0.71435935 -0.71435935 -0.71435935 -0.71435935  1.39985569\n",
      " -0.71435935 -0.71435935  1.39985569  1.39985569 -0.71435935 -0.71435935\n",
      "  1.39985569  1.39985569  1.39985569  1.39985569 -0.71435935 -0.71435935\n",
      " -0.71435935  1.39985569 -0.71435935 -0.71435935 -0.71435935 -0.71435935\n",
      " -0.71435935  1.39985569 -0.71435935 -0.71435935  1.39985569  1.39985569\n",
      "  1.39985569 -0.71435935 -0.71435935 -0.71435935  1.39985569  1.39985569\n",
      "  1.39985569  1.39985569 -0.71435935 -0.71435935  1.39985569 -0.71435935\n",
      " -0.71435935 -0.71435935 -0.71435935 -0.71435935  1.39985569  1.39985569\n",
      " -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935\n",
      " -0.71435935  1.39985569  1.39985569  1.39985569  1.39985569  1.39985569\n",
      "  1.39985569 -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935\n",
      "  1.39985569 -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935\n",
      "  1.39985569 -0.71435935 -0.71435935  1.39985569 -0.71435935 -0.71435935\n",
      " -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935  1.39985569\n",
      " -0.71435935 -0.71435935 -0.71435935 -0.71435935  1.39985569 -0.71435935\n",
      " -0.71435935 -0.71435935  1.39985569 -0.71435935 -0.71435935 -0.71435935\n",
      " -0.71435935 -0.71435935 -0.71435935 -0.71435935  1.39985569  1.39985569\n",
      " -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935\n",
      " -0.71435935 -0.71435935  1.39985569 -0.71435935  1.39985569 -0.71435935\n",
      " -0.71435935 -0.71435935  1.39985569  1.39985569 -0.71435935 -0.71435935\n",
      " -0.71435935  1.39985569  1.39985569 -0.71435935  1.39985569  1.39985569\n",
      " -0.71435935  1.39985569 -0.71435935  1.39985569 -0.71435935 -0.71435935\n",
      "  1.39985569 -0.71435935  1.39985569 -0.71435935  1.39985569  1.39985569\n",
      "  1.39985569 -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935\n",
      " -0.71435935 -0.71435935 -0.71435935  1.39985569  1.39985569 -0.71435935\n",
      " -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935\n",
      " -0.71435935 -0.71435935 -0.71435935 -0.71435935  1.39985569 -0.71435935\n",
      " -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935  1.39985569\n",
      "  1.39985569 -0.71435935 -0.71435935 -0.71435935  1.39985569 -0.71435935\n",
      " -0.71435935 -0.71435935 -0.71435935 -0.71435935  1.39985569 -0.71435935\n",
      " -0.71435935  1.39985569 -0.71435935 -0.71435935 -0.71435935 -0.71435935\n",
      "  1.39985569 -0.71435935  1.39985569 -0.71435935  1.39985569 -0.71435935\n",
      "  1.39985569  1.39985569 -0.71435935  1.39985569 -0.71435935 -0.71435935\n",
      " -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935\n",
      " -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935\n",
      "  1.39985569 -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935\n",
      " -0.71435935 -0.71435935 -0.71435935  1.39985569  1.39985569 -0.71435935\n",
      " -0.71435935 -0.71435935  1.39985569 -0.71435935 -0.71435935 -0.71435935\n",
      " -0.71435935 -0.71435935  1.39985569 -0.71435935  1.39985569 -0.71435935\n",
      "  1.39985569 -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935\n",
      " -0.71435935 -0.71435935 -0.71435935  1.39985569  1.39985569 -0.71435935\n",
      "  1.39985569  1.39985569  1.39985569  1.39985569 -0.71435935 -0.71435935\n",
      " -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935\n",
      " -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935\n",
      "  1.39985569  1.39985569  1.39985569 -0.71435935 -0.71435935 -0.71435935\n",
      " -0.71435935  1.39985569  1.39985569 -0.71435935 -0.71435935  1.39985569\n",
      " -0.71435935 -0.71435935  1.39985569 -0.71435935 -0.71435935 -0.71435935\n",
      " -0.71435935 -0.71435935 -0.71435935  1.39985569  1.39985569  1.39985569\n",
      " -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935\n",
      " -0.71435935  1.39985569  1.39985569  1.39985569  1.39985569  1.39985569\n",
      " -0.71435935  1.39985569 -0.71435935  1.39985569  1.39985569  1.39985569\n",
      "  1.39985569  1.39985569  1.39985569]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  features_normalized.loc[valid_mask, col] = normalized_data\n",
      "/Users/rancze/Documents/GitHub/GitHub - CURSOR/vestibular_vr_pipeline/sleap/visualization.py:132: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[ 1.39985569  1.39985569  1.39985569  1.39985569  1.39985569  1.39985569\n",
      "  1.39985569  1.39985569 -0.71435935  1.39985569 -0.71435935  1.39985569\n",
      "  1.39985569  1.39985569 -0.71435935 -0.71435935 -0.71435935 -0.71435935\n",
      " -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935\n",
      " -0.71435935 -0.71435935  1.39985569  1.39985569  1.39985569 -0.71435935\n",
      "  1.39985569 -0.71435935 -0.71435935  1.39985569 -0.71435935  1.39985569\n",
      "  1.39985569 -0.71435935 -0.71435935 -0.71435935 -0.71435935  1.39985569\n",
      "  1.39985569 -0.71435935 -0.71435935 -0.71435935  1.39985569 -0.71435935\n",
      "  1.39985569  1.39985569 -0.71435935 -0.71435935 -0.71435935  1.39985569\n",
      " -0.71435935 -0.71435935 -0.71435935  1.39985569 -0.71435935  1.39985569\n",
      "  1.39985569 -0.71435935 -0.71435935 -0.71435935 -0.71435935  1.39985569\n",
      " -0.71435935  1.39985569 -0.71435935 -0.71435935  1.39985569 -0.71435935\n",
      "  1.39985569 -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935\n",
      "  1.39985569  1.39985569 -0.71435935 -0.71435935 -0.71435935 -0.71435935\n",
      " -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935\n",
      " -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935  1.39985569\n",
      "  1.39985569 -0.71435935 -0.71435935  1.39985569 -0.71435935 -0.71435935\n",
      " -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935  1.39985569\n",
      " -0.71435935  1.39985569 -0.71435935 -0.71435935 -0.71435935  1.39985569\n",
      "  1.39985569 -0.71435935  1.39985569 -0.71435935 -0.71435935  1.39985569\n",
      " -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935  1.39985569\n",
      "  1.39985569 -0.71435935 -0.71435935  1.39985569  1.39985569  1.39985569\n",
      "  1.39985569  1.39985569  1.39985569  1.39985569  1.39985569 -0.71435935\n",
      " -0.71435935 -0.71435935  1.39985569 -0.71435935 -0.71435935 -0.71435935\n",
      " -0.71435935 -0.71435935  1.39985569 -0.71435935 -0.71435935  1.39985569\n",
      "  1.39985569  1.39985569 -0.71435935  1.39985569  1.39985569  1.39985569\n",
      "  1.39985569  1.39985569 -0.71435935 -0.71435935  1.39985569 -0.71435935\n",
      " -0.71435935 -0.71435935  1.39985569  1.39985569  1.39985569 -0.71435935\n",
      " -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935\n",
      "  1.39985569  1.39985569  1.39985569  1.39985569 -0.71435935  1.39985569\n",
      "  1.39985569 -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935\n",
      "  1.39985569  1.39985569 -0.71435935 -0.71435935 -0.71435935 -0.71435935\n",
      " -0.71435935 -0.71435935  1.39985569 -0.71435935 -0.71435935  1.39985569\n",
      "  1.39985569  1.39985569 -0.71435935 -0.71435935 -0.71435935 -0.71435935\n",
      " -0.71435935 -0.71435935 -0.71435935  1.39985569  1.39985569 -0.71435935\n",
      "  1.39985569 -0.71435935 -0.71435935  1.39985569  1.39985569 -0.71435935\n",
      " -0.71435935 -0.71435935 -0.71435935  1.39985569  1.39985569 -0.71435935\n",
      " -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935\n",
      "  1.39985569  1.39985569 -0.71435935 -0.71435935 -0.71435935 -0.71435935\n",
      "  1.39985569  1.39985569 -0.71435935 -0.71435935 -0.71435935  1.39985569\n",
      "  1.39985569 -0.71435935  1.39985569  1.39985569 -0.71435935 -0.71435935\n",
      "  1.39985569 -0.71435935  1.39985569 -0.71435935 -0.71435935  1.39985569\n",
      "  1.39985569 -0.71435935  1.39985569  1.39985569  1.39985569 -0.71435935\n",
      " -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935  1.39985569\n",
      "  1.39985569 -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935\n",
      " -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935\n",
      " -0.71435935  1.39985569 -0.71435935 -0.71435935 -0.71435935 -0.71435935\n",
      "  1.39985569  1.39985569  1.39985569  1.39985569  1.39985569 -0.71435935\n",
      "  1.39985569 -0.71435935  1.39985569 -0.71435935 -0.71435935  1.39985569\n",
      " -0.71435935  1.39985569 -0.71435935 -0.71435935 -0.71435935 -0.71435935\n",
      "  1.39985569  1.39985569 -0.71435935  1.39985569  1.39985569 -0.71435935\n",
      "  1.39985569  1.39985569 -0.71435935  1.39985569 -0.71435935 -0.71435935\n",
      " -0.71435935 -0.71435935 -0.71435935  1.39985569 -0.71435935  1.39985569\n",
      " -0.71435935 -0.71435935 -0.71435935  1.39985569 -0.71435935 -0.71435935\n",
      " -0.71435935 -0.71435935 -0.71435935  1.39985569 -0.71435935 -0.71435935\n",
      " -0.71435935  1.39985569 -0.71435935 -0.71435935 -0.71435935 -0.71435935\n",
      " -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935  1.39985569\n",
      "  1.39985569  1.39985569  1.39985569  1.39985569  1.39985569 -0.71435935\n",
      " -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935\n",
      " -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935\n",
      " -0.71435935 -0.71435935  1.39985569  1.39985569 -0.71435935  1.39985569\n",
      "  1.39985569 -0.71435935 -0.71435935 -0.71435935 -0.71435935  1.39985569\n",
      "  1.39985569 -0.71435935 -0.71435935 -0.71435935 -0.71435935  1.39985569\n",
      " -0.71435935 -0.71435935  1.39985569 -0.71435935 -0.71435935 -0.71435935\n",
      " -0.71435935 -0.71435935 -0.71435935 -0.71435935  1.39985569  1.39985569\n",
      "  1.39985569 -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935\n",
      " -0.71435935  1.39985569  1.39985569  1.39985569  1.39985569  1.39985569\n",
      " -0.71435935 -0.71435935  1.39985569  1.39985569 -0.71435935  1.39985569\n",
      "  1.39985569  1.39985569 -0.71435935  1.39985569  1.39985569  1.39985569\n",
      "  1.39985569  1.39985569  1.39985569  1.39985569  1.39985569  1.39985569\n",
      "  1.39985569  1.39985569  1.39985569 -0.71435935 -0.71435935  1.39985569\n",
      " -0.71435935  1.39985569  1.39985569  1.39985569 -0.71435935 -0.71435935\n",
      " -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935\n",
      " -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935\n",
      "  1.39985569  1.39985569  1.39985569 -0.71435935  1.39985569 -0.71435935\n",
      " -0.71435935 -0.71435935  1.39985569 -0.71435935  1.39985569  1.39985569\n",
      " -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935  1.39985569\n",
      "  1.39985569 -0.71435935 -0.71435935 -0.71435935 -0.71435935  1.39985569\n",
      "  1.39985569 -0.71435935  1.39985569 -0.71435935 -0.71435935 -0.71435935\n",
      "  1.39985569 -0.71435935 -0.71435935 -0.71435935  1.39985569 -0.71435935\n",
      "  1.39985569 -0.71435935 -0.71435935 -0.71435935 -0.71435935  1.39985569\n",
      " -0.71435935  1.39985569 -0.71435935 -0.71435935 -0.71435935 -0.71435935\n",
      "  1.39985569  1.39985569 -0.71435935 -0.71435935 -0.71435935 -0.71435935\n",
      " -0.71435935 -0.71435935 -0.71435935  1.39985569  1.39985569 -0.71435935\n",
      " -0.71435935 -0.71435935 -0.71435935  1.39985569 -0.71435935 -0.71435935\n",
      " -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935  1.39985569\n",
      "  1.39985569 -0.71435935 -0.71435935  1.39985569 -0.71435935 -0.71435935\n",
      " -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935\n",
      "  1.39985569  1.39985569 -0.71435935 -0.71435935 -0.71435935 -0.71435935\n",
      "  1.39985569  1.39985569 -0.71435935 -0.71435935 -0.71435935 -0.71435935\n",
      " -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935\n",
      " -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935\n",
      " -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935\n",
      " -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935  1.39985569\n",
      " -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935  1.39985569\n",
      " -0.71435935 -0.71435935 -0.71435935 -0.71435935  1.39985569 -0.71435935\n",
      " -0.71435935  1.39985569  1.39985569 -0.71435935 -0.71435935  1.39985569\n",
      "  1.39985569  1.39985569  1.39985569 -0.71435935 -0.71435935 -0.71435935\n",
      "  1.39985569 -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935\n",
      "  1.39985569 -0.71435935 -0.71435935  1.39985569  1.39985569  1.39985569\n",
      " -0.71435935 -0.71435935 -0.71435935  1.39985569  1.39985569  1.39985569\n",
      "  1.39985569 -0.71435935 -0.71435935  1.39985569 -0.71435935 -0.71435935\n",
      " -0.71435935 -0.71435935 -0.71435935  1.39985569  1.39985569 -0.71435935\n",
      " -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935\n",
      "  1.39985569  1.39985569  1.39985569  1.39985569  1.39985569  1.39985569\n",
      " -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935  1.39985569\n",
      " -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935  1.39985569\n",
      " -0.71435935 -0.71435935  1.39985569 -0.71435935 -0.71435935 -0.71435935\n",
      " -0.71435935 -0.71435935 -0.71435935 -0.71435935  1.39985569 -0.71435935\n",
      " -0.71435935 -0.71435935 -0.71435935  1.39985569 -0.71435935 -0.71435935\n",
      " -0.71435935  1.39985569 -0.71435935 -0.71435935 -0.71435935 -0.71435935\n",
      " -0.71435935 -0.71435935 -0.71435935  1.39985569  1.39985569 -0.71435935\n",
      " -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935\n",
      " -0.71435935  1.39985569 -0.71435935  1.39985569 -0.71435935 -0.71435935\n",
      " -0.71435935  1.39985569  1.39985569 -0.71435935 -0.71435935 -0.71435935\n",
      "  1.39985569  1.39985569 -0.71435935  1.39985569  1.39985569 -0.71435935\n",
      "  1.39985569 -0.71435935  1.39985569 -0.71435935 -0.71435935  1.39985569\n",
      " -0.71435935  1.39985569 -0.71435935  1.39985569  1.39985569  1.39985569\n",
      " -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935\n",
      " -0.71435935 -0.71435935  1.39985569  1.39985569 -0.71435935 -0.71435935\n",
      " -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935\n",
      " -0.71435935 -0.71435935 -0.71435935  1.39985569 -0.71435935 -0.71435935\n",
      " -0.71435935 -0.71435935 -0.71435935 -0.71435935  1.39985569  1.39985569\n",
      " -0.71435935 -0.71435935 -0.71435935  1.39985569 -0.71435935 -0.71435935\n",
      " -0.71435935 -0.71435935 -0.71435935  1.39985569 -0.71435935 -0.71435935\n",
      "  1.39985569 -0.71435935 -0.71435935 -0.71435935 -0.71435935  1.39985569\n",
      " -0.71435935  1.39985569 -0.71435935  1.39985569 -0.71435935  1.39985569\n",
      "  1.39985569 -0.71435935  1.39985569 -0.71435935 -0.71435935 -0.71435935\n",
      " -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935\n",
      " -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935  1.39985569\n",
      " -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935\n",
      " -0.71435935 -0.71435935  1.39985569  1.39985569 -0.71435935 -0.71435935\n",
      " -0.71435935  1.39985569 -0.71435935 -0.71435935 -0.71435935 -0.71435935\n",
      " -0.71435935  1.39985569 -0.71435935  1.39985569 -0.71435935  1.39985569\n",
      " -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935\n",
      " -0.71435935 -0.71435935  1.39985569  1.39985569 -0.71435935  1.39985569\n",
      "  1.39985569  1.39985569  1.39985569 -0.71435935 -0.71435935 -0.71435935\n",
      " -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935\n",
      " -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935  1.39985569\n",
      "  1.39985569  1.39985569 -0.71435935 -0.71435935 -0.71435935 -0.71435935\n",
      "  1.39985569  1.39985569 -0.71435935 -0.71435935  1.39985569 -0.71435935\n",
      " -0.71435935  1.39985569 -0.71435935 -0.71435935 -0.71435935 -0.71435935\n",
      " -0.71435935 -0.71435935  1.39985569  1.39985569  1.39985569 -0.71435935\n",
      " -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935 -0.71435935\n",
      "  1.39985569  1.39985569  1.39985569  1.39985569  1.39985569 -0.71435935\n",
      "  1.39985569 -0.71435935  1.39985569  1.39985569  1.39985569  1.39985569\n",
      "  1.39985569  1.39985569  1.39985569]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  features_normalized.loc[valid_mask, col] = normalized_data\n",
      "/Users/rancze/Documents/GitHub/GitHub - CURSOR/vestibular_vr_pipeline/sleap/visualization.py:132: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[ 2.25924029  2.25924029  2.25924029  2.25924029  2.25924029  2.25924029\n",
      "  2.25924029  2.25924029 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      "  2.25924029  2.25924029 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      " -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      " -0.44262667 -0.44262667 -0.44262667  2.25924029  2.25924029 -0.44262667\n",
      " -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      "  2.25924029 -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      "  2.25924029 -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      " -0.44262667  2.25924029 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      " -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      "  2.25924029 -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      " -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      " -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      " -0.44262667  2.25924029 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      " -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      " -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      "  2.25924029 -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      " -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      " -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      "  2.25924029 -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      " -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      "  2.25924029 -0.44262667 -0.44262667 -0.44262667  2.25924029  2.25924029\n",
      "  2.25924029  2.25924029  2.25924029  2.25924029  2.25924029 -0.44262667\n",
      " -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      " -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      "  2.25924029  2.25924029 -0.44262667 -0.44262667  2.25924029  2.25924029\n",
      "  2.25924029  2.25924029 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      " -0.44262667 -0.44262667 -0.44262667  2.25924029  2.25924029 -0.44262667\n",
      " -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      " -0.44262667  2.25924029  2.25924029  2.25924029 -0.44262667 -0.44262667\n",
      "  2.25924029 -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      " -0.44262667  2.25924029 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      " -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      "  2.25924029  2.25924029 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      " -0.44262667 -0.44262667 -0.44262667 -0.44262667  2.25924029 -0.44262667\n",
      " -0.44262667 -0.44262667 -0.44262667 -0.44262667  2.25924029 -0.44262667\n",
      " -0.44262667 -0.44262667 -0.44262667 -0.44262667  2.25924029 -0.44262667\n",
      " -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      " -0.44262667  2.25924029 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      " -0.44262667  2.25924029 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      "  2.25924029 -0.44262667 -0.44262667  2.25924029 -0.44262667 -0.44262667\n",
      " -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      "  2.25924029 -0.44262667 -0.44262667  2.25924029  2.25924029 -0.44262667\n",
      " -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      "  2.25924029 -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      " -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      " -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      " -0.44262667  2.25924029  2.25924029  2.25924029  2.25924029 -0.44262667\n",
      " -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      " -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      " -0.44262667  2.25924029 -0.44262667 -0.44262667  2.25924029 -0.44262667\n",
      " -0.44262667  2.25924029 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      " -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      " -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      " -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      " -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      " -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      "  2.25924029  2.25924029  2.25924029  2.25924029  2.25924029 -0.44262667\n",
      " -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      " -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      " -0.44262667 -0.44262667 -0.44262667  2.25924029 -0.44262667 -0.44262667\n",
      "  2.25924029 -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      "  2.25924029 -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      " -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      " -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667  2.25924029\n",
      "  2.25924029 -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      " -0.44262667 -0.44262667  2.25924029  2.25924029  2.25924029  2.25924029\n",
      " -0.44262667 -0.44262667 -0.44262667  2.25924029 -0.44262667 -0.44262667\n",
      "  2.25924029  2.25924029 -0.44262667 -0.44262667  2.25924029  2.25924029\n",
      "  2.25924029  2.25924029  2.25924029  2.25924029  2.25924029  2.25924029\n",
      "  2.25924029  2.25924029  2.25924029 -0.44262667 -0.44262667 -0.44262667\n",
      " -0.44262667 -0.44262667  2.25924029  2.25924029 -0.44262667 -0.44262667\n",
      " -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      " -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      " -0.44262667  2.25924029  2.25924029 -0.44262667 -0.44262667 -0.44262667\n",
      " -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667  2.25924029\n",
      " -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      "  2.25924029 -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      "  2.25924029 -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      " -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      " -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      " -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      " -0.44262667  2.25924029 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      " -0.44262667 -0.44262667 -0.44262667 -0.44262667  2.25924029 -0.44262667\n",
      " -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      " -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      "  2.25924029 -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      " -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      " -0.44262667  2.25924029 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      " -0.44262667  2.25924029 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      " -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      " -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      " -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      " -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      " -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      " -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      " -0.44262667 -0.44262667  2.25924029 -0.44262667 -0.44262667 -0.44262667\n",
      "  2.25924029  2.25924029  2.25924029 -0.44262667 -0.44262667 -0.44262667\n",
      " -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      " -0.44262667 -0.44262667 -0.44262667 -0.44262667  2.25924029  2.25924029\n",
      " -0.44262667 -0.44262667 -0.44262667 -0.44262667  2.25924029  2.25924029\n",
      "  2.25924029 -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      " -0.44262667 -0.44262667 -0.44262667 -0.44262667  2.25924029 -0.44262667\n",
      " -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      " -0.44262667  2.25924029  2.25924029  2.25924029  2.25924029  2.25924029\n",
      " -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      " -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      " -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      " -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      " -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      " -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      " -0.44262667 -0.44262667 -0.44262667 -0.44262667  2.25924029 -0.44262667\n",
      " -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      " -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      " -0.44262667 -0.44262667  2.25924029 -0.44262667 -0.44262667 -0.44262667\n",
      " -0.44262667  2.25924029 -0.44262667 -0.44262667  2.25924029 -0.44262667\n",
      " -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      " -0.44262667 -0.44262667 -0.44262667 -0.44262667  2.25924029  2.25924029\n",
      " -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      " -0.44262667 -0.44262667 -0.44262667  2.25924029 -0.44262667 -0.44262667\n",
      " -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      " -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      " -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667  2.25924029\n",
      " -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      " -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      " -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      " -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      "  2.25924029 -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      " -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      " -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      " -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      " -0.44262667 -0.44262667 -0.44262667  2.25924029 -0.44262667 -0.44262667\n",
      " -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      " -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      " -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      " -0.44262667 -0.44262667 -0.44262667  2.25924029 -0.44262667 -0.44262667\n",
      "  2.25924029  2.25924029  2.25924029 -0.44262667 -0.44262667 -0.44262667\n",
      " -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      " -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      "  2.25924029  2.25924029 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      " -0.44262667  2.25924029 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      " -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      " -0.44262667 -0.44262667 -0.44262667  2.25924029  2.25924029 -0.44262667\n",
      " -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667 -0.44262667\n",
      " -0.44262667  2.25924029  2.25924029  2.25924029  2.25924029 -0.44262667\n",
      " -0.44262667 -0.44262667 -0.44262667  2.25924029  2.25924029  2.25924029\n",
      "  2.25924029  2.25924029  2.25924029]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  features_normalized.loc[valid_mask, col] = normalized_data\n",
      "/Users/rancze/Documents/GitHub/GitHub - CURSOR/vestibular_vr_pipeline/sleap/visualization.py:132: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[-0.37321704 -1.44180916  0.69537508  0.69537508  0.69537508 -1.44180916\n",
      " -1.44180916  0.69537508 -1.44180916 -1.44180916  0.69537508  0.69537508\n",
      "  0.69537508 -1.44180916 -1.44180916  0.69537508 -1.44180916  0.69537508\n",
      " -1.44180916  0.69537508  0.69537508 -1.44180916 -1.44180916  0.69537508\n",
      "  0.69537508  0.69537508  0.69537508 -1.44180916  0.69537508 -1.44180916\n",
      "  0.69537508  0.69537508  0.69537508  0.69537508  0.69537508  0.69537508\n",
      " -1.44180916 -1.44180916  0.69537508  0.69537508  0.69537508  0.69537508\n",
      " -1.44180916 -1.44180916  0.69537508  0.69537508  0.69537508 -1.44180916\n",
      " -1.44180916  0.69537508 -1.44180916 -1.44180916  0.69537508  0.69537508\n",
      "  0.69537508  0.69537508  0.69537508  0.69537508 -1.44180916 -1.44180916\n",
      " -1.44180916  0.69537508 -1.44180916  0.69537508  0.69537508  0.69537508\n",
      "  0.69537508 -1.44180916  0.69537508 -1.44180916  0.69537508  0.69537508\n",
      " -1.44180916  0.69537508  0.69537508 -1.44180916  0.69537508  0.69537508\n",
      "  0.69537508 -1.44180916  0.69537508  0.69537508  0.69537508 -1.44180916\n",
      " -1.44180916  0.69537508  0.69537508  0.69537508  0.69537508  0.69537508\n",
      "  0.69537508 -1.44180916 -1.44180916  0.69537508  0.69537508 -1.44180916\n",
      " -1.44180916  0.69537508 -1.44180916  0.69537508 -1.44180916 -1.44180916\n",
      "  0.69537508  0.69537508  0.69537508  0.69537508  0.69537508  0.69537508\n",
      " -1.44180916 -1.44180916  0.69537508  0.69537508  0.69537508  0.69537508\n",
      " -1.44180916  0.69537508 -1.44180916  0.69537508 -1.44180916 -1.44180916\n",
      "  0.69537508 -1.44180916 -1.44180916  0.69537508 -1.44180916 -1.44180916\n",
      "  0.69537508  0.69537508  0.69537508  0.69537508  0.69537508  0.69537508\n",
      "  0.69537508  0.69537508  0.69537508  0.69537508 -1.44180916 -1.44180916\n",
      "  0.69537508  0.69537508  0.69537508 -1.44180916 -1.44180916  0.69537508\n",
      "  0.69537508  0.69537508  0.69537508  0.69537508  0.69537508  0.69537508\n",
      "  0.69537508  0.69537508  0.69537508  0.69537508  0.69537508  0.69537508\n",
      " -1.44180916 -1.44180916 -1.44180916 -1.44180916  0.69537508  0.69537508\n",
      "  0.69537508  0.69537508  0.69537508 -1.44180916  0.69537508 -1.44180916\n",
      "  0.69537508  0.69537508  0.69537508  0.69537508  0.69537508 -1.44180916\n",
      " -1.44180916  0.69537508 -1.44180916 -1.44180916 -1.44180916 -1.44180916\n",
      "  0.69537508  0.69537508  0.69537508  0.69537508  0.69537508  0.69537508\n",
      "  0.69537508 -1.44180916 -1.44180916  0.69537508  0.69537508  0.69537508\n",
      "  0.69537508  0.69537508  0.69537508  0.69537508  0.69537508  0.69537508\n",
      " -1.44180916 -1.44180916  0.69537508  0.69537508  0.69537508  0.69537508\n",
      "  0.69537508 -1.44180916 -1.44180916  0.69537508  0.69537508  0.69537508\n",
      "  0.69537508  0.69537508  0.69537508  0.69537508 -1.44180916 -1.44180916\n",
      "  0.69537508  0.69537508  0.69537508  0.69537508 -1.44180916 -1.44180916\n",
      "  0.69537508  0.69537508  0.69537508  0.69537508  0.69537508  0.69537508\n",
      "  0.69537508 -1.44180916 -1.44180916 -1.44180916 -1.44180916  0.69537508\n",
      "  0.69537508 -1.44180916 -1.44180916  0.69537508  0.69537508  0.69537508\n",
      "  0.69537508  0.69537508  0.69537508  0.69537508  0.69537508  0.69537508\n",
      "  0.69537508  0.69537508  0.69537508  0.69537508  0.69537508  0.69537508\n",
      "  0.69537508 -1.44180916  0.69537508 -1.44180916 -1.44180916 -1.44180916\n",
      "  0.69537508  0.69537508  0.69537508  0.69537508  0.69537508  0.69537508\n",
      " -1.44180916 -1.44180916  0.69537508  0.69537508 -1.44180916 -1.44180916\n",
      "  0.69537508  0.69537508  0.69537508  0.69537508  0.69537508  0.69537508\n",
      " -1.44180916 -1.44180916 -1.44180916 -1.44180916  0.69537508 -1.44180916\n",
      " -1.44180916 -1.44180916  0.69537508  0.69537508 -1.44180916  0.69537508\n",
      "  0.69537508  0.69537508  0.69537508  0.69537508  0.69537508  0.69537508\n",
      " -1.44180916 -1.44180916  0.69537508  0.69537508  0.69537508  0.69537508\n",
      "  0.69537508  0.69537508  0.69537508  0.69537508 -1.44180916 -1.44180916\n",
      "  0.69537508  0.69537508 -1.44180916 -1.44180916 -1.44180916 -1.44180916\n",
      "  0.69537508  0.69537508 -1.44180916 -1.44180916 -1.44180916 -1.44180916\n",
      "  0.69537508  0.69537508  0.69537508  0.69537508  0.69537508  0.69537508\n",
      "  0.69537508  0.69537508  0.69537508  0.69537508 -1.44180916  0.69537508\n",
      " -1.44180916 -1.44180916 -1.44180916 -1.44180916 -1.44180916  0.69537508\n",
      "  0.69537508  0.69537508  0.69537508  0.69537508  0.69537508  0.69537508\n",
      " -1.44180916 -1.44180916  0.69537508  0.69537508 -1.44180916 -1.44180916\n",
      " -1.44180916 -1.44180916 -1.44180916 -1.44180916  0.69537508  0.69537508\n",
      "  0.69537508  0.69537508  0.69537508  0.69537508  0.69537508  0.69537508\n",
      "  0.69537508  0.69537508  0.69537508 -1.44180916  0.69537508  0.69537508\n",
      " -1.44180916  0.69537508  0.69537508  0.69537508  0.69537508  0.69537508\n",
      " -1.44180916  0.69537508 -1.44180916  0.69537508  0.69537508  0.69537508\n",
      "  0.69537508  0.69537508  0.69537508  0.69537508 -1.44180916 -1.44180916\n",
      "  0.69537508  0.69537508  0.69537508  0.69537508  0.69537508 -1.44180916\n",
      "  0.69537508 -1.44180916  0.69537508  0.69537508  0.69537508  0.69537508\n",
      "  0.69537508  0.69537508  0.69537508  0.69537508  0.69537508 -1.44180916\n",
      " -1.44180916  0.69537508 -1.44180916 -1.44180916 -1.44180916 -1.44180916\n",
      " -1.44180916 -1.44180916 -1.44180916 -1.44180916  0.69537508 -0.37321704\n",
      "  0.69537508 -1.44180916 -1.44180916  0.69537508  0.69537508 -1.44180916\n",
      " -1.44180916  0.69537508 -1.44180916 -1.44180916  0.69537508  0.69537508\n",
      "  0.69537508  0.69537508  0.69537508 -1.44180916 -1.44180916  0.69537508\n",
      " -1.44180916 -1.44180916 -1.44180916 -1.44180916  0.69537508  0.69537508\n",
      "  0.69537508  0.69537508 -1.44180916 -1.44180916  0.69537508  0.69537508\n",
      " -1.44180916  0.69537508 -1.44180916  0.69537508  0.69537508  0.69537508\n",
      "  0.69537508  0.69537508  0.69537508  0.69537508  0.69537508 -1.44180916\n",
      " -1.44180916  0.69537508  0.69537508  0.69537508  0.69537508  0.69537508\n",
      " -1.44180916 -1.44180916  0.69537508  0.69537508  0.69537508  0.69537508\n",
      " -1.44180916 -1.44180916  0.69537508 -1.44180916 -1.44180916  0.69537508\n",
      "  0.69537508  0.69537508  0.69537508  0.69537508  0.69537508 -1.44180916\n",
      " -1.44180916 -1.44180916 -1.44180916  0.69537508  0.69537508  0.69537508\n",
      "  0.69537508 -1.44180916  0.69537508 -1.44180916  0.69537508  0.69537508\n",
      "  0.69537508  0.69537508 -1.44180916 -1.44180916  0.69537508  0.69537508\n",
      "  0.69537508  0.69537508  0.69537508  0.69537508 -1.44180916  0.69537508\n",
      "  0.69537508  0.69537508 -1.44180916 -1.44180916  0.69537508  0.69537508\n",
      "  0.69537508  0.69537508  0.69537508  0.69537508  0.69537508  0.69537508\n",
      "  0.69537508  0.69537508 -1.44180916  0.69537508 -1.44180916 -1.44180916\n",
      "  0.69537508  0.69537508  0.69537508  0.69537508  0.69537508  0.69537508\n",
      "  0.69537508 -1.44180916 -1.44180916  0.69537508  0.69537508  0.69537508\n",
      "  0.69537508 -1.44180916 -1.44180916 -1.44180916  0.69537508  0.69537508\n",
      " -1.44180916 -1.44180916 -1.44180916  0.69537508  0.69537508  0.69537508\n",
      " -1.44180916 -1.44180916 -1.44180916 -1.44180916 -1.44180916 -1.44180916\n",
      " -1.44180916 -1.44180916  0.69537508 -1.44180916 -1.44180916  0.69537508\n",
      " -1.44180916 -1.44180916 -1.44180916 -1.44180916  0.69537508 -1.44180916\n",
      "  0.69537508 -1.44180916  0.69537508  0.69537508  0.69537508 -1.44180916\n",
      " -1.44180916  0.69537508  0.69537508  0.69537508  0.69537508  0.69537508\n",
      "  0.69537508  0.69537508  0.69537508  0.69537508  0.69537508  0.69537508\n",
      "  0.69537508  0.69537508 -1.44180916 -1.44180916  0.69537508  0.69537508\n",
      "  0.69537508 -1.44180916 -1.44180916  0.69537508  0.69537508  0.69537508\n",
      "  0.69537508  0.69537508  0.69537508  0.69537508  0.69537508  0.69537508\n",
      "  0.69537508  0.69537508  0.69537508  0.69537508  0.69537508  0.69537508\n",
      " -1.44180916 -1.44180916  0.69537508 -1.44180916 -1.44180916 -1.44180916\n",
      " -1.44180916  0.69537508  0.69537508  0.69537508 -1.44180916 -1.44180916\n",
      "  0.69537508  0.69537508  0.69537508  0.69537508  0.69537508  0.69537508\n",
      "  0.69537508  0.69537508 -1.44180916 -1.44180916 -1.44180916 -1.44180916\n",
      "  0.69537508  0.69537508  0.69537508  0.69537508  0.69537508  0.69537508\n",
      "  0.69537508  0.69537508  0.69537508  0.69537508  0.69537508  0.69537508\n",
      "  0.69537508  0.69537508  0.69537508  0.69537508  0.69537508  0.69537508\n",
      "  0.69537508  0.69537508  0.69537508  0.69537508  0.69537508  0.69537508\n",
      "  0.69537508  0.69537508  0.69537508  0.69537508  0.69537508  0.69537508\n",
      "  0.69537508  0.69537508  0.69537508  0.69537508  0.69537508  0.69537508\n",
      "  0.69537508  0.69537508  0.69537508  0.69537508 -1.44180916 -1.44180916\n",
      "  0.69537508  0.69537508  0.69537508  0.69537508  0.69537508  0.69537508\n",
      "  0.69537508  0.69537508 -1.44180916 -1.44180916  0.69537508  0.69537508\n",
      "  0.69537508  0.69537508 -1.44180916 -1.44180916  0.69537508  0.69537508\n",
      "  0.69537508  0.69537508  0.69537508  0.69537508  0.69537508  0.69537508\n",
      "  0.69537508  0.69537508  0.69537508  0.69537508  0.69537508  0.69537508\n",
      "  0.69537508 -1.44180916  0.69537508  0.69537508 -1.44180916 -1.44180916\n",
      " -1.44180916  0.69537508  0.69537508  0.69537508  0.69537508  0.69537508\n",
      "  0.69537508  0.69537508  0.69537508 -1.44180916 -1.44180916  0.69537508\n",
      "  0.69537508  0.69537508  0.69537508  0.69537508  0.69537508  0.69537508\n",
      "  0.69537508  0.69537508 -1.44180916 -1.44180916 -1.44180916 -1.44180916\n",
      "  0.69537508 -1.44180916 -1.44180916  0.69537508  0.69537508 -1.44180916\n",
      " -1.44180916  0.69537508  0.69537508  0.69537508  0.69537508  0.69537508\n",
      "  0.69537508  0.69537508  0.69537508 -1.44180916 -1.44180916  0.69537508\n",
      "  0.69537508  0.69537508  0.69537508  0.69537508  0.69537508  0.69537508\n",
      "  0.69537508  0.69537508  0.69537508  0.69537508  0.69537508  0.69537508\n",
      "  0.69537508 -1.44180916  0.69537508 -1.44180916  0.69537508 -1.44180916\n",
      " -1.44180916  0.69537508  0.69537508  0.69537508  0.69537508 -1.44180916\n",
      " -1.44180916  0.69537508 -1.44180916 -1.44180916  0.69537508 -1.44180916\n",
      " -1.44180916  0.69537508 -1.44180916 -1.44180916 -1.44180916 -1.44180916\n",
      "  0.69537508 -1.44180916 -1.44180916 -1.44180916 -1.44180916  0.69537508\n",
      "  0.69537508  0.69537508  0.69537508  0.69537508  0.69537508  0.69537508\n",
      "  0.69537508  0.69537508 -1.44180916  0.69537508 -1.44180916 -1.44180916\n",
      " -1.44180916  0.69537508  0.69537508  0.69537508  0.69537508  0.69537508\n",
      " -1.44180916 -1.44180916  0.69537508 -1.44180916 -1.44180916 -1.44180916\n",
      " -1.44180916  0.69537508 -1.44180916 -1.44180916 -1.44180916 -1.44180916\n",
      "  0.69537508  0.69537508  0.69537508  0.69537508  0.69537508  0.69537508\n",
      "  0.69537508  0.69537508  0.69537508  0.69537508  0.69537508  0.69537508\n",
      " -1.44180916 -1.44180916  0.69537508  0.69537508  0.69537508  0.69537508\n",
      "  0.69537508 -1.44180916 -1.44180916  0.69537508  0.69537508  0.69537508\n",
      "  0.69537508  0.69537508  0.69537508  0.69537508  0.69537508  0.69537508\n",
      "  0.69537508  0.69537508  0.69537508 -1.44180916  0.69537508 -1.44180916\n",
      "  0.69537508  0.69537508  0.69537508  0.69537508  0.69537508  0.69537508\n",
      "  0.69537508  0.69537508  0.69537508  0.69537508  0.69537508  0.69537508\n",
      "  0.69537508 -1.44180916 -1.44180916  0.69537508 -1.44180916 -1.44180916\n",
      "  0.69537508  0.69537508  0.69537508]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  features_normalized.loc[valid_mask, col] = normalized_data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   üîç Plotting category 'Category A: Basic Properties' with 7 features (plot_idx=0)\n",
      "   ‚úÖ Plotted category 'Category A: Basic Properties' (subplot 1/6)\n",
      "   üîç Plotting category 'Category B: Pre-Saccade' with 5 features (plot_idx=1)\n",
      "   ‚úÖ Plotted category 'Category B: Pre-Saccade' (subplot 2/6)\n",
      "   üîç Plotting category 'Category C: Post-Saccade' with 6 features (plot_idx=2)\n",
      "   ‚úÖ Plotted category 'Category C: Post-Saccade' (subplot 3/6)\n",
      "   üîç Plotting category 'Category D: Temporal Context' with 10 features (plot_idx=3)\n",
      "   ‚úÖ Plotted category 'Category D: Temporal Context' (subplot 4/6)\n",
      "   üîç Plotting category 'Category G: Amplitude/Direction Consistency' with 3 features (plot_idx=4)\n",
      "   ‚úÖ Plotted category 'Category G: Amplitude/Direction Consistency' (subplot 5/6)\n",
      "   üîç Plotting category 'Category H: Rule-Based Classification' with 1 features (plot_idx=5)\n",
      "   ‚úÖ Plotted category 'Category H: Rule-Based Classification' (subplot 6/6)\n",
      "\n",
      "üîç DIAGNOSTIC: Panel 1 summary\n",
      "   Total categories plotted: 6\n",
      "   Total subplots created: 6\n",
      "\n",
      "================================================================================\n",
      "PANEL 2: Rule-Based Classification Comparison (Key Features)\n",
      "================================================================================\n",
      "\n",
      "üîç DIAGNOSTIC: Preparing Panel 2 data\n",
      "   Key features requested: 15\n",
      "   Available key features: 15\n",
      "   Has rule_based_class_label column: True\n",
      "   ‚úÖ Processed 879 rows\n",
      "   ‚ö†Ô∏è Skipped 0 rows with 'Unknown' or NaN class label\n",
      "   ‚úÖ Created 13152 data points\n",
      "   ‚úÖ class_plot_df shape: (13152, 3)\n",
      "   ‚úÖ Columns in class_plot_df: ['Feature', 'Value', 'Rule_Based_Class']\n",
      "   ‚úÖ Unique Rule_Based_Class values: ['Compensatory' 'Orienting']\n",
      "   ‚úÖ Unique Features: ['amplitude' 'duration' 'peak_velocity' 'pre_saccade_mean_velocity'\n",
      " 'pre_saccade_position_drift' 'post_saccade_position_variance'\n",
      " 'post_saccade_position_change' 'bout_size' 'time_until_next_saccade'\n",
      " 'is_first_in_bout' 'is_isolated' 'bout_duration'\n",
      " 'amplitude_relative_to_bout_mean' 'rule_based_confidence'\n",
      " 'time_since_previous_saccade']\n",
      "   ‚úÖ Sample data (first 3 rows):\n",
      "         Feature     Value Rule_Based_Class\n",
      "0      amplitude -0.876991     Compensatory\n",
      "1       duration  0.095427     Compensatory\n",
      "2  peak_velocity  1.151539     Compensatory\n",
      "\n",
      "   ‚úÖ Creating Panel 2 figure with 15 features\n",
      "   Figure layout: 5 rows x 3 cols = 15 subplots\n",
      "\n",
      "‚úÖ Panel 2 complete: Compared 15 key features across classes\n",
      "\n",
      "‚úÖ Feature visualization complete!\n",
      "   Normalized 33 features\n",
      "   Visualized 6 categories\n",
      "   Created 2 panels: Category Distributions, Class Comparison\n"
     ]
    }
   ],
   "source": [
    "# Visualize Feature Distributions: Panel 1 (Violin Plots by Category) + Panel 2 (Key Features by Class)\n",
    "############################################################################################################\n",
    "\n",
    "if features_combined is not None and len(features_combined) > 0:\n",
    "    # Define feature categories\n",
    "    feature_categories = {\n",
    "        'Category A: Basic Properties': [\n",
    "            'amplitude', 'duration', 'peak_velocity', 'direction', \n",
    "            'start_time', 'end_time', 'time'\n",
    "        ],\n",
    "        'Category B: Pre-Saccade': [\n",
    "            'pre_saccade_mean_velocity', 'pre_saccade_position_drift',\n",
    "            'pre_saccade_position_variance', 'pre_saccade_drift_rate',\n",
    "            'pre_saccade_window_duration'\n",
    "        ],\n",
    "        'Category C: Post-Saccade': [\n",
    "            'post_saccade_position_variance', 'post_saccade_position_change',\n",
    "            'post_saccade_position_change_pct', 'post_saccade_mean_velocity',\n",
    "            'post_saccade_drift_rate', 'post_saccade_window_duration'\n",
    "        ],\n",
    "        'Category D: Temporal Context': [\n",
    "            'time_since_previous_saccade', 'time_until_next_saccade',\n",
    "            'bout_size', 'position_in_bout', 'is_first_in_bout',\n",
    "            'is_last_in_bout', 'is_isolated', 'bout_duration',\n",
    "            'inter_saccade_interval_mean', 'inter_saccade_interval_std'\n",
    "        ],\n",
    "        'Category G: Amplitude/Direction Consistency': [\n",
    "            'amplitude_relative_to_bout_mean', 'amplitude_consistency_in_bout',\n",
    "            'direction_relative_to_previous'\n",
    "        ],\n",
    "        'Category H: Rule-Based Classification': [\n",
    "            'rule_based_class', 'rule_based_confidence'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Use the visualization function\n",
    "    visualize_ml_features(\n",
    "        features_combined=features_combined,\n",
    "        feature_categories=feature_categories,\n",
    "        video_labels=VIDEO_LABELS,\n",
    "        show_plots=True,\n",
    "        verbose=True\n",
    "    )\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No features available for visualization. Run feature extraction first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "276fcae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment ID: Cohort3_20250409_Cohort3_rotation_B6J2782_2025-04-28\n",
      "\n",
      "================================================================================\n",
      "Launching GUI Annotation Tool\n",
      "================================================================================\n",
      "Experiment ID: Cohort3_20250409_Cohort3_rotation_B6J2782_2025-04-28\n",
      "Video: VideoData1 (VideoData1 (L: Left))\n",
      "Saccades: 413\n",
      "Features: 413\n",
      "Annotations file: /Users/rancze/Documents/Data/vestVR/20250409_Cohort3_rotation/Visual_mismatch_day4/saccade_annotations_master.csv\n",
      "================================================================================\n",
      "\n",
      "‚ÑπÔ∏è Instructions:\n",
      "  - Use keyboard shortcuts: 1=Compensatory, 2=Orienting, 3=Saccade-and-Fixate, 4=Non-Saccade\n",
      "  - Navigation: N=Next, P=Previous, S=Save\n",
      "  - Click on saccades in the table to select them\n",
      "  - Close the GUI window when done annotating\n",
      "================================================================================\n",
      "\n",
      "‚ö†Ô∏è Annotations file not found: /Users/rancze/Documents/Data/vestVR/20250409_Cohort3_rotation/Visual_mismatch_day4/saccade_annotations_master.csv\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rancze/opt/anaconda3/envs/aeon/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3675: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# LAUNCH GUI ANNOTATION TOOL\n",
    "############################################################################################################\n",
    "# Use this cell to launch the GUI for manual annotation of saccades\n",
    "# The GUI starts with rule-based classifications and allows you to correct them to 4 classes\n",
    "\n",
    "# Get experiment ID\n",
    "experiment_id = extract_experiment_id(data_path)\n",
    "print(f\"Experiment ID: {experiment_id}\")\n",
    "\n",
    "# Choose which video to annotate (VideoData1 or VideoData2)\n",
    "# Change this to annotate a different eye\n",
    "video_key = 'VideoData1'  # Options: 'VideoData1' or 'VideoData2'\n",
    "\n",
    "if video_key in saccade_results:\n",
    "    # Get saccade results for this video\n",
    "    video_saccade_results = saccade_results[video_key]\n",
    "    \n",
    "    # Get features for this video (if available)\n",
    "    video_features = None\n",
    "    if features_combined is not None and len(features_combined) > 0:\n",
    "        # Filter features for this video\n",
    "        # The video_label column should contain the video key\n",
    "        if 'video_label' in features_combined.columns:\n",
    "            # Find features for this video (check if video_label contains VideoData1 or VideoData2)\n",
    "            video_num = video_key.replace('VideoData', '')\n",
    "            video_features = features_combined[\n",
    "                features_combined['video_label'].str.contains(video_num, na=False)\n",
    "            ].copy()\n",
    "        else:\n",
    "            # If no video_label column, try to split by eye\n",
    "            if video_key == 'VideoData1':\n",
    "                eye_label = 'Left' if video1_eye == 'L' else 'Right'\n",
    "            else:\n",
    "                eye_label = 'Right' if video1_eye == 'L' else 'Left'\n",
    "            \n",
    "            if 'eye' in features_combined.columns:\n",
    "                video_features = features_combined[features_combined['eye'] == eye_label].copy()\n",
    "            else:\n",
    "                # Fallback: use all features (less ideal but will work)\n",
    "                video_features = features_combined.copy()\n",
    "    \n",
    "    # Set annotations file path (save in data_path parent directory)\n",
    "    annotations_file = data_path.parent / 'saccade_annotations_master.csv'\n",
    "    # Or use a project-wide location:\n",
    "    # annotations_file = Path('/Users/rancze/Documents/Data/vestVR/saccade_annotations_master.csv')\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Launching GUI Annotation Tool\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Experiment ID: {experiment_id}\")\n",
    "    print(f\"Video: {video_key} ({VIDEO_LABELS.get(video_key, video_key)})\")\n",
    "    print(f\"Saccades: {len(video_saccade_results.get('all_saccades_df', pd.DataFrame()))}\")\n",
    "    print(f\"Features: {len(video_features) if video_features is not None else 0}\")\n",
    "    print(f\"Annotations file: {annotations_file}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    print(\"‚ÑπÔ∏è Instructions:\")\n",
    "    print(\"  - Use keyboard shortcuts: 1=Compensatory, 2=Orienting, 3=Saccade-and-Fixate, 4=Non-Saccade\")\n",
    "    print(\"  - Navigation: N=Next, P=Previous, S=Save\")\n",
    "    print(\"  - Click on saccades in the table to select them\")\n",
    "    print(\"  - Close the GUI window when done annotating\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Launch GUI (this will block until GUI is closed)\n",
    "    launch_annotation_gui(\n",
    "        saccade_results=video_saccade_results,\n",
    "        features_df=video_features,\n",
    "        experiment_id=experiment_id,\n",
    "        annotations_file_path=annotations_file\n",
    "    )\n",
    "    \n",
    "    # After GUI closes, show statistics\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"Annotation session complete!\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Load and display annotations\n",
    "    annotations = load_annotations(annotations_file, experiment_id=experiment_id)\n",
    "    if len(annotations) > 0:\n",
    "        print(f\"\\n‚úÖ Annotated {len(annotations)} saccades for this experiment\")\n",
    "        print_annotation_stats(annotations_file, experiment_id=experiment_id)\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è No annotations saved for this experiment\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è {video_key} not found in saccade_results\")\n",
    "    print(f\"Available keys: {list(saccade_results.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca81678",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "aeon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
