{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0697e807",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d17fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.io as pio\n",
    "import gc\n",
    "import io\n",
    "from contextlib import redirect_stdout\n",
    "\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.colors import LogNorm\n",
    "from fastkde.fastKDE import fastKDE\n",
    "from scipy.stats import linregress\n",
    "from scipy.signal import butter, filtfilt\n",
    "from scipy.signal import correlate\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "from harp_resources import process, utils\n",
    "from sleap import load_and_process as lp\n",
    "from sleap import processing_functions as pf\n",
    "from sleap import saccade_processing as sp\n",
    "from sleap.saccade_processing import analyze_eye_video_saccades\n",
    "\n",
    "# Reload modules to pick up latest changes (useful after code updates)\n",
    "# Set force_reload_modules = True to always reload, or False to use cached versions\n",
    "force_reload_modules = True  # Set to False for faster execution when modules haven't changed\n",
    "if force_reload_modules:\n",
    "    import importlib\n",
    "    import sleap.load_and_process\n",
    "    import sleap.processing_functions\n",
    "    import sleap.saccade_processing\n",
    "    importlib.reload(sleap.load_and_process)\n",
    "    importlib.reload(sleap.processing_functions)\n",
    "    importlib.reload(sleap.saccade_processing)\n",
    "    # Re-import aliases after reload\n",
    "    lp = sleap.load_and_process\n",
    "    pf = sleap.processing_functions\n",
    "    sp = sleap.saccade_processing\n",
    "    from sleap.saccade_processing import analyze_eye_video_saccades\n",
    "\n",
    "def get_eye_label(key):\n",
    "    \"\"\"Return mapped user-viewable eye label for video key.\"\"\"\n",
    "    return VIDEO_LABELS.get(key, key)\n",
    "\n",
    "NaNs_removed = False # keep as false here, it is to checking if NaNs already removed if the notebook cell is rerun\n",
    "\n",
    "\n",
    "# symbols to use ‚úÖ ‚ÑπÔ∏è ‚ö†Ô∏è ‚ùó"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66db980e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# set up variables and load data \n",
    "############################################################################################################\n",
    "\n",
    "# User-editable friendly labels for plotting and console output:\n",
    "\n",
    "debug = True  # Set to True to enable debug output across all cells (file loading, processing, etc.)\n",
    "\n",
    "video1_eye = 'L'  # Options: 'L' or 'R'; which eye does VideoData1 represent? ('L' = Left, 'R' = Right)\n",
    "plot_QC_timeseries = False\n",
    "score_cutoff = 0.2 # for filtering out inferred points with low confidence, they get interpolated \n",
    "outlier_sd_threshold = 10 # for removing outliers from the data, they get interpolated \n",
    "\n",
    "# Pupil diameter filter settings (Butterworth low-pass)\n",
    "pupil_filter_cutoff_hz = 10  # Hz\n",
    "pupil_filter_order = 6\n",
    "\n",
    "# Parameters for blink detection\n",
    "min_blink_duration_ms = 50  # minimum blink duration in milliseconds\n",
    "blink_merge_window_ms = 100  # NOT CURRENTLY USED: merge window was removed to preserve good data between separate blinks\n",
    "long_blink_warning_ms = 2000  # warn if blinks exceed this duration (in ms) - user should verify these are real blinks\n",
    "blink_instance_score_threshold = 3.8  # hard threshold for blink detection - frames with instance.score below this value are considered blinks, calculated as 9 pupil points *0.2 + left/right as 1   \n",
    "\n",
    "# for saccades\n",
    "refractory_period = 0.1  # sec\n",
    "## Separate adaptive saccade threshold (k) for each video:\n",
    "k1 = 4  # for VideoData1 (L)\n",
    "k2 = 5  # for VideoData2 (R)\n",
    "\n",
    "# for adaptive saccade threshold - Number of standard deviations (adjustable: 2-4 range works well) \n",
    "onset_offset_fraction = 0.2  # to determine saccade onset and offset, i.e. o.2 is 20% of the peak velocity\n",
    "n_before = 10  # Number of points before detection peak to extract for peri-saccade-segments, points, so independent of FPS \n",
    "n_after = 30   # Number of points after detection peak to extract\n",
    "\n",
    "plot_saccade_detection_QC = True\n",
    "\n",
    "# Parameters for orienting vs compensatory saccade classification\n",
    "classify_orienting_compensatory = True  # Set to True to classify saccades as orienting vs compensatory\n",
    "bout_window = 1.5  # Time window (seconds) for grouping saccades into bouts\n",
    "pre_saccade_window = 0.3  # Time window (seconds) before saccade onset to analyze\n",
    "max_intersaccade_interval_for_classification = 5.0  # Maximum time (seconds) to extend post-saccade window until next saccade for classification\n",
    "pre_saccade_velocity_threshold = 50.0  # Velocity threshold (px/s) for detecting pre-saccade drift\n",
    "pre_saccade_drift_threshold = 10.0  # Position drift threshold (px) before saccade for compensatory classification\n",
    "post_saccade_variance_threshold = 100.0  # Position variance threshold (px¬≤) after saccade for orienting classification\n",
    "post_saccade_position_change_threshold_percent = 50.0  # Position change threshold (% of saccade amplitude) - if post-saccade change > amplitude * this%, classify as compensatory\n",
    "\n",
    "# Adaptive threshold parameters (percentile-based)\n",
    "use_adaptive_thresholds = True  # Set to True to use adaptive thresholds based on feature distributions, False to use fixed thresholds\n",
    "adaptive_percentile_pre_velocity = 75  # Percentile for pre-saccade velocity threshold (upper percentile for compensatory detection)\n",
    "adaptive_percentile_pre_drift = 75  # Percentile for pre-saccade drift threshold (upper percentile for compensatory detection)\n",
    "adaptive_percentile_post_variance = 25  # Percentile for post-saccade variance threshold (lower percentile for orienting detection - low variance = stable)\n",
    "\n",
    "\"\"\"\n",
    "CLASSIFICATION PARAMETERS EXPLANATION:\n",
    "======================================\n",
    "\n",
    "1. bout_window (1.5 seconds)\n",
    "   - Purpose: Groups saccades that occur within this time window into \"bouts\"\n",
    "   - How it works: If two saccades occur within 1.5s of each other, they're grouped into the same bout\n",
    "   - Reasoning: Compensatory saccades occur in rapid succession during head rotation\n",
    "   - Typical values: 1.0-2.0 seconds (adjust based on your data)\n",
    "\n",
    "2. pre_saccade_window (0.3 seconds)\n",
    "   - Purpose: Time window BEFORE each saccade onset to analyze for drift/stability\n",
    "   - How it works: Measures mean velocity and position drift in the 0.3s before saccade starts\n",
    "     * IMPORTANT: The window is constrained to not extend before the peak time of the previous saccade\n",
    "     * This ensures we only measure the inter-saccade interval, not the period during the previous saccade\n",
    "   - Reasoning: \n",
    "     * Compensatory: Eye drifts slowly before saccade (compensating for head rotation)\n",
    "     * Orienting: Eye is stable before saccade (at rest before quick shift)\n",
    "   - Typical values: 0.2-0.5 seconds (should capture pre-saccade behavior)\n",
    "\n",
    "3. max_intersaccade_interval_for_classification (5.0 seconds)\n",
    "   - Purpose: Maximum time to extend post-saccade window until the next saccade occurs\n",
    "   - How it works: \n",
    "     * For each saccade, the post-saccade window dynamically extends until the next saccade starts\n",
    "     * If the next saccade occurs within max_intersaccade_interval_for_classification, use that interval\n",
    "     * If no next saccade occurs within this time, cap at max_intersaccade_interval_for_classification\n",
    "     * Measures position change/variance in this dynamic window\n",
    "   - Reasoning:\n",
    "     * Compensatory saccades: Eye position continues to change until next compensatory saccade (large position change)\n",
    "     * Orienting saccades: Eye settles at new position and stays stable (small position change)\n",
    "   - Typical values: 3-10 seconds (should capture behavior until next saccade or reasonable maximum)\n",
    "   - This parameter is key for distinguishing compensatory vs orienting based on eye position stability\n",
    "\n",
    "5. pre_saccade_velocity_threshold (50.0 px/s)\n",
    "   - Purpose: Threshold for mean absolute velocity in pre-saccade window\n",
    "   - How it works: If mean(|velocity|) > threshold ‚Üí evidence of drift (compensatory)\n",
    "   - Reasoning: Compensatory saccades follow slow drift (velocity > 0)\n",
    "   - Typical values: 30-100 px/s (adjust based on your sampling rate and noise level)\n",
    "   - Too low: May misclassify orienting saccades with noise as compensatory\n",
    "   - Too high: May miss true compensatory drift\n",
    "\n",
    "6. pre_saccade_drift_threshold (10.0 px)\n",
    "   - Purpose: Threshold for position change in pre-saccade window\n",
    "   - How it works: If |position_end - position_start| > threshold ‚Üí evidence of drift (compensatory)\n",
    "   - Reasoning: Compensatory saccades follow slow position drift (eye moves slowly)\n",
    "   - Typical values: 5-20 px (adjust based on your typical saccade amplitudes)\n",
    "   - Too low: May misclassify orienting saccades with small movements as compensatory\n",
    "   - Too high: May miss true compensatory drift\n",
    "\n",
    "7. post_saccade_variance_threshold (100.0 px¬≤)\n",
    "   - Purpose: Threshold for position variance in post-saccade window\n",
    "   - How it works: If variance < threshold ‚Üí stable position (orienting)\n",
    "   - Reasoning: Orienting saccades settle at new stable position (low variance)\n",
    "   - Typical values: 50-200 px¬≤ (adjust based on your noise level)\n",
    "   - Too low: May misclassify orienting saccades as compensatory\n",
    "   - Too high: May misclassify compensatory saccades as orienting\n",
    "   - Note: Less reliable for saccades in bouts (next saccade may occur soon after)\n",
    "\n",
    "8. post_saccade_position_change_threshold_percent (50.0%)\n",
    "   - Purpose: Threshold for position change in dynamic post-saccade window, expressed as percentage of saccade amplitude\n",
    "   - How it works: \n",
    "     * Calculates total position change from end of saccade until next saccade (or max interval)\n",
    "     * Compares to saccade amplitude: if post_change > amplitude * threshold_percent ‚Üí compensatory\n",
    "     * Compensatory saccades: Eye continues moving after saccade (large position change relative to amplitude)\n",
    "     * Orienting saccades: Eye settles at new position (small position change relative to amplitude)\n",
    "   - Reasoning: \n",
    "     * Compensatory: Eye position continues changing until next compensatory saccade (change comparable to or larger than saccade amplitude)\n",
    "     * Orienting: Eye settles and stays stable (change much smaller than saccade amplitude)\n",
    "   - Typical values: 30-70% (adjust based on your data)\n",
    "   - Too low: May misclassify orienting saccades with small noise as compensatory\n",
    "   - Too high: May miss true compensatory drift patterns\n",
    "   - Key advantage: Relative to amplitude, so adapts to different saccade sizes\n",
    "\n",
    "PRACTICAL TUNING GUIDE (For QC Adjustments):\n",
    "=============================================\n",
    "When you find a miscategorized saccade during QC, here's what to adjust:\n",
    "\n",
    "‚ö†Ô∏è PROBLEM: Two orienting saccades close together are misclassified as compensatory\n",
    "   ‚Üí SOLUTION: Decrease bout_window (e.g., 1.5 ‚Üí 1.0 seconds)\n",
    "   ‚Üí WHY: Makes the classifier less likely to group saccades into bouts\n",
    "\n",
    "‚ö†Ô∏è PROBLEM: Compensatory saccades misclassified as orienting (happens in bouts)\n",
    "   ‚Üí SOLUTION: Increase bout_window (e.g., 1.5 ‚Üí 2.0 seconds)\n",
    "   ‚Üí WHY: Better captures rapid compensatory saccade sequences\n",
    "\n",
    "‚ö†Ô∏è PROBLEM: Compensatory saccades misclassified as orienting (isolated saccades)\n",
    "   ‚Üí SOLUTION: Decrease pre_saccade_velocity_threshold or pre_saccade_drift_threshold\n",
    "   ‚Üí WHY: Makes it easier to detect slow pre-saccade drift that indicates compensation\n",
    "   ‚Üí OR: Increase post_saccade_position_change_threshold_percent (e.g., 50 ‚Üí 70)\n",
    "   ‚Üí WHY: Requires larger post-saccade position change to classify as compensatory\n",
    "\n",
    "‚ö†Ô∏è PROBLEM: Orienting saccades misclassified as compensatory (too sensitive to drift)\n",
    "   ‚Üí SOLUTION: Increase pre_saccade_velocity_threshold or pre_saccade_drift_threshold\n",
    "   ‚Üí WHY: Requires stronger evidence of drift before classifying as compensatory\n",
    "   ‚Üí OR: Decrease post_saccade_position_change_threshold_percent (e.g., 50 ‚Üí 30)\n",
    "   ‚Üí WHY: Makes it harder to classify based on post-saccade position change\n",
    "\n",
    "‚ö†Ô∏è PROBLEM: Eye position continues moving after orienting saccades (causes misclassification)\n",
    "   ‚Üí SOLUTION: Increase max_intersaccade_interval_for_classification (e.g., 5.0 ‚Üí 7.0 seconds)\n",
    "   ‚Üí WHY: Gives more time for eye to settle, better captures true stability\n",
    "\n",
    "‚ö†Ô∏è PROBLEM: Using adaptive thresholds but getting inconsistent results\n",
    "   ‚Üí SOLUTION: Set use_adaptive_thresholds = False and use fixed thresholds\n",
    "   ‚Üí WHY: Fixed thresholds give more predictable, reproducible results\n",
    "\n",
    "üìù QUICK REFERENCE:\n",
    "   - bout_window: Controls how close saccades need to be to form a bout (default: 1.5s)\n",
    "   - pre_saccade_velocity_threshold: How fast eye moves before saccade = compensatory (default: 50 px/s)\n",
    "   - pre_saccade_drift_threshold: How much position changes before saccade = compensatory (default: 10 px)\n",
    "   - post_saccade_position_change_threshold_percent: % of saccade amplitude as position change threshold (default: 50%)\n",
    "   - max_intersaccade_interval_for_classification: Max time to look for next saccade (default: 5.0s)\n",
    "\n",
    "CLASSIFICATION ORDER AND LOGIC:\n",
    "================================\n",
    "\n",
    "Stage 1: Temporal Clustering (Bout Detection)\n",
    "  - Groups saccades within bout_window into bouts\n",
    "  - Saccades > bout_window apart start a new bout\n",
    "  - Result: Each saccade gets a bout_id and bout_size\n",
    "\n",
    "Stage 2: Feature Extraction\n",
    "  For each saccade:\n",
    "    a) Extract pre-saccade features (pre_saccade_window before onset):\n",
    "       - Mean absolute velocity (pre_saccade_mean_velocity)\n",
    "       - Position drift (pre_saccade_position_drift)\n",
    "    b) Extract post-saccade features (DYNAMIC window):\n",
    "       - Window extends from saccade offset until next saccade start (or max_intersaccade_interval_for_classification)\n",
    "       - Position variance (post_saccade_position_variance) - measures stability until next saccade\n",
    "       - Position change (post_saccade_position_change) - total change in position until next saccade\n",
    "       - This captures whether eye settles (orienting) or continues changing (compensatory)\n",
    "\n",
    "Stage 3: Classification (ORIGINAL SIMPLE LOGIC - Conservative Starting Point)\n",
    "  Rule 1: If in a bout (bout_size >= 2)\n",
    "    - Automatically classify as compensatory\n",
    "    - Rationale: Compensatory saccades occur in rapid succession during head rotation\n",
    "  \n",
    "  Rule 2: If isolated (bout_size == 1)\n",
    "    - If pre_vel > pre_saccade_velocity_threshold OR pre_drift > pre_saccade_drift_threshold\n",
    "      ‚Üí Compensatory (evidence of drift/compensation before saccade)\n",
    "    - Else if pre_vel ‚â§ pre_saccade_velocity_threshold AND post_var < post_saccade_variance_threshold\n",
    "      ‚Üí Orienting (stable before and stable after saccade)\n",
    "    - Else\n",
    "      ‚Üí Compensatory (conservative default - when uncertain)\n",
    "\n",
    "RATIONALE FOR ORDER:\n",
    "====================\n",
    "1. Temporal clustering first: Identifies which saccades are potentially related (bouts)\n",
    "2. Feature extraction: Measures key characteristics of each saccade\n",
    "3. Classification using simple rules:\n",
    "   - Bouts: Automatically compensatory (conservative - assumes bouts indicate compensatory behavior)\n",
    "   - Isolated: Feature-based classification (pre-saccade drift + post-saccade stability)\n",
    "\n",
    "KEY INSIGHT:\n",
    "============\n",
    "- Compensatory saccades: Show DRIFT before them (slow compensation for head rotation)\n",
    "- Orienting saccades: Show STABLE periods before them (eye at rest before quick shift)\n",
    "- The pre-saccade window captures the inter-saccade interval, which is the key discriminator\n",
    "\n",
    "NOTE ON CURRENT LOGIC:\n",
    "======================\n",
    "This is the original conservative starting point:\n",
    "- All saccades in bouts (>=2 saccades within bout_window) are classified as compensatory\n",
    "- Only isolated saccades are classified using features\n",
    "- This may over-classify compensatory (especially if two orienting saccades happen close together)\n",
    "- Adjust bout_window or add refinement logic if needed\n",
    "\"\"\"\n",
    "\n",
    "video2_eye = 'R' if video1_eye == 'L' else 'L' # Automatically assign eye for VideoData2\n",
    "eye_fullname = {'L': 'Left', 'R': 'Right'} # Map for full names (used in labels)\n",
    "# Update VIDEO_LABELS based on selection\n",
    "VIDEO_LABELS = {\n",
    "    'VideoData1': f\"VideoData1 ({video1_eye}: {eye_fullname[video1_eye]})\",\n",
    "    'VideoData2': f\"VideoData2 ({video2_eye}: {eye_fullname[video2_eye]})\"\n",
    "}\n",
    "\n",
    "data_path = Path('/Users/rancze/Documents/Data/vestVR/20250409_Cohort3_rotation/Visual_mismatch_day4/B6J2782-2025-04-28T14-22-03') \n",
    "#data_path = Path('/Users/rancze/Documents/Data/vestVR/Cohort1/No_iso_correction/Visual_mismatch_day3/B6J2717-2024-12-10T12-17-03') # only has sleap data 1\n",
    "save_path = data_path.parent / f\"{data_path.name}_processedData\"\n",
    "\n",
    "VideoData1, VideoData2, VideoData1_Has_Sleap, VideoData2_Has_Sleap = lp.load_videography_data(data_path, debug=debug)\n",
    "\n",
    "# Load manual blink data if available\n",
    "manual_blinks_v1 = None\n",
    "manual_blinks_v2 = None\n",
    "\n",
    "manual_blinks_v1_path = data_path / \"Video1_manual_blinks.csv\"\n",
    "manual_blinks_v2_path = data_path / \"Video2_manual_blinks.csv\"\n",
    "\n",
    "if manual_blinks_v1_path.exists():\n",
    "    try:\n",
    "        manual_blinks_df_v1 = pd.read_csv(manual_blinks_v1_path)\n",
    "        # Expected columns: blink_number, start_frame, end_frame\n",
    "        if all(col in manual_blinks_df_v1.columns for col in ['blink_number', 'start_frame', 'end_frame']):\n",
    "            manual_blinks_v1 = [\n",
    "                {'num': int(row['blink_number']), 'start': int(row['start_frame']), 'end': int(row['end_frame'])}\n",
    "                for _, row in manual_blinks_df_v1.iterrows()\n",
    "            ]\n",
    "            print(f\"‚úÖ Loaded {len(manual_blinks_v1)} manual blinks for VideoData1 from {manual_blinks_v1_path.name}\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è WARNING: {manual_blinks_v1_path.name} exists but doesn't have expected columns (blink_number, start_frame, end_frame)\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è WARNING: Failed to load {manual_blinks_v1_path.name}: {e}\")\n",
    "\n",
    "if manual_blinks_v2_path.exists():\n",
    "    try:\n",
    "        manual_blinks_df_v2 = pd.read_csv(manual_blinks_v2_path)\n",
    "        # Expected columns: blink_number, start_frame, end_frame\n",
    "        if all(col in manual_blinks_df_v2.columns for col in ['blink_number', 'start_frame', 'end_frame']):\n",
    "            manual_blinks_v2 = [\n",
    "                {'num': int(row['blink_number']), 'start': int(row['start_frame']), 'end': int(row['end_frame'])}\n",
    "                for _, row in manual_blinks_df_v2.iterrows()\n",
    "            ]\n",
    "            print(f\"‚úÖ Loaded {len(manual_blinks_v2)} manual blinks for VideoData2 from {manual_blinks_v2_path.name}\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è WARNING: {manual_blinks_v2_path.name} exists but doesn't have expected columns (blink_number, start_frame, end_frame)\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è WARNING: Failed to load {manual_blinks_v2_path.name}: {e}\")\n",
    "\n",
    "columns_of_interest = ['left.x','left.y','center.x','center.y','right.x','right.y','p1.x','p1.y','p2.x','p2.y','p3.x','p3.y','p4.x','p4.y','p5.x','p5.y','p6.x','p6.y','p7.x','p7.y','p8.x','p8.y']\n",
    "\n",
    "if VideoData1_Has_Sleap:\n",
    "    VideoData1 = VideoData1.drop(columns=['track']) # drop the track column as it is empty\n",
    "    coordinates_dict1_raw=lp.get_coordinates_dict(VideoData1, columns_of_interest)\n",
    "    FPS_1 = 1 / VideoData1[\"Seconds\"].diff().mean()  # frame rate for VideoData1 TODO where to save it, is it useful?\n",
    "    print ()\n",
    "    print(f\"{get_eye_label('VideoData1')}: FPS = {FPS_1}\")\n",
    "\n",
    "if VideoData2_Has_Sleap:\n",
    "    VideoData2 = VideoData2.drop(columns=['track']) # drop the track column as it is empty\n",
    "    coordinates_dict2_raw=lp.get_coordinates_dict(VideoData2, columns_of_interest)\n",
    "    FPS_2 = 1 / VideoData2[\"Seconds\"].diff().mean()  # frame rate for VideoData2\n",
    "    print(f\"{get_eye_label('VideoData2')}: FPS = {FPS_2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2596e61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot timeseries of coordinates in browser for both VideoData1 and VideoData2\n",
    "############################################################################################################\n",
    "if plot_QC_timeseries:\n",
    "    print(f'‚ö†Ô∏è Check for long discontinuities and outliers in the data, we will try to deal with them later')\n",
    "    print(f'‚ÑπÔ∏è Figures open in browser window, takes a bit of time.')\n",
    "\n",
    "    # Helper list variables\n",
    "    subplot_titles = (\n",
    "        \"X coordinates for pupil centre and left-right eye corner\",\n",
    "        \"Y coordinates for pupil centre and left-right eye corner\",\n",
    "        \"X coordinates for iris points\",\n",
    "        \"Y coordinates for iris points\"\n",
    "    )\n",
    "    eye_x = ['left.x', 'center.x', 'right.x']\n",
    "    eye_y = ['left.y', 'center.y', 'right.y']\n",
    "    iris_x = ['p1.x', 'p2.x', 'p3.x', 'p4.x', 'p5.x', 'p6.x', 'p7.x', 'p8.x']\n",
    "    iris_y = ['p1.y', 'p2.y', 'p3.y', 'p4.y', 'p5.y', 'p6.y', 'p7.y', 'p8.y']\n",
    "\n",
    "    # --- VideoData1 ---\n",
    "    if VideoData1_Has_Sleap:\n",
    "        fig1 = make_subplots(\n",
    "            rows=4, cols=1,\n",
    "            shared_xaxes=True,\n",
    "            vertical_spacing=0.05,\n",
    "            subplot_titles=subplot_titles\n",
    "        )\n",
    "\n",
    "        # Row 1: left.x, center.x, right.x\n",
    "        for col in eye_x:\n",
    "            fig1.add_trace(go.Scatter(x=VideoData1['Seconds'], y=VideoData1[col], mode='lines', name=col), row=1, col=1)\n",
    "        # Row 2: left.y, center.y, right.y\n",
    "        for col in eye_y:\n",
    "            fig1.add_trace(go.Scatter(x=VideoData1['Seconds'], y=VideoData1[col], mode='lines', name=col), row=2, col=1)\n",
    "        # Row 3: p1.x ... p8.x\n",
    "        for col in iris_x:\n",
    "            fig1.add_trace(go.Scatter(x=VideoData1['Seconds'], y=VideoData1[col], mode='lines', name=col), row=3, col=1)\n",
    "        # Row 4: p1.y ... p8.y\n",
    "        for col in iris_y:\n",
    "            fig1.add_trace(go.Scatter(x=VideoData1['Seconds'], y=VideoData1[col], mode='lines', name=col), row=4, col=1)\n",
    "\n",
    "        fig1.update_layout(\n",
    "            height=1200,\n",
    "            title_text=f\"Time series subplots for coordinates [{get_eye_label('VideoData1')}]\",\n",
    "            showlegend=True\n",
    "        )\n",
    "        fig1.update_xaxes(title_text=\"Seconds\", row=4, col=1)\n",
    "        fig1.update_yaxes(title_text=\"X Position\", row=1, col=1)\n",
    "        fig1.update_yaxes(title_text=\"Y Position\", row=2, col=1)\n",
    "        fig1.update_yaxes(title_text=\"X Position\", row=3, col=1)\n",
    "        fig1.update_yaxes(title_text=\"Y Position\", row=4, col=1)\n",
    "\n",
    "        fig1.show(renderer='browser')\n",
    "\n",
    "    # --- VideoData2 ---\n",
    "    if VideoData2_Has_Sleap:\n",
    "        fig2 = make_subplots(\n",
    "            rows=4, cols=1,\n",
    "            shared_xaxes=True,\n",
    "            vertical_spacing=0.05,\n",
    "            subplot_titles=subplot_titles\n",
    "        )\n",
    "        # Row 1: left.x, center.x, right.x\n",
    "        for col in eye_x:\n",
    "            fig2.add_trace(go.Scatter(x=VideoData2['Seconds'], y=VideoData2[col], mode='lines', name=col), row=1, col=1)\n",
    "        # Row 2: left.y, center.y, right.y\n",
    "        for col in eye_y:\n",
    "            fig2.add_trace(go.Scatter(x=VideoData2['Seconds'], y=VideoData2[col], mode='lines', name=col), row=2, col=1)\n",
    "        # Row 3: p1.x ... p8.x\n",
    "        for col in iris_x:\n",
    "            fig2.add_trace(go.Scatter(x=VideoData2['Seconds'], y=VideoData2[col], mode='lines', name=col), row=3, col=1)\n",
    "        # Row 4: p1.y ... p8.y\n",
    "        for col in iris_y:\n",
    "            fig2.add_trace(go.Scatter(x=VideoData2['Seconds'], y=VideoData2[col], mode='lines', name=col), row=4, col=1)\n",
    "\n",
    "        fig2.update_layout(\n",
    "            height=1200,\n",
    "            title_text=f\"Time series subplots for coordinates [{get_eye_label('VideoData2')}]\",\n",
    "            showlegend=True\n",
    "        )\n",
    "        fig2.update_xaxes(title_text=\"Seconds\", row=4, col=1)\n",
    "        fig2.update_yaxes(title_text=\"X Position\", row=1, col=1)\n",
    "        fig2.update_yaxes(title_text=\"Y Position\", row=2, col=1)\n",
    "        fig2.update_yaxes(title_text=\"X Position\", row=3, col=1)\n",
    "        fig2.update_yaxes(title_text=\"Y Position\", row=4, col=1)\n",
    "\n",
    "        fig2.show(renderer='browser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e20024b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# QC plot XY coordinate distributions to visualize outliers \n",
    "############################################################################################################\n",
    "\n",
    "columns_of_interest = ['left', 'right', 'center', 'p1', 'p2', 'p3', 'p4', 'p5', 'p6', 'p7', 'p8']\n",
    "\n",
    "# Filter out NaN values and calculate the min and max values for X and Y coordinates for both dict1 and dict2\n",
    "\n",
    "def min_max_dict(coordinates_dict):\n",
    "    x_min = min([coordinates_dict[f'{col}.x'][~np.isnan(coordinates_dict[f'{col}.x'])].min() for col in columns_of_interest])\n",
    "    x_max = max([coordinates_dict[f'{col}.x'][~np.isnan(coordinates_dict[f'{col}.x'])].max() for col in columns_of_interest])\n",
    "    y_min = min([coordinates_dict[f'{col}.y'][~np.isnan(coordinates_dict[f'{col}.y'])].min() for col in columns_of_interest])\n",
    "    y_max = max([coordinates_dict[f'{col}.y'][~np.isnan(coordinates_dict[f'{col}.y'])].max() for col in columns_of_interest])\n",
    "    return x_min, x_max, y_min, y_max\n",
    "\n",
    "# Only plot panels for 1 and 2 if VideoData1_Has_Sleap and/or VideoData2_Has_Sleap are true\n",
    "\n",
    "# Compute min/max as before for global axes limits\n",
    "if VideoData1_Has_Sleap:\n",
    "    x_min1, x_max1, y_min1, y_max1 = pf.min_max_dict(coordinates_dict1_raw, columns_of_interest)\n",
    "if VideoData2_Has_Sleap:\n",
    "    x_min2, x_max2, y_min2, y_max2 = pf.min_max_dict(coordinates_dict2_raw, columns_of_interest)\n",
    "\n",
    "# Use global min and max for consistency only if both VideoData1_Has_Sleap and VideoData2_Has_Sleap are True\n",
    "if VideoData1_Has_Sleap and VideoData2_Has_Sleap:\n",
    "    x_min = min(x_min1, x_min2)\n",
    "    x_max = max(x_max1, x_max2)\n",
    "    y_min = min(y_min1, y_min2)\n",
    "    y_max = max(y_max1, y_max2)\n",
    "elif VideoData1_Has_Sleap:\n",
    "    x_min, x_max, y_min, y_max = x_min1, x_max1, y_min1, y_max1\n",
    "elif VideoData2_Has_Sleap:\n",
    "    x_min, x_max, y_min, y_max = x_min2, x_max2, y_min2, y_max2\n",
    "else:\n",
    "    raise ValueError(\"Neither VideoData1 nor VideoData2 has Sleap data available.\")\n",
    "\n",
    "# Create the figure and axes\n",
    "fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(18, 12))\n",
    "fig.suptitle(\n",
    "    f\"XY coordinate distribution of different points for {get_eye_label('VideoData1')} and {get_eye_label('VideoData2')} before outlier removal and NaN interpolation\", \n",
    "    fontsize=14\n",
    ")\n",
    "\n",
    "# Define colormap for p1-p8\n",
    "colors = ['blue', 'green', 'red', 'cyan', 'magenta', 'yellow', 'black', 'orange']\n",
    "\n",
    "# Panel 1: left, right, center (dict1)\n",
    "if 'VideoData1_Has_Sleap' in globals() and VideoData1_Has_Sleap:\n",
    "    ax[0, 0].set_title(f\"{get_eye_label('VideoData1')}: left, right, center\")\n",
    "    ax[0, 0].scatter(coordinates_dict1_raw['left.x'], coordinates_dict1_raw['left.y'], color='black', label='left', s=10)\n",
    "    ax[0, 0].scatter(coordinates_dict1_raw['right.x'], coordinates_dict1_raw['right.y'], color='grey', label='right', s=10)\n",
    "    ax[0, 0].scatter(coordinates_dict1_raw['center.x'], coordinates_dict1_raw['center.y'], color='red', label='center', s=10)\n",
    "    ax[0, 0].set_xlim([x_min, x_max])\n",
    "    ax[0, 0].set_ylim([y_min, y_max])\n",
    "    ax[0, 0].set_xlabel('x coordinates (pixels)')\n",
    "    ax[0, 0].set_ylabel('y coordinates (pixels)')\n",
    "    ax[0, 0].legend(loc='upper right')\n",
    "else:\n",
    "    ax[0, 0].axis('off')\n",
    "\n",
    "# Panel 2: p1 to p8 (dict1)\n",
    "if 'VideoData1_Has_Sleap' in globals() and VideoData1_Has_Sleap:\n",
    "    ax[0, 1].set_title(f\"{get_eye_label('VideoData1')}: p1 to p8\")\n",
    "    for idx, col in enumerate(columns_of_interest[3:]):\n",
    "        ax[0, 1].scatter(coordinates_dict1_raw[f'{col}.x'], coordinates_dict1_raw[f'{col}.y'], color=colors[idx], label=col, s=5)\n",
    "    ax[0, 1].set_xlim([x_min, x_max])\n",
    "    ax[0, 1].set_ylim([y_min, y_max])\n",
    "    ax[0, 1].set_xlabel('x coordinates (pixels)')\n",
    "    ax[0, 1].set_ylabel('y coordinates (pixels)')\n",
    "    ax[0, 1].legend(loc='upper right')\n",
    "else:\n",
    "    ax[0, 1].axis('off')\n",
    "\n",
    "# Panel 3: left, right, center (dict2)\n",
    "if 'VideoData2_Has_Sleap' in globals() and VideoData2_Has_Sleap:\n",
    "    ax[1, 0].set_title(f\"{get_eye_label('VideoData2')}: left, right, center\")\n",
    "    ax[1, 0].scatter(coordinates_dict2_raw['left.x'], coordinates_dict2_raw['left.y'], color='black', label='left', s=10)\n",
    "    ax[1, 0].scatter(coordinates_dict2_raw['right.x'], coordinates_dict2_raw['right.y'], color='grey', label='right', s=10)\n",
    "    ax[1, 0].scatter(coordinates_dict2_raw['center.x'], coordinates_dict2_raw['center.y'], color='red', label='center', s=10)\n",
    "    ax[1, 0].set_xlim([x_min, x_max])\n",
    "    ax[1, 0].set_ylim([y_min, y_max])\n",
    "    ax[1, 0].set_xlabel('x coordinates (pixels)')\n",
    "    ax[1, 0].set_ylabel('y coordinates (pixels)')\n",
    "    ax[1, 0].legend(loc='upper right')\n",
    "else:\n",
    "    ax[1, 0].axis('off')\n",
    "\n",
    "# Panel 4: p1 to p8 (dict2)\n",
    "if 'VideoData2_Has_Sleap' in globals() and VideoData2_Has_Sleap:\n",
    "    ax[1, 1].set_title(f\"{get_eye_label('VideoData2')}: p1 to p8\")\n",
    "    for idx, col in enumerate(columns_of_interest[3:]):\n",
    "        ax[1, 1].scatter(coordinates_dict2_raw[f'{col}.x'], coordinates_dict2_raw[f'{col}.y'], color=colors[idx], label=col, s=5)\n",
    "    ax[1, 1].set_xlim([x_min, x_max])\n",
    "    ax[1, 1].set_ylim([y_min, y_max])\n",
    "    ax[1, 1].set_xlabel('x coordinates (pixels)')\n",
    "    ax[1, 1].set_ylabel('y coordinates (pixels)')\n",
    "    ax[1, 1].legend(loc='upper right')\n",
    "else:\n",
    "    ax[1, 1].axis('off')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.96])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a27b19",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Center coordinates, filter low-confidence points, remove outliers, and interpolate\n",
    "############################################################################################################\n",
    "\n",
    "# Detect and print confidence scores analysis (runs before any filtering)\n",
    "#########\n",
    "\n",
    "if not debug:\n",
    "    print(\"‚ÑπÔ∏è Debug output suppressed. Set debug=True to see detailed confidence score analysis.\")\n",
    "\n",
    "score_columns = ['left.score','center.score','right.score','p1.score','p2.score','p3.score','p4.score','p5.score','p6.score','p7.score','p8.score']\n",
    "\n",
    "# VideoData1 confidence score analysis\n",
    "if debug and 'VideoData1_Has_Sleap' in globals() and VideoData1_Has_Sleap:\n",
    "    total_points1 = len(VideoData1)\n",
    "    print(f'\\n‚ÑπÔ∏è VideoData1 - Top 3 columns with most frames below {score_cutoff} confidence score:')\n",
    "\n",
    "    video1_stats = []\n",
    "    for col in score_columns:\n",
    "        if col in VideoData1.columns:\n",
    "            count_below = (VideoData1[col] < score_cutoff).sum()\n",
    "            pct_below = (count_below / total_points1) * 100 if total_points1 > 0 else 0\n",
    "\n",
    "            below_mask = VideoData1[col] < score_cutoff\n",
    "            longest = 0\n",
    "            run = 0\n",
    "            for val in below_mask:\n",
    "                if val:\n",
    "                    run += 1\n",
    "                    if run > longest:\n",
    "                        longest = run\n",
    "                else:\n",
    "                    run = 0\n",
    "            video1_stats.append((col, count_below, pct_below, longest))\n",
    "\n",
    "    video1_stats.sort(key=lambda x: x[1], reverse=True)\n",
    "    for i, (col, count, pct, longest) in enumerate(video1_stats[:3]):\n",
    "        print(f\"VideoData1 - #{i+1}: {col} | Values below {score_cutoff}: {count} ({pct:.2f}%) | Longest consecutive frame series: {longest}\")\n",
    "\n",
    "# VideoData2 confidence score analysis\n",
    "if debug and 'VideoData2_Has_Sleap' in globals() and VideoData2_Has_Sleap:\n",
    "    total_points2 = len(VideoData2)\n",
    "    print(f'\\n‚ÑπÔ∏è VideoData2 - Top 3 columns with most frames below {score_cutoff} confidence score:')\n",
    "\n",
    "    video2_stats = []\n",
    "    for col in score_columns:\n",
    "        if col in VideoData2.columns:\n",
    "            count_below = (VideoData2[col] < score_cutoff).sum()\n",
    "            pct_below = (count_below / total_points2) * 100 if total_points2 > 0 else 0\n",
    "\n",
    "            below_mask = VideoData2[col] < score_cutoff\n",
    "            longest = 0\n",
    "            run = 0\n",
    "            for val in below_mask:\n",
    "                if val:\n",
    "                    run += 1\n",
    "                    if run > longest:\n",
    "                        longest = run\n",
    "                else:\n",
    "                    run = 0\n",
    "            video2_stats.append((col, count_below, pct_below, longest))\n",
    "\n",
    "    video2_stats.sort(key=lambda x: x[1], reverse=True)\n",
    "    for i, (col, count, pct, longest) in enumerate(video2_stats[:3]):\n",
    "        print(f\"VideoData2 - #{i+1}: {col} | Values below {score_cutoff}: {count} ({pct:.2f}%) | Longest consecutive frame series: {longest}\")\n",
    "\n",
    "\n",
    "## Center coordinates to the median pupil centre\n",
    "columns_of_interest = ['left.x','left.y','center.x','center.y','right.x','right.y','p1.x','p1.y','p2.x','p2.y','p3.x','p3.y','p4.x','p4.y','p5.x','p5.y','p6.x','p6.y','p7.x','p7.y','p8.x','p8.y']\n",
    "\n",
    "print ()\n",
    "print(\"=== Centering coordinates to the median pupil centre ===\")\n",
    "# VideoData1 processing\n",
    "if 'VideoData1_Has_Sleap' in globals() and VideoData1_Has_Sleap:\n",
    "    # Calculate the mean of the center x and y points\n",
    "    mean_center_x1 = VideoData1['center.x'].median()\n",
    "    mean_center_y1 = VideoData1['center.y'].median()\n",
    "\n",
    "    print(f\"{get_eye_label('VideoData1')} - Centering on median pupil centre: \\nMean center.x: {mean_center_x1}, Mean center.y: {mean_center_y1}\")\n",
    "\n",
    "    # Translate the coordinates\n",
    "    for col in columns_of_interest:\n",
    "        if '.x' in col:\n",
    "            VideoData1[col] = VideoData1[col] - mean_center_x1\n",
    "        elif '.y' in col:\n",
    "            VideoData1[col] = VideoData1[col] - mean_center_y1\n",
    "\n",
    "    VideoData1_centered = VideoData1.copy()\n",
    "\n",
    "# VideoData2 processing\n",
    "if 'VideoData2_Has_Sleap' in globals() and VideoData2_Has_Sleap:\n",
    "    # Calculate the mean of the center x and y points\n",
    "    mean_center_x2 = VideoData2['center.x'].median()\n",
    "    mean_center_y2 = VideoData2['center.y'].median()\n",
    "\n",
    "    print(f\"{get_eye_label('VideoData2')} - Centering on median pupil centre: \\nMean center.x: {mean_center_x2}, Mean center.y: {mean_center_y2}\")\n",
    "\n",
    "    # Translate the coordinates\n",
    "    for col in columns_of_interest:\n",
    "        if '.x' in col:\n",
    "            VideoData2[col] = VideoData2[col] - mean_center_x2\n",
    "        elif '.y' in col:\n",
    "            VideoData2[col] = VideoData2[col] - mean_center_y2\n",
    "\n",
    "    VideoData2_centered = VideoData2.copy()\n",
    "\n",
    "############################################################################################################\n",
    "# remove low confidence points (score < threshold)\n",
    "############################################################################################################\n",
    "if not NaNs_removed:\n",
    "    if debug:\n",
    "        print(\"\\n=== Score-based Filtering - point scores below threshold are replaced by interpolation ===\")\n",
    "        print(f\"Score threshold: {score_cutoff}\")\n",
    "    # List of point names (without .x, .y, .score)\n",
    "    point_names = ['left', 'right', 'center', 'p1', 'p2', 'p3', 'p4', 'p5', 'p6', 'p7', 'p8']\n",
    "\n",
    "    # VideoData1 score-based filtering\n",
    "    if 'VideoData1_Has_Sleap' in globals() and VideoData1_Has_Sleap:\n",
    "        total_low_score1 = 0\n",
    "        low_score_counts1 = {}\n",
    "        for point in point_names:\n",
    "            if f'{point}.score' in VideoData1.columns:\n",
    "                # Find indices where score is below threshold\n",
    "                low_score_mask = VideoData1[f'{point}.score'] < score_cutoff\n",
    "                low_score_count = low_score_mask.sum()\n",
    "                low_score_counts1[f'{point}.x'] = low_score_count\n",
    "                low_score_counts1[f'{point}.y'] = low_score_count\n",
    "                total_low_score1 += low_score_count * 2  # *2 because we're removing both x and y\n",
    "                \n",
    "                # Set x and y to NaN for low confidence points\n",
    "                VideoData1.loc[low_score_mask, f'{point}.x'] = np.nan\n",
    "                VideoData1.loc[low_score_mask, f'{point}.y'] = np.nan\n",
    "        \n",
    "        # Find the channel with the maximum number of low-score points\n",
    "        max_low_score_channel1 = max(low_score_counts1, key=low_score_counts1.get)\n",
    "        max_low_score_count1 = low_score_counts1[max_low_score_channel1]\n",
    "        \n",
    "        # Print the channel with the maximum number of low-score points\n",
    "        if debug:\n",
    "            print(f\"{get_eye_label('VideoData1')} - Channel with the maximum number of low-confidence points: {max_low_score_channel1}, Number of low-confidence points: {max_low_score_count1}\")\n",
    "            print(f\"{get_eye_label('VideoData1')} - A total number of {total_low_score1} low-confidence coordinate values were replaced by interpolation\")\n",
    "\n",
    "    # VideoData2 score-based filtering\n",
    "    if 'VideoData2_Has_Sleap' in globals() and VideoData2_Has_Sleap:\n",
    "        total_low_score2 = 0\n",
    "        low_score_counts2 = {}\n",
    "        for point in point_names:\n",
    "            if f'{point}.score' in VideoData2.columns:\n",
    "                # Find indices where score is below threshold\n",
    "                low_score_mask = VideoData2[f'{point}.score'] < score_cutoff\n",
    "                low_score_count = low_score_mask.sum()\n",
    "                low_score_counts2[f'{point}.x'] = low_score_count\n",
    "                low_score_counts2[f'{point}.y'] = low_score_count\n",
    "                total_low_score2 += low_score_count * 2  # *2 because we're removing both x and y\n",
    "                \n",
    "                # Set x and y to NaN for low confidence points\n",
    "                VideoData2.loc[low_score_mask, f'{point}.x'] = np.nan\n",
    "                VideoData2.loc[low_score_mask, f'{point}.y'] = np.nan\n",
    "        \n",
    "        # Find the channel with the maximum number of low-score points\n",
    "        max_low_score_channel2 = max(low_score_counts2, key=low_score_counts2.get)\n",
    "        max_low_score_count2 = low_score_counts2[max_low_score_channel2]\n",
    "        \n",
    "        # Print the channel with the maximum number of low-score points\n",
    "        if debug:\n",
    "            print(f\"{get_eye_label('VideoData2')} - Channel with the maximum number of low-confidence points: {max_low_score_channel2}, Number of low-confidence points: {max_low_score_count2}\")\n",
    "            print(f\"{get_eye_label('VideoData2')} - A total number of {total_low_score2} low-confidence coordinate values were replaced by interpolation\")\n",
    "\n",
    "    ############################################################################################################\n",
    "    # remove outliers (x times SD)\n",
    "    # then interpolates on all NaN values (skipped frames, low confidence inference points, outliers)\n",
    "    ############################################################################################################\n",
    "\n",
    "    if debug:\n",
    "        print(\"\\n=== Outlier Analysis - outlier points are replaced by interpolation ===\")\n",
    "\n",
    "    # VideoData1 outlier analysis and interpolation\n",
    "    if 'VideoData1_Has_Sleap' in globals() and VideoData1_Has_Sleap:\n",
    "        # Calculate the standard deviation for each column of interest\n",
    "        std_devs1 = {col: VideoData1[col].std() for col in columns_of_interest}\n",
    "\n",
    "        # Calculate the number of outliers for each column\n",
    "        outliers1 = {col: ((VideoData1[col] - VideoData1[col].mean()).abs() > outlier_sd_threshold * std_devs1[col]).sum() for col in columns_of_interest}\n",
    "\n",
    "        # Find the channel with the maximum number of outliers\n",
    "        max_outliers_channel1 = max(outliers1, key=outliers1.get)\n",
    "        max_outliers_count1 = outliers1[max_outliers_channel1]\n",
    "        total_outliers1 = sum(outliers1.values())\n",
    "\n",
    "        # Print the channel with the maximum number of outliers and the number\n",
    "        if debug:\n",
    "            print(f\"{get_eye_label('VideoData1')} - Channel with the maximum number of outliers: {max_outliers_channel1}, Number of outliers: {max_outliers_count1}\")\n",
    "            print(f\"{get_eye_label('VideoData1')} - A total number of {total_outliers1} outliers were replaced by interpolation\")\n",
    "\n",
    "        # Replace outliers by interpolating between the previous and subsequent non-NaN value\n",
    "        for col in columns_of_interest:\n",
    "            outlier_indices = VideoData1[((VideoData1[col] - VideoData1[col].mean()).abs() > outlier_sd_threshold * std_devs1[col])].index\n",
    "            VideoData1.loc[outlier_indices, col] = np.nan\n",
    "\n",
    "        #VideoData1.interpolate(inplace=True)\n",
    "        VideoData1 = VideoData1.interpolate(method='linear', limit_direction='both')\n",
    "\n",
    "    # VideoData2 outlier analysis and interpolation\n",
    "    if 'VideoData2_Has_Sleap' in globals() and VideoData2_Has_Sleap:\n",
    "        # Calculate the standard deviation for each column of interest\n",
    "        std_devs2 = {col: VideoData2[col].std() for col in columns_of_interest}\n",
    "\n",
    "        # Calculate the number of outliers for each column\n",
    "        outliers2 = {col: ((VideoData2[col] - VideoData2[col].mean()).abs() > outlier_sd_threshold * std_devs2[col]).sum() for col in columns_of_interest}\n",
    "\n",
    "        # Find the channel with the maximum number of outliers\n",
    "        max_outliers_channel2 = max(outliers2, key=outliers2.get)\n",
    "        max_outliers_count2 = outliers2[max_outliers_channel2]\n",
    "        total_outliers2 = sum(outliers2.values())\n",
    "\n",
    "        # Print the channel with the maximum number of outliers and the number\n",
    "        if debug:\n",
    "            print(f\"{get_eye_label('VideoData2')} - Channel with the maximum number of outliers: {max_outliers_channel2}, Number of outliers: {max_outliers_count2}\")\n",
    "            print(f\"{get_eye_label('VideoData2')} - A total number of {total_outliers2} outliers were replaced by interpolation\")\n",
    "\n",
    "        # Replace outliers by interpolating between the previous and subsequent non-NaN value\n",
    "        for col in columns_of_interest:\n",
    "            outlier_indices = VideoData2[((VideoData2[col] - VideoData2[col].mean()).abs() > outlier_sd_threshold * std_devs2[col])].index\n",
    "            VideoData2.loc[outlier_indices, col] = np.nan\n",
    "\n",
    "        #VideoData2.interpolate(inplace=True)\n",
    "        VideoData2 = VideoData2.interpolate(method='linear', limit_direction='both')\n",
    "\n",
    "    # Set flag after both VideoData1 and VideoData2 processing is complete\n",
    "    NaNs_removed = True\n",
    "else:\n",
    "    print(\"=== Interpolation already done, skipping ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f930a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instance.score distribution and hard threshold for blink detection\n",
    "############################################################################################################\n",
    "# Plotting the distribution of instance scores and using hard threshold for blink detection.\n",
    "# When instance score is low, that's typically because of a blink or similar occlusion, as there are long sequences of low scores.\n",
    "# Frames with instance.score below the hard threshold are considered potential blinks.\n",
    "\n",
    "if not debug:\n",
    "    print(\"‚ÑπÔ∏è Debug output suppressed. Set debug=True to see detailed instance score distribution analysis.\")\n",
    "\n",
    "if debug:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"INSTANCE.SCORE DISTRIBUTION AND BLINK DETECTION THRESHOLD\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\nHard threshold: instance.score < {blink_instance_score_threshold}\")\n",
    "    print(f\"  Frames with instance.score below this threshold will be considered potential blinks.\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "# Only analyze for dataset(s) that exist\n",
    "has_v1 = \"VideoData1_Has_Sleap\" in globals() and VideoData1_Has_Sleap\n",
    "has_v2 = \"VideoData2_Has_Sleap\" in globals() and VideoData2_Has_Sleap\n",
    "\n",
    "# Get FPS for time calculations\n",
    "if has_v1:\n",
    "    if 'FPS_1' in globals():\n",
    "        fps_1_for_threshold = FPS_1\n",
    "    else:\n",
    "        fps_1_for_threshold = 1 / VideoData1[\"Seconds\"].diff().mean()\n",
    "else:\n",
    "    fps_1_for_threshold = None\n",
    "\n",
    "if has_v2:\n",
    "    if 'FPS_2' in globals():\n",
    "        fps_2_for_threshold = FPS_2\n",
    "    else:\n",
    "        fps_2_for_threshold = 1 / VideoData2[\"Seconds\"].diff().mean()\n",
    "else:\n",
    "    fps_2_for_threshold = None\n",
    "\n",
    "# Plot histograms with hard threshold marked\n",
    "fig = None\n",
    "if has_v1 or has_v2:\n",
    "    plt.figure(figsize=(12,5))\n",
    "    plot_index = 1\n",
    "\n",
    "if has_v1:\n",
    "    plt.subplot(1, 2 if has_v2 else 1, plot_index)\n",
    "    plt.hist(VideoData1['instance.score'].dropna(), bins=30, color='skyblue', edgecolor='black')\n",
    "    plt.axvline(blink_instance_score_threshold, color='red', linestyle='--', linewidth=2, \n",
    "                label=f'Hard threshold = {blink_instance_score_threshold}')\n",
    "    plt.yscale('log')\n",
    "    plt.title(\"Distribution of instance.score (VideoData1)\")\n",
    "    plt.xlabel(\"instance.score\")\n",
    "    plt.ylabel(\"Frequency (log scale)\")\n",
    "    plt.legend()\n",
    "    plot_index += 1\n",
    "\n",
    "if has_v2:\n",
    "    plt.subplot(1, 2 if has_v1 else 1, plot_index)\n",
    "    plt.hist(VideoData2['instance.score'].dropna(), bins=30, color='salmon', edgecolor='black')\n",
    "    plt.axvline(blink_instance_score_threshold, color='red', linestyle='--', linewidth=2,\n",
    "                label=f'Hard threshold = {blink_instance_score_threshold}')\n",
    "    plt.yscale('log')\n",
    "    plt.title(\"Distribution of instance.score (VideoData2)\")\n",
    "    plt.xlabel(\"instance.score\")\n",
    "    plt.ylabel(\"Frequency (log scale)\")\n",
    "    plt.legend()\n",
    "    plot_index += 1\n",
    "\n",
    "if has_v1 or has_v2:\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Report the statistics for available VideoData\n",
    "# Always show key stats: number/percentile below threshold and longest consecutive segment\n",
    "if has_v1:\n",
    "    print(f\"\\nVideoData1 - Instance Score Threshold Analysis:\")\n",
    "    print(f\"  Hard threshold: {blink_instance_score_threshold}\")\n",
    "    \n",
    "    # Calculate percentile for reference\n",
    "    v1_percentile = (VideoData1['instance.score'] < blink_instance_score_threshold).sum() / len(VideoData1) * 100\n",
    "    v1_num_low = (VideoData1['instance.score'] < blink_instance_score_threshold).sum()\n",
    "    v1_total = len(VideoData1)\n",
    "    v1_pct_low = (v1_num_low / v1_total) * 100\n",
    "    \n",
    "    # Find longest consecutive segments\n",
    "    low_sections_v1 = pf.find_longest_lowscore_sections(VideoData1['instance.score'], blink_instance_score_threshold, top_n=1)\n",
    "    longest_consecutive_v1 = low_sections_v1[0]['length'] if low_sections_v1 else 0\n",
    "    longest_consecutive_v1_ms = (longest_consecutive_v1 / fps_1_for_threshold) * 1000 if fps_1_for_threshold and longest_consecutive_v1 > 0 else None\n",
    "    \n",
    "    # Always print key stats\n",
    "    print(f\"  Frames below threshold: {v1_num_low} / {v1_total} ({v1_pct_low:.2f}%)\")\n",
    "    print(f\"  Longest consecutive segment: {longest_consecutive_v1} frames\", end=\"\")\n",
    "    if longest_consecutive_v1_ms:\n",
    "        print(f\" ({longest_consecutive_v1_ms:.1f}ms)\")\n",
    "    else:\n",
    "        print()\n",
    "    \n",
    "    # Detailed stats only in debug mode\n",
    "    if debug:\n",
    "        print(f\"\\n  Detailed statistics:\")\n",
    "        print(f\"  Percentile: {v1_percentile:.2f}% (i.e., {v1_percentile:.2f}% of frames have instance.score < {blink_instance_score_threshold})\")\n",
    "        \n",
    "        # Report the top 5 longest consecutive sections\n",
    "        low_sections_v1 = pf.find_longest_lowscore_sections(VideoData1['instance.score'], blink_instance_score_threshold, top_n=5)\n",
    "        if len(low_sections_v1) > 0:\n",
    "            print(f\"\\n  Top 5 longest consecutive sections where instance.score < threshold:\")\n",
    "            for i, sec in enumerate(low_sections_v1, 1):\n",
    "                start_idx = sec['start_idx']\n",
    "                end_idx = sec['end_idx']\n",
    "                start_time = VideoData1.index[start_idx] if isinstance(VideoData1.index, pd.DatetimeIndex) else start_idx\n",
    "                end_time = VideoData1.index[end_idx] if isinstance(VideoData1.index, pd.DatetimeIndex) else end_idx\n",
    "                sec_duration_ms = (sec['length'] / fps_1_for_threshold) * 1000 if fps_1_for_threshold else None\n",
    "                if sec_duration_ms:\n",
    "                    print(f\"    Section {i}: index {start_idx}-{end_idx} (length {sec['length']} frames, {sec_duration_ms:.1f}ms)\")\n",
    "                else:\n",
    "                    print(f\"    Section {i}: index {start_idx}-{end_idx} (length {sec['length']} frames)\")\n",
    "\n",
    "if has_v2:\n",
    "    print(f\"\\nVideoData2 - Instance Score Threshold Analysis:\")\n",
    "    print(f\"  Hard threshold: {blink_instance_score_threshold}\")\n",
    "    \n",
    "    # Calculate percentile for reference\n",
    "    v2_percentile = (VideoData2['instance.score'] < blink_instance_score_threshold).sum() / len(VideoData2) * 100\n",
    "    v2_num_low = (VideoData2['instance.score'] < blink_instance_score_threshold).sum()\n",
    "    v2_total = len(VideoData2)\n",
    "    v2_pct_low = (v2_num_low / v2_total) * 100\n",
    "    \n",
    "    # Find longest consecutive segments\n",
    "    low_sections_v2 = pf.find_longest_lowscore_sections(VideoData2['instance.score'], blink_instance_score_threshold, top_n=1)\n",
    "    longest_consecutive_v2 = low_sections_v2[0]['length'] if low_sections_v2 else 0\n",
    "    longest_consecutive_v2_ms = (longest_consecutive_v2 / fps_2_for_threshold) * 1000 if fps_2_for_threshold and longest_consecutive_v2 > 0 else None\n",
    "    \n",
    "    # Always print key stats\n",
    "    print(f\"  Frames below threshold: {v2_num_low} / {v2_total} ({v2_pct_low:.2f}%)\")\n",
    "    print(f\"  Longest consecutive segment: {longest_consecutive_v2} frames\", end=\"\")\n",
    "    if longest_consecutive_v2_ms:\n",
    "        print(f\" ({longest_consecutive_v2_ms:.1f}ms)\")\n",
    "    else:\n",
    "        print()\n",
    "    \n",
    "    # Detailed stats only in debug mode\n",
    "    if debug:\n",
    "        print(f\"\\n  Detailed statistics:\")\n",
    "        print(f\"  Percentile: {v2_percentile:.2f}% (i.e., {v2_percentile:.2f}% of frames have instance.score < {blink_instance_score_threshold})\")\n",
    "        \n",
    "        # Report the top 5 longest consecutive sections\n",
    "        low_sections_v2 = pf.find_longest_lowscore_sections(VideoData2['instance.score'], blink_instance_score_threshold, top_n=5)\n",
    "        if len(low_sections_v2) > 0:\n",
    "            print(f\"\\n  Top 5 longest consecutive sections where instance.score < threshold:\")\n",
    "            for i, sec in enumerate(low_sections_v2, 1):\n",
    "                start_idx = sec['start_idx']\n",
    "                end_idx = sec['end_idx']\n",
    "                start_time = VideoData2.index[start_idx] if isinstance(VideoData2.index, pd.DatetimeIndex) else start_idx\n",
    "                end_time = VideoData2.index[end_idx] if isinstance(VideoData2.index, pd.DatetimeIndex) else end_idx\n",
    "                sec_duration_ms = (sec['length'] / fps_2_for_threshold) * 1000 if fps_2_for_threshold else None\n",
    "                if sec_duration_ms:\n",
    "                    print(f\"    Section {i}: index {start_idx}-{end_idx} (length {sec['length']} frames, {sec_duration_ms:.1f}ms)\")\n",
    "                else:\n",
    "                    print(f\"    Section {i}: index {start_idx}-{end_idx} (length {sec['length']} frames)\")\n",
    "\n",
    "if debug:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"Note: This threshold will be used for blink detection in the next cell.\")\n",
    "    print(\"      Frames with instance.score below this threshold are considered potential blinks.\")\n",
    "    print(\"=\" * 80)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd2d6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Blink detection using instance.score - mark blinks and set coordinates to NaN (keep them as NaN, no interpolation)\n",
    "############################################################################################################\n",
    "\n",
    "if not debug:\n",
    "    print(\"‚ÑπÔ∏è Debug output suppressed. Set debug=True to see detailed blink detection information.\")\n",
    "\n",
    "# Capture all print output to save to file\n",
    "\n",
    "class TeeOutput:\n",
    "    \"\"\"Output to both stdout and a string buffer\"\"\"\n",
    "    def __init__(self, stdout, buffer):\n",
    "        self.stdout = stdout\n",
    "        self.buffer = buffer\n",
    "    \n",
    "    def write(self, s):\n",
    "        self.stdout.write(s)\n",
    "        self.buffer.write(s)\n",
    "    \n",
    "    def flush(self):\n",
    "        self.stdout.flush()\n",
    "        self.buffer.flush()\n",
    "\n",
    "output_buffer = io.StringIO()\n",
    "original_stdout = sys.stdout\n",
    "sys.stdout = TeeOutput(original_stdout, output_buffer)\n",
    "\n",
    "# Run blink detection code with output captured\n",
    "if debug:\n",
    "    print(\"\\n=== Blink Detection ===\")\n",
    "\n",
    "# VideoData1 blink detection\n",
    "if 'VideoData1_Has_Sleap' in globals() and VideoData1_Has_Sleap:\n",
    "    if debug:\n",
    "        print(f\"\\n{get_eye_label('VideoData1')} - Blink Detection\")\n",
    "    \n",
    "    # Calculate frame-based durations (using FPS_1 if available, otherwise estimate)\n",
    "    if 'FPS_1' in globals():\n",
    "        fps_1 = FPS_1\n",
    "    else:\n",
    "        fps_1 = 1 / VideoData1[\"Seconds\"].diff().mean()\n",
    "    \n",
    "    long_blink_warning_frames = int(long_blink_warning_ms / 1000 * fps_1)\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"  FPS: {fps_1:.2f}\")\n",
    "        print(f\"  Long blink warning threshold: {long_blink_warning_frames} frames ({long_blink_warning_ms}ms)\")\n",
    "    \n",
    "    # Use hard threshold from user parameters\n",
    "    blink_threshold_v1 = blink_instance_score_threshold\n",
    "    if debug:\n",
    "        print(f\"  Using hard threshold: {blink_threshold_v1:.4f}\")\n",
    "    \n",
    "    # Find all blink segments - use very lenient min_frames (1) to capture all segments\n",
    "    # No filtering by frame count - short blinks are OK to interpolate\n",
    "    # No merging - we want to preserve good data between separate blinks\n",
    "    all_blink_segments_v1 = pf.find_blink_segments(\n",
    "        VideoData1['instance.score'], \n",
    "        blink_threshold_v1, \n",
    "        min_frames=1,  # Very lenient to capture all segments\n",
    "        max_frames=999999  # Very high limit - essentially no maximum\n",
    "    )\n",
    "    \n",
    "    # Always print key blink detection stats\n",
    "    print(f\"{get_eye_label('VideoData1')} - Found {len(all_blink_segments_v1)} blink segments\")\n",
    "    \n",
    "    # Filter out blinks shorter than 4 frames\n",
    "    min_frames_threshold = 4\n",
    "    blink_segments_v1 = [blink for blink in all_blink_segments_v1 if blink['length'] >= min_frames_threshold]\n",
    "    short_blink_segments_v1 = [blink for blink in all_blink_segments_v1 if blink['length'] < min_frames_threshold]\n",
    "    \n",
    "    # Always print filtering stats\n",
    "    print(f\"  After filtering <{min_frames_threshold} frames: {len(blink_segments_v1)} blink segment(s), {len(short_blink_segments_v1)} short segment(s) will be interpolated\")\n",
    "    \n",
    "    # Merge blinks within 10 frames into blink bouts\n",
    "    merge_window_frames = 10\n",
    "    blink_bouts_v1 = pf.merge_nearby_blinks(blink_segments_v1, merge_window_frames)\n",
    "    \n",
    "    # Always print bout count\n",
    "    print(f\"  After merging blinks within {merge_window_frames} frames: {len(blink_bouts_v1)} blink bout(s)\")\n",
    "    \n",
    "    # Check for long blinks and warn if needed\n",
    "    long_blinks_warnings_v1 = []\n",
    "    for i, blink in enumerate(blink_segments_v1, 1):\n",
    "        start_idx = blink['start_idx']\n",
    "        end_idx = blink['end_idx']\n",
    "        start_time = VideoData1['Seconds'].iloc[start_idx]\n",
    "        end_time = VideoData1['Seconds'].iloc[end_idx]\n",
    "        duration_ms = (end_time - start_time) * 1000\n",
    "        \n",
    "        # Warn about very long blinks (may need manual verification)\n",
    "        if duration_ms > long_blink_warning_ms:\n",
    "            frame_start = int(VideoData1['frame_idx'].iloc[start_idx])\n",
    "            frame_end = int(VideoData1['frame_idx'].iloc[end_idx])\n",
    "            long_blinks_warnings_v1.append({\n",
    "                'blink_num': i,\n",
    "                'frames': f\"{frame_start}-{frame_end}\",\n",
    "                'duration_ms': duration_ms\n",
    "            })\n",
    "    \n",
    "    # Print warnings for long blinks\n",
    "    if len(long_blinks_warnings_v1) > 0:\n",
    "        print(f\"\\n   ‚ö†Ô∏è WARNING: Found {len(long_blinks_warnings_v1)} blink(s) longer than {long_blink_warning_ms}ms:\")\n",
    "        for warn in long_blinks_warnings_v1:\n",
    "            print(f\"      Blink {warn['blink_num']}: frames {warn['frames']}, duration {warn['duration_ms']:.1f}ms - Please verify this is a real blink in the video\")\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"\\n  Detailed blink detection information:\")\n",
    "        print(f\"  FPS: {fps_1:.2f}\")\n",
    "        print(f\"  Long blink warning threshold: {long_blink_warning_frames} frames ({long_blink_warning_ms}ms)\")\n",
    "        print(f\"  Using hard threshold: {blink_threshold_v1:.4f}\")\n",
    "        print(f\"  Detected {len(blink_segments_v1)} blink segment(s)\\n\")\n",
    "    \n",
    "    # Print all detected blinks once (detailed)\n",
    "    if debug and len(blink_segments_v1) > 0:\n",
    "        print(f\"  Detected blinks:\")\n",
    "        for i, blink in enumerate(blink_segments_v1, 1):\n",
    "            start_idx = blink['start_idx']\n",
    "            end_idx = blink['end_idx']\n",
    "            \n",
    "            # Calculate time range\n",
    "            start_time = VideoData1['Seconds'].iloc[start_idx]\n",
    "            end_time = VideoData1['Seconds'].iloc[end_idx]\n",
    "            duration_ms = (end_time - start_time) * 1000\n",
    "            \n",
    "            # Get actual frame numbers from frame_idx column\n",
    "            if 'frame_idx' in VideoData1.columns:\n",
    "                actual_start_frame = int(VideoData1['frame_idx'].iloc[start_idx])\n",
    "                actual_end_frame = int(VideoData1['frame_idx'].iloc[end_idx])\n",
    "                frame_info = f\"frames {actual_start_frame}-{actual_end_frame}\"\n",
    "            else:\n",
    "                actual_start_frame = start_idx\n",
    "                actual_end_frame = end_idx\n",
    "                frame_info = f\"frames {actual_start_frame}-{actual_end_frame}\"\n",
    "            \n",
    "            print(f\"    Blink {i}: {frame_info}, {blink['length']} frames, {duration_ms:.1f}ms, mean score: {blink['mean_score']:.4f}\")\n",
    "        \n",
    "        # DIAGNOSTIC: Direct comparison of manual vs auto-detected blinks (detailed)\n",
    "        if debug and ('manual_blinks_v1' in globals() and manual_blinks_v1 is not None):\n",
    "            # Get auto-detected blink frame ranges\n",
    "            auto_blinks_v1 = []\n",
    "            for i, blink in enumerate(blink_segments_v1, 1):\n",
    "                start_idx = blink['start_idx']\n",
    "                end_idx = blink['end_idx']\n",
    "                frame_start = int(VideoData1['frame_idx'].iloc[start_idx])\n",
    "                frame_end = int(VideoData1['frame_idx'].iloc[end_idx])\n",
    "                auto_blinks_v1.append({'num': i, 'start': frame_start, 'end': frame_end, 'start_idx': start_idx, 'end_idx': end_idx, 'length': blink['length']})\n",
    "            \n",
    "            print(f\"\\n   üîç MANUAL vs AUTO-DETECTED BLINK COMPARISON:\")\n",
    "            print(f\"      Manual blinks: {len(manual_blinks_v1)}\")\n",
    "            print(f\"      Auto-detected blinks: {len(auto_blinks_v1)}\")\n",
    "            \n",
    "            # Match manual blinks to auto-detected ones (find overlapping frames)\n",
    "            # Allow matching with multiple auto-detected blinks if manual blink is over-merged\n",
    "            print(f\"\\n   Manual ‚Üí Auto matching (overlap analysis):\")\n",
    "            for manual in manual_blinks_v1:\n",
    "                manual_length = manual['end'] - manual['start'] + 1\n",
    "                total_overlap = 0\n",
    "                matching_auto_blinks = []\n",
    "                \n",
    "                # Find all auto-detected blinks that overlap with this manual blink\n",
    "                for auto in auto_blinks_v1:\n",
    "                    overlap_start = max(manual['start'], auto['start'])\n",
    "                    overlap_end = min(manual['end'], auto['end'])\n",
    "                    if overlap_start <= overlap_end:\n",
    "                        overlap_frames = overlap_end - overlap_start + 1\n",
    "                        total_overlap += overlap_frames\n",
    "                        matching_auto_blinks.append({\n",
    "                            'auto': auto,\n",
    "                            'overlap': overlap_frames\n",
    "                        })\n",
    "                \n",
    "                # Calculate total overlap percentage\n",
    "                total_overlap_pct = (total_overlap / manual_length) * 100 if manual_length > 0 else 0\n",
    "                \n",
    "                # Match if total overlap is >= 40% (less stringent to handle over-merged manual blinks)\n",
    "                if total_overlap >= manual_length * 0.40:  # At least 40% overlap\n",
    "                    if len(matching_auto_blinks) == 1:\n",
    "                        # Single match\n",
    "                        match_info = matching_auto_blinks[0]\n",
    "                        best_match = match_info['auto']\n",
    "                        overlap = match_info['overlap']\n",
    "                        start_diff = best_match['start'] - manual['start']\n",
    "                        end_diff = best_match['end'] - manual['end']\n",
    "                        match_str = f\"‚úÖ MATCH: Auto blink {best_match['num']} (frames {best_match['start']}-{best_match['end']})\"\n",
    "                        match_str += f\", {overlap} frames overlap ({total_overlap_pct:.1f}%)\"\n",
    "                        if start_diff != 0 or end_diff != 0:\n",
    "                            match_str += f\", offset: start={start_diff:+d}, end={end_diff:+d}\"\n",
    "                        print(f\"      Manual {manual['num']}: {manual['start']}-{manual['end']} ‚Üí {match_str}\")\n",
    "                    else:\n",
    "                        # Multiple matches (manual blink is over-merged)\n",
    "                        auto_nums = [m['auto']['num'] for m in matching_auto_blinks]\n",
    "                        auto_ranges = [f\"{m['auto']['start']}-{m['auto']['end']}\" for m in matching_auto_blinks]\n",
    "                        match_str = f\"‚úÖ MATCH: Auto blinks {auto_nums} (frames: {', '.join(auto_ranges)})\"\n",
    "                        match_str += f\", total {total_overlap} frames overlap ({total_overlap_pct:.1f}%)\"\n",
    "                        print(f\"      Manual {manual['num']}: {manual['start']}-{manual['end']} ‚Üí {match_str}\")\n",
    "                else:\n",
    "                    print(f\"      Manual {manual['num']}: {manual['start']}-{manual['end']} ‚Üí ‚ùå NO MATCH FOUND\")\n",
    "            \n",
    "            # Also check which auto-detected blinks don't have manual matches\n",
    "            print(f\"\\n   Auto-detected blinks without manual matches:\")\n",
    "            unmatched_auto = []\n",
    "            for auto in auto_blinks_v1:\n",
    "                has_match = False\n",
    "                for manual in manual_blinks_v1:\n",
    "                    overlap_start = max(manual['start'], auto['start'])\n",
    "                    overlap_end = min(manual['end'], auto['end'])\n",
    "                    if overlap_start <= overlap_end:\n",
    "                        overlap_frames = overlap_end - overlap_start + 1\n",
    "                        manual_length = manual['end'] - manual['start'] + 1\n",
    "                        # Use same threshold as matching logic (40% of manual blink)\n",
    "                        if overlap_frames >= manual_length * 0.40:\n",
    "                            has_match = True\n",
    "                            break\n",
    "                if not has_match:\n",
    "                    unmatched_auto.append(auto)\n",
    "            \n",
    "            if len(unmatched_auto) > 0:\n",
    "                for auto in unmatched_auto:\n",
    "                    print(f\"      Auto {auto['num']}: frames {auto['start']}-{auto['end']}, length={auto['length']} frames\")\n",
    "            else:\n",
    "                print(f\"      (all auto-detected blinks have manual matches)\")\n",
    "        elif debug:\n",
    "            print(f\"\\n   ‚ö†Ô∏è WARNING: Manual blink comparison skipped - Video1_manual_blinks.csv not found in data_path\")\n",
    "        \n",
    "        # Interpolate over short blinks (keep long blinks as NaN)\n",
    "        if debug:\n",
    "            print(f\"\\n  Interpolating short blinks (< {min_frames_threshold} frames):\")\n",
    "        short_blink_frames_v1 = 0\n",
    "        if len(short_blink_segments_v1) > 0:\n",
    "            # Mark short blinks as NaN\n",
    "            for blink in short_blink_segments_v1:\n",
    "                start_idx = blink['start_idx']\n",
    "                end_idx = blink['end_idx']\n",
    "                VideoData1.loc[VideoData1.index[start_idx:end_idx+1], columns_of_interest] = np.nan\n",
    "                short_blink_frames_v1 += blink['length']\n",
    "            \n",
    "            # Interpolate all NaNs (this fills short blinks)\n",
    "            VideoData1[columns_of_interest] = VideoData1[columns_of_interest].interpolate(method='linear', limit_direction='both')\n",
    "            \n",
    "            if debug:\n",
    "                print(f\"    Interpolated {short_blink_frames_v1} frames from {len(short_blink_segments_v1)} short blink segment(s)\")\n",
    "        else:\n",
    "            if debug:\n",
    "                print(f\"    No short blinks to interpolate\")\n",
    "        \n",
    "        # Mark long blinks by setting coordinates to NaN (these remain as NaN, not interpolated)\n",
    "        recording_start_time = VideoData1['Seconds'].iloc[0]\n",
    "        total_blink_frames_v1 = 0\n",
    "        for blink in blink_segments_v1:\n",
    "            start_idx = blink['start_idx']\n",
    "            end_idx = blink['end_idx']\n",
    "            VideoData1.loc[VideoData1.index[start_idx:end_idx+1], columns_of_interest] = np.nan\n",
    "            total_blink_frames_v1 += blink['length']\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"  Total long blink frames marked (kept as NaN): {total_blink_frames_v1} frames \"\n",
    "                  f\"({total_blink_frames_v1/fps_1*1000:.1f}ms)\")\n",
    "        \n",
    "        # Calculate blink bout rate\n",
    "        recording_duration_min = (VideoData1['Seconds'].iloc[-1] - VideoData1['Seconds'].iloc[0]) / 60\n",
    "        blink_bout_rate = len(blink_bouts_v1) / recording_duration_min if recording_duration_min > 0 else 0\n",
    "        if debug:\n",
    "            print(f\"  Blink bout rate: {blink_bout_rate:.2f} blink bouts/minute\")\n",
    "    else:\n",
    "        if debug:\n",
    "            print(\"  No blinks detected\")\n",
    "\n",
    "# VideoData2 blink detection\n",
    "if 'VideoData2_Has_Sleap' in globals() and VideoData2_Has_Sleap:\n",
    "    if debug:\n",
    "        print(f\"\\n{get_eye_label('VideoData2')} - Blink Detection\")\n",
    "    \n",
    "    # Calculate frame-based durations (using FPS_2 if available, otherwise estimate)\n",
    "    if 'FPS_2' in globals():\n",
    "        fps_2 = FPS_2\n",
    "    else:\n",
    "        fps_2 = 1 / VideoData2[\"Seconds\"].diff().mean()\n",
    "    \n",
    "    long_blink_warning_frames = int(long_blink_warning_ms / 1000 * fps_2)\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"  FPS: {fps_2:.2f}\")\n",
    "        print(f\"  Long blink warning threshold: {long_blink_warning_frames} frames ({long_blink_warning_ms}ms)\")\n",
    "    \n",
    "    # Use hard threshold from user parameters\n",
    "    blink_threshold_v2 = blink_instance_score_threshold\n",
    "    if debug:\n",
    "        print(f\"  Using hard threshold: {blink_threshold_v2:.4f}\")\n",
    "    \n",
    "    # Find all blink segments - use very lenient min_frames (1) to capture all segments\n",
    "    # No filtering by frame count - short blinks are OK to interpolate\n",
    "    # No merging - we want to preserve good data between separate blinks\n",
    "    all_blink_segments_v2 = pf.find_blink_segments(\n",
    "        VideoData2['instance.score'], \n",
    "        blink_threshold_v2, \n",
    "        min_frames=1,  # Very lenient to capture all segments\n",
    "        max_frames=999999  # Very high limit - essentially no maximum\n",
    "    )\n",
    "    \n",
    "    # Always print key blink detection stats\n",
    "    print(f\"{get_eye_label('VideoData2')} - Found {len(all_blink_segments_v2)} blink segments\")\n",
    "    \n",
    "    # Filter out blinks shorter than 4 frames\n",
    "    min_frames_threshold = 4\n",
    "    blink_segments_v2 = [blink for blink in all_blink_segments_v2 if blink['length'] >= min_frames_threshold]\n",
    "    short_blink_segments_v2 = [blink for blink in all_blink_segments_v2 if blink['length'] < min_frames_threshold]\n",
    "    \n",
    "    # Always print filtering stats\n",
    "    print(f\"  After filtering <{min_frames_threshold} frames: {len(blink_segments_v2)} blink segment(s), {len(short_blink_segments_v2)} short segment(s) will be interpolated\")\n",
    "    \n",
    "    # Merge blinks within 10 frames into blink bouts\n",
    "    merge_window_frames = 10\n",
    "    blink_bouts_v2 = pf.merge_nearby_blinks(blink_segments_v2, merge_window_frames)\n",
    "    \n",
    "    # Always print bout count\n",
    "    print(f\"  After merging blinks within {merge_window_frames} frames: {len(blink_bouts_v2)} blink bout(s)\")\n",
    "    \n",
    "    # Check for long blinks and warn if needed\n",
    "    long_blinks_warnings_v2 = []\n",
    "    for i, blink in enumerate(blink_segments_v2, 1):\n",
    "        start_idx = blink['start_idx']\n",
    "        end_idx = blink['end_idx']\n",
    "        start_time = VideoData2['Seconds'].iloc[start_idx]\n",
    "        end_time = VideoData2['Seconds'].iloc[end_idx]\n",
    "        duration_ms = (end_time - start_time) * 1000\n",
    "        \n",
    "        # Warn about very long blinks (may need manual verification)\n",
    "        if duration_ms > long_blink_warning_ms:\n",
    "            frame_start = int(VideoData2['frame_idx'].iloc[start_idx])\n",
    "            frame_end = int(VideoData2['frame_idx'].iloc[end_idx])\n",
    "            long_blinks_warnings_v2.append({\n",
    "                'blink_num': i,\n",
    "                'frames': f\"{frame_start}-{frame_end}\",\n",
    "                'duration_ms': duration_ms\n",
    "            })\n",
    "    \n",
    "    # Print warnings for long blinks\n",
    "    if len(long_blinks_warnings_v2) > 0:\n",
    "        print(f\"\\n   ‚ö†Ô∏è WARNING: Found {len(long_blinks_warnings_v2)} blink(s) longer than {long_blink_warning_ms}ms:\")\n",
    "        for warn in long_blinks_warnings_v2:\n",
    "            print(f\"      Blink {warn['blink_num']}: frames {warn['frames']}, duration {warn['duration_ms']:.1f}ms - Please verify this is a real blink in the video\")\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"\\n  Detailed blink detection information:\")\n",
    "        print(f\"  FPS: {fps_2:.2f}\")\n",
    "        print(f\"  Long blink warning threshold: {long_blink_warning_frames} frames ({long_blink_warning_ms}ms)\")\n",
    "        print(f\"  Using hard threshold: {blink_threshold_v2:.4f}\")\n",
    "        print(f\"  Detected {len(blink_segments_v2)} blink segment(s)\\n\")\n",
    "    \n",
    "    # Print all detected blinks once (detailed)\n",
    "    if debug and len(blink_segments_v2) > 0:\n",
    "        print(f\"  Detected blinks:\")\n",
    "        for i, blink in enumerate(blink_segments_v2, 1):\n",
    "            start_idx = blink['start_idx']\n",
    "            end_idx = blink['end_idx']\n",
    "            \n",
    "            # Calculate time range\n",
    "            start_time = VideoData2['Seconds'].iloc[start_idx]\n",
    "            end_time = VideoData2['Seconds'].iloc[end_idx]\n",
    "            duration_ms = (end_time - start_time) * 1000\n",
    "            \n",
    "            # Get actual frame numbers from frame_idx column\n",
    "            if 'frame_idx' in VideoData2.columns:\n",
    "                actual_start_frame = int(VideoData2['frame_idx'].iloc[start_idx])\n",
    "                actual_end_frame = int(VideoData2['frame_idx'].iloc[end_idx])\n",
    "                frame_info = f\"frames {actual_start_frame}-{actual_end_frame}\"\n",
    "            else:\n",
    "                actual_start_frame = start_idx\n",
    "                actual_end_frame = end_idx\n",
    "                frame_info = f\"frames {actual_start_frame}-{actual_end_frame}\"\n",
    "            \n",
    "            print(f\"    Blink {i}: {frame_info}, {blink['length']} frames, {duration_ms:.1f}ms, mean score: {blink['mean_score']:.4f}\")\n",
    "        \n",
    "        # DIAGNOSTIC: Direct comparison of manual vs auto-detected blinks (detailed)\n",
    "        if debug and ('manual_blinks_v2' in globals() and manual_blinks_v2 is not None):\n",
    "            # Get auto-detected blink frame ranges\n",
    "            auto_blinks_v2 = []\n",
    "            for i, blink in enumerate(blink_segments_v2, 1):\n",
    "                start_idx = blink['start_idx']\n",
    "                end_idx = blink['end_idx']\n",
    "                frame_start = int(VideoData2['frame_idx'].iloc[start_idx])\n",
    "                frame_end = int(VideoData2['frame_idx'].iloc[end_idx])\n",
    "                auto_blinks_v2.append({'num': i, 'start': frame_start, 'end': frame_end, 'start_idx': start_idx, 'end_idx': end_idx, 'length': blink['length']})\n",
    "            \n",
    "            print(f\"\\n   üîç MANUAL vs AUTO-DETECTED BLINK COMPARISON:\")\n",
    "            print(f\"      Manual blinks: {len(manual_blinks_v2)}\")\n",
    "            print(f\"      Auto-detected blinks: {len(auto_blinks_v2)}\")\n",
    "            \n",
    "            # Match manual blinks to auto-detected ones (find overlapping frames)\n",
    "            # Allow matching with multiple auto-detected blinks if manual blink is over-merged\n",
    "            print(f\"\\n   Manual ‚Üí Auto matching (overlap analysis):\")\n",
    "            for manual in manual_blinks_v2:\n",
    "                manual_length = manual['end'] - manual['start'] + 1\n",
    "                total_overlap = 0\n",
    "                matching_auto_blinks = []\n",
    "                \n",
    "                # Find all auto-detected blinks that overlap with this manual blink\n",
    "                for auto in auto_blinks_v2:\n",
    "                    overlap_start = max(manual['start'], auto['start'])\n",
    "                    overlap_end = min(manual['end'], auto['end'])\n",
    "                    if overlap_start <= overlap_end:\n",
    "                        overlap_frames = overlap_end - overlap_start + 1\n",
    "                        total_overlap += overlap_frames\n",
    "                        matching_auto_blinks.append({\n",
    "                            'auto': auto,\n",
    "                            'overlap': overlap_frames\n",
    "                        })\n",
    "                \n",
    "                # Calculate total overlap percentage\n",
    "                total_overlap_pct = (total_overlap / manual_length) * 100 if manual_length > 0 else 0\n",
    "                \n",
    "                # Match if total overlap is >= 40% (less stringent to handle over-merged manual blinks)\n",
    "                if total_overlap >= manual_length * 0.40:  # At least 40% overlap\n",
    "                    if len(matching_auto_blinks) == 1:\n",
    "                        # Single match\n",
    "                        match_info = matching_auto_blinks[0]\n",
    "                        best_match = match_info['auto']\n",
    "                        overlap = match_info['overlap']\n",
    "                        start_diff = best_match['start'] - manual['start']\n",
    "                        end_diff = best_match['end'] - manual['end']\n",
    "                        match_str = f\"‚úÖ MATCH: Auto blink {best_match['num']} (frames {best_match['start']}-{best_match['end']})\"\n",
    "                        match_str += f\", {overlap} frames overlap ({total_overlap_pct:.1f}%)\"\n",
    "                        if start_diff != 0 or end_diff != 0:\n",
    "                            match_str += f\", offset: start={start_diff:+d}, end={end_diff:+d}\"\n",
    "                        print(f\"      Manual {manual['num']}: {manual['start']}-{manual['end']} ‚Üí {match_str}\")\n",
    "                    else:\n",
    "                        # Multiple matches (manual blink is over-merged)\n",
    "                        auto_nums = [m['auto']['num'] for m in matching_auto_blinks]\n",
    "                        auto_ranges = [f\"{m['auto']['start']}-{m['auto']['end']}\" for m in matching_auto_blinks]\n",
    "                        match_str = f\"‚úÖ MATCH: Auto blinks {auto_nums} (frames: {', '.join(auto_ranges)})\"\n",
    "                        match_str += f\", total {total_overlap} frames overlap ({total_overlap_pct:.1f}%)\"\n",
    "                        print(f\"      Manual {manual['num']}: {manual['start']}-{manual['end']} ‚Üí {match_str}\")\n",
    "                else:\n",
    "                    print(f\"      Manual {manual['num']}: {manual['start']}-{manual['end']} ‚Üí ‚ùå NO MATCH FOUND\")\n",
    "            \n",
    "            # Also check which auto-detected blinks don't have manual matches\n",
    "            print(f\"\\n   Auto-detected blinks without manual matches:\")\n",
    "            unmatched_auto = []\n",
    "            for auto in auto_blinks_v2:\n",
    "                has_match = False\n",
    "                for manual in manual_blinks_v2:\n",
    "                    overlap_start = max(manual['start'], auto['start'])\n",
    "                    overlap_end = min(manual['end'], auto['end'])\n",
    "                    if overlap_start <= overlap_end:\n",
    "                        overlap_frames = overlap_end - overlap_start + 1\n",
    "                        manual_length = manual['end'] - manual['start'] + 1\n",
    "                        # Use same threshold as matching logic (40% of manual blink)\n",
    "                        if overlap_frames >= manual_length * 0.40:\n",
    "                            has_match = True\n",
    "                            break\n",
    "                if not has_match:\n",
    "                    unmatched_auto.append(auto)\n",
    "            \n",
    "            if len(unmatched_auto) > 0:\n",
    "                for auto in unmatched_auto:\n",
    "                    print(f\"      Auto {auto['num']}: frames {auto['start']}-{auto['end']}, length={auto['length']} frames\")\n",
    "            else:\n",
    "                print(f\"      (all auto-detected blinks have manual matches)\")\n",
    "        elif debug:\n",
    "            print(f\"\\n   ‚ö†Ô∏è WARNING: Manual blink comparison skipped - Video2_manual_blinks.csv not found in data_path\")\n",
    "        \n",
    "        # Interpolate over short blinks (keep long blinks as NaN)\n",
    "        if debug:\n",
    "            print(f\"\\n  Interpolating short blinks (< {min_frames_threshold} frames):\")\n",
    "        short_blink_frames_v2 = 0\n",
    "        if len(short_blink_segments_v2) > 0:\n",
    "            # Mark short blinks as NaN\n",
    "            for blink in short_blink_segments_v2:\n",
    "                start_idx = blink['start_idx']\n",
    "                end_idx = blink['end_idx']\n",
    "                VideoData2.loc[VideoData2.index[start_idx:end_idx+1], columns_of_interest] = np.nan\n",
    "                short_blink_frames_v2 += blink['length']\n",
    "            \n",
    "            # Interpolate all NaNs (this fills short blinks)\n",
    "            VideoData2[columns_of_interest] = VideoData2[columns_of_interest].interpolate(method='linear', limit_direction='both')\n",
    "            \n",
    "            if debug:\n",
    "                print(f\"    Interpolated {short_blink_frames_v2} frames from {len(short_blink_segments_v2)} short blink segment(s)\")\n",
    "        else:\n",
    "            if debug:\n",
    "                print(f\"    No short blinks to interpolate\")\n",
    "        \n",
    "        # Mark long blinks by setting coordinates to NaN (these remain as NaN, not interpolated)\n",
    "        recording_start_time = VideoData2['Seconds'].iloc[0]\n",
    "        total_blink_frames_v2 = 0\n",
    "        for blink in blink_segments_v2:\n",
    "            start_idx = blink['start_idx']\n",
    "            end_idx = blink['end_idx']\n",
    "            VideoData2.loc[VideoData2.index[start_idx:end_idx+1], columns_of_interest] = np.nan\n",
    "            total_blink_frames_v2 += blink['length']\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"  Total long blink frames marked (kept as NaN): {total_blink_frames_v2} frames \"\n",
    "                  f\"({total_blink_frames_v2/fps_2*1000:.1f}ms)\")\n",
    "        \n",
    "        # Calculate blink bout rate\n",
    "        recording_duration_min = (VideoData2['Seconds'].iloc[-1] - VideoData2['Seconds'].iloc[0]) / 60\n",
    "        blink_bout_rate = len(blink_bouts_v2) / recording_duration_min if recording_duration_min > 0 else 0\n",
    "        if debug:\n",
    "            print(f\"  Blink bout rate: {blink_bout_rate:.2f} blink bouts/minute\")\n",
    "    else:\n",
    "        if debug:\n",
    "            print(\"  No blinks detected\")\n",
    "\n",
    "print(\"\\n‚úÖ Blink detection complete. Blink periods remain as NaN (not interpolated).\")\n",
    "\n",
    "# Compare blink bout timing between VideoData1 and VideoData2 (between eyes)\n",
    "if ('VideoData1_Has_Sleap' in globals() and VideoData1_Has_Sleap and \n",
    "    'VideoData2_Has_Sleap' in globals() and VideoData2_Has_Sleap):\n",
    "    if debug:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"BLINK BOUT TIMING COMPARISON: VideoData1 vs VideoData2 (Between Eyes)\")\n",
    "        print(\"=\"*80)\n",
    "    \n",
    "    # Get blink bout frame ranges for both videos (if they exist)\n",
    "    # Check if blink_bouts variables exist (they are created during blink detection)\n",
    "    try:\n",
    "        has_bouts_v1 = 'blink_bouts_v1' in globals() and len(blink_bouts_v1) > 0\n",
    "    except:\n",
    "        has_bouts_v1 = False\n",
    "    \n",
    "    try:\n",
    "        has_bouts_v2 = 'blink_bouts_v2' in globals() and len(blink_bouts_v2) > 0\n",
    "    except:\n",
    "        has_bouts_v2 = False\n",
    "    \n",
    "    if has_bouts_v1 and has_bouts_v2:\n",
    "        \n",
    "        # Convert bout indices to frame numbers\n",
    "        bouts_v1 = []\n",
    "        for i, bout in enumerate(blink_bouts_v1, 1):\n",
    "            start_idx = bout['start_idx']\n",
    "            end_idx = bout['end_idx']\n",
    "            if 'frame_idx' in VideoData1.columns:\n",
    "                start_frame = int(VideoData1['frame_idx'].iloc[start_idx])\n",
    "                end_frame = int(VideoData1['frame_idx'].iloc[end_idx])\n",
    "            else:\n",
    "                start_frame = start_idx\n",
    "                end_frame = end_idx\n",
    "            bouts_v1.append({\n",
    "                'num': i,\n",
    "                'start_frame': start_frame,\n",
    "                'end_frame': end_frame,\n",
    "                'start_idx': start_idx,\n",
    "                'end_idx': end_idx,\n",
    "                'length': bout['length']\n",
    "            })\n",
    "        \n",
    "        bouts_v2 = []\n",
    "        for i, bout in enumerate(blink_bouts_v2, 1):\n",
    "            start_idx = bout['start_idx']\n",
    "            end_idx = bout['end_idx']\n",
    "            if 'frame_idx' in VideoData2.columns:\n",
    "                start_frame = int(VideoData2['frame_idx'].iloc[start_idx])\n",
    "                end_frame = int(VideoData2['frame_idx'].iloc[end_idx])\n",
    "            else:\n",
    "                start_frame = start_idx\n",
    "                end_frame = end_idx\n",
    "            bouts_v2.append({\n",
    "                'num': i,\n",
    "                'start_frame': start_frame,\n",
    "                'end_frame': end_frame,\n",
    "                'start_idx': start_idx,\n",
    "                'end_idx': end_idx,\n",
    "                'length': bout['length']\n",
    "            })\n",
    "        \n",
    "        # Find concurrent bouts (overlapping in time, synchronized by Seconds)\n",
    "        concurrent_bouts = []\n",
    "        v1_independent = []\n",
    "        v2_independent = []\n",
    "        \n",
    "        v2_matched = set()  # Track which VideoData2 bouts have been matched\n",
    "        \n",
    "        for bout1 in bouts_v1:\n",
    "            # Get time range for bout1\n",
    "            v1_start_time = VideoData1['Seconds'].iloc[bout1['start_idx']]\n",
    "            v1_end_time = VideoData1['Seconds'].iloc[bout1['end_idx']]\n",
    "            \n",
    "            found_match = False\n",
    "            for bout2 in bouts_v2:\n",
    "                # Get time range for bout2\n",
    "                v2_start_time = VideoData2['Seconds'].iloc[bout2['start_idx']]\n",
    "                v2_end_time = VideoData2['Seconds'].iloc[bout2['end_idx']]\n",
    "                \n",
    "                # Check if bouts overlap in time (any overlapping time period)\n",
    "                overlap_start_time = max(v1_start_time, v2_start_time)\n",
    "                overlap_end_time = min(v1_end_time, v2_end_time)\n",
    "                \n",
    "                if overlap_start_time <= overlap_end_time:\n",
    "                    # Concurrent - they overlap in time\n",
    "                    # Calculate overlap duration\n",
    "                    overlap_duration = overlap_end_time - overlap_start_time\n",
    "                    \n",
    "                    concurrent_bouts.append({\n",
    "                        'v1_num': bout1['num'],\n",
    "                        'v1_start_frame': bout1['start_frame'],\n",
    "                        'v1_end_frame': bout1['end_frame'],\n",
    "                        'v1_start_time': v1_start_time,\n",
    "                        'v1_end_time': v1_end_time,\n",
    "                        'v2_num': bout2['num'],\n",
    "                        'v2_start_frame': bout2['start_frame'],\n",
    "                        'v2_end_frame': bout2['end_frame'],\n",
    "                        'v2_start_time': v2_start_time,\n",
    "                        'v2_end_time': v2_end_time,\n",
    "                        'overlap_start_time': overlap_start_time,\n",
    "                        'overlap_end_time': overlap_end_time,\n",
    "                        'overlap_duration': overlap_duration\n",
    "                    })\n",
    "                    v2_matched.add(bout2['num'])\n",
    "                    found_match = True\n",
    "                    break\n",
    "            \n",
    "            if not found_match:\n",
    "                v1_independent.append(bout1)\n",
    "        \n",
    "        # Find VideoData2 bouts that don't have matches\n",
    "        for bout2 in bouts_v2:\n",
    "            if bout2['num'] not in v2_matched:\n",
    "                v2_independent.append(bout2)\n",
    "        \n",
    "        # Calculate statistics\n",
    "        total_v1_bouts = len(bouts_v1)\n",
    "        total_v2_bouts = len(bouts_v2)\n",
    "        total_concurrent = len(concurrent_bouts)\n",
    "        total_v1_independent = len(v1_independent)\n",
    "        total_v2_independent = len(v2_independent)\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"\\nBlink bout counts:\")\n",
    "            print(f\"  VideoData1: {total_v1_bouts} blink bout(s)\")\n",
    "            print(f\"  VideoData2: {total_v2_bouts} blink bout(s)\")\n",
    "            print(f\"  Concurrent: {total_concurrent} bout(s) (overlapping frames)\")\n",
    "            print(f\"  VideoData1 only: {total_v1_independent} bout(s)\")\n",
    "            print(f\"  VideoData2 only: {total_v2_independent} bout(s)\")\n",
    "            \n",
    "            if total_v1_bouts > 0 and total_v2_bouts > 0:\n",
    "                concurrent_pct_v1 = (total_concurrent / total_v1_bouts) * 100\n",
    "                concurrent_pct_v2 = (total_concurrent / total_v2_bouts) * 100\n",
    "                print(f\"\\nConcurrency percentage:\")\n",
    "                print(f\"  {concurrent_pct_v1:.1f}% of VideoData1 bouts are concurrent with VideoData2\")\n",
    "                print(f\"  {concurrent_pct_v2:.1f}% of VideoData2 bouts are concurrent with VideoData1\")\n",
    "                \n",
    "                # Calculate timing offsets for concurrent bouts\n",
    "                if len(concurrent_bouts) > 0:\n",
    "                    time_offsets_ms = []\n",
    "                    for cb in concurrent_bouts:\n",
    "                        # Calculate offset from start times (already in Seconds)\n",
    "                        offset_ms = (cb['v1_start_time'] - cb['v2_start_time']) * 1000\n",
    "                        time_offsets_ms.append(offset_ms)\n",
    "                        cb['time_offset_ms'] = offset_ms\n",
    "                    \n",
    "                    mean_offset = np.mean(time_offsets_ms)\n",
    "                    std_offset = np.std(time_offsets_ms)\n",
    "                    print(f\"\\nTiming offset for concurrent bouts:\")\n",
    "                    print(f\"  Mean offset (VideoData1 - VideoData2): {mean_offset:.2f} ms\")\n",
    "                    print(f\"  Std offset: {std_offset:.2f} ms\")\n",
    "                    print(f\"  Range: {min(time_offsets_ms):.2f} to {max(time_offsets_ms):.2f} ms\")\n",
    "            \n",
    "            # Visualization removed per request\n",
    "            print(\"=\"*80)\n",
    "    elif has_bouts_v1 or has_bouts_v2:\n",
    "        print(f\"\\n‚ö†Ô∏è Cannot compare blink bouts - only one eye has blink bouts detected:\")\n",
    "        if has_bouts_v1:\n",
    "            print(f\"  VideoData1: {len(blink_bouts_v1)} blink bout(s)\")\n",
    "        else:\n",
    "            print(f\"  VideoData1: 0 blink bout(s)\")\n",
    "        if has_bouts_v2:\n",
    "            print(f\"  VideoData2: {len(blink_bouts_v2)} blink bout(s)\")\n",
    "        else:\n",
    "            print(f\"  VideoData2: 0 blink bout(s)\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è Cannot compare blink bouts - neither video has blink bouts detected\")\n",
    "\n",
    "# Save blink detection results to CSV files\n",
    "if 'VideoData1_Has_Sleap' in globals() and VideoData1_Has_Sleap:\n",
    "    if len(blink_segments_v1) > 0:\n",
    "        # Helper function to check if an auto-detected blink matches any manual blink\n",
    "        def check_manual_match(auto_start, auto_end, manual_blinks_list):\n",
    "            \"\"\"Check if auto-detected blink matches any manual blink (>=40% overlap to handle over-merged manual blinks)\"\"\"\n",
    "            if manual_blinks_list is None:\n",
    "                return 0\n",
    "            for manual in manual_blinks_list:\n",
    "                overlap_start = max(manual['start'], auto_start)\n",
    "                overlap_end = min(manual['end'], auto_end)\n",
    "                if overlap_start <= overlap_end:\n",
    "                    overlap_frames = overlap_end - overlap_start + 1\n",
    "                    manual_length = manual['end'] - manual['start'] + 1\n",
    "                    # Use 40% threshold to match the diagnostic comparison logic\n",
    "                    if overlap_frames >= manual_length * 0.40:\n",
    "                        return 1\n",
    "            return 0\n",
    "        \n",
    "        # Collect blink information\n",
    "        blink_data_v1 = []\n",
    "        manual_blinks_for_csv = None\n",
    "        if 'manual_blinks_v1' in globals() and manual_blinks_v1 is not None:\n",
    "            manual_blinks_for_csv = manual_blinks_v1\n",
    "            \n",
    "        for i, blink in enumerate(blink_segments_v1, 1):\n",
    "            start_idx = blink['start_idx']\n",
    "            end_idx = blink['end_idx']\n",
    "            \n",
    "            # Get actual frame numbers from frame_idx column\n",
    "            if 'frame_idx' in VideoData1.columns:\n",
    "                first_frame = int(VideoData1['frame_idx'].iloc[start_idx])\n",
    "                last_frame = int(VideoData1['frame_idx'].iloc[end_idx])\n",
    "            else:\n",
    "                first_frame = start_idx\n",
    "                last_frame = end_idx\n",
    "            \n",
    "            # Check if this blink matches a manual one\n",
    "            matches_manual = check_manual_match(first_frame, last_frame, manual_blinks_for_csv)\n",
    "            \n",
    "            blink_data_v1.append({\n",
    "                'blink_number': i,\n",
    "                'first_frame': first_frame,\n",
    "                'last_frame': last_frame,\n",
    "                'matches_manual': matches_manual\n",
    "            })\n",
    "        \n",
    "        # Create DataFrame and save to CSV\n",
    "        blink_df_v1 = pd.DataFrame(blink_data_v1)\n",
    "        blink_csv_path_v1 = data_path / \"blink_detection_VideoData1.csv\"\n",
    "        blink_df_v1.to_csv(blink_csv_path_v1, index=False)\n",
    "        print(f\"\\n‚úÖ Blink detection results (VideoData1) saved to: {blink_csv_path_v1}\")\n",
    "        print(f\"   Saved {len(blink_data_v1)} blinks\")\n",
    "\n",
    "if 'VideoData2_Has_Sleap' in globals() and VideoData2_Has_Sleap:\n",
    "    if len(blink_segments_v2) > 0:\n",
    "        # Helper function to check if an auto-detected blink matches any manual blink\n",
    "        def check_manual_match(auto_start, auto_end, manual_blinks_list):\n",
    "            \"\"\"Check if auto-detected blink matches any manual blink (>=40% overlap to handle over-merged manual blinks)\"\"\"\n",
    "            if manual_blinks_list is None:\n",
    "                return 0\n",
    "            for manual in manual_blinks_list:\n",
    "                overlap_start = max(manual['start'], auto_start)\n",
    "                overlap_end = min(manual['end'], auto_end)\n",
    "                if overlap_start <= overlap_end:\n",
    "                    overlap_frames = overlap_end - overlap_start + 1\n",
    "                    manual_length = manual['end'] - manual['start'] + 1\n",
    "                    # Use 40% threshold to match the diagnostic comparison logic\n",
    "                    if overlap_frames >= manual_length * 0.40:\n",
    "                        return 1\n",
    "            return 0\n",
    "        \n",
    "        # Collect blink information\n",
    "        blink_data_v2 = []\n",
    "        manual_blinks_for_csv = None\n",
    "        if 'manual_blinks_v2' in globals() and manual_blinks_v2 is not None:\n",
    "            manual_blinks_for_csv = manual_blinks_v2\n",
    "            \n",
    "        for i, blink in enumerate(blink_segments_v2, 1):\n",
    "            start_idx = blink['start_idx']\n",
    "            end_idx = blink['end_idx']\n",
    "            \n",
    "            # Get actual frame numbers from frame_idx column\n",
    "            if 'frame_idx' in VideoData2.columns:\n",
    "                first_frame = int(VideoData2['frame_idx'].iloc[start_idx])\n",
    "                last_frame = int(VideoData2['frame_idx'].iloc[end_idx])\n",
    "            else:\n",
    "                first_frame = start_idx\n",
    "                last_frame = end_idx\n",
    "            \n",
    "            # Check if this blink matches a manual one\n",
    "            matches_manual = check_manual_match(first_frame, last_frame, manual_blinks_for_csv)\n",
    "            \n",
    "            blink_data_v2.append({\n",
    "                'blink_number': i,\n",
    "                'first_frame': first_frame,\n",
    "                'last_frame': last_frame,\n",
    "                'matches_manual': matches_manual\n",
    "            })\n",
    "        \n",
    "        # Create DataFrame and save to CSV\n",
    "        blink_df_v2 = pd.DataFrame(blink_data_v2)\n",
    "        blink_csv_path_v2 = data_path / \"blink_detection_VideoData2.csv\"\n",
    "        blink_df_v2.to_csv(blink_csv_path_v2, index=False)\n",
    "        print(f\"\\n‚úÖ Blink detection results (VideoData2) saved to: {blink_csv_path_v2}\")\n",
    "        print(f\"   Saved {len(blink_data_v2)} blinks\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìπ MANUAL QC CHECK:\")\n",
    "print(\"=\"*80)\n",
    "print(\"For instructions on how to prepare videos for manual blink detection QC,\")\n",
    "print(\"see: https://github.com/ranczlab/vestibular_vr_pipeline/issues/86\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Restore original stdout and save captured output to file\n",
    "sys.stdout = original_stdout\n",
    "\n",
    "# Get the captured output\n",
    "captured_output = output_buffer.getvalue()\n",
    "\n",
    "# Save to file in data_path folder\n",
    "output_file = data_path / \"blink_detection_QC.txt\"\n",
    "output_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "with open(output_file, 'w') as f:\n",
    "    f.write(captured_output)\n",
    "\n",
    "print(f\"\\n‚úÖ Blink detection output saved to: {output_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968fd27b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# QC plot timeseries of interpolation corrected NaN and (TODO low confidence coordinates in browser \n",
    "############################################################################################################\n",
    "\n",
    "if plot_QC_timeseries:\n",
    "    print(f'‚ÑπÔ∏è Figure opens in browser window, takes a bit of time.')\n",
    "    \n",
    "    # VideoData1 QC Plot\n",
    "    if 'VideoData1_Has_Sleap' in globals() and VideoData1_Has_Sleap:\n",
    "        fig1 = make_subplots(\n",
    "            rows=4, cols=1,\n",
    "            shared_xaxes=True,\n",
    "            vertical_spacing=0.05,\n",
    "            subplot_titles=(\n",
    "                f\"VideoData1 - X coordinates for pupil centre and left-right eye corner\",\n",
    "                f\"VideoData1 - Y coordinates for pupil centre and left-right eye corner\",\n",
    "                f\"VideoData1 - X coordinates for iris points\",\n",
    "                f\"VideoData1 - Y coordinates for iris points\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Row 1: Plot left.x, center.x, right.x\n",
    "        fig1.add_trace(go.Scatter(x=VideoData1['Seconds'], y=VideoData1['left.x'], mode='lines', name='left.x'), row=1, col=1)\n",
    "        fig1.add_trace(go.Scatter(x=VideoData1['Seconds'], y=VideoData1['center.x'], mode='lines', name='center.x'), row=1, col=1)\n",
    "        fig1.add_trace(go.Scatter(x=VideoData1['Seconds'], y=VideoData1['right.x'], mode='lines', name='right.x'), row=1, col=1)\n",
    "\n",
    "        # Row 2: Plot left.y, center.y, right.y\n",
    "        fig1.add_trace(go.Scatter(x=VideoData1['Seconds'], y=VideoData1['left.y'], mode='lines', name='left.y'), row=2, col=1)\n",
    "        fig1.add_trace(go.Scatter(x=VideoData1['Seconds'], y=VideoData1['center.y'], mode='lines', name='center.y'), row=2, col=1)\n",
    "        fig1.add_trace(go.Scatter(x=VideoData1['Seconds'], y=VideoData1['right.y'], mode='lines', name='right.y'), row=2, col=1)\n",
    "\n",
    "        # Row 3: Plot p.x coordinates for p1 to p8\n",
    "        for col in ['p1.x', 'p2.x', 'p3.x', 'p4.x', 'p5.x', 'p6.x', 'p7.x', 'p8.x']:\n",
    "            fig1.add_trace(go.Scatter(x=VideoData1['Seconds'], y=VideoData1[col], mode='lines', name=col), row=3, col=1)\n",
    "\n",
    "        # Row 4: Plot p.y coordinates for p1 to p8\n",
    "        for col in ['p1.y', 'p2.y', 'p3.y', 'p4.y', 'p5.y', 'p6.y', 'p7.y', 'p8.y']:\n",
    "            fig1.add_trace(go.Scatter(x=VideoData1['Seconds'], y=VideoData1[col], mode='lines', name=col), row=4, col=1)\n",
    "\n",
    "        fig1.update_layout(\n",
    "            height=1200,\n",
    "            title_text=f\"VideoData1 - Time series subplots for coordinates (QC after interpolation)\",\n",
    "            showlegend=True\n",
    "        )\n",
    "        fig1.update_xaxes(title_text=\"Seconds\", row=4, col=1)\n",
    "        fig1.update_yaxes(title_text=\"X Position\", row=1, col=1)\n",
    "        fig1.update_yaxes(title_text=\"Y Position\", row=2, col=1)\n",
    "        fig1.update_yaxes(title_text=\"X Position\", row=3, col=1)\n",
    "        fig1.update_yaxes(title_text=\"Y Position\", row=4, col=1)\n",
    "\n",
    "        fig1.show(renderer='browser')\n",
    "    \n",
    "    # VideoData2 QC Plot\n",
    "    if 'VideoData2_Has_Sleap' in globals() and VideoData2_Has_Sleap:\n",
    "        fig2 = make_subplots(\n",
    "            rows=4, cols=1,\n",
    "            shared_xaxes=True,\n",
    "            vertical_spacing=0.05,\n",
    "            subplot_titles=(\n",
    "                f\"VideoData2 - X coordinates for pupil centre and left-right eye corner\",\n",
    "                f\"VideoData2 - Y coordinates for pupil centre and left-right eye corner\",\n",
    "                f\"VideoData2 - X coordinates for iris points\",\n",
    "                f\"VideoData2 - Y coordinates for iris points\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Row 1: Plot left.x, center.x, right.x\n",
    "        fig2.add_trace(go.Scatter(x=VideoData2['Seconds'], y=VideoData2['left.x'], mode='lines', name='left.x'), row=1, col=1)\n",
    "        fig2.add_trace(go.Scatter(x=VideoData2['Seconds'], y=VideoData2['center.x'], mode='lines', name='center.x'), row=1, col=1)\n",
    "        fig2.add_trace(go.Scatter(x=VideoData2['Seconds'], y=VideoData2['right.x'], mode='lines', name='right.x'), row=1, col=1)\n",
    "\n",
    "        # Row 2: Plot left.y, center.y, right.y\n",
    "        fig2.add_trace(go.Scatter(x=VideoData2['Seconds'], y=VideoData2['left.y'], mode='lines', name='left.y'), row=2, col=1)\n",
    "        fig2.add_trace(go.Scatter(x=VideoData2['Seconds'], y=VideoData2['center.y'], mode='lines', name='center.y'), row=2, col=1)\n",
    "        fig2.add_trace(go.Scatter(x=VideoData2['Seconds'], y=VideoData2['right.y'], mode='lines', name='right.y'), row=2, col=1)\n",
    "\n",
    "        # Row 3: Plot p.x coordinates for p1 to p8\n",
    "        for col in ['p1.x', 'p2.x', 'p3.x', 'p4.x', 'p5.x', 'p6.x', 'p7.x', 'p8.x']:\n",
    "            fig2.add_trace(go.Scatter(x=VideoData2['Seconds'], y=VideoData2[col], mode='lines', name=col), row=3, col=1)\n",
    "\n",
    "        # Row 4: Plot p.y coordinates for p1 to p8\n",
    "        for col in ['p1.y', 'p2.y', 'p3.y', 'p4.y', 'p5.y', 'p6.y', 'p7.y', 'p8.y']:\n",
    "            fig2.add_trace(go.Scatter(x=VideoData2['Seconds'], y=VideoData2[col], mode='lines', name=col), row=4, col=1)\n",
    "\n",
    "        fig2.update_layout(\n",
    "            height=1200,\n",
    "            title_text=f\"VideoData2 - Time series subplots for coordinates (QC after interpolation)\",\n",
    "            showlegend=True\n",
    "        )\n",
    "        fig2.update_xaxes(title_text=\"Seconds\", row=4, col=1)\n",
    "        fig2.update_yaxes(title_text=\"X Position\", row=1, col=1)\n",
    "        fig2.update_yaxes(title_text=\"Y Position\", row=2, col=1)\n",
    "        fig2.update_yaxes(title_text=\"X Position\", row=3, col=1)\n",
    "        fig2.update_yaxes(title_text=\"Y Position\", row=4, col=1)\n",
    "\n",
    "        fig2.show(renderer='browser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9844a3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QC plot XY coordinate distributions after NaN and ( TODO - low confidence inference points) are interpolated \n",
    "##############################################################################################################\n",
    "\n",
    "columns_of_interest = ['left.x','left.y','center.x','center.y','right.x','right.y','p1.x','p1.y','p2.x','p2.y','p3.x','p3.y','p4.x','p4.y','p5.x','p5.y','p6.x','p6.y','p7.x','p7.y','p8.x','p8.y']\n",
    "\n",
    "# Create coordinates_dict for both datasets\n",
    "if VideoData1_Has_Sleap:\n",
    "    coordinates_dict1_processed = lp.get_coordinates_dict(VideoData1, columns_of_interest)\n",
    "if VideoData2_Has_Sleap:\n",
    "    coordinates_dict2_processed = lp.get_coordinates_dict(VideoData2, columns_of_interest)\n",
    "\n",
    "columns_of_interest = ['left', 'right', 'center', 'p1', 'p2', 'p3', 'p4', 'p5', 'p6', 'p7', 'p8']\n",
    "\n",
    "# Filter out NaN values and calculate the min and max values for X and Y coordinates for both dict1 and dict2\n",
    "def min_max_dict(coordinates_dict):\n",
    "    x_min = min([coordinates_dict[f'{col}.x'][~np.isnan(coordinates_dict[f'{col}.x'])].min() for col in columns_of_interest])\n",
    "    x_max = max([coordinates_dict[f'{col}.x'][~np.isnan(coordinates_dict[f'{col}.x'])].max() for col in columns_of_interest])\n",
    "    y_min = min([coordinates_dict[f'{col}.y'][~np.isnan(coordinates_dict[f'{col}.y'])].min() for col in columns_of_interest])\n",
    "    y_max = max([coordinates_dict[f'{col}.y'][~np.isnan(coordinates_dict[f'{col}.y'])].max() for col in columns_of_interest])\n",
    "    return x_min, x_max, y_min, y_max\n",
    "\n",
    "if VideoData1_Has_Sleap:\n",
    "    x_min1, x_max1, y_min1, y_max1 = min_max_dict(coordinates_dict1_processed)\n",
    "if VideoData2_Has_Sleap:\n",
    "    x_min2, x_max2, y_min2, y_max2 = min_max_dict(coordinates_dict2_processed)\n",
    "\n",
    "# Use global min and max for consistency across subplots\n",
    "if VideoData1_Has_Sleap and VideoData2_Has_Sleap:\n",
    "    x_min = min(x_min1, x_min2)\n",
    "    x_max = max(x_max1, x_max2)\n",
    "    y_min = min(y_min1, y_min2)\n",
    "    y_max = max(y_max1, y_max2)\n",
    "elif VideoData1_Has_Sleap:\n",
    "    x_min, x_max, y_min, y_max = x_min1, x_max1, y_min1, y_max1\n",
    "elif VideoData2_Has_Sleap:\n",
    "    x_min, x_max, y_min, y_max = x_min2, x_max2, y_min2, y_max2\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(18, 12))\n",
    "fig.suptitle(\n",
    "    f\"XY coordinate distribution of different points for {get_eye_label('VideoData1')} and {get_eye_label('VideoData2')} post outlier removal and NaN interpolation\",\n",
    "    fontsize=14\n",
    ")\n",
    "\n",
    "# Define colormap for p1-p8\n",
    "colors = ['blue', 'green', 'red', 'cyan', 'magenta', 'yellow', 'black', 'orange']\n",
    "\n",
    "# Panel 1: left, right, center (VideoData1)\n",
    "if VideoData1_Has_Sleap:\n",
    "    ax[0, 0].set_title(f\"{get_eye_label('VideoData1')}: left, right, center\")\n",
    "    ax[0, 0].scatter(coordinates_dict1_processed['left.x'], coordinates_dict1_processed['left.y'], color='black', label='left', s=10)\n",
    "    ax[0, 0].scatter(coordinates_dict1_processed['right.x'], coordinates_dict1_processed['right.y'], color='grey', label='right', s=10)\n",
    "    ax[0, 0].scatter(coordinates_dict1_processed['center.x'], coordinates_dict1_processed['center.y'], color='red', label='center', s=10)\n",
    "    ax[0, 0].set_xlim([x_min, x_max])\n",
    "    ax[0, 0].set_ylim([y_min, y_max])\n",
    "    ax[0, 0].set_xlabel('x coordinates (pixels)')\n",
    "    ax[0, 0].set_ylabel('y coordinates (pixels)')\n",
    "    ax[0, 0].legend(loc='upper right')\n",
    "\n",
    "    # Panel 2: p1 to p8 (VideoData1)\n",
    "    ax[0, 1].set_title(f\"{get_eye_label('VideoData1')}: p1 to p8\")\n",
    "    for idx, col in enumerate(columns_of_interest[3:]):\n",
    "        ax[0, 1].scatter(coordinates_dict1_processed[f'{col}.x'], coordinates_dict1_processed[f'{col}.y'], color=colors[idx], label=col, s=5)\n",
    "    ax[0, 1].set_xlim([x_min, x_max])\n",
    "    ax[0, 1].set_ylim([y_min, y_max])\n",
    "    ax[0, 1].set_xlabel('x coordinates (pixels)')\n",
    "    ax[0, 1].set_ylabel('y coordinates (pixels)')\n",
    "    ax[0, 1].legend(loc='upper right')\n",
    "\n",
    "# Panel 3: left, right, center (VideoData2)\n",
    "if VideoData2_Has_Sleap:\n",
    "    ax[1, 0].set_title(f\"{get_eye_label('VideoData2')}: left, right, center\")\n",
    "    ax[1, 0].scatter(coordinates_dict2_processed['left.x'], coordinates_dict2_processed['left.y'], color='black', label='left', s=10)\n",
    "    ax[1, 0].scatter(coordinates_dict2_processed['right.x'], coordinates_dict2_processed['right.y'], color='grey', label='right', s=10)\n",
    "    ax[1, 0].scatter(coordinates_dict2_processed['center.x'], coordinates_dict2_processed['center.y'], color='red', label='center', s=10)\n",
    "    ax[1, 0].set_xlim([x_min, x_max])\n",
    "    ax[1, 0].set_ylim([y_min, y_max])\n",
    "    ax[1, 0].set_xlabel('x coordinates (pixels)')\n",
    "    ax[1, 0].set_ylabel('y coordinates (pixels)')\n",
    "    ax[1, 0].legend(loc='upper right')\n",
    "\n",
    "    # Panel 4: p1 to p8 (VideoData2)\n",
    "    ax[1, 1].set_title(f\"{get_eye_label('VideoData2')}: p1 to p8\")\n",
    "    for idx, col in enumerate(columns_of_interest[3:]):\n",
    "        ax[1, 1].scatter(coordinates_dict2_processed[f'{col}.x'], coordinates_dict2_processed[f'{col}.y'], color=colors[idx], label=col, s=5)\n",
    "    ax[1, 1].set_xlim([x_min, x_max])\n",
    "    ax[1, 1].set_ylim([y_min, y_max])\n",
    "    ax[1, 1].set_xlabel('x coordinates (pixels)')\n",
    "    ax[1, 1].set_ylabel('y coordinates (pixels)')\n",
    "    ax[1, 1].legend(loc='upper right')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.96])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b97f728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit ellipses on the 8 points to determine pupil centre and diameter\n",
    "############################################################################################################\n",
    "\n",
    "columns_of_interest = ['left.x','left.y','center.x','center.y','right.x','right.y','p1.x','p1.y','p2.x','p2.y','p3.x','p3.y','p4.x','p4.y','p5.x','p5.y','p6.x','p6.y','p7.x','p7.y','p8.x','p8.y']\n",
    "\n",
    "# VideoData1 processing\n",
    "if VideoData1_Has_Sleap:\n",
    "    print(f\"=== VideoData1 Ellipse Fitting for Pupil Diameter ===\")\n",
    "    coordinates_dict1_processed = lp.get_coordinates_dict(VideoData1, columns_of_interest)\n",
    "\n",
    "    theta1 = lp.find_horizontal_axis_angle(VideoData1, 'left', 'center')\n",
    "    center_point1 = lp.get_left_right_center_point(coordinates_dict1_processed)\n",
    "\n",
    "    columns_of_interest_reformatted = ['left', 'right', 'center', 'p1', 'p2', 'p3', 'p4', 'p5', 'p6', 'p7', 'p8']\n",
    "    remformatted_coordinates_dict1 = lp.get_reformatted_coordinates_dict(coordinates_dict1_processed, columns_of_interest_reformatted)\n",
    "    centered_coordinates_dict1 = lp.get_centered_coordinates_dict(remformatted_coordinates_dict1, center_point1)\n",
    "    rotated_coordinates_dict1 = lp.get_rotated_coordinates_dict(centered_coordinates_dict1, theta1)\n",
    "\n",
    "    columns_of_interest_ellipse = ['p1', 'p2', 'p3', 'p4', 'p5', 'p6', 'p7', 'p8']\n",
    "    ellipse_parameters_data1, ellipse_center_points_data1 = lp.get_fitted_ellipse_parameters(rotated_coordinates_dict1, columns_of_interest_ellipse)\n",
    "\n",
    "    average_diameter1 = np.mean([ellipse_parameters_data1[:,0], ellipse_parameters_data1[:,1]], axis=0)\n",
    "\n",
    "    SleapVideoData1 = process.convert_arrays_to_dataframe(['Seconds', 'Ellipse.Diameter', 'Ellipse.Angle', 'Ellipse.Center.X', 'Ellipse.Center.Y'], [VideoData1['Seconds'].values, average_diameter1, ellipse_parameters_data1[:,2], ellipse_center_points_data1[:,0], ellipse_center_points_data1[:,1]])\n",
    "\n",
    "# VideoData2 processing\n",
    "if VideoData2_Has_Sleap:\n",
    "    print(f\"=== VideoData2 Ellipse Fitting for Pupil Diameter ===\")\n",
    "    coordinates_dict2_processed = lp.get_coordinates_dict(VideoData2, columns_of_interest)\n",
    "\n",
    "    theta2 = lp.find_horizontal_axis_angle(VideoData2, 'left', 'center')\n",
    "    center_point2 = lp.get_left_right_center_point(coordinates_dict2_processed)\n",
    "\n",
    "    columns_of_interest_reformatted = ['left', 'right', 'center', 'p1', 'p2', 'p3', 'p4', 'p5', 'p6', 'p7', 'p8']\n",
    "    remformatted_coordinates_dict2 = lp.get_reformatted_coordinates_dict(coordinates_dict2_processed, columns_of_interest_reformatted)\n",
    "    centered_coordinates_dict2 = lp.get_centered_coordinates_dict(remformatted_coordinates_dict2, center_point2)\n",
    "    rotated_coordinates_dict2 = lp.get_rotated_coordinates_dict(centered_coordinates_dict2, theta2)\n",
    "\n",
    "    columns_of_interest_ellipse = ['p1', 'p2', 'p3', 'p4', 'p5', 'p6', 'p7', 'p8']\n",
    "    ellipse_parameters_data2, ellipse_center_points_data2 = lp.get_fitted_ellipse_parameters(rotated_coordinates_dict2, columns_of_interest_ellipse)\n",
    "\n",
    "    average_diameter2 = np.mean([ellipse_parameters_data2[:,0], ellipse_parameters_data2[:,1]], axis=0)\n",
    "\n",
    "    SleapVideoData2 = process.convert_arrays_to_dataframe(['Seconds', 'Ellipse.Diameter', 'Ellipse.Angle', 'Ellipse.Center.X', 'Ellipse.Center.Y'], [VideoData2['Seconds'].values, average_diameter2, ellipse_parameters_data2[:,2], ellipse_center_points_data2[:,0], ellipse_center_points_data2[:,1]])\n",
    "\n",
    "############################################################################################################\n",
    "# Filter pupil diameter using 10 Hz Butterworth low-pass filter\n",
    "############################################################################################################\n",
    "\n",
    "# VideoData1 filtering\n",
    "if VideoData1_Has_Sleap:\n",
    "    print(f\"\\n=== Filtering pupil diameter for VideoData1  ===\")\n",
    "    # Butterworth filter parameters - pupil_filter_cutoff_hz low-pass filter\n",
    "    fs1 = 1 / np.median(np.diff(SleapVideoData1['Seconds']))  # Sampling frequency (Hz)\n",
    "    order = pupil_filter_order\n",
    "\n",
    "    b1, a1 = butter(order, pupil_filter_cutoff_hz / (0.5 * fs1), btype='low')\n",
    "    \n",
    "    # Handle NaN values before filtering (from blink detection)\n",
    "    # Replace NaN with forward-fill for filtering purposes only (to avoid filtfilt issues)\n",
    "    diameter_data = SleapVideoData1['Ellipse.Diameter'].copy()\n",
    "    # Use ffill() and bfill() instead of deprecated fillna(method='ffill')\n",
    "    diameter_data_filled = diameter_data.ffill().bfill()\n",
    "    \n",
    "    # Apply Butterworth filter\n",
    "    if not diameter_data_filled.isna().all():\n",
    "        filtered = filtfilt(b1, a1, diameter_data_filled)\n",
    "        # Restore NaN values at original NaN positions (from blinks)\n",
    "        filtered = pd.Series(filtered, index=diameter_data.index)\n",
    "        filtered[diameter_data.isna()] = np.nan\n",
    "        SleapVideoData1['Ellipse.Diameter.Filt'] = filtered\n",
    "    else:\n",
    "        # If all values are NaN, just copy\n",
    "        SleapVideoData1['Ellipse.Diameter.Filt'] = diameter_data\n",
    "\n",
    "# VideoData2 filtering\n",
    "if VideoData2_Has_Sleap:\n",
    "    print(f\"=== Filtering pupil diameter for VideoData1 ===\")\n",
    "    # Butterworth filter parameters - pupil_filter_cutoff_hz low-pass filter\n",
    "    fs2 = 1 / np.median(np.diff(SleapVideoData2['Seconds']))  # Sampling frequency (Hz)\n",
    "    order = pupil_filter_order\n",
    "\n",
    "    b2, a2 = butter(order, pupil_filter_cutoff_hz / (0.5 * fs2), btype='low')\n",
    "    \n",
    "    # Handle NaN values before filtering (from blink detection)\n",
    "    # Replace NaN with forward-fill for filtering purposes only (to avoid filtfilt issues)\n",
    "    diameter_data = SleapVideoData2['Ellipse.Diameter'].copy()\n",
    "    # Use ffill() and bfill() instead of deprecated fillna(method='ffill')\n",
    "    diameter_data_filled = diameter_data.ffill().bfill()\n",
    "    \n",
    "    # Apply Butterworth filter\n",
    "    if not diameter_data_filled.isna().all():\n",
    "        filtered = filtfilt(b2, a2, diameter_data_filled)\n",
    "        # Restore NaN values at original NaN positions (from blinks)\n",
    "        filtered = pd.Series(filtered, index=diameter_data.index)\n",
    "        filtered[diameter_data.isna()] = np.nan\n",
    "        SleapVideoData2['Ellipse.Diameter.Filt'] = filtered\n",
    "    else:\n",
    "        # If all values are NaN, just copy\n",
    "        SleapVideoData2['Ellipse.Diameter.Filt'] = diameter_data\n",
    "\n",
    "print(\"‚úÖ Done calculating pupil diameter and angle for both VideoData1 and VideoData2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d3d3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-correlate pupil diameter for left and right eye \n",
    "############################################################################################################\n",
    "\n",
    "if VideoData1_Has_Sleap and VideoData2_Has_Sleap:\n",
    "    # # Create subplots for both comparison and cross-correlation\n",
    "    # fig = make_subplots(\n",
    "    #     rows=2, cols=1,\n",
    "    #     subplot_titles=[\"Pupil Diameter Comparison\", \"Cross-Correlation Analysis\"],\n",
    "    #     vertical_spacing=0.15\n",
    "    # )\n",
    "\n",
    "    # # Add SleapVideoData1 pupil diameter\n",
    "    # fig.add_trace(\n",
    "    #     go.Scatter(\n",
    "    #         x=SleapVideoData1['Seconds'],\n",
    "    #         y=SleapVideoData1['Ellipse.Diameter'],\n",
    "    #         mode='lines',\n",
    "    #         name=f\"VideoData1 Pupil Diameter\",\n",
    "    #         line=dict(color='blue')\n",
    "    #     ),\n",
    "    #     row=1, col=1\n",
    "    # )\n",
    "\n",
    "    # # Add SleapVideoData2 pupil diameter\n",
    "    # fig.add_trace(\n",
    "    #     go.Scatter(\n",
    "    #         x=SleapVideoData2['Seconds'],\n",
    "    #         y=SleapVideoData2['Ellipse.Diameter'],\n",
    "    #         mode='lines',\n",
    "    #         name=f\"VideoData2 Pupil Diameter\",\n",
    "    #         line=dict(color='red')\n",
    "    #     ),\n",
    "    #     row=1, col=1\n",
    "    # )\n",
    "\n",
    "    # Cross-correlation analysis\n",
    "    print(\"=== Cross-Correlation Analysis ===\")\n",
    "\n",
    "    # Get pupil diameter data\n",
    "    # Use filtered diameter data (with NaN restored at blink positions)\n",
    "    pupil1 = SleapVideoData1['Ellipse.Diameter.Filt'].values\n",
    "    pupil2 = SleapVideoData2['Ellipse.Diameter.Filt'].values\n",
    "\n",
    "    # Handle different lengths by using the shorter dataset length\n",
    "    min_length = min(len(pupil1), len(pupil2))\n",
    "\n",
    "    # Truncate both datasets to the same length (preserving time alignment)\n",
    "    pupil1_truncated = pupil1[:min_length]\n",
    "    pupil2_truncated = pupil2[:min_length]\n",
    "\n",
    "    # Remove NaN values for correlation - preserve time alignment by only keeping pairs where BOTH are valid\n",
    "    # This ensures cross-correlation is computed on temporally aligned data\n",
    "    valid_mask1 = ~np.isnan(pupil1_truncated)\n",
    "    valid_mask2 = ~np.isnan(pupil2_truncated)\n",
    "    valid_mask = valid_mask1 & valid_mask2  # Only use indices where both arrays have valid data\n",
    "\n",
    "    # Extract aligned pairs (preserves temporal alignment)\n",
    "    pupil1_clean = pupil1_truncated[valid_mask]\n",
    "    pupil2_clean = pupil2_truncated[valid_mask]\n",
    "\n",
    "    # Check if we have enough data\n",
    "    if len(pupil1_clean) < 2 or len(pupil2_clean) < 2:\n",
    "        print(\"‚ùå Error: Not enough valid data points for correlation analysis\")\n",
    "    else:\n",
    "        # Z-score normalize both signals before cross-correlation\n",
    "        # This accounts for different camera magnifications/orientations by comparing relative changes\n",
    "        # Formula: z = (x - mean) / std\n",
    "        pupil1_mean = np.mean(pupil1_clean)\n",
    "        pupil1_std = np.std(pupil1_clean)\n",
    "        pupil2_mean = np.mean(pupil2_clean)\n",
    "        pupil2_std = np.std(pupil2_clean)\n",
    "        \n",
    "        if pupil1_std > 0 and pupil2_std > 0:\n",
    "            pupil1_z = (pupil1_clean - pupil1_mean) / pupil1_std\n",
    "            pupil2_z = (pupil2_clean - pupil2_mean) / pupil2_std\n",
    "            print(f\"Applied z-score normalization to pupil diameter signals (accounts for different camera magnifications)\")\n",
    "            print(f\"  VideoData1: mean={pupil1_mean:.2f}, std={pupil1_std:.2f}\")\n",
    "            print(f\"  VideoData2: mean={pupil2_mean:.2f}, std={pupil2_std:.2f}\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Warning: Zero variance detected, using raw signals (no normalization)\")\n",
    "            pupil1_z = pupil1_clean\n",
    "            pupil2_z = pupil2_clean\n",
    "        \n",
    "        # Calculate cross-correlation using z-scored signals\n",
    "        try:\n",
    "            correlation = correlate(pupil1_z, pupil2_z, mode='full')\n",
    "            \n",
    "            # Calculate lags (in samples)\n",
    "            lags = np.arange(-len(pupil2_z) + 1, len(pupil1_z))\n",
    "            \n",
    "            # Convert lags to time (assuming same sampling rate)\n",
    "            dt = np.median(np.diff(SleapVideoData1['Seconds']))\n",
    "            lag_times = lags * dt\n",
    "            \n",
    "            # Find peak correlation and corresponding lag\n",
    "            peak_idx = np.argmax(correlation)\n",
    "            peak_correlation = correlation[peak_idx]\n",
    "            peak_lag_samples = lags[peak_idx]\n",
    "            peak_lag_time = lag_times[peak_idx]\n",
    "            peak_lag_time_display = peak_lag_time # for final QC figure \n",
    "            \n",
    "            print(f\"Peak lag (time): {peak_lag_time:.4f} seconds\")\n",
    "\n",
    "        \n",
    "            # Normalize correlation to [-1, 1] range (for z-scored signals, this is standard normalization)\n",
    "            norm_factor = np.sqrt(np.sum(pupil1_z**2) * np.sum(pupil2_z**2))\n",
    "            if norm_factor > 0:\n",
    "                correlation_normalized = correlation / norm_factor\n",
    "                peak_correlation_normalized = correlation_normalized[peak_idx]\n",
    "                print(f\"Peak normalized correlation: {peak_correlation_normalized:.4f}\")\n",
    "            else:\n",
    "                print(\"‚ùå Error: Cannot normalize correlation (zero variance)\")\n",
    "                correlation_normalized = correlation\n",
    "                peak_correlation_normalized = 0\n",
    "            \n",
    "            # # Plot cross-correlation\n",
    "            # fig.add_trace(\n",
    "            #     go.Scatter(\n",
    "            #         x=lag_times,\n",
    "            #         y=correlation_normalized,\n",
    "            #         mode='lines',\n",
    "            #         name=\"Cross-Correlation\",\n",
    "            #         line=dict(color='green')\n",
    "            #     ),\n",
    "            #     row=2, col=1\n",
    "            # )\n",
    "            \n",
    "            # # Add vertical line at peak\n",
    "            # fig.add_vline(\n",
    "            #     x=peak_lag_time,\n",
    "            #     line_dash=\"dash\",\n",
    "            #     line_color=\"red\",\n",
    "            #     annotation_text=f\"Peak: {peak_correlation_normalized:.3f}\",\n",
    "            #     row=2, col=1\n",
    "            # )\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error in cross-correlation calculation: {e}\")\n",
    "            # # Add empty trace to maintain plot structure\n",
    "            # fig.add_trace(\n",
    "            #     go.Scatter(\n",
    "            #         x=[0], y=[0],\n",
    "            #         mode='lines',\n",
    "            #         name=\"Cross-Correlation (Error)\",\n",
    "            #         line=dict(color='gray')\n",
    "            #     ),\n",
    "            #     row=2, col=1\n",
    "            # )\n",
    "\n",
    "    # # Update axes labels\n",
    "    # fig.update_xaxes(title_text=\"Time (seconds)\", row=1, col=1)\n",
    "    # fig.update_yaxes(title_text=\"Pupil Diameter\", row=1, col=1)\n",
    "    # fig.update_xaxes(title_text=\"Lag (seconds)\", row=2, col=1)\n",
    "    # fig.update_yaxes(title_text=\"Normalized Correlation\", row=2, col=1)\n",
    "\n",
    "    # fig.update_layout(\n",
    "    #     height=800,\n",
    "    #     width=1000,\n",
    "    #     title_text=f\"SLEAP Pupil Diameter Analysis: Comparison & Cross-Correlation\"\n",
    "    # )\n",
    "\n",
    "    # fig.show()\n",
    "\n",
    "    # Additional correlation statistics\n",
    "    if len(pupil1_clean) >= 2 and len(pupil2_clean) >= 2:\n",
    "        try:\n",
    "            # Calculate Pearson correlation coefficient on z-scored signals\n",
    "            # Note: For z-scored signals, Pearson correlation is equivalent to the normalized cross-correlation at zero lag\n",
    "            pearson_r, pearson_p = pearsonr(pupil1_z, pupil2_z)\n",
    "            pearson_r_display = pearson_r\n",
    "            pearson_p_display = pearson_p\n",
    "            \n",
    "            print(f\"\\n=== Additional Statistics ===\")\n",
    "            print(f\"Pearson correlation coefficient: {pearson_r:.2f}\")\n",
    "\n",
    "            # Handle extremely small p-values\n",
    "            if pearson_p < 1e-300:\n",
    "                print(f'Pearson p-value: < 1e-300 (extremely significant)')\n",
    "            else:\n",
    "                print(f'Pearson p-value: {pearson_p:.5e}')\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error in additional statistics: {e}\")\n",
    "            pearson_r_display = None\n",
    "            pearson_p_display = None\n",
    "    else:\n",
    "        print(\"‚ùå Cannot calculate additional statistics - insufficient data\")\n",
    "else:\n",
    "    print(\"Only one eye is present, no pupil diameter cross-correlation can be done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929ce1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if Second values match 1:1 between VideoData and SleapVideoData then merge them into VideoData\n",
    "############################################################################################################\n",
    "\n",
    "if VideoData1_Has_Sleap is True:\n",
    "    if VideoData1['Seconds'].equals(SleapVideoData1['Seconds']) is False:\n",
    "        print(f\"‚ùó {get_eye_label('VideoData1')}: The 'Seconds' columns DO NOT correspond 1:1 between the two DataFrames. This should not happen\")\n",
    "    else:\n",
    "        VideoData1 = VideoData1.merge(SleapVideoData1, on='Seconds', how='outer')\n",
    "        del SleapVideoData1\n",
    "\n",
    "if VideoData2_Has_Sleap is True:\n",
    "    if VideoData2['Seconds'].equals(SleapVideoData2['Seconds']) is False:\n",
    "        print(f\"‚ùó {get_eye_label('VideoData2')}: The 'Seconds' columns DO NOT correspond 1:1 between the two DataFrames. This should not happen\")\n",
    "    else:\n",
    "        VideoData2 = VideoData2.merge(SleapVideoData2, on='Seconds', how='outer')\n",
    "        del SleapVideoData2\n",
    "gc.collect()\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a444300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare SLEAP center.x and .y with fitted ellipse centre distributions for both VideoData1 and VideoData2\n",
    "############################################################################################################\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1) Compute correlations for VideoData1\n",
    "# ------------------------------------------------------------------\n",
    "if VideoData1_Has_Sleap is True:\n",
    "    print(f\"=== VideoData1 Analysis ===\")\n",
    "    slope_x1, intercept_x1, r_value_x1, p_value_x1, std_err_x1 = linregress(\n",
    "        VideoData1[\"Ellipse.Center.X\"], \n",
    "        VideoData1[\"center.x\"]\n",
    "    )\n",
    "    r_squared_x1 = r_value_x1**2\n",
    "    print(f\"{get_eye_label('VideoData1')} - R^2 between center point and ellipse center X data: {r_squared_x1:.4f}\")\n",
    "\n",
    "    slope_y1, intercept_y1, r_value_y1, p_value_y1, std_err_y1 = linregress(\n",
    "        VideoData1[\"Ellipse.Center.Y\"], \n",
    "        VideoData1[\"center.y\"]\n",
    "    )\n",
    "    r_squared_y1 = r_value_y1**2\n",
    "    print(f\"{get_eye_label('VideoData1')} - R^2 between center point and ellipse center Y data: {r_squared_y1:.4f}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2) Compute correlations for VideoData2\n",
    "# ------------------------------------------------------------------\n",
    "if VideoData2_Has_Sleap is True:\n",
    "    print(f\"\\n=== VideoData2 Analysis ===\")\n",
    "    slope_x2, intercept_x2, r_value_x2, p_value_x2, std_err_x2 = linregress(\n",
    "        VideoData2[\"Ellipse.Center.X\"], \n",
    "        VideoData2[\"center.x\"]\n",
    "    )\n",
    "    r_squared_x2 = r_value_x2**2\n",
    "    print(f\"{get_eye_label('VideoData2')} - R^2 between center point and ellipse center X data: {r_squared_x2:.4f}\")\n",
    "\n",
    "    slope_y2, intercept_y2, r_value_y2, p_value_y2, std_err_y2 = linregress(\n",
    "        VideoData2[\"Ellipse.Center.Y\"], \n",
    "        VideoData2[\"center.y\"]\n",
    "    )\n",
    "    r_squared_y2 = r_value_y2**2\n",
    "    print(f\"{get_eye_label('VideoData2')} - R^2 between center point and ellipse center Y data: {r_squared_y2:.4f}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3) Center of Mass Analysis (if both VideoData1 and VideoData2 are available)\n",
    "# ------------------------------------------------------------------\n",
    "if VideoData1_Has_Sleap is True and VideoData2_Has_Sleap is True:\n",
    "    print(f\"\\n=== Center of Mass Distance Analysis ===\")\n",
    "    \n",
    "    # Calculate center of mass (mean) for VideoData1\n",
    "    com_center_x1 = VideoData1[\"center.x\"].mean()\n",
    "    com_center_y1 = VideoData1[\"center.y\"].mean()\n",
    "    com_ellipse_x1 = VideoData1[\"Ellipse.Center.X\"].mean()\n",
    "    com_ellipse_y1 = VideoData1[\"Ellipse.Center.Y\"].mean()\n",
    "    \n",
    "    # Calculate absolute distances for VideoData1\n",
    "    dist_x1 = abs(com_center_x1 - com_ellipse_x1)\n",
    "    dist_y1 = abs(com_center_y1 - com_ellipse_y1)\n",
    "    \n",
    "    print(f\"\\n{get_eye_label('VideoData1')}:\")\n",
    "    print(f\"  Center of mass for center.x/y: ({com_center_x1:.4f}, {com_center_y1:.4f})\")\n",
    "    print(f\"  Center of mass for Ellipse.Center.X/Y: ({com_ellipse_x1:.4f}, {com_ellipse_y1:.4f})\")\n",
    "    print(f\"  Absolute distance in X: {dist_x1:.4f} pixels\")\n",
    "    print(f\"  Absolute distance in Y: {dist_y1:.4f} pixels\")\n",
    "    \n",
    "    # Calculate center of mass (mean) for VideoData2\n",
    "    com_center_x2 = VideoData2[\"center.x\"].mean()\n",
    "    com_center_y2 = VideoData2[\"center.y\"].mean()\n",
    "    com_ellipse_x2 = VideoData2[\"Ellipse.Center.X\"].mean()\n",
    "    com_ellipse_y2 = VideoData2[\"Ellipse.Center.Y\"].mean()\n",
    "    \n",
    "    # Calculate absolute distances for VideoData2\n",
    "    dist_x2 = abs(com_center_x2 - com_ellipse_x2)\n",
    "    dist_y2 = abs(com_center_y2 - com_ellipse_y2)\n",
    "    \n",
    "    print(f\"\\n{get_eye_label('VideoData2')}:\")\n",
    "    print(f\"  Center of mass for center.x/y: ({com_center_x2:.4f}, {com_center_y2:.4f})\")\n",
    "    print(f\"  Center of mass for Ellipse.Center.X/Y: ({com_ellipse_x2:.4f}, {com_ellipse_y2:.4f})\")\n",
    "    print(f\"  Absolute distance in X: {dist_x2:.4f} pixels\")\n",
    "    print(f\"  Absolute distance in Y: {dist_y2:.4f} pixels\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4) Re-center Ellipse.Center.X and Ellipse.Center.Y using median\n",
    "# ------------------------------------------------------------------\n",
    "print(\"\\n=== Re-centering Ellipse.Center coordinates ===\")\n",
    "\n",
    "# Re-center VideoData1 Ellipse.Center coordinates\n",
    "if VideoData1_Has_Sleap is True:\n",
    "    # Calculate median\n",
    "    median_ellipse_x1 = VideoData1[\"Ellipse.Center.X\"].median()\n",
    "    median_ellipse_y1 = VideoData1[\"Ellipse.Center.Y\"].median()\n",
    "    \n",
    "    # Center the coordinates\n",
    "    VideoData1[\"Ellipse.Center.X\"] = VideoData1[\"Ellipse.Center.X\"] - median_ellipse_x1\n",
    "    VideoData1[\"Ellipse.Center.Y\"] = VideoData1[\"Ellipse.Center.Y\"] - median_ellipse_y1\n",
    "    \n",
    "    print(f\"{get_eye_label('VideoData1')} - Re-centered Ellipse.Center using median: ({median_ellipse_x1:.4f}, {median_ellipse_y1:.4f})\")\n",
    "\n",
    "# Re-center VideoData2 Ellipse.Center coordinates\n",
    "if VideoData2_Has_Sleap is True:\n",
    "    # Calculate median\n",
    "    median_ellipse_x2 = VideoData2[\"Ellipse.Center.X\"].median()\n",
    "    median_ellipse_y2 = VideoData2[\"Ellipse.Center.Y\"].median()\n",
    "    \n",
    "    # Center the coordinates\n",
    "    VideoData2[\"Ellipse.Center.X\"] = VideoData2[\"Ellipse.Center.X\"] - median_ellipse_x2\n",
    "    VideoData2[\"Ellipse.Center.Y\"] = VideoData2[\"Ellipse.Center.Y\"] - median_ellipse_y2\n",
    "    \n",
    "    print(f\"{get_eye_label('VideoData2')} - Re-centered Ellipse.Center using median: ({median_ellipse_x2:.4f}, {median_ellipse_y2:.4f})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ff1475",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Make and save summary QC plot using matplotlib with scatter plots for 2D distributions\n",
    "\n",
    "# Initialize the statistics variables (these are calculated in Cell 11)\n",
    "try:\n",
    "    pearson_r_display\n",
    "except NameError:\n",
    "    pearson_r_display = None\n",
    "    pearson_p_display = None\n",
    "    peak_lag_time_display = None\n",
    "    print(\"‚ö†Ô∏è Note: Statistics not found. They should be calculated in Cell 11.\")\n",
    "\n",
    "# Calculate correlation for Ellipse.Center.X between VideoData1 and VideoData2 (if both exist)\n",
    "pearson_r_center = None\n",
    "pearson_p_center = None\n",
    "peak_lag_time_center = None\n",
    "\n",
    "if VideoData1_Has_Sleap and VideoData2_Has_Sleap:\n",
    "    # Get the Center.X data\n",
    "    center_x1 = VideoData1['Ellipse.Center.X'].values\n",
    "    center_x2 = VideoData2['Ellipse.Center.X'].values\n",
    "    \n",
    "    min_length = min(len(center_x1), len(center_x2))\n",
    "    center_x1_truncated = center_x1[:min_length]\n",
    "    center_x2_truncated = center_x2[:min_length]\n",
    "    \n",
    "    valid_mask1 = ~np.isnan(center_x1_truncated)\n",
    "    valid_mask2 = ~np.isnan(center_x2_truncated)\n",
    "    valid_mask = valid_mask1 & valid_mask2\n",
    "    \n",
    "    center_x1_clean = center_x1_truncated[valid_mask]\n",
    "    center_x2_clean = center_x2_truncated[valid_mask]\n",
    "    \n",
    "    if len(center_x1_clean) >= 2 and len(center_x2_clean) >= 2:\n",
    "        try:\n",
    "            # Calculate Pearson correlation\n",
    "            pearson_r_center, pearson_p_center = pearsonr(center_x1_clean, center_x2_clean)\n",
    "            \n",
    "            # Calculate cross-correlation for peak lag\n",
    "            correlation = correlate(center_x1_clean, center_x2_clean, mode='full')\n",
    "            lags = np.arange(-len(center_x2_clean) + 1, len(center_x1_clean))\n",
    "            dt = np.median(np.diff(VideoData1['Seconds']))\n",
    "            lag_times = lags * dt\n",
    "            peak_idx = np.argmax(correlation)\n",
    "            peak_lag_time_center = lag_times[peak_idx]\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error calculating Ellipse.Center.X correlation stats: {e}\")\n",
    "\n",
    "# Create the QC summary figure using matplotlib with custom grid layout\n",
    "fig = plt.figure(figsize=(20, 18))\n",
    "fig.suptitle(str(data_path), fontsize=16, y=0.995)\n",
    "\n",
    "# Create a grid layout:\n",
    "# - Top row (full width): VideoData1 Time Series\n",
    "# - Second row (full width): VideoData2 Time Series  \n",
    "# - Third row (two columns): 2D scatter plots (VideoData1 left, VideoData2 right)\n",
    "# - Fourth row (two columns): Pupil diameter (left), Ellipse.Center.X correlation (right)\n",
    "\n",
    "gs = fig.add_gridspec(4, 2, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# Panel 1: VideoData1 center coordinates - Time Series (full width)\n",
    "if VideoData1_Has_Sleap:\n",
    "    ax1 = fig.add_subplot(gs[0, :])\n",
    "    ax1.plot(VideoData1_centered['Seconds'], VideoData1_centered['center.x'],\n",
    "            linewidth=0.5, c='blue', alpha=0.6, label='center.x original')\n",
    "    ax1.plot(VideoData1['Seconds'], VideoData1['Ellipse.Center.X'],\n",
    "            linewidth=0.5, c='red', alpha=0.6, label='Ellipse Center.X')\n",
    "    ax1.set_xlabel('Time (s)')\n",
    "    ax1.set_ylabel('Position (pixels)')\n",
    "    ax1.set_title(f\"{get_eye_label('VideoData1')} - center.X Time Series\")\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Panel 2: VideoData2 center coordinates - Time Series (full width)\n",
    "if VideoData2_Has_Sleap:\n",
    "    ax2 = fig.add_subplot(gs[1, :])\n",
    "    ax2.plot(VideoData2_centered['Seconds'], VideoData2_centered['center.x'],\n",
    "            linewidth=0.5, c='blue', alpha=0.6, label='center.x original')\n",
    "    ax2.plot(VideoData2['Seconds'], VideoData2['Ellipse.Center.X'],\n",
    "            linewidth=0.5, c='red', alpha=0.6, label='Ellipse Center.X')\n",
    "    ax2.set_xlabel('Time (s)')\n",
    "    ax2.set_ylabel('Position (pixels)')\n",
    "    ax2.set_title(f\"{get_eye_label('VideoData2')} - center.X Time Series\")\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Panel 3: VideoData1 center coordinates - Scatter plot (left half)\n",
    "if VideoData1_Has_Sleap:\n",
    "    ax3 = fig.add_subplot(gs[2, 0])\n",
    "    \n",
    "    # Ellipse.Center (blue)\n",
    "    x_ellipse1 = VideoData1['Ellipse.Center.X'].to_numpy()\n",
    "    y_ellipse1 = VideoData1['Ellipse.Center.Y'].to_numpy()\n",
    "    mask1 = ~(np.isnan(x_ellipse1) | np.isnan(y_ellipse1))\n",
    "    \n",
    "    ax3.scatter(x_ellipse1[mask1], y_ellipse1[mask1],\n",
    "               s=1, alpha=0.3, c='blue', label='Ellipse.Center')\n",
    "    \n",
    "    # Center (red) - from centered data\n",
    "    x_center1 = VideoData1_centered['center.x'].to_numpy()\n",
    "    y_center1 = VideoData1_centered['center.y'].to_numpy()\n",
    "    mask2 = ~(np.isnan(x_center1) | np.isnan(y_center1))\n",
    "    \n",
    "    ax3.scatter(x_center1[mask2], y_center1[mask2],\n",
    "               s=1, alpha=0.3, c='red', label='center.x original')\n",
    "    \n",
    "    ax3.set_xlabel('Center X (pixels)')\n",
    "    ax3.set_ylabel('Center Y (pixels)')\n",
    "    ax3.set_title(f\"{get_eye_label('VideoData1')} - Center X-Y Distribution (center.X vs Ellipse)\")\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add R¬≤ statistics for VideoData1 (bottom left)\n",
    "    try:\n",
    "        if 'r_squared_x1' in globals() and 'r_squared_y1' in globals():\n",
    "            stats_text = f'R¬≤ X: {r_squared_x1:.2g}\\nR¬≤ Y: {r_squared_y1:.2g}'\n",
    "            ax3.text(0.02, 0.02, stats_text, transform=ax3.transAxes,\n",
    "                    verticalalignment='bottom', horizontalalignment='left',\n",
    "                    bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8),\n",
    "                    fontsize=9, family='monospace')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Add center of mass distance for VideoData1 (bottom right)\n",
    "    try:\n",
    "        if 'dist_x1' in globals() and 'dist_y1' in globals():\n",
    "            distance_text = f'COM Dist X: {dist_x1:.3g}\\nCOM Dist Y: {dist_y1:.3g}'\n",
    "            ax3.text(0.98, 0.02, distance_text, transform=ax3.transAxes,\n",
    "                    verticalalignment='bottom', horizontalalignment='right',\n",
    "                    bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8),\n",
    "                    fontsize=9, family='monospace')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Panel 4: VideoData2 center coordinates - Scatter plot (right half)\n",
    "if VideoData2_Has_Sleap:\n",
    "    ax4 = fig.add_subplot(gs[2, 1])\n",
    "    \n",
    "    # Ellipse.Center (blue)\n",
    "    x_ellipse2 = VideoData2['Ellipse.Center.X'].to_numpy()\n",
    "    y_ellipse2 = VideoData2['Ellipse.Center.Y'].to_numpy()\n",
    "    mask3 = ~(np.isnan(x_ellipse2) | np.isnan(y_ellipse2))\n",
    "    \n",
    "    ax4.scatter(x_ellipse2[mask3], y_ellipse2[mask3],\n",
    "               s=1, alpha=0.3, c='blue', label='Ellipse.Center')\n",
    "    \n",
    "    # Center (red) - from centered data\n",
    "    x_center2 = VideoData2_centered['center.x'].to_numpy()\n",
    "    y_center2 = VideoData2_centered['center.y'].to_numpy()\n",
    "    mask4 = ~(np.isnan(x_center2) | np.isnan(y_center2))\n",
    "    \n",
    "    ax4.scatter(x_center2[mask4], y_center2[mask4],\n",
    "               s=1, alpha=0.3, c='red', label='center.X Center')\n",
    "    \n",
    "    ax4.set_xlabel('Center X (pixels)')\n",
    "    ax4.set_ylabel('Center Y (pixels)')\n",
    "    ax4.set_title(f\"{get_eye_label('VideoData2')} - Center X-Y Distribution (center.X vs Ellipse)\")\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add R¬≤ statistics for VideoData2 (bottom left)\n",
    "    try:\n",
    "        if 'r_squared_x2' in globals() and 'r_squared_y2' in globals():\n",
    "            stats_text = f'R¬≤ X: {r_squared_x2:.2g}\\nR¬≤ Y: {r_squared_y2:.2g}'\n",
    "            ax4.text(0.02, 0.02, stats_text, transform=ax4.transAxes,\n",
    "                    verticalalignment='bottom', horizontalalignment='left',\n",
    "                    bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8),\n",
    "                    fontsize=9, family='monospace')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Add center of mass distance for VideoData2 (bottom right)\n",
    "    try:\n",
    "        if 'dist_x2' in globals() and 'dist_y2' in globals():\n",
    "            distance_text = f'COM Dist X: {dist_x2:.3g}\\nCOM Dist Y: {dist_y2:.3g}'\n",
    "            ax4.text(0.98, 0.02, distance_text, transform=ax4.transAxes,\n",
    "                    verticalalignment='bottom', horizontalalignment='right',\n",
    "                    bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8),\n",
    "                    fontsize=9, family='monospace')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Panel 5: Pupil diameter comparison (bottom left)\n",
    "ax5 = fig.add_subplot(gs[3, 0])\n",
    "if VideoData1_Has_Sleap and VideoData2_Has_Sleap:\n",
    "    ax5.plot(VideoData1['Seconds'], VideoData1['Ellipse.Diameter.Filt'],\n",
    "            linewidth=0.5, c='#FF7F00', alpha=0.6, label='VideoData1 Diameter')\n",
    "    ax5.plot(VideoData2['Seconds'], VideoData2['Ellipse.Diameter.Filt'],\n",
    "            linewidth=0.5, c='#9370DB', alpha=0.6, label='VideoData2 Diameter')\n",
    "elif VideoData1_Has_Sleap:\n",
    "    ax5.plot(VideoData1['Seconds'], VideoData1['Ellipse.Diameter.Filt'],\n",
    "            linewidth=0.5, c='#FF7F00', alpha=0.6, label='VideoData1 Diameter')\n",
    "elif VideoData2_Has_Sleap:\n",
    "    ax5.plot(VideoData2['Seconds'], VideoData2['Ellipse.Diameter.Filt'],\n",
    "            linewidth=0.5, c='#9370DB', alpha=0.6, label='VideoData2 Diameter')\n",
    "\n",
    "ax5.set_xlabel('Time (s)')\n",
    "ax5.set_ylabel('Diameter (pixels)')\n",
    "ax5.set_title('Pupil Diameter Comparison')\n",
    "ax5.legend()\n",
    "ax5.grid(True, alpha=0.3)\n",
    "\n",
    "# Add statistics text to Panel 5\n",
    "if pearson_r_display is not None and pearson_p_display is not None and peak_lag_time_display is not None:\n",
    "    stats_text = (f'Pearson r = {pearson_r_display:.4f}\\n'\n",
    "                  f'Pearson p = {pearson_p_display:.4e}\\n'\n",
    "                  f'Peak lag = {peak_lag_time_display:.4f} s')\n",
    "    ax5.text(0.98, 0.98, stats_text, transform=ax5.transAxes,\n",
    "            verticalalignment='top', horizontalalignment='right',\n",
    "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8),\n",
    "            fontsize=10, family='monospace')\n",
    "else:\n",
    "    ax5.text(0.5, 0.5, 'Statistics not available\\n(See Cell 11 for correlation analysis)', \n",
    "            transform=ax5.transAxes, ha='center', va='center', fontsize=10)\n",
    "\n",
    "# Panel 6: Ellipse.Center.X comparison (bottom right) with dual y-axis\n",
    "ax6 = fig.add_subplot(gs[3, 1])\n",
    "ax6_twin = ax6.twinx()  # Create a second y-axis\n",
    "\n",
    "if VideoData1_Has_Sleap and VideoData2_Has_Sleap:\n",
    "    # Plot the individual traces\n",
    "    ax6.plot(VideoData1['Seconds'], VideoData1['Ellipse.Center.X'],\n",
    "            linewidth=0.5, c='#FF7F00', alpha=0.6, label='VideoData1 Ellipse.Center.X')\n",
    "    ax6.plot(VideoData2['Seconds'], VideoData2['Ellipse.Center.X'],\n",
    "            linewidth=0.5, c='#9370DB', alpha=0.6, label='VideoData2 Ellipse.Center.X')\n",
    "    \n",
    "    # Plot the difference on the right axis\n",
    "    # Align the data to the same length and normalize for fair comparison\n",
    "    min_length = min(len(VideoData1), len(VideoData2))\n",
    "    \n",
    "    # Normalize data (z-score) to account for different scales\n",
    "    center_x1_aligned = VideoData1['Ellipse.Center.X'].iloc[:min_length]\n",
    "    center_x2_aligned = VideoData2['Ellipse.Center.X'].iloc[:min_length]\n",
    "    \n",
    "    # Calculate mean and std for normalization\n",
    "    mean1 = center_x1_aligned.mean()\n",
    "    std1 = center_x1_aligned.std()\n",
    "    mean2 = center_x2_aligned.mean()\n",
    "    std2 = center_x2_aligned.std()\n",
    "    \n",
    "    # Normalize both datasets\n",
    "    center_x1_norm = (center_x1_aligned - mean1) / std1\n",
    "    center_x2_norm = (center_x2_aligned - mean2) / std2\n",
    "    \n",
    "    # Calculate difference of normalized data\n",
    "    center_x_diff = center_x1_norm - center_x2_norm\n",
    "    seconds_aligned = VideoData1['Seconds'].iloc[:min_length]\n",
    "    ax6_twin.plot(seconds_aligned, center_x_diff,\n",
    "                  linewidth=0.5, c='green', alpha=0.6, label='Difference (normalized)')\n",
    "    \n",
    "elif VideoData1_Has_Sleap:\n",
    "    ax6.plot(VideoData1['Seconds'], VideoData1['Ellipse.Center.X'],\n",
    "            linewidth=0.5, c='#FF7F00', alpha=0.6, label='VideoData1 Ellipse.Center.X')\n",
    "elif VideoData2_Has_Sleap:\n",
    "    ax6.plot(VideoData2['Seconds'], VideoData2['Ellipse.Center.X'],\n",
    "            linewidth=0.5, c='#9370DB', alpha=0.6, label='VideoData2 Ellipse.Center.X')\n",
    "\n",
    "ax6.set_xlabel('Time (s)')\n",
    "ax6.set_ylabel('Center X (pixels)', color='black')\n",
    "ax6.set_title('Ellipse.Center.X Comparison')\n",
    "ax6.tick_params(axis='y', labelcolor='black')\n",
    "if VideoData1_Has_Sleap and VideoData2_Has_Sleap:\n",
    "    ax6_twin.set_ylabel('Normalized Difference (z-score)', color='green')\n",
    "    ax6_twin.tick_params(axis='y', labelcolor='green')\n",
    "\n",
    "# Combine legends from both axes\n",
    "lines1, labels1 = ax6.get_legend_handles_labels()\n",
    "if VideoData1_Has_Sleap and VideoData2_Has_Sleap:\n",
    "    lines2, labels2 = ax6_twin.get_legend_handles_labels()\n",
    "    ax6.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "else:\n",
    "    ax6.legend(loc='upper left')\n",
    "\n",
    "ax6.grid(True, alpha=0.3)\n",
    "\n",
    "# Add statistics text to Panel 6\n",
    "if pearson_r_center is not None and pearson_p_center is not None and peak_lag_time_center is not None:\n",
    "    stats_text = (f'Pearson r = {pearson_r_center:.4f}\\n'\n",
    "                  f'Pearson p = {pearson_p_center:.4e}\\n'\n",
    "                  f'Peak lag = {peak_lag_time_center:.4f} s')\n",
    "    ax6.text(0.98, 0.98, stats_text, transform=ax6.transAxes,\n",
    "            verticalalignment='top', horizontalalignment='right',\n",
    "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8),\n",
    "            fontsize=10, family='monospace')\n",
    "else:\n",
    "    ax6.text(0.5, 0.5, 'Statistics not available\\n(both eyes required)', \n",
    "            transform=ax6.transAxes, ha='center', va='center', fontsize=10)\n",
    "\n",
    "# Save as PDF (editable vector format)\n",
    "save_path.mkdir(parents=True, exist_ok=True)\n",
    "pdf_path = save_path / \"Eye_data_QC.pdf\"\n",
    "plt.savefig(pdf_path, dpi=300, bbox_inches='tight', format='pdf')\n",
    "print(f\"‚úÖ QC figure saved as PDF (editable): {pdf_path}\")\n",
    "\n",
    "# Also save as 600 dpi PNG (high-resolution for printing)\n",
    "png_path = save_path / \"Eye_data_QC.png\"\n",
    "plt.savefig(png_path, dpi=600, bbox_inches='tight', format='png')\n",
    "print(f\"‚úÖ QC figure saved as PNG (600 dpi for printing): {png_path}\")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1de938",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Create interactive time series plots using plotly for browser viewing\n",
    "if plot_QC_timeseries:\n",
    "    # Create subplots for the time series (3 rows now instead of 2)\n",
    "    # Need to enable secondary_y for the third panel\n",
    "    fig = make_subplots(\n",
    "        rows=3, cols=1,\n",
    "        shared_xaxes=True,\n",
    "        vertical_spacing=0.08,\n",
    "        subplot_titles=(\n",
    "            f\"{get_eye_label('VideoData1')} - center.X Time Series\",\n",
    "            f\"{get_eye_label('VideoData2')} - center.X Time Series\",\n",
    "            \"Ellipse.Center.X Comparison with Difference\"\n",
    "        ),\n",
    "        specs=[[{}], [{}], [{\"secondary_y\": True}]]  # Enable secondary_y for row 3\n",
    "    )\n",
    "\n",
    "    # Panel 1: VideoData1 center coordinates - Time Series\n",
    "    if VideoData1_Has_Sleap:\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=VideoData1_centered['Seconds'],\n",
    "            y=VideoData1_centered['center.x'],\n",
    "            mode='lines',\n",
    "            name='center.x original',\n",
    "            line=dict(color='blue', width=0.5),\n",
    "            opacity=0.6\n",
    "        ), row=1, col=1)\n",
    "        \n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=VideoData1['Seconds'],\n",
    "            y=VideoData1['Ellipse.Center.X'],\n",
    "            mode='lines',\n",
    "            name='Ellipse Center.X',\n",
    "            line=dict(color='red', width=0.5),\n",
    "            opacity=0.6\n",
    "        ), row=1, col=1)\n",
    "\n",
    "    # Panel 2: VideoData2 center coordinates - Time Series\n",
    "    if VideoData2_Has_Sleap:\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=VideoData2_centered['Seconds'],\n",
    "            y=VideoData2_centered['center.x'],\n",
    "            mode='lines',\n",
    "            name='center.x original',\n",
    "            line=dict(color='blue', width=0.5),\n",
    "            opacity=0.6\n",
    "        ), row=2, col=1)\n",
    "        \n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=VideoData2['Seconds'],\n",
    "            y=VideoData2['Ellipse.Center.X'],\n",
    "            mode='lines',\n",
    "            name='Ellipse Center.X',\n",
    "            line=dict(color='red', width=0.5),\n",
    "            opacity=0.6\n",
    "        ), row=2, col=1)\n",
    "\n",
    "    # Panel 3: Ellipse.Center.X Comparison with difference\n",
    "    if VideoData1_Has_Sleap and VideoData2_Has_Sleap:\n",
    "        # Plot the individual traces\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=VideoData1['Seconds'],\n",
    "            y=VideoData1['Ellipse.Center.X'],\n",
    "            mode='lines',\n",
    "            name='VideoData1 Ellipse.Center.X',\n",
    "            line=dict(color='#FF7F00', width=0.5),  # Orange\n",
    "            opacity=0.6\n",
    "        ), row=3, col=1)\n",
    "        \n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=VideoData2['Seconds'],\n",
    "            y=VideoData2['Ellipse.Center.X'],\n",
    "            mode='lines',\n",
    "            name='VideoData2 Ellipse.Center.X',\n",
    "            line=dict(color='#9370DB', width=0.5),  # Purple\n",
    "            opacity=0.6\n",
    "        ), row=3, col=1)\n",
    "        \n",
    "        # Plot the difference on secondary y-axis\n",
    "        # Align the data to the same length and normalize for fair comparison\n",
    "        min_length = min(len(VideoData1), len(VideoData2))\n",
    "        \n",
    "        # Normalize data (z-score) to account for different scales\n",
    "        center_x1_aligned = VideoData1['Ellipse.Center.X'].iloc[:min_length]\n",
    "        center_x2_aligned = VideoData2['Ellipse.Center.X'].iloc[:min_length]\n",
    "        \n",
    "        # Calculate mean and std for normalization\n",
    "        mean1 = center_x1_aligned.mean()\n",
    "        std1 = center_x1_aligned.std()\n",
    "        mean2 = center_x2_aligned.mean()\n",
    "        std2 = center_x2_aligned.std()\n",
    "        \n",
    "        # Normalize both datasets\n",
    "        center_x1_norm = (center_x1_aligned - mean1) / std1\n",
    "        center_x2_norm = (center_x2_aligned - mean2) / std2\n",
    "        \n",
    "        # Calculate difference of normalized data\n",
    "        center_x_diff = center_x1_norm - center_x2_norm\n",
    "        seconds_aligned = VideoData1['Seconds'].iloc[:min_length]\n",
    "        \n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=seconds_aligned,\n",
    "            y=center_x_diff,\n",
    "            mode='lines',\n",
    "            name='Difference (normalized)',\n",
    "            line=dict(color='green', width=0.5),\n",
    "            opacity=0.6\n",
    "        ), row=3, col=1, secondary_y=True)\n",
    "        \n",
    "    elif VideoData1_Has_Sleap:\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=VideoData1['Seconds'],\n",
    "            y=VideoData1['Ellipse.Center.X'],\n",
    "            mode='lines',\n",
    "            name='VideoData1 Ellipse.Center.X',\n",
    "            line=dict(color='#FF7F00', width=0.5),\n",
    "            opacity=0.6\n",
    "        ), row=3, col=1)\n",
    "    elif VideoData2_Has_Sleap:\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=VideoData2['Seconds'],\n",
    "            y=VideoData2['Ellipse.Center.X'],\n",
    "            mode='lines',\n",
    "            name='VideoData2 Ellipse.Center.X',\n",
    "            line=dict(color='#9370DB', width=0.5),\n",
    "            opacity=0.6\n",
    "        ), row=3, col=1)\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        height=1200,  # Increased height for 3 panels\n",
    "        title_text=f'{data_path} - Eye Tracking Time Series QC',\n",
    "        showlegend=True,\n",
    "        hovermode='x unified'\n",
    "    )\n",
    "\n",
    "    # Update axes\n",
    "    fig.update_xaxes(title_text=\"Time (s)\", row=3, col=1)\n",
    "    fig.update_yaxes(title_text=\"Position (pixels)\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Position (pixels)\", row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"Center X (pixels)\", row=3, col=1)\n",
    "\n",
    "    # Update secondary y-axis for difference plot\n",
    "    if VideoData1_Has_Sleap and VideoData2_Has_Sleap:\n",
    "        fig.update_yaxes(title_text=\"Normalized Difference (z-score)\", row=3, col=1, secondary_y=True)\n",
    "\n",
    "    # Show in browser\n",
    "    fig.show(renderer='browser')\n",
    "\n",
    "    # Also save as HTML\n",
    "    save_path.mkdir(parents=True, exist_ok=True)\n",
    "    html_path = save_path / \"Eye_data_QC_time_series.html\"\n",
    "    fig.write_html(html_path)\n",
    "    print(f\"‚úÖ Interactive time series plot saved to: {html_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42a62f6",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# save as df to csv to be loaded in the photometry/harp/etc. analysis notebook \n",
    "############################################################################################################\n",
    "# reindex to aeon datetime to be done in the other notebook\n",
    " \n",
    "if VideoData1_Has_Sleap:\n",
    "    # Save  DataFrame as CSV to proper path and filename\n",
    "    save_path1 = save_path / \"Video_Sleap_Data1\" / \"Video_Sleap_Data1_1904-01-01T00-00-00.csv\"\n",
    "    save_path1.parent.mkdir(parents=True, exist_ok=True)\n",
    "    #save_path1.parent.mkdir(parents=True, exist_ok=True)\n",
    "    VideoData1.to_csv(save_path1)\n",
    "\n",
    "if VideoData2_Has_Sleap:\n",
    "    # Save  DataFrame as CSV to proper path and filename\n",
    "    save_path2 = save_path / \"Video_Sleap_Data2\" / \"Video_Sleap_Data2_1904-01-01T00-00-00.csv\"\n",
    "    save_path2.parent.mkdir(parents=True, exist_ok=True)\n",
    "    #save_path2.parent.mkdir(parents=True, exist_ok=True)\n",
    "    VideoData2.to_csv(save_path2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d832c53",
   "metadata": {},
   "source": [
    "# Saccade detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc720c7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# # TEMPORARY - param setting \n",
    "# # Parameters for orienting vs compensatory saccade classification\n",
    "# classify_orienting_compensatory = True  # Set to True to classify saccades as orienting vs compensatory\n",
    "# bout_window = 1.5  # Time window (seconds) for grouping saccades into bouts\n",
    "# pre_saccade_window = 0.3  # Time window (seconds) before saccade onset to analyze\n",
    "# max_intersaccade_interval_for_classification = 5.0  # Maximum time (seconds) to extend post-saccade window until next saccade for classification\n",
    "# pre_saccade_velocity_threshold = 50.0  # Velocity threshold (px/s) for detecting pre-saccade drift\n",
    "# pre_saccade_drift_threshold = 10.0  # Position drift threshold (px) before saccade for compensatory classification\n",
    "# post_saccade_variance_threshold = 100.0  # Position variance threshold (px¬≤) after saccade for orienting classification\n",
    "# post_saccade_position_change_threshold_percent = 50.0  # Position change threshold (% of saccade amplitude) - if post-saccade change > amplitude * this%, classify as compensatory\n",
    "\n",
    "# # Adaptive threshold parameters (percentile-based)\n",
    "# use_adaptive_thresholds = False  # Set to True to use adaptive thresholds based on feature distributions, False to use fixed thresholds\n",
    "# adaptive_percentile_pre_velocity = 75  # Percentile for pre-saccade velocity threshold (upper percentile for compensatory detection)\n",
    "# adaptive_percentile_pre_drift = 75  # Percentile for pre-saccade drift threshold (upper percentile for compensatory detection)\n",
    "# adaptive_percentile_post_variance = 25  # Percentile for post-saccade variance threshold (lower percentile for orienting detection - low variance = stable)\n",
    "\n",
    "# for saccades\n",
    "refractory_period = 0.1  # sec\n",
    "## Separate adaptive saccade threshold (k) for each video:\n",
    "k1 = 6  # for VideoData1 (L)\n",
    "k2 = 6  # for VideoData2 (R)\n",
    "\n",
    "# for adaptive saccade threshold - Number of standard deviations (adjustable: 2-4 range works well) \n",
    "onset_offset_fraction = 0.2  # to determine saccade onset and offset, i.e. o.2 is 20% of the peak velocity\n",
    "n_before = 10  # Number of points before detection peak to extract for peri-saccade-segments, points, so independent of FPS \n",
    "n_after = 30   # Number of points after detection peak to extract\n",
    "\n",
    "# Additional saccade detection parameters\n",
    "baseline_n_points = 5  # Number of points before threshold crossing to use for baseline calculation\n",
    "saccade_smoothing_window = 5  # Rolling median window size for position smoothing (frames)\n",
    "saccade_peak_width = 1  # Minimum peak width in samples for find_peaks (frames)\n",
    "\n",
    "plot_saccade_detection_QC = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8999e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "saccade_results = {}\n",
    "\n",
    "# Helper: map detected directions (upward/downward) to NT/TN based on eye assignment\n",
    "# Left eye: upward‚ÜíNT, downward‚ÜíTN; Right eye: upward‚ÜíTN, downward‚ÜíNT\n",
    "def get_direction_map_for_video(video_key):\n",
    "    eye = video1_eye if video_key == 'VideoData1' else video2_eye\n",
    "    if eye == 'L':\n",
    "        return {'upward': 'NT', 'downward': 'TN'}\n",
    "    else:\n",
    "        return {'upward': 'TN', 'downward': 'NT'}\n",
    "\n",
    "if VideoData1_Has_Sleap:\n",
    "    print(f\"\\nüîé === Source: ({get_eye_label('VideoData1')}) ===\\n\")\n",
    "    df1 = VideoData1[['Ellipse.Center.X', 'Seconds']].copy()\n",
    "    dir_map_v1 = get_direction_map_for_video('VideoData1')\n",
    "    saccade_results['VideoData1'] = analyze_eye_video_saccades(\n",
    "        df1, FPS_1, get_eye_label('VideoData1'),\n",
    "        k=k1, refractory_period=refractory_period,\n",
    "        onset_offset_fraction=onset_offset_fraction,\n",
    "        n_before=n_before, n_after=n_after, baseline_n_points=baseline_n_points,\n",
    "        saccade_smoothing_window=saccade_smoothing_window,\n",
    "        saccade_peak_width=saccade_peak_width,\n",
    "        upward_label=dir_map_v1['upward'],\n",
    "        downward_label=dir_map_v1['downward'],\n",
    "        classify_orienting_compensatory=classify_orienting_compensatory,\n",
    "        bout_window=bout_window,\n",
    "        pre_saccade_window=pre_saccade_window,\n",
    "        max_intersaccade_interval_for_classification=max_intersaccade_interval_for_classification,\n",
    "        pre_saccade_velocity_threshold=pre_saccade_velocity_threshold,\n",
    "        pre_saccade_drift_threshold=pre_saccade_drift_threshold,\n",
    "        post_saccade_variance_threshold=post_saccade_variance_threshold,\n",
    "        post_saccade_position_change_threshold_percent=post_saccade_position_change_threshold_percent,\n",
    "        use_adaptive_thresholds=use_adaptive_thresholds,\n",
    "        adaptive_percentile_pre_velocity=adaptive_percentile_pre_velocity,\n",
    "        adaptive_percentile_pre_drift=adaptive_percentile_pre_drift,\n",
    "        adaptive_percentile_post_variance=adaptive_percentile_post_variance\n",
    "    )\n",
    "\n",
    "\n",
    "if VideoData2_Has_Sleap:\n",
    "    print(f\"\\nüîé === Source: ({get_eye_label('VideoData2')}) ===\\n\")\n",
    "    df2 = VideoData2[['Ellipse.Center.X', 'Seconds']].copy()\n",
    "    dir_map_v2 = get_direction_map_for_video('VideoData2')\n",
    "    saccade_results['VideoData2'] = analyze_eye_video_saccades(\n",
    "        df2, FPS_2, get_eye_label('VideoData2'),\n",
    "        k=k2, refractory_period=refractory_period,\n",
    "        onset_offset_fraction=onset_offset_fraction,\n",
    "        n_before=n_before, n_after=n_after, baseline_n_points=baseline_n_points,\n",
    "        saccade_smoothing_window=saccade_smoothing_window,\n",
    "        saccade_peak_width=saccade_peak_width,\n",
    "        upward_label=dir_map_v2['upward'],\n",
    "        downward_label=dir_map_v2['downward'],\n",
    "        classify_orienting_compensatory=classify_orienting_compensatory,\n",
    "        bout_window=bout_window,\n",
    "        pre_saccade_window=pre_saccade_window,\n",
    "        max_intersaccade_interval_for_classification=max_intersaccade_interval_for_classification,\n",
    "        pre_saccade_velocity_threshold=pre_saccade_velocity_threshold,\n",
    "        pre_saccade_drift_threshold=pre_saccade_drift_threshold,\n",
    "        post_saccade_variance_threshold=post_saccade_variance_threshold,\n",
    "        post_saccade_position_change_threshold_percent=post_saccade_position_change_threshold_percent,\n",
    "        use_adaptive_thresholds=use_adaptive_thresholds,\n",
    "        adaptive_percentile_pre_velocity=adaptive_percentile_pre_velocity,\n",
    "        adaptive_percentile_pre_drift=adaptive_percentile_pre_drift,\n",
    "        adaptive_percentile_post_variance=adaptive_percentile_post_variance\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d27747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADAPTIVE THRESHOLD DIAGNOSTIC PLOTS (only if debug=True)\n",
    "#-------------------------------------------------------------------------------\n",
    "# Plot distributions of classification features to help determine meaningful adaptive thresholds\n",
    "if debug and len(saccade_results) > 0:\n",
    "    print(\"\\nüìä Generating adaptive threshold diagnostic plots...\")\n",
    "    \n",
    "    for video_key, res in saccade_results.items():\n",
    "        all_saccades_df = res.get('all_saccades_df', pd.DataFrame())\n",
    "        \n",
    "        if len(all_saccades_df) == 0:\n",
    "            print(f\"‚ö†Ô∏è  No saccades found for {get_eye_label(video_key)}, skipping diagnostic plots\")\n",
    "            continue\n",
    "        \n",
    "        # Filter out NaN values for plotting\n",
    "        pre_vel = all_saccades_df['pre_saccade_mean_velocity'].dropna()\n",
    "        pre_drift = all_saccades_df['pre_saccade_position_drift'].dropna()\n",
    "        post_var = all_saccades_df['post_saccade_position_variance'].dropna()\n",
    "        post_change = all_saccades_df['post_saccade_position_change'].dropna()\n",
    "        amplitude = all_saccades_df['amplitude'].dropna()\n",
    "        \n",
    "        # Calculate post_change / amplitude ratio (for percentage threshold visualization)\n",
    "        # Align by index to ensure matching\n",
    "        aligned_indices = post_change.index.intersection(amplitude.index)\n",
    "        post_change_aligned = post_change.loc[aligned_indices]\n",
    "        amplitude_aligned = amplitude.loc[aligned_indices]\n",
    "        post_change_ratio = (post_change_aligned / amplitude_aligned) * 100  # Convert to percentage\n",
    "        \n",
    "        # Calculate current thresholds for visualization\n",
    "        if use_adaptive_thresholds:\n",
    "            # Calculate adaptive thresholds from current data\n",
    "            if len(pre_vel) >= 3:\n",
    "                current_pre_vel_threshold = np.percentile(pre_vel, adaptive_percentile_pre_velocity)\n",
    "            else:\n",
    "                current_pre_vel_threshold = pre_saccade_velocity_threshold\n",
    "            \n",
    "            if len(pre_drift) >= 3:\n",
    "                current_pre_drift_threshold = np.percentile(pre_drift, adaptive_percentile_pre_drift)\n",
    "            else:\n",
    "                current_pre_drift_threshold = pre_saccade_drift_threshold\n",
    "            \n",
    "            if len(post_var) >= 3:\n",
    "                current_post_var_threshold = np.percentile(post_var, adaptive_percentile_post_variance)\n",
    "            else:\n",
    "                current_post_var_threshold = post_saccade_variance_threshold\n",
    "        else:\n",
    "            # Use fixed thresholds\n",
    "            current_pre_vel_threshold = pre_saccade_velocity_threshold\n",
    "            current_pre_drift_threshold = pre_saccade_drift_threshold\n",
    "            current_post_var_threshold = post_saccade_variance_threshold\n",
    "        \n",
    "        # Create figure with 2x2 subplots\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "        fig.suptitle(f'Adaptive Threshold Diagnostic Plots: {get_eye_label(video_key)}\\n'\n",
    "                    f'(n={len(all_saccades_df)} saccades)', fontsize=14, fontweight='bold')\n",
    "        \n",
    "        # Plot 1: Pre-saccade mean velocity\n",
    "        ax = axes[0, 0]\n",
    "        if len(pre_vel) > 0:\n",
    "            ax.hist(pre_vel, bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "            ax.axvline(current_pre_vel_threshold, color='red', linestyle='--', linewidth=2, \n",
    "                      label=f'Threshold: {current_pre_vel_threshold:.2f} px/s')\n",
    "            if use_adaptive_thresholds:\n",
    "                ax.axvline(np.percentile(pre_vel, 50), color='gray', linestyle=':', linewidth=1, \n",
    "                          label=f'Median: {np.percentile(pre_vel, 50):.2f} px/s')\n",
    "                ax.axvline(np.percentile(pre_vel, 75), color='orange', linestyle=':', linewidth=1, \n",
    "                          label=f'75th: {np.percentile(pre_vel, 75):.2f} px/s')\n",
    "            ax.set_xlabel('Pre-saccade Mean Velocity (px/s)')\n",
    "            ax.set_ylabel('Count')\n",
    "            ax.set_title(f'Pre-saccade Velocity Distribution\\n'\n",
    "                        f'{\"Adaptive\" if use_adaptive_thresholds else \"Fixed\"} threshold at '\n",
    "                        f'{adaptive_percentile_pre_velocity if use_adaptive_thresholds else \"fixed\"}th percentile')\n",
    "            ax.legend()\n",
    "            ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 2: Pre-saccade position drift\n",
    "        ax = axes[0, 1]\n",
    "        if len(pre_drift) > 0:\n",
    "            ax.hist(pre_drift, bins=50, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "            ax.axvline(current_pre_drift_threshold, color='red', linestyle='--', linewidth=2,\n",
    "                      label=f'Threshold: {current_pre_drift_threshold:.2f} px')\n",
    "            if use_adaptive_thresholds:\n",
    "                ax.axvline(np.percentile(pre_drift, 50), color='gray', linestyle=':', linewidth=1,\n",
    "                          label=f'Median: {np.percentile(pre_drift, 50):.2f} px')\n",
    "                ax.axvline(np.percentile(pre_drift, 75), color='orange', linestyle=':', linewidth=1,\n",
    "                          label=f'75th: {np.percentile(pre_drift, 75):.2f} px')\n",
    "            ax.set_xlabel('Pre-saccade Position Drift (px)')\n",
    "            ax.set_ylabel('Count')\n",
    "            ax.set_title(f'Pre-saccade Drift Distribution\\n'\n",
    "                        f'{\"Adaptive\" if use_adaptive_thresholds else \"Fixed\"} threshold at '\n",
    "                        f'{adaptive_percentile_pre_drift if use_adaptive_thresholds else \"fixed\"}th percentile')\n",
    "            ax.legend()\n",
    "            ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 3: Post-saccade position variance\n",
    "        ax = axes[1, 0]\n",
    "        if len(post_var) > 0:\n",
    "            ax.hist(post_var, bins=50, alpha=0.7, color='plum', edgecolor='black')\n",
    "            ax.axvline(current_post_var_threshold, color='red', linestyle='--', linewidth=2,\n",
    "                      label=f'Threshold: {current_post_var_threshold:.2f} px¬≤')\n",
    "            if use_adaptive_thresholds:\n",
    "                ax.axvline(np.percentile(post_var, 25), color='orange', linestyle=':', linewidth=1,\n",
    "                          label=f'25th: {np.percentile(post_var, 25):.2f} px¬≤')\n",
    "                ax.axvline(np.percentile(post_var, 50), color='gray', linestyle=':', linewidth=1,\n",
    "                          label=f'Median: {np.percentile(post_var, 50):.2f} px¬≤')\n",
    "            ax.set_xlabel('Post-saccade Position Variance (px¬≤)')\n",
    "            ax.set_ylabel('Count')\n",
    "            ax.set_title(f'Post-saccade Variance Distribution\\n'\n",
    "                        f'{\"Adaptive\" if use_adaptive_thresholds else \"Fixed\"} threshold at '\n",
    "                        f'{adaptive_percentile_post_variance if use_adaptive_thresholds else \"fixed\"}th percentile')\n",
    "            ax.legend()\n",
    "            ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 4: Post-saccade position change (as percentage of amplitude)\n",
    "        ax = axes[1, 1]\n",
    "        if len(post_change_ratio) > 0:\n",
    "            ax.hist(post_change_ratio, bins=50, alpha=0.7, color='salmon', edgecolor='black')\n",
    "            ax.axvline(post_saccade_position_change_threshold_percent, color='red', linestyle='--', \n",
    "                      linewidth=2, label=f'Threshold: {post_saccade_position_change_threshold_percent:.1f}%')\n",
    "            ax.axvline(np.percentile(post_change_ratio, 50), color='gray', linestyle=':', linewidth=1,\n",
    "                      label=f'Median: {np.percentile(post_change_ratio, 50):.1f}%')\n",
    "            ax.axvline(np.percentile(post_change_ratio, 75), color='orange', linestyle=':', linewidth=1,\n",
    "                      label=f'75th: {np.percentile(post_change_ratio, 75):.1f}%')\n",
    "            ax.set_xlabel('Post-saccade Position Change / Amplitude (%)')\n",
    "            ax.set_ylabel('Count')\n",
    "            ax.set_title(f'Post-saccade Position Change Ratio\\n'\n",
    "                        f'Fixed threshold: {post_saccade_position_change_threshold_percent:.1f}% of amplitude')\n",
    "            ax.legend()\n",
    "            ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print summary statistics\n",
    "        print(f\"\\nüìà Summary Statistics for {get_eye_label(video_key)}:\")\n",
    "        if len(pre_vel) > 0:\n",
    "            print(f\"  Pre-saccade velocity: mean={pre_vel.mean():.2f}, median={pre_vel.median():.2f}, \"\n",
    "                  f\"std={pre_vel.std():.2f} px/s\")\n",
    "        if len(pre_drift) > 0:\n",
    "            print(f\"  Pre-saccade drift: mean={pre_drift.mean():.2f}, median={pre_drift.median():.2f}, \"\n",
    "                  f\"std={pre_drift.std():.2f} px\")\n",
    "        if len(post_var) > 0:\n",
    "            print(f\"  Post-saccade variance: mean={post_var.mean():.2f}, median={post_var.median():.2f}, \"\n",
    "                  f\"std={post_var.std():.2f} px¬≤\")\n",
    "        if len(post_change_ratio) > 0:\n",
    "            print(f\"  Post-saccade change ratio: mean={post_change_ratio.mean():.1f}%, \"\n",
    "                  f\"median={post_change_ratio.median():.1f}%, std={post_change_ratio.std():.1f}%\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a07f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZE ALL SACCADES - SIDE BY SIDE\n",
    "#-------------------------------------------------------------------------------\n",
    "# Plot all upward and downward saccades aligned by time with position and velocity traces\n",
    "\n",
    "# Create figure with 4 columns in a single row: up pos, up vel, down pos, down vel\n",
    "\n",
    "for video_key, res in saccade_results.items():\n",
    "    dir_map = get_direction_map_for_video(video_key)\n",
    "    label_up = dir_map['upward']\n",
    "    label_down = dir_map['downward']\n",
    "\n",
    "    upward_saccades_df = res['upward_saccades_df']\n",
    "    downward_saccades_df = res['downward_saccades_df']\n",
    "    peri_saccades = res['peri_saccades']\n",
    "    upward_segments = res['upward_segments']\n",
    "    downward_segments = res['downward_segments']\n",
    "    # Any other variables you need...\n",
    "\n",
    "    fig_all = make_subplots(\n",
    "        rows=1, cols=4,\n",
    "        shared_yaxes=False,  # Each panel can have different y-axis scale\n",
    "        shared_xaxes=False,\n",
    "        subplot_titles=(\n",
    "            f'Position - {label_up} Saccades',\n",
    "            f'Velocity - {label_up} Saccades',\n",
    "            f'Position - {label_down} Saccades',\n",
    "            f'Velocity - {label_down} Saccades'\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Extract segments for each direction\n",
    "    upward_segments_all = [seg for seg in peri_saccades if seg['saccade_direction'].iloc[0] == 'upward']\n",
    "    downward_segments_all = [seg for seg in peri_saccades if seg['saccade_direction'].iloc[0] == 'downward']\n",
    "\n",
    "    # Filter outliers\n",
    "    upward_segments, upward_outliers_meta, upward_outlier_segments = sp.filter_outlier_saccades(upward_segments_all, 'upward')\n",
    "    downward_segments, downward_outliers_meta, downward_outlier_segments = sp.filter_outlier_saccades(downward_segments_all, 'downward')\n",
    "\n",
    "    if debug:\n",
    "        print(f\"Plotting {len(upward_segments)} {label_up} and {len(downward_segments)} {label_down} saccades...\")\n",
    "        if len(upward_outliers_meta) > 0 or len(downward_outliers_meta) > 0:\n",
    "            print(f\"   Excluded {len(upward_outliers_meta)} {label_up} outlier(s) and {len(downward_outliers_meta)} {label_down} outlier(s)\")\n",
    "\n",
    "    if debug and len(upward_outliers_meta) > 0:\n",
    "        print(f\"\\n   {label_up} outliers (first 5):\")\n",
    "        for i, out in enumerate(upward_outliers_meta[:5]):\n",
    "            pass\n",
    "        if len(upward_outliers_meta) > 5:\n",
    "            print(f\"      ... and {len(upward_outliers_meta) - 5} more\")\n",
    "\n",
    "    if debug and len(downward_outliers_meta) > 0:\n",
    "        print(f\"\\n   {label_down} outliers (first 5):\")\n",
    "        for i, out in enumerate(downward_outliers_meta[:5]):\n",
    "            pass\n",
    "        if len(downward_outliers_meta) > 5:\n",
    "            print(f\"      ... and {len(downward_outliers_meta) - 5} more\")\n",
    "\n",
    "    # Plot upward saccades\n",
    "    for i, segment in enumerate(upward_segments):\n",
    "        color_opacity = 0.15 if len(upward_segments) > 20 else 0.3\n",
    "        \n",
    "        # Position trace (using baselined values)\n",
    "        fig_all.add_trace(\n",
    "            go.Scatter(\n",
    "                x=segment['Time_rel_threshold'],\n",
    "                y=segment['X_smooth_baselined'],\n",
    "                mode='lines',\n",
    "                name=f'Up #{i+1}',\n",
    "                line=dict(color='green', width=1),\n",
    "                showlegend=False,\n",
    "                opacity=color_opacity\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        # Velocity trace\n",
    "        fig_all.add_trace(\n",
    "            go.Scatter(\n",
    "                x=segment['Time_rel_threshold'],\n",
    "                y=segment['vel_x_smooth'],\n",
    "                mode='lines',\n",
    "                name=f'Up #{i+1}',\n",
    "                line=dict(color='green', width=1),\n",
    "                        showlegend=False,\n",
    "                opacity=color_opacity\n",
    "            ),\n",
    "            row=1, col=2\n",
    "        )\n",
    "\n",
    "    # Plot downward saccades\n",
    "    for i, segment in enumerate(downward_segments):\n",
    "        color_opacity = 0.15 if len(downward_segments) > 20 else 0.3\n",
    "        \n",
    "        # Position trace (using baselined values)\n",
    "        fig_all.add_trace(\n",
    "            go.Scatter(\n",
    "                x=segment['Time_rel_threshold'],\n",
    "                y=segment['X_smooth_baselined'],\n",
    "                mode='lines',\n",
    "                name=f'Down #{i+1}',\n",
    "                line=dict(color='purple', width=1),\n",
    "                showlegend=False,\n",
    "                opacity=color_opacity\n",
    "            ),\n",
    "            row=1, col=3\n",
    "        )\n",
    "        \n",
    "        # Velocity trace\n",
    "        fig_all.add_trace(\n",
    "            go.Scatter(\n",
    "                x=segment['Time_rel_threshold'],\n",
    "                y=segment['vel_x_smooth'],\n",
    "                mode='lines',\n",
    "                name=f'Down #{i+1}',\n",
    "                line=dict(color='purple', width=1),\n",
    "                showlegend=False,\n",
    "                opacity=color_opacity\n",
    "            ),\n",
    "            row=1, col=4\n",
    "        )\n",
    "\n",
    "    # Add mean traces for reference\n",
    "    if len(upward_segments) > 0:\n",
    "        # Calculate mean for upward by aligning segments by Time_rel_threshold (not by array index)\n",
    "        # Find the threshold crossing index (where Time_rel_threshold ‚âà 0) for each segment and align based on that\n",
    "        aligned_positions = []\n",
    "        aligned_velocities = []\n",
    "        aligned_times = []\n",
    "        \n",
    "        for seg in upward_segments:\n",
    "            # Find index where Time_rel_threshold is closest to 0 (the threshold crossing)\n",
    "            threshold_idx = np.abs(seg['Time_rel_threshold'].values).argmin()\n",
    "            \n",
    "            # Extract data centered on threshold crossing: n_before points before, threshold crossing, n_after points after\n",
    "            start_idx = max(0, threshold_idx - n_before)\n",
    "            end_idx = min(len(seg), threshold_idx + n_after + 1)\n",
    "            \n",
    "            # Extract aligned segment\n",
    "            aligned_seg = seg.iloc[start_idx:end_idx].copy()\n",
    "            \n",
    "            # Find where the threshold crossing actually is within the extracted segment\n",
    "            threshold_in_seg_idx = threshold_idx - start_idx\n",
    "            \n",
    "            # Ensure threshold crossing is at index n_before by padding at start if needed\n",
    "            if threshold_in_seg_idx < n_before:\n",
    "                # Need to pad at the start to align threshold crossing to index n_before\n",
    "                pad_length = n_before - threshold_in_seg_idx\n",
    "                # Estimate time step from original segment for padding\n",
    "                if len(seg) > 1:\n",
    "                    dt_est = np.diff(seg['Time_rel_threshold'].values).mean()\n",
    "                else:\n",
    "                    dt_est = 0.0083  # default estimate if we can't calculate\n",
    "                \n",
    "                # Create padding with NaN values\n",
    "                pad_times = aligned_seg['Time_rel_threshold'].iloc[0] - dt_est * np.arange(pad_length, 0, -1)\n",
    "                pad_df = pd.DataFrame({\n",
    "                    'X_smooth_baselined': [np.nan] * pad_length,\n",
    "                    'vel_x_smooth': [np.nan] * pad_length,\n",
    "                    'Time_rel_threshold': pad_times\n",
    "                })\n",
    "                aligned_seg = pd.concat([pad_df, aligned_seg.reset_index(drop=True)], ignore_index=True)\n",
    "            \n",
    "            aligned_positions.append(aligned_seg['X_smooth_baselined'].values)\n",
    "            aligned_velocities.append(aligned_seg['vel_x_smooth'].values)\n",
    "            aligned_times.append(aligned_seg['Time_rel_threshold'].values)\n",
    "        \n",
    "        # Find minimum length after alignment\n",
    "        min_length = min(len(pos) for pos in aligned_positions)\n",
    "        max_length = max(len(pos) for pos in aligned_positions)\n",
    "        \n",
    "        if min_length != max_length and debug:\n",
    "            print(f\"‚ö†Ô∏è  Warning: {label_up} segments have variable lengths after alignment ({min_length} to {max_length} points). Using minimum length {min_length}.\")\n",
    "        \n",
    "        # Truncate all segments to same length and stack\n",
    "        upward_positions = np.array([pos[:min_length] for pos in aligned_positions])\n",
    "        upward_velocities = np.array([vel[:min_length] for vel in aligned_velocities])\n",
    "        upward_times = aligned_times[0][:min_length]  # Use first segment's time values\n",
    "        \n",
    "        # Calculate mean across all segments (axis=0 means across segments, keeping time dimension)\n",
    "        upward_mean_pos = np.mean(upward_positions, axis=0)\n",
    "        upward_mean_vel = np.mean(upward_velocities, axis=0)\n",
    "        \n",
    "        fig_all.add_trace(\n",
    "            go.Scatter(\n",
    "                x=upward_times,\n",
    "                y=upward_mean_pos,\n",
    "                mode='lines',\n",
    "                name=f'{label_up} Mean Position',\n",
    "                line=dict(color='red', width=3)\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        fig_all.add_trace(\n",
    "            go.Scatter(\n",
    "                x=upward_times,\n",
    "                y=upward_mean_vel,\n",
    "                mode='lines',\n",
    "                name=f'{label_up} Mean Velocity',\n",
    "                line=dict(color='red', width=3)\n",
    "            ),\n",
    "            row=1, col=2\n",
    "        )\n",
    "\n",
    "    if len(downward_segments) > 0:\n",
    "        # Calculate mean for downward by taking mean across all segments at each index position\n",
    "        # Find minimum length to handle variable-length segments\n",
    "        min_length_down = min(len(seg) for seg in downward_segments)\n",
    "        max_length_down = max(len(seg) for seg in downward_segments)\n",
    "        \n",
    "        if min_length_down != max_length_down and debug:\n",
    "            print(f\"‚ö†Ô∏è  Warning: {label_down} segments have variable lengths ({min_length_down} to {max_length_down} points). Using minimum length {min_length_down}.\")\n",
    "        \n",
    "        # Stack all segments as arrays (each row is one saccade, columns are time points)\n",
    "        # Use only first min_length points to ensure all arrays have same shape\n",
    "        downward_positions = np.array([seg['X_smooth_baselined'].values[:min_length_down] for seg in downward_segments])\n",
    "        downward_velocities = np.array([seg['vel_x_smooth'].values[:min_length_down] for seg in downward_segments])\n",
    "        downward_times = downward_segments[0]['Time_rel_threshold'].values[:min_length_down]  # Use first segment's time values\n",
    "        \n",
    "        # Calculate mean across all segments (axis=0 means across segments, keeping time dimension)\n",
    "        downward_mean_pos = np.mean(downward_positions, axis=0)\n",
    "        downward_mean_vel = np.mean(downward_velocities, axis=0)\n",
    "        \n",
    "        fig_all.add_trace(\n",
    "            go.Scatter(\n",
    "                x=downward_times,\n",
    "                y=downward_mean_pos,\n",
    "                mode='lines',\n",
    "                name=f'{label_down} Mean Position',\n",
    "                line=dict(color='purple', width=3)\n",
    "            ),\n",
    "            row=1, col=3\n",
    "        )\n",
    "        \n",
    "        fig_all.add_trace(\n",
    "            go.Scatter(\n",
    "                x=downward_times,\n",
    "                y=downward_mean_vel,\n",
    "                mode='lines',\n",
    "                name=f'{label_down} Mean Velocity',\n",
    "                line=dict(color='purple', width=3)\n",
    "            ),\n",
    "            row=1, col=4\n",
    "        )\n",
    "\n",
    "    # Add vertical line at time=0 (saccade onset)\n",
    "    for col in [1, 2, 3, 4]:\n",
    "        fig_all.add_shape(\n",
    "            type=\"line\",\n",
    "            x0=0, x1=0,\n",
    "            y0=-999, y1=999,\n",
    "            line=dict(color='black', width=1, dash='dash'),\n",
    "            row=1, col=col\n",
    "        )\n",
    "\n",
    "    # Update layout\n",
    "    fig_all.update_layout(\n",
    "        title_text=f'All Detected Saccades Overlaid ({get_eye_label(video_key)}) - Position and Velocity Profiles<br><sub>Position traces are baselined (avg of points -{n_before} to -{n_before-5}). All traces in semi-transparent, mean in bold. Time=0 is threshold crossing. {n_before} points before, {n_after} after.</sub>',\n",
    "        height=500,\n",
    "        width=1600,\n",
    "        showlegend=False\n",
    "    )\n",
    "\n",
    "    # Calculate x-axis limits based on actual min/max time values from segments (with small padding)\n",
    "    # Y-axis will use auto-range (no explicit range setting)\n",
    "\n",
    "    # Collect all time values for proper x-axis scaling\n",
    "    all_upward_times = []\n",
    "    all_downward_times = []\n",
    "\n",
    "    for seg in upward_segments:\n",
    "        all_upward_times.extend(seg['Time_rel_threshold'].values)\n",
    "\n",
    "    for seg in downward_segments:\n",
    "        all_downward_times.extend(seg['Time_rel_threshold'].values)\n",
    "\n",
    "    # Set x-axis ranges using actual min/max from all segment times (with small padding to show all data)\n",
    "    padding_factor = 0.02  # 2% padding on each side for readability\n",
    "    if len(all_upward_times) > 0:\n",
    "        up_x_range = np.max(all_upward_times) - np.min(all_upward_times)\n",
    "        padding = up_x_range * padding_factor if up_x_range > 0 else 0.01\n",
    "        up_x_min = np.min(all_upward_times) - padding\n",
    "        up_x_max = np.max(all_upward_times) + padding\n",
    "    else:\n",
    "        up_x_min, up_x_max = -0.2, 0.4\n",
    "\n",
    "    if len(all_downward_times) > 0:\n",
    "        down_x_range = np.max(all_downward_times) - np.min(all_downward_times)\n",
    "        padding = down_x_range * padding_factor if down_x_range > 0 else 0.01\n",
    "        down_x_min = np.min(all_downward_times) - padding\n",
    "        down_x_max = np.max(all_downward_times) + padding\n",
    "    else:\n",
    "        down_x_min, down_x_max = -0.2, 0.4\n",
    "\n",
    "    # Calculate y-axis ranges separately for position and velocity from filtered data\n",
    "    # Collect position and velocity values from filtered segments\n",
    "    upward_pos_values = []\n",
    "    downward_pos_values = []\n",
    "    upward_vel_values = []\n",
    "    downward_vel_values = []\n",
    "\n",
    "    for seg in upward_segments:\n",
    "        upward_pos_values.extend(seg['X_smooth_baselined'].values)\n",
    "        # Filter out NaN values when collecting velocity data\n",
    "        vel_values = seg['vel_x_smooth'].values\n",
    "        upward_vel_values.extend(vel_values[~np.isnan(vel_values)])\n",
    "\n",
    "    for seg in downward_segments:\n",
    "        downward_pos_values.extend(seg['X_smooth_baselined'].values)\n",
    "        # Filter out NaN values when collecting velocity data\n",
    "        vel_values = seg['vel_x_smooth'].values\n",
    "        downward_vel_values.extend(vel_values[~np.isnan(vel_values)])\n",
    "\n",
    "    # Position: find min/max for upward and downward, use wider range for both panels in row 1\n",
    "    if len(upward_pos_values) > 0 and len(downward_pos_values) > 0:\n",
    "        up_pos_min = np.min(upward_pos_values)\n",
    "        up_pos_max = np.max(upward_pos_values)\n",
    "        down_pos_min = np.min(downward_pos_values)\n",
    "        down_pos_max = np.max(downward_pos_values)\n",
    "        # Use the wider range (smaller min, larger max)\n",
    "        pos_min = min(up_pos_min, down_pos_min)\n",
    "        pos_max = max(up_pos_max, down_pos_max)\n",
    "    elif len(upward_pos_values) > 0:\n",
    "        pos_min = np.min(upward_pos_values)\n",
    "        pos_max = np.max(upward_pos_values)\n",
    "    elif len(downward_pos_values) > 0:\n",
    "        pos_min = np.min(downward_pos_values)\n",
    "        pos_max = np.max(downward_pos_values)\n",
    "    else:\n",
    "        pos_min, pos_max = -50, 50\n",
    "\n",
    "    # Velocity: find min/max for upward and downward, use wider range for both panels in row 2\n",
    "    # Get min and max directly from all velocity traces being plotted, with padding to prevent clipping\n",
    "    if len(upward_vel_values) > 0 and len(downward_vel_values) > 0:\n",
    "        # Get actual min/max from all velocity values\n",
    "        all_vel_min = min(np.min(upward_vel_values), np.min(downward_vel_values))\n",
    "        all_vel_max = max(np.max(upward_vel_values), np.max(downward_vel_values))\n",
    "\n",
    "    elif len(upward_vel_values) > 0:\n",
    "        all_vel_min = np.min(upward_vel_values)\n",
    "        all_vel_max = np.max(upward_vel_values)\n",
    "\n",
    "    elif len(downward_vel_values) > 0:\n",
    "        all_vel_min = np.min(downward_vel_values)\n",
    "        all_vel_max = np.max(downward_vel_values)\n",
    "\n",
    "    else:\n",
    "        all_vel_min, all_vel_max = -1000, 1000\n",
    "        if debug:\n",
    "            print(f\"   ‚ö†Ô∏è  No velocity values found, using default range: [{all_vel_min:.2f}, {all_vel_max:.2f}] px/s\")\n",
    "\n",
    "    # Add padding to prevent clipping (20% padding on each side)\n",
    "    vel_range = all_vel_max - all_vel_min\n",
    "    if vel_range > 0:\n",
    "        padding = vel_range * 0.20  # 20% padding\n",
    "        vel_min = all_vel_min - padding\n",
    "        vel_max = all_vel_max + padding\n",
    "    else:\n",
    "        # If range is zero or very small, use default padding\n",
    "        vel_min = all_vel_min - 1.0\n",
    "        vel_max = all_vel_max + 1.0\n",
    "\n",
    "    # Update axes - x-axis with ranges, y-axis with explicit ranges based on filtered data\n",
    "    fig_all.update_xaxes(title_text=\"Time relative to threshold crossing (s)\", range=[up_x_min, up_x_max], row=1, col=2)\n",
    "    fig_all.update_xaxes(title_text=\"Time relative to threshold crossing (s)\", range=[down_x_min, down_x_max], row=1, col=4)\n",
    "    fig_all.update_xaxes(title_text=\"\", range=[up_x_min, up_x_max], row=1, col=1)\n",
    "    fig_all.update_xaxes(title_text=\"\", range=[down_x_min, down_x_max], row=1, col=3)\n",
    "\n",
    "    # Set explicit y-axis ranges - position panels share same range, velocity panels share same range\n",
    "    fig_all.update_yaxes(title_text=\"X Position (px)\", range=[pos_min, pos_max], row=1, col=1)\n",
    "    fig_all.update_yaxes(title_text=\"X Position (px)\", range=[pos_min, pos_max], row=1, col=3)\n",
    "    fig_all.update_yaxes(title_text=\"Velocity (px/s)\", range=[vel_min, vel_max], row=1, col=2)\n",
    "    fig_all.update_yaxes(title_text=\"Velocity (px/s)\", range=[vel_min, vel_max], row=1, col=4)\n",
    "\n",
    "\n",
    "    fig_all.show()\n",
    "\n",
    "    # Print statistics\n",
    "    if debug:\n",
    "        print(f\"\\n=== OVERLAY SUMMARY ===\")\n",
    "        if len(upward_segments) > 0:\n",
    "            up_amps = [seg['saccade_amplitude'].iloc[0] for seg in upward_segments]\n",
    "            up_durs = [seg['saccade_duration'].iloc[0] for seg in upward_segments]\n",
    "            print(f\"{label_up} saccades: {len(upward_segments)}\")\n",
    "            print(f\"  Mean amplitude: {np.mean(up_amps):.2f} px\")\n",
    "            print(f\"  Mean duration: {np.mean(up_durs):.3f} s\")\n",
    "\n",
    "        if len(downward_segments) > 0:\n",
    "            down_amps = [seg['saccade_amplitude'].iloc[0] for seg in downward_segments]\n",
    "            down_durs = [seg['saccade_duration'].iloc[0] for seg in downward_segments]\n",
    "            print(f\"{label_down} saccades: {len(downward_segments)}\")\n",
    "            print(f\"  Mean amplitude: {np.mean(down_amps):.2f} px\")\n",
    "            print(f\"  Mean duration: {np.mean(down_durs):.3f} s\")\n",
    "\n",
    "        print(f\"\\n‚è±Ô∏è  Time alignment: All saccades aligned to threshold crossing (Time_rel_threshold=0)\")\n",
    "        if len(all_upward_times) > 0 and len(all_downward_times) > 0:\n",
    "            # Use the wider range for reporting\n",
    "            overall_x_min = min(np.min(all_upward_times), np.min(all_downward_times))\n",
    "            overall_x_max = max(np.max(all_upward_times), np.max(all_downward_times))\n",
    "            print(f\"üìè Window: {n_before} points before + {n_after} points after threshold crossing (actual time range: {overall_x_min:.3f} to {overall_x_max:.3f} s)\")\n",
    "        elif len(all_upward_times) > 0:\n",
    "            print(f\"üìè Window: {n_before} points before + {n_after} points after threshold crossing ({label_up} actual time range: {np.min(all_upward_times):.3f} to {np.max(all_upward_times):.3f} s)\")\n",
    "        elif len(all_downward_times) > 0:\n",
    "            print(f\"üìè Window: {n_before} points before + {n_after} points after threshold crossing ({label_down} actual time range: {np.min(all_downward_times):.3f} to {np.max(all_downward_times):.3f} s)\")\n",
    "        else:\n",
    "            print(f\"üìè Window: {n_before} points before + {n_after} points after threshold crossing\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43db5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SACCADE AMPLITUDE QC VISUALIZATION\n",
    "#-------------------------------------------------------------------------------\n",
    "# 1. Distribution of saccade amplitudes\n",
    "# 2. Correlation between saccade amplitude and duration\n",
    "# 3. Peri-saccade segments colored by amplitude (outlier detection)\n",
    "\n",
    "for video_key, res in saccade_results.items():\n",
    "    dir_map = get_direction_map_for_video(video_key)\n",
    "    label_up = dir_map['upward']\n",
    "    label_down = dir_map['downward']\n",
    "\n",
    "    upward_saccades_df = res['upward_saccades_df']\n",
    "    downward_saccades_df = res['downward_saccades_df']\n",
    "    peri_saccades = res['peri_saccades']   \n",
    "    upward_segments = res['upward_segments']\n",
    "    downward_segments = res['downward_segments']\n",
    "    # Any other variables you need...\n",
    "\n",
    "    fig_qc = make_subplots(\n",
    "        rows=3, cols=2,\n",
    "        subplot_titles=(\n",
    "            f'Amplitude Distribution - {label_up} Saccades', \n",
    "            f'Amplitude Distribution - {label_down} Saccades',\n",
    "            f'Amplitude vs Duration - {label_up} Saccades',\n",
    "            f'Amplitude vs Duration - {label_down} Saccades',\n",
    "            f'Peri-Saccade Segments - {label_up} (colored by amplitude)',\n",
    "            f'Peri-Saccade Segments - {label_down} (colored by amplitude)'\n",
    "        ),\n",
    "        vertical_spacing=0.10,\n",
    "        horizontal_spacing=0.1,\n",
    "        row_heights=[0.25, 0.25, 0.5]  # Make segment plots larger\n",
    "    )\n",
    "\n",
    "    # 1. Amplitude distributions\n",
    "    if len(upward_saccades_df) > 0:\n",
    "        # Histogram for upward saccades\n",
    "        fig_qc.add_trace(\n",
    "            go.Histogram(\n",
    "                x=upward_saccades_df['amplitude'],\n",
    "                nbinsx=50,\n",
    "                name=f'{label_up}',\n",
    "                marker_color='red',\n",
    "                opacity=0.6\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        # Scatter plot for upward saccades\n",
    "        fig_qc.add_trace(\n",
    "            go.Scatter(\n",
    "                x=upward_saccades_df['duration'],\n",
    "                y=upward_saccades_df['amplitude'],\n",
    "                mode='markers',\n",
    "                name=f'{label_up}',\n",
    "                marker=dict(color='red', size=6, opacity=0.6)\n",
    "            ),\n",
    "            row=2, col=1\n",
    "        )\n",
    "        \n",
    "        # Add correlation line for upward saccades\n",
    "        corr_up = upward_saccades_df[['amplitude', 'duration']].corr().iloc[0, 1]\n",
    "        z_up = np.polyfit(upward_saccades_df['duration'], upward_saccades_df['amplitude'], 1)\n",
    "        p_up = np.poly1d(z_up)\n",
    "        fig_qc.add_trace(\n",
    "            go.Scatter(\n",
    "                x=upward_saccades_df['duration'],\n",
    "                y=p_up(upward_saccades_df['duration']),\n",
    "                mode='lines',\n",
    "                name=f'R={corr_up:.2f}',\n",
    "                line=dict(color='darkgreen', width=2),\n",
    "                showlegend=False\n",
    "            ),\n",
    "            row=2, col=1\n",
    "        )\n",
    "\n",
    "    if len(downward_saccades_df) > 0:\n",
    "        # Histogram for downward saccades\n",
    "        fig_qc.add_trace(\n",
    "            go.Histogram(\n",
    "                x=downward_saccades_df['amplitude'],\n",
    "                nbinsx=50,\n",
    "                name=f'{label_down}',\n",
    "                marker_color='purple',\n",
    "                opacity=0.6\n",
    "            ),\n",
    "            row=1, col=2\n",
    "        )\n",
    "        \n",
    "        # Scatter plot for downward saccades\n",
    "        fig_qc.add_trace(\n",
    "            go.Scatter(\n",
    "                x=downward_saccades_df['duration'],\n",
    "                y=downward_saccades_df['amplitude'],\n",
    "                mode='markers',\n",
    "                name=f'{label_down}',\n",
    "                marker=dict(color='purple', size=6, opacity=0.6)\n",
    "            ),\n",
    "            row=2, col=2\n",
    "        )\n",
    "        \n",
    "        # Add correlation line for downward saccades\n",
    "        corr_down = downward_saccades_df[['amplitude', 'duration']].corr().iloc[0, 1]\n",
    "        z_down = np.polyfit(downward_saccades_df['duration'], downward_saccades_df['amplitude'], 1)\n",
    "        p_down = np.poly1d(z_down)\n",
    "        fig_qc.add_trace(\n",
    "            go.Scatter(\n",
    "                x=downward_saccades_df['duration'],\n",
    "                y=p_down(downward_saccades_df['duration']),\n",
    "                mode='lines',\n",
    "                name=f'R={corr_down:.2f}',\n",
    "                line=dict(color='darkviolet', width=2),\n",
    "                showlegend=False\n",
    "            ),\n",
    "            row=2, col=2\n",
    "        )\n",
    "\n",
    "    # 3. Plot peri-saccade segments colored by amplitude\n",
    "    # Reuse already-extracted and baselined segments from peri_saccades (no re-extraction or re-baselining)\n",
    "\n",
    "    # Extract upward and downward segments for QC visualization from already-baselined peri_saccades\n",
    "    # (Removed extract_qc_segments function - segments are now baselined only once during initial extraction)\n",
    "    if 'peri_saccades' in globals() and len(peri_saccades) > 0:\n",
    "        upward_segments_all = [seg for seg in peri_saccades if seg['saccade_direction'].iloc[0] == 'upward']\n",
    "        downward_segments_all = [seg for seg in peri_saccades if seg['saccade_direction'].iloc[0] == 'downward']\n",
    "    else:\n",
    "        upward_segments_all = []\n",
    "        downward_segments_all = []\n",
    "\n",
    "    # Plot upward segments\n",
    "    if len(upward_segments_all) > 0:\n",
    "        upward_amplitudes = [seg['saccade_amplitude'].iloc[0] for seg in upward_segments_all]\n",
    "        upward_colors, upward_min_amp, upward_max_amp = sp.get_color_mapping(upward_amplitudes)\n",
    "        \n",
    "        for i, (segment, color) in enumerate(zip(upward_segments_all, upward_colors)):\n",
    "            fig_qc.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=segment['Time_rel_threshold'],\n",
    "                    y=segment['X_smooth_baselined'],\n",
    "                    mode='lines',\n",
    "                    name=f'{label_up} #{i+1}',\n",
    "                    line=dict(color=color, width=1.5),\n",
    "                    showlegend=False,\n",
    "                    opacity=0.7,\n",
    "                    hovertemplate=f'Amplitude: {segment[\"saccade_amplitude\"].iloc[0]:.2f} px<br>' +\n",
    "                                'Time: %{x:.3f} s<br>' +\n",
    "                                'Position: %{y:.2f} px<extra></extra>'\n",
    "                ),\n",
    "                row=3, col=1\n",
    "            )\n",
    "        \n",
    "        # Add dummy trace for colorbar (hidden but provides colorbar)\n",
    "        fig_qc.add_trace(\n",
    "            go.Scatter(\n",
    "                x=[None],  # Hidden trace\n",
    "                y=[None],\n",
    "                mode='markers',\n",
    "                marker=dict(\n",
    "                    size=1,\n",
    "                    color=[upward_min_amp, upward_max_amp],\n",
    "                    colorscale='Plasma',\n",
    "                    cmin=upward_min_amp,\n",
    "                    cmax=upward_max_amp,\n",
    "                    showscale=True,\n",
    "                    colorbar=dict(\n",
    "                        title=dict(text=f\"Amplitude ({label_up})\", side=\"right\"),\n",
    "                        x=0.47,  # Position to the right of the left column plot\n",
    "                        xpad=10,\n",
    "                        len=0.45,  # Set colorbar length relative to subplot\n",
    "                        y=0.5,  # Center vertically on the subplot\n",
    "                        yanchor=\"middle\"\n",
    "                    )\n",
    "                ),\n",
    "                showlegend=False,\n",
    "                hoverinfo='skip'\n",
    "            ),\n",
    "            row=3, col=1\n",
    "        )\n",
    "\n",
    "    # Plot downward segments\n",
    "    if len(downward_segments_all) > 0:\n",
    "        downward_amplitudes = [seg['saccade_amplitude'].iloc[0] for seg in downward_segments_all]\n",
    "        downward_colors, downward_min_amp, downward_max_amp = sp.get_color_mapping(downward_amplitudes)\n",
    "        \n",
    "        for i, (segment, color) in enumerate(zip(downward_segments_all, downward_colors)):\n",
    "            fig_qc.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=segment['Time_rel_threshold'],\n",
    "                    y=segment['X_smooth_baselined'],\n",
    "                    mode='lines',\n",
    "                    name=f'{label_down} #{i+1}',\n",
    "                    line=dict(color=color, width=1.5),\n",
    "                    showlegend=False,\n",
    "                    opacity=0.7,\n",
    "                    hovertemplate=f'Amplitude: {segment[\"saccade_amplitude\"].iloc[0]:.2f} px<br>' +\n",
    "                                'Time: %{x:.3f} s<br>' +\n",
    "                                'Position: %{y:.2f} px<extra></extra>'\n",
    "                ),\n",
    "                row=3, col=2\n",
    "            )\n",
    "        \n",
    "        # Add dummy trace for colorbar (hidden but provides colorbar)\n",
    "        fig_qc.add_trace(\n",
    "            go.Scatter(\n",
    "                x=[None],  # Hidden trace\n",
    "                y=[None],\n",
    "                mode='markers',\n",
    "                marker=dict(\n",
    "                    size=1,\n",
    "                    color=[downward_min_amp, downward_max_amp],\n",
    "                    colorscale='Plasma',\n",
    "                    cmin=downward_min_amp,\n",
    "                    cmax=downward_max_amp,\n",
    "                    showscale=True,\n",
    "                    colorbar=dict(\n",
    "                        title=dict(text=f\"Amplitude ({label_down})\", side=\"right\"),\n",
    "                        x=0.97,  # Position to the right of the right column plot\n",
    "                        xpad=10,\n",
    "                        len=0.45,  # Set colorbar length relative to subplot\n",
    "                        y=0.5,  # Center vertically on the subplot\n",
    "                        yanchor=\"middle\"\n",
    "                    )\n",
    "                ),\n",
    "                showlegend=False,\n",
    "                hoverinfo='skip'\n",
    "            ),\n",
    "            row=3, col=2\n",
    "        )\n",
    "\n",
    "    # Update layout\n",
    "    fig_qc.update_layout(\n",
    "        title_text=f'Saccade Amplitude QC: Distributions, Correlations, and Segments (Outlier Detection, {get_eye_label(video_key)})',\n",
    "        height=1200,\n",
    "        showlegend=False\n",
    "    )\n",
    "\n",
    "    # Update axes labels\n",
    "    fig_qc.update_xaxes(title_text=\"Amplitude (px)\", row=1, col=1)\n",
    "    fig_qc.update_xaxes(title_text=\"Amplitude (px)\", row=1, col=2)\n",
    "    fig_qc.update_xaxes(title_text=\"Duration (s)\", row=2, col=1)\n",
    "    fig_qc.update_xaxes(title_text=\"Duration (s)\", row=2, col=2)\n",
    "    fig_qc.update_xaxes(title_text=\"Time relative to threshold crossing (s)\", row=3, col=1)\n",
    "    fig_qc.update_xaxes(title_text=\"Time relative to threshold crossing (s)\", row=3, col=2)\n",
    "    fig_qc.update_yaxes(title_text=\"Count\", row=1, col=1)\n",
    "    fig_qc.update_yaxes(title_text=\"Count\", row=1, col=2)\n",
    "    fig_qc.update_yaxes(title_text=\"Amplitude (px)\", row=2, col=1)\n",
    "    fig_qc.update_yaxes(title_text=\"Amplitude (px)\", row=2, col=2)\n",
    "    fig_qc.update_yaxes(title_text=\"Position (baselined, px)\", row=3, col=1)\n",
    "    fig_qc.update_yaxes(title_text=\"Position (baselined, px)\", row=3, col=2)\n",
    "\n",
    "    fig_qc.show()\n",
    "\n",
    "    # Print correlation statistics\n",
    "    if debug:\n",
    "        print(\"\\n=== SACCADE AMPLITUDE-DURATION CORRELATION ===\\n\")\n",
    "        if upward_saccades_df is not None and len(upward_saccades_df) > 0:\n",
    "            print(f\"{get_eye_label(video_key)} saccades (n={len(upward_saccades_df)}):\")\n",
    "            print(f\"  Correlation (amplitude vs duration): {corr_up:.3f}\")\n",
    "            print(f\"  Mean amplitude: {upward_saccades_df['amplitude'].mean():.2f} px\")\n",
    "            print(f\"  Mean duration: {upward_saccades_df['duration'].mean():.3f} s\")\n",
    "            print(f\"  Amp range: {upward_saccades_df['amplitude'].min():.2f} - {upward_saccades_df['amplitude'].max():.2f} px\")\n",
    "\n",
    "        if downward_saccades_df is not None and len(downward_saccades_df) > 0:\n",
    "            print(f\"\\n{get_eye_label(video_key)} saccades (n={len(downward_saccades_df)}):\")\n",
    "            print(f\"  Correlation (amplitude vs duration): {corr_down:.3f}\")\n",
    "            print(f\"  Mean amplitude: {downward_saccades_df['amplitude'].mean():.2f} px\")\n",
    "            print(f\"  Mean duration: {downward_saccades_df['duration'].mean():.3f} s\")\n",
    "            print(f\"  Amp range: {downward_saccades_df['amplitude'].min():.2f} - {downward_saccades_df['amplitude'].max():.2f} px\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ccf47e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# VISUALIZE DETECTED SACCADES (Adaptive Method)\n",
    "#-------------------------------------------------------------------------------\n",
    "# Create overlay plot showing detected saccades with duration lines and peak arrows\n",
    "if plot_saccade_detection_QC:\n",
    "    for video_key, res in saccade_results.items():\n",
    "        dir_map = get_direction_map_for_video(video_key)\n",
    "        label_up = dir_map['upward']\n",
    "        label_down = dir_map['downward']\n",
    "\n",
    "        upward_saccades_df = res['upward_saccades_df']\n",
    "        downward_saccades_df = res['downward_saccades_df']\n",
    "        peri_saccades = res['peri_saccades']   \n",
    "        upward_segments = res['upward_segments']\n",
    "        downward_segments = res['downward_segments']\n",
    "        # Any other variables you need...\n",
    "\n",
    "        fig = make_subplots(\n",
    "            rows=2, cols=1,\n",
    "            shared_xaxes=True,\n",
    "            vertical_spacing=0.1,\n",
    "            subplot_titles=('X Position (px)', 'Velocity (px/s) with Detected Saccades')\n",
    "        )\n",
    "\n",
    "        # Add smoothed X position to the first subplot\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=res['df']['Seconds'],\n",
    "                y=res['df']['X_smooth'],\n",
    "                mode='lines',\n",
    "                name='Smoothed X',\n",
    "                line=dict(color='blue', width=2)\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "\n",
    "        # Add smoothed velocity to the second subplot\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=res['df']['Seconds'],\n",
    "                y=res['df']['vel_x_smooth'],\n",
    "                mode='lines',\n",
    "                name='Smoothed Velocity',\n",
    "                line=dict(color='red', width=2)\n",
    "            ),\n",
    "            row=2, col=1\n",
    "        )\n",
    "\n",
    "        # Add adaptive threshold lines for reference\n",
    "        fig.add_hline(\n",
    "            y=res['vel_thresh'],\n",
    "            line_dash=\"dash\",\n",
    "            line_color=\"green\",\n",
    "            opacity=0.5,\n",
    "            annotation_text=f\"Adaptive threshold (¬±{res['vel_thresh']:.0f} px/s)\",\n",
    "            row=2, col=1\n",
    "        )\n",
    "\n",
    "        fig.add_hline(\n",
    "            y=-res['vel_thresh'],\n",
    "            line_dash=\"dash\",\n",
    "            line_color=\"green\",\n",
    "            opacity=0.5,\n",
    "            row=2, col=1\n",
    "        )\n",
    "\n",
    "        # Set offset for saccade indicator lines (above the trace for upward, below for downward)\n",
    "        vel_max = res['df']['vel_x_smooth'].max()\n",
    "        vel_min = res['df']['vel_x_smooth'].min()\n",
    "        vel_range = vel_max - vel_min\n",
    "        line_offset = vel_range * 0.15  # 15% of velocity range\n",
    "\n",
    "        # Plot upward saccades with duration lines and peak arrows\n",
    "        if len(upward_saccades_df) > 0:\n",
    "            for idx, row in upward_saccades_df.iterrows():\n",
    "                start_time = row['start_time']\n",
    "                end_time = row['end_time']\n",
    "                peak_time = row['time']\n",
    "                peak_velocity = row['velocity']\n",
    "                \n",
    "                # Draw horizontal line spanning the saccade duration\n",
    "                # Line is positioned above the velocity trace\n",
    "                y_line_pos = vel_max + line_offset\n",
    "                \n",
    "                fig.add_shape(\n",
    "                    type=\"line\",\n",
    "                    x0=start_time, y0=y_line_pos,\n",
    "                    x1=end_time, y1=y_line_pos,\n",
    "                    line=dict(color='green', width=3),\n",
    "                    row=2, col=1\n",
    "                )\n",
    "                \n",
    "                # Add arrow annotation at the peak position pointing to the actual peak velocity\n",
    "                # Arrow points from the line to the peak velocity value on the velocity trace\n",
    "                y_arrow_start = y_line_pos\n",
    "                y_arrow_end = peak_velocity\n",
    "                \n",
    "                fig.add_annotation(\n",
    "                    x=peak_time,\n",
    "                    y=y_arrow_start,\n",
    "                    ax=0,\n",
    "                    ay=y_arrow_end - y_arrow_start,  # arrow points to peak velocity\n",
    "                    arrowhead=2,  # filled arrowhead\n",
    "                    arrowsize=2,\n",
    "                    arrowwidth=2,\n",
    "                    arrowcolor='green',\n",
    "                    row=2, col=1,\n",
    "                    showarrow=True\n",
    "                )\n",
    "            \n",
    "            # Add legend entry for upward saccades\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=[None],\n",
    "                    y=[None],\n",
    "                    mode='markers',\n",
    "                    name=f'{label_up} Saccades (duration lines)',\n",
    "                    marker=dict(symbol='line-ns', size=15, color='green', line=dict(width=3))\n",
    "                ),\n",
    "                row=2, col=1\n",
    "            )\n",
    "\n",
    "        # Plot downward saccades with duration lines and peak arrows\n",
    "        if len(downward_saccades_df) > 0:\n",
    "            for idx, row in downward_saccades_df.iterrows():\n",
    "                start_time = row['start_time']\n",
    "                end_time = row['end_time']\n",
    "                peak_time = row['time']\n",
    "                peak_velocity = row['velocity']\n",
    "                \n",
    "                # Draw horizontal line spanning the saccade duration\n",
    "                # Line is positioned below the velocity trace\n",
    "                y_line_pos = vel_min - line_offset\n",
    "                \n",
    "                fig.add_shape(\n",
    "                    type=\"line\",\n",
    "                    x0=start_time, y0=y_line_pos,\n",
    "                    x1=end_time, y1=y_line_pos,\n",
    "                    line=dict(color='purple', width=3),\n",
    "                    row=2, col=1\n",
    "                )\n",
    "                \n",
    "                # Add arrow annotation at the peak position pointing to the actual peak velocity\n",
    "                # Arrow points from the line to the peak velocity value on the velocity trace\n",
    "                y_arrow_start = y_line_pos\n",
    "                y_arrow_end = peak_velocity\n",
    "                \n",
    "                fig.add_annotation(\n",
    "                    x=peak_time,\n",
    "                    y=y_arrow_start,\n",
    "                    ax=0,\n",
    "                    ay=y_arrow_end - y_arrow_start,  # arrow points to peak velocity\n",
    "                    arrowhead=2,  # filled arrowhead\n",
    "                    arrowsize=2,\n",
    "                    arrowwidth=2,\n",
    "                    arrowcolor='purple',\n",
    "                    row=2, col=1,\n",
    "                    showarrow=True\n",
    "                )\n",
    "            \n",
    "            # Add legend entry for downward saccades\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=[None],\n",
    "                    y=[None],\n",
    "                    mode='markers',\n",
    "                    name=f'{label_down} Saccades (duration lines)',\n",
    "                    marker=dict(symbol='line-ns', size=15, color='purple', line=dict(width=3))\n",
    "                ),\n",
    "                row=2, col=1\n",
    "            )\n",
    "\n",
    "        # Update layout\n",
    "        fig.update_layout(\n",
    "            title=f'Detected Saccades ({get_eye_label(video_key)}): Duration Lines + Peak Arrows (QC Visualization)',\n",
    "            height=600,\n",
    "            showlegend=True,\n",
    "            legend=dict(x=0.01, y=0.99)\n",
    "        )\n",
    "\n",
    "        # Update axes\n",
    "        fig.update_xaxes(title_text=\"Time (s)\", row=2, col=1)\n",
    "        fig.update_yaxes(title_text=\"X Position (px)\", row=1, col=1)\n",
    "        fig.update_yaxes(title_text=\"Velocity (px/s)\", row=2, col=1)\n",
    "\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7442c9a3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# VISUALIZE AND ANALYZE SACCADE CLASSIFICATION (Orienting vs Compensatory)\n",
    "#-------------------------------------------------------------------------------\n",
    "# Create validation plots and statistical comparisons for saccade classification\n",
    "\n",
    "for video_key, res in saccade_results.items():\n",
    "    dir_map = get_direction_map_for_video(video_key)\n",
    "    label_up = dir_map['upward']\n",
    "    label_down = dir_map['downward']\n",
    "    \n",
    "    all_saccades_df = res.get('all_saccades_df', pd.DataFrame())\n",
    "    \n",
    "    if len(all_saccades_df) == 0:\n",
    "        print(f\"\\n‚ö†Ô∏è No saccades found for {get_eye_label(video_key)}\")\n",
    "        continue\n",
    "    \n",
    "    # Check if classification was performed\n",
    "    if 'saccade_type' not in all_saccades_df.columns:\n",
    "        print(f\"\\n‚ö†Ô∏è Classification not performed for {get_eye_label(video_key)}\")\n",
    "        continue\n",
    "    \n",
    "    orienting_saccades = all_saccades_df[all_saccades_df['saccade_type'] == 'orienting']\n",
    "    compensatory_saccades = all_saccades_df[all_saccades_df['saccade_type'] == 'compensatory']\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"CLASSIFICATION ANALYSIS: {get_eye_label(video_key)}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Statistical comparisons\n",
    "    from scipy import stats\n",
    "    \n",
    "    print(f\"\\nüìä Statistical Comparisons:\")\n",
    "    print(f\"  Orienting saccades: {len(orienting_saccades)}\")\n",
    "    print(f\"  Compensatory saccades: {len(compensatory_saccades)}\")\n",
    "    \n",
    "    if len(orienting_saccades) > 0 and len(compensatory_saccades) > 0:\n",
    "        # Amplitude comparison\n",
    "        orienting_amps = orienting_saccades['amplitude'].values\n",
    "        compensatory_amps = compensatory_saccades['amplitude'].values\n",
    "        amp_stat, amp_p = stats.mannwhitneyu(orienting_amps, compensatory_amps, alternative='two-sided')\n",
    "        print(f\"\\n  Amplitude (px):\")\n",
    "        print(f\"    Orienting: {orienting_amps.mean():.2f} ¬± {orienting_amps.std():.2f} (median: {np.median(orienting_amps):.2f})\")\n",
    "        print(f\"    Compensatory: {compensatory_amps.mean():.2f} ¬± {compensatory_amps.std():.2f} (median: {np.median(compensatory_amps):.2f})\")\n",
    "        print(f\"    Mann-Whitney U test: U={amp_stat:.1f}, p={amp_p:.4f}\")\n",
    "        \n",
    "        # Duration comparison\n",
    "        orienting_durs = orienting_saccades['duration'].values\n",
    "        compensatory_durs = compensatory_saccades['duration'].values\n",
    "        dur_stat, dur_p = stats.mannwhitneyu(orienting_durs, compensatory_durs, alternative='two-sided')\n",
    "        print(f\"\\n  Duration (s):\")\n",
    "        print(f\"    Orienting: {orienting_durs.mean():.3f} ¬± {orienting_durs.std():.3f} (median: {np.median(orienting_durs):.3f})\")\n",
    "        print(f\"    Compensatory: {compensatory_durs.mean():.3f} ¬± {compensatory_durs.std():.3f} (median: {np.median(compensatory_durs):.3f})\")\n",
    "        print(f\"    Mann-Whitney U test: U={dur_stat:.1f}, p={dur_p:.4f}\")\n",
    "        \n",
    "        # Pre-saccade velocity comparison\n",
    "        orienting_pre_vel = orienting_saccades['pre_saccade_mean_velocity'].values\n",
    "        compensatory_pre_vel = compensatory_saccades['pre_saccade_mean_velocity'].values\n",
    "        pre_vel_stat, pre_vel_p = stats.mannwhitneyu(orienting_pre_vel, compensatory_pre_vel, alternative='two-sided')\n",
    "        print(f\"\\n  Pre-saccade velocity (px/s):\")\n",
    "        print(f\"    Orienting: {orienting_pre_vel.mean():.2f} ¬± {orienting_pre_vel.std():.2f} (median: {np.median(orienting_pre_vel):.2f})\")\n",
    "        print(f\"    Compensatory: {compensatory_pre_vel.mean():.2f} ¬± {compensatory_pre_vel.std():.2f} (median: {np.median(compensatory_pre_vel):.2f})\")\n",
    "        print(f\"    Mann-Whitney U test: U={pre_vel_stat:.1f}, p={pre_vel_p:.4f}\")\n",
    "        \n",
    "        # Pre-saccade drift comparison\n",
    "        orienting_pre_drift = orienting_saccades['pre_saccade_position_drift'].values\n",
    "        compensatory_pre_drift = compensatory_saccades['pre_saccade_position_drift'].values\n",
    "        pre_drift_stat, pre_drift_p = stats.mannwhitneyu(orienting_pre_drift, compensatory_pre_drift, alternative='two-sided')\n",
    "        print(f\"\\n  Pre-saccade position drift (px):\")\n",
    "        print(f\"    Orienting: {orienting_pre_drift.mean():.2f} ¬± {orienting_pre_drift.std():.2f} (median: {np.median(orienting_pre_drift):.2f})\")\n",
    "        print(f\"    Compensatory: {compensatory_pre_drift.mean():.2f} ¬± {compensatory_pre_drift.std():.2f} (median: {np.median(compensatory_pre_drift):.2f})\")\n",
    "        print(f\"    Mann-Whitney U test: U={pre_drift_stat:.1f}, p={pre_drift_p:.4f}\")\n",
    "        \n",
    "        # Post-saccade variance comparison\n",
    "        orienting_post_var = orienting_saccades['post_saccade_position_variance'].values\n",
    "        compensatory_post_var = compensatory_saccades['post_saccade_position_variance'].values\n",
    "        post_var_stat, post_var_p = stats.mannwhitneyu(orienting_post_var, compensatory_post_var, alternative='two-sided')\n",
    "        print(f\"\\n  Post-saccade position variance (px¬≤):\")\n",
    "        print(f\"    Orienting: {orienting_post_var.mean():.2f} ¬± {orienting_post_var.std():.2f} (median: {np.median(orienting_post_var):.2f})\")\n",
    "        print(f\"    Compensatory: {compensatory_post_var.mean():.2f} ¬± {compensatory_post_var.std():.2f} (median: {np.median(compensatory_post_var):.2f})\")\n",
    "        print(f\"    Mann-Whitney U test: U={post_var_stat:.1f}, p={post_var_p:.4f}\")\n",
    "        \n",
    "        # Bout size for compensatory saccades\n",
    "        if len(compensatory_saccades) > 0:\n",
    "            bout_sizes = compensatory_saccades['bout_size'].values\n",
    "            print(f\"\\n  Bout size (compensatory saccades only):\")\n",
    "            print(f\"    Mean: {bout_sizes.mean():.2f} ¬± {bout_sizes.std():.2f} saccades\")\n",
    "            print(f\"    Range: {bout_sizes.min():.0f} - {bout_sizes.max():.0f} saccades\")\n",
    "            print(f\"    Median: {np.median(bout_sizes):.0f} saccades\")\n",
    "    else:\n",
    "        print(f\"  ‚ö†Ô∏è Cannot perform statistical comparisons - need both types present\")\n",
    "    \n",
    "    # Create visualization figure\n",
    "    fig_class = make_subplots(\n",
    "        rows=2, cols=3,\n",
    "        subplot_titles=(\n",
    "            'Amplitude Distribution',\n",
    "            'Duration Distribution',\n",
    "            'Pre-saccade Velocity Distribution',\n",
    "            'Pre-saccade Position Drift',\n",
    "            'Post-saccade Position Variance',\n",
    "            'Bout Size Distribution (Compensatory)'\n",
    "        ),\n",
    "        vertical_spacing=0.12,\n",
    "        horizontal_spacing=0.1\n",
    "    )\n",
    "    \n",
    "    # Row 1, Col 1: Amplitude distributions\n",
    "    if len(orienting_saccades) > 0:\n",
    "        fig_class.add_trace(\n",
    "            go.Histogram(\n",
    "                x=orienting_saccades['amplitude'],\n",
    "                nbinsx=30,\n",
    "                name='Orienting',\n",
    "                marker_color='blue',\n",
    "                opacity=0.6\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "    if len(compensatory_saccades) > 0:\n",
    "        fig_class.add_trace(\n",
    "            go.Histogram(\n",
    "                x=compensatory_saccades['amplitude'],\n",
    "                nbinsx=30,\n",
    "                name='Compensatory',\n",
    "                marker_color='orange',\n",
    "                opacity=0.6\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "    \n",
    "    # Row 1, Col 2: Duration distributions\n",
    "    if len(orienting_saccades) > 0:\n",
    "        fig_class.add_trace(\n",
    "            go.Histogram(\n",
    "                x=orienting_saccades['duration'],\n",
    "                nbinsx=30,\n",
    "                name='Orienting',\n",
    "                marker_color='blue',\n",
    "                opacity=0.6,\n",
    "                showlegend=False\n",
    "            ),\n",
    "            row=1, col=2\n",
    "        )\n",
    "    if len(compensatory_saccades) > 0:\n",
    "        fig_class.add_trace(\n",
    "            go.Histogram(\n",
    "                x=compensatory_saccades['duration'],\n",
    "                nbinsx=30,\n",
    "                name='Compensatory',\n",
    "                marker_color='orange',\n",
    "                opacity=0.6,\n",
    "                showlegend=False\n",
    "            ),\n",
    "            row=1, col=2\n",
    "        )\n",
    "    \n",
    "    # Row 1, Col 3: Pre-saccade velocity distributions\n",
    "    if len(orienting_saccades) > 0:\n",
    "        fig_class.add_trace(\n",
    "            go.Histogram(\n",
    "                x=orienting_saccades['pre_saccade_mean_velocity'],\n",
    "                nbinsx=30,\n",
    "                name='Orienting',\n",
    "                marker_color='blue',\n",
    "                opacity=0.6,\n",
    "                showlegend=False\n",
    "            ),\n",
    "            row=1, col=3\n",
    "        )\n",
    "    if len(compensatory_saccades) > 0:\n",
    "        fig_class.add_trace(\n",
    "            go.Histogram(\n",
    "                x=compensatory_saccades['pre_saccade_mean_velocity'],\n",
    "                nbinsx=30,\n",
    "                name='Compensatory',\n",
    "                marker_color='orange',\n",
    "                opacity=0.6,\n",
    "                showlegend=False\n",
    "            ),\n",
    "            row=1, col=3\n",
    "        )\n",
    "    \n",
    "    # Row 2, Col 1: Pre-saccade drift distributions\n",
    "    if len(orienting_saccades) > 0:\n",
    "        fig_class.add_trace(\n",
    "            go.Histogram(\n",
    "                x=orienting_saccades['pre_saccade_position_drift'],\n",
    "                nbinsx=30,\n",
    "                name='Orienting',\n",
    "                marker_color='blue',\n",
    "                opacity=0.6,\n",
    "                showlegend=False\n",
    "            ),\n",
    "            row=2, col=1\n",
    "        )\n",
    "    if len(compensatory_saccades) > 0:\n",
    "        fig_class.add_trace(\n",
    "            go.Histogram(\n",
    "                x=compensatory_saccades['pre_saccade_position_drift'],\n",
    "                nbinsx=30,\n",
    "                name='Compensatory',\n",
    "                marker_color='orange',\n",
    "                opacity=0.6,\n",
    "                showlegend=False\n",
    "            ),\n",
    "            row=2, col=1\n",
    "        )\n",
    "    \n",
    "    # Row 2, Col 2: Post-saccade variance distributions\n",
    "    if len(orienting_saccades) > 0:\n",
    "        fig_class.add_trace(\n",
    "            go.Histogram(\n",
    "                x=orienting_saccades['post_saccade_position_variance'],\n",
    "                nbinsx=30,\n",
    "                name='Orienting',\n",
    "                marker_color='blue',\n",
    "                opacity=0.6,\n",
    "                showlegend=False\n",
    "            ),\n",
    "            row=2, col=2\n",
    "        )\n",
    "    if len(compensatory_saccades) > 0:\n",
    "        fig_class.add_trace(\n",
    "            go.Histogram(\n",
    "                x=compensatory_saccades['post_saccade_position_variance'],\n",
    "                nbinsx=30,\n",
    "                name='Compensatory',\n",
    "                marker_color='orange',\n",
    "                opacity=0.6,\n",
    "                showlegend=False\n",
    "            ),\n",
    "            row=2, col=2\n",
    "        )\n",
    "    \n",
    "    # Row 2, Col 3: Bout size distribution (compensatory only)\n",
    "    if len(compensatory_saccades) > 0:\n",
    "        fig_class.add_trace(\n",
    "            go.Histogram(\n",
    "                x=compensatory_saccades['bout_size'],\n",
    "                nbinsx=20,\n",
    "                name='Compensatory Bout Size',\n",
    "                marker_color='orange',\n",
    "                opacity=0.6,\n",
    "                showlegend=False\n",
    "            ),\n",
    "            row=2, col=3\n",
    "        )\n",
    "    else:\n",
    "        # Add empty trace to maintain layout\n",
    "        fig_class.add_trace(\n",
    "            go.Histogram(x=[], name='No compensatory saccades'),\n",
    "            row=2, col=3\n",
    "        )\n",
    "    \n",
    "    # Update layout\n",
    "    fig_class.update_layout(\n",
    "        title_text=f'Saccade Classification Analysis: Orienting vs Compensatory ({get_eye_label(video_key)})',\n",
    "        height=800,\n",
    "        showlegend=True,\n",
    "        legend=dict(x=0.02, y=0.98)\n",
    "    )\n",
    "    \n",
    "    # Update axes labels\n",
    "    fig_class.update_xaxes(title_text=\"Amplitude (px)\", row=1, col=1)\n",
    "    fig_class.update_xaxes(title_text=\"Duration (s)\", row=1, col=2)\n",
    "    fig_class.update_xaxes(title_text=\"Velocity (px/s)\", row=1, col=3)\n",
    "    fig_class.update_xaxes(title_text=\"Drift (px)\", row=2, col=1)\n",
    "    fig_class.update_xaxes(title_text=\"Variance (px¬≤)\", row=2, col=2)\n",
    "    fig_class.update_xaxes(title_text=\"Bout Size (saccades)\", row=2, col=3)\n",
    "    \n",
    "    fig_class.update_yaxes(title_text=\"Count\", row=1, col=1)\n",
    "    fig_class.update_yaxes(title_text=\"Count\", row=1, col=2)\n",
    "    fig_class.update_yaxes(title_text=\"Count\", row=1, col=3)\n",
    "    fig_class.update_yaxes(title_text=\"Count\", row=2, col=1)\n",
    "    fig_class.update_yaxes(title_text=\"Count\", row=2, col=2)\n",
    "    fig_class.update_yaxes(title_text=\"Count\", row=2, col=3)\n",
    "    \n",
    "    fig_class.show()\n",
    "    \n",
    "    # Time series visualization with classification\n",
    "    fig_ts = make_subplots(\n",
    "        rows=2, cols=1,\n",
    "        shared_xaxes=True,\n",
    "        vertical_spacing=0.1,\n",
    "        subplot_titles=('X Position (px)', 'Velocity (px/s) with Classified Saccades')\n",
    "    )\n",
    "    \n",
    "    # Add position trace\n",
    "    fig_ts.add_trace(\n",
    "        go.Scatter(\n",
    "            x=res['df']['Seconds'],\n",
    "            y=res['df']['X_smooth'],\n",
    "            mode='lines',\n",
    "            name='Smoothed X',\n",
    "            line=dict(color='blue', width=2)\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Add velocity trace\n",
    "    fig_ts.add_trace(\n",
    "        go.Scatter(\n",
    "            x=res['df']['Seconds'],\n",
    "            y=res['df']['vel_x_smooth'],\n",
    "            mode='lines',\n",
    "            name='Smoothed Velocity',\n",
    "            line=dict(color='red', width=2)\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # Add adaptive threshold lines\n",
    "    fig_ts.add_hline(\n",
    "        y=res['vel_thresh'],\n",
    "        line_dash=\"dash\",\n",
    "        line_color=\"green\",\n",
    "        opacity=0.5,\n",
    "        annotation_text=f\"Adaptive threshold (¬±{res['vel_thresh']:.0f} px/s)\",\n",
    "        row=2, col=1\n",
    "    )\n",
    "    fig_ts.add_hline(\n",
    "        y=-res['vel_thresh'],\n",
    "        line_dash=\"dash\",\n",
    "        line_color=\"green\",\n",
    "        opacity=0.5,\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # Calculate offset for saccade indicator lines\n",
    "    vel_max = res['df']['vel_x_smooth'].max()\n",
    "    vel_min = res['df']['vel_x_smooth'].min()\n",
    "    vel_range = vel_max - vel_min\n",
    "    line_offset = vel_range * 0.15\n",
    "    \n",
    "    # Plot orienting saccades (blue)\n",
    "    orienting_in_df = all_saccades_df[all_saccades_df['saccade_type'] == 'orienting']\n",
    "    if len(orienting_in_df) > 0:\n",
    "        for idx, row in orienting_in_df.iterrows():\n",
    "            start_time = row['start_time']\n",
    "            end_time = row['end_time']\n",
    "            peak_time = row['time']\n",
    "            peak_velocity = row['velocity']\n",
    "            \n",
    "            # Draw horizontal line\n",
    "            y_line_pos = vel_max + line_offset\n",
    "            fig_ts.add_shape(\n",
    "                type=\"line\",\n",
    "                x0=start_time, y0=y_line_pos,\n",
    "                x1=end_time, y1=y_line_pos,\n",
    "                line=dict(color='blue', width=3),\n",
    "                row=2, col=1\n",
    "            )\n",
    "            \n",
    "            # Add arrow\n",
    "            fig_ts.add_annotation(\n",
    "                x=peak_time,\n",
    "                y=y_line_pos,\n",
    "                ax=0,\n",
    "                ay=peak_velocity - y_line_pos,\n",
    "                arrowhead=2,\n",
    "                arrowsize=2,\n",
    "                arrowwidth=2,\n",
    "                arrowcolor='blue',\n",
    "                row=2, col=1,\n",
    "                showarrow=True\n",
    "            )\n",
    "        \n",
    "        # Legend entry\n",
    "        fig_ts.add_trace(\n",
    "            go.Scatter(\n",
    "                x=[None], y=[None],\n",
    "                mode='markers',\n",
    "                name='Orienting Saccades',\n",
    "                marker=dict(symbol='line-ns', size=15, color='blue', line=dict(width=3))\n",
    "            ),\n",
    "            row=2, col=1\n",
    "        )\n",
    "    \n",
    "    # Plot compensatory saccades (orange)\n",
    "    compensatory_in_df = all_saccades_df[all_saccades_df['saccade_type'] == 'compensatory']\n",
    "    if len(compensatory_in_df) > 0:\n",
    "        for idx, row in compensatory_in_df.iterrows():\n",
    "            start_time = row['start_time']\n",
    "            end_time = row['end_time']\n",
    "            peak_time = row['time']\n",
    "            peak_velocity = row['velocity']\n",
    "            \n",
    "            # Draw horizontal line (below velocity trace)\n",
    "            y_line_pos = vel_min - line_offset\n",
    "            fig_ts.add_shape(\n",
    "                type=\"line\",\n",
    "                x0=start_time, y0=y_line_pos,\n",
    "                x1=end_time, y1=y_line_pos,\n",
    "                line=dict(color='orange', width=3),\n",
    "                row=2, col=1\n",
    "            )\n",
    "            \n",
    "            # Add arrow\n",
    "            fig_ts.add_annotation(\n",
    "                x=peak_time,\n",
    "                y=y_line_pos,\n",
    "                ax=0,\n",
    "                ay=peak_velocity - y_line_pos,\n",
    "                arrowhead=2,\n",
    "                arrowsize=2,\n",
    "                arrowwidth=2,\n",
    "                arrowcolor='orange',\n",
    "                row=2, col=1,\n",
    "                showarrow=True\n",
    "            )\n",
    "        \n",
    "        # Legend entry\n",
    "        fig_ts.add_trace(\n",
    "            go.Scatter(\n",
    "                x=[None], y=[None],\n",
    "                mode='markers',\n",
    "                name='Compensatory Saccades',\n",
    "                marker=dict(symbol='line-ns', size=15, color='orange', line=dict(width=3))\n",
    "            ),\n",
    "            row=2, col=1\n",
    "        )\n",
    "    \n",
    "    # Update layout\n",
    "    fig_ts.update_layout(\n",
    "        title=f'Time Series with Saccade Classification ({get_eye_label(video_key)})<br><sub>Blue: Orienting, Orange: Compensatory</sub>',\n",
    "        height=600,\n",
    "        showlegend=True,\n",
    "        legend=dict(x=0.01, y=0.99)\n",
    "    )\n",
    "    \n",
    "    # Update axes\n",
    "    fig_ts.update_xaxes(title_text=\"Time (s)\", row=2, col=1)\n",
    "    fig_ts.update_yaxes(title_text=\"X Position (px)\", row=1, col=1)\n",
    "    fig_ts.update_yaxes(title_text=\"Velocity (px/s)\", row=2, col=1)\n",
    "    \n",
    "    fig_ts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ec6686",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "aeon",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
