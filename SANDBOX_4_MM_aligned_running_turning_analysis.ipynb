{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Dict, Iterable, List, Optional, Tuple\n",
    "\n",
    "import math\n",
    "import os\n",
    "import warnings\n",
    "from collections import OrderedDict\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "try:\n",
    "    from scipy import stats\n",
    "except Exception:  # pragma: no cover - SciPy is optional\n",
    "    stats = None\n",
    "    warnings.warn(\n",
    "        \"SciPy is not available; paired t-test p-values will be reported as NaN.\",\n",
    "        RuntimeWarning,\n",
    "    )\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"figure.dpi\": 120,\n",
    "    \"axes.titlesize\": 12,\n",
    "    \"axes.labelsize\": 11,\n",
    "})\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "# ----------------------------------------------------------------------\n",
    "#HERE: COMMENT OUT PATHS YOU DON'T WANT TO RUN. For example, run analysis only for experimental day types. \n",
    "# Will automatically separate halt and no halt\n",
    "# Root directories containing cohort-level folders with aligned data.\n",
    "DATA_DIRS: List[Path] = [\n",
    "    # Path(\"/Volumes/RanczLab2/DATA_NEW/Cohort1_rotation/Visual_mismatch_day4/\").expanduser(),\n",
    "    # Path(\"/Volumes/RanczLab2/DATA_NEW/Cohort3_rotation/Visual_mismatch_day4/\").expanduser(),\n",
    "    Path(\"/Volumes/RanczLab2/DATA_NEW/Cohort1_rotation/Visual_mismatch_day3\").expanduser(),\n",
    "    Path(\"/Volumes/RanczLab2/DATA_NEW/Cohort3_rotation/Visual_mismatch_day3/\").expanduser(),\n",
    "]\n",
    "\n",
    "OUTPUT_SUBDIR_NAME = \"turning_analysis\"\n",
    "\n",
    "# Diagnostic settings\n",
    "ENABLE_DECAY_DIAGNOSTICS = False  # Set True to run detailed diagnostics on decay fit failures\n",
    "MAX_DIAGNOSTIC_EXAMPLES = 3       # Maximum number of failed fits to diagnose in detail\n",
    "\n",
    "# Event file suffixes that encode turn direction in their names.\n",
    "EVENT_SUFFIXES: List[str] = [\n",
    "    \"_Apply halt_2s_right_turns_baselined_data.csv\",\n",
    "    \"_Apply halt_2s_left_turns_baselined_data.csv\",\n",
    "    \"_No halt_right_turns_baselined_data.csv\",\n",
    "    \"_No halt_left_turns_baselined_data.csv\",\n",
    "]\n",
    "\n",
    "# Optional subset of mice to analyse (use [] to include every mouse found).\n",
    "SELECTED_MICE: List[str] = []\n",
    "\n",
    "# Data columns\n",
    "TIME_COLUMN = \"Time (s)\"\n",
    "VELOCITY_COLUMN = \"Motor_Velocity\"  # Motor turning velocity (deg/s)\n",
    "RUNNING_COLUMN = \"Velocity_0X_Baseline\"      # Forward running velocity (m/s)\n",
    "\n",
    "# Eye tracking columns (verify these match your CSV column names!)\n",
    "SACCADE_COLUMN = \"saccade_probability_eye1\"  # Saccade probability - NO SCALING APPLIED, uses raw values from CSV\n",
    "PUPIL_COLUMN = \"Pupil.Diameter_eye1_Baseline\"     # Pupil diameter (COMBINED across all turns)\n",
    "EYE_POSITION_COLUMN = \"Ellipse.Center.X_eye1_Baseline\"  # Eye horizontal position (SEPARATED by turn direction)\n",
    "\n",
    "# If saccade probability values are very small (< 0.01), check:\n",
    "# 1. Is this the correct column name?\n",
    "# 2. Are values per-frame probability (naturally small) vs aggregate probability?\n",
    "# 3. Set SHOW_AVAILABLE_COLUMNS = True below to see all available columns\n",
    "\n",
    "# ==================================================================================\n",
    "# TIME WINDOW CONFIGURATION\n",
    "# ==================================================================================\n",
    "# All windows are (start, end) tuples in seconds, relative to turn onset (t=0)\n",
    "# \n",
    "# USAGE GUIDE:\n",
    "# - BASELINE_WINDOW: Pre-halt baseline for direction change detection & normalization\n",
    "# - ANALYSIS_WINDOW_MEAN_PEAK: ⚙️ PRIMARY CONFIGURABLE WINDOW ⚙️\n",
    "#   Controls the analysis window for:\n",
    "#   • Post-halt mean velocity (turning & running)\n",
    "#   • Peak velocity (turning & running)\n",
    "#   • AUC calculation\n",
    "#   Default: (0.0, 1.0) = analyze the first 1 second after halt onset\n",
    "# - EXTENDED_RESPONSE_WINDOW: For mean absolute velocity (combines left/right turns)\n",
    "# - FULL_RESPONSE_WINDOW: For exponential decay fitting (longer window needed)\n",
    "# - TEMPORAL_DYNAMICS_WINDOWS: Fine-grained windows for temporal pattern analysis\n",
    "# ==================================================================================\n",
    "\n",
    "# Baseline (pre-turn) window\n",
    "BASELINE_WINDOW = (-2.0, 0.0)  # Used for: direction detection, baseline normalization\n",
    "\n",
    "# Post-turn response windows (used for primary metrics)\n",
    "# ⚙️ USER CONFIGURABLE: Change this window to control mean and peak velocity analysis\n",
    "ANALYSIS_WINDOW_MEAN_PEAK = (0.0, 2.0)  # Window for post-halt mean & peak velocity analysis\n",
    "\n",
    "EARLY_RESPONSE_WINDOW = ANALYSIS_WINDOW_MEAN_PEAK  # Used for: peak velocity, AUC, post-turn mean\n",
    "EXTENDED_RESPONSE_WINDOW = (0.0, 2.0)   # Used for: mean absolute velocity (combines left/right)\n",
    "FULL_RESPONSE_WINDOW = (0.0, 3.0)       # Used for: exponential decay fitting\n",
    "\n",
    "# Fine-grained temporal dynamics windows (used for alternative metrics)\n",
    "TEMPORAL_EARLY_WINDOW = (0.0, 0.5)   # Immediate response phase\n",
    "TEMPORAL_MID_WINDOW = (0.5, 1.0)     # Mid response phase\n",
    "TEMPORAL_LATE_WINDOW = (1.0, 2.0)    # Sustained response phase\n",
    "TEMPORAL_FULL_WINDOW = (0.0, 2.0)    # Full temporal analysis window\n",
    "\n",
    "# Legacy compatibility (kept for function signatures, but values derived from above)\n",
    "PRE_WINDOW = BASELINE_WINDOW           # Alias for backward compatibility\n",
    "POST_WINDOW = EARLY_RESPONSE_WINDOW   # Alias for backward compatibility\n",
    "PEAK_WINDOW = EARLY_RESPONSE_WINDOW   # Peak finding window (same as early response)\n",
    "AUC_WINDOW = EARLY_RESPONSE_WINDOW    # Primary AUC window (same as early response)\n",
    "DECAY_FIT_WINDOW = FULL_RESPONSE_WINDOW  # Decay fitting window\n",
    "\n",
    "# Other thresholds\n",
    "ZERO_THRESHOLD = 1e-2      # Threshold (deg/s) to treat near-zero values as zero\n",
    "LATENCY_FRACTION = 0.5     # [DEPRECATED] Fraction of peak for latency calculation\n",
    "\n",
    "# Mapping from inferred turn direction to expected velocity sign after time 0.\n",
    "# Here we expect the actual turn to be opposite the label in the filename.\n",
    "EXPECTED_DIRECTION_SIGN = {\n",
    "    \"right\": -1,  # filenames tagged \"right\" should result in negative velocity\n",
    "    \"left\": 1,    # filenames tagged \"left\" should result in positive velocity\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory: /Volumes/RanczLab2/DATA_NEW/turning_analysis__Cohort1_rotation_Visual_mismatch_day4__Cohort3_rotation_Visual_mismatch_day4\n"
     ]
    }
   ],
   "source": [
    "# Utility functions\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "def _iter_raw_mouse_dirs(data_dirs: Iterable[Path]) -> Iterable[Tuple[str, Path]]:\n",
    "    \"\"\"Yield (mouse_id, raw_dir) pairs for each mouse found in the data dirs.\"\"\"\n",
    "    for data_dir in data_dirs:\n",
    "        base = Path(data_dir)\n",
    "        if not base.exists():\n",
    "            print(f\"⚠️ Data directory not found: {base}\")\n",
    "            continue\n",
    "\n",
    "        if base.is_dir() and base.name.endswith(\"_processedData\"):\n",
    "            raw_dir = base.with_name(base.name.replace(\"_processedData\", \"\"))\n",
    "            if raw_dir.exists():\n",
    "                mouse_id = raw_dir.name.split(\"-\")[0]\n",
    "                yield mouse_id, raw_dir\n",
    "            else:\n",
    "                print(f\"⚠️ Raw directory for processed data not found: {base}\")\n",
    "            continue\n",
    "\n",
    "        if not base.is_dir():\n",
    "            continue\n",
    "\n",
    "        candidate_mouse_dirs = [\n",
    "            subdir\n",
    "            for subdir in base.iterdir()\n",
    "            if subdir.is_dir() and \"-\" in subdir.name and not subdir.name.endswith(\"_processedData\")\n",
    "        ]\n",
    "        if candidate_mouse_dirs:\n",
    "            for subdir in candidate_mouse_dirs:\n",
    "                mouse_id = subdir.name.split(\"-\")[0]\n",
    "                yield mouse_id, subdir\n",
    "            continue\n",
    "\n",
    "        mouse_id = base.name.split(\"-\")[0]\n",
    "        yield mouse_id, base\n",
    "\n",
    "\n",
    "def find_turn_event_files(\n",
    "    data_dirs: Iterable[Path],\n",
    "    event_suffixes: Iterable[str],\n",
    "    selected_mice: Optional[Iterable[str]] = None,\n",
    ") -> List[Dict[str, object]]:\n",
    "    \"\"\"Locate aligned CSV files whose names encode turn direction.\"\"\"\n",
    "    selected = set(selected_mice or [])\n",
    "    matches: List[Dict[str, object]] = []\n",
    "\n",
    "    for mouse_id, raw_dir in _iter_raw_mouse_dirs(data_dirs):\n",
    "        if selected and mouse_id not in selected:\n",
    "            continue\n",
    "\n",
    "        aligned_dir = raw_dir.parent / f\"{raw_dir.name}_processedData\" / \"aligned_data\"\n",
    "        if not aligned_dir.exists():\n",
    "            continue\n",
    "\n",
    "        for suffix in event_suffixes:\n",
    "            csv_path = aligned_dir / f\"{mouse_id}{suffix}\"\n",
    "            if not csv_path.exists():\n",
    "                continue\n",
    "            direction = infer_direction_from_name(csv_path.name)\n",
    "            group = infer_event_group(suffix)\n",
    "            matches.append(\n",
    "                {\n",
    "                    \"mouse\": mouse_id,\n",
    "                    \"direction\": direction,\n",
    "                    \"group\": group,\n",
    "                    \"event_suffix\": suffix,\n",
    "                    \"csv_path\": csv_path,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return matches\n",
    "\n",
    "\n",
    "def infer_direction_from_name(filename: str) -> str:\n",
    "    name = filename.lower()\n",
    "    if \"right\" in name:\n",
    "        return \"right\"\n",
    "    if \"left\" in name:\n",
    "        return \"left\"\n",
    "    return \"unknown\"\n",
    "\n",
    "\n",
    "def infer_event_group(name: str) -> str:\n",
    "    lower = name.lower()\n",
    "    if \"no_halt\" in lower or \"no halt\" in lower:\n",
    "        return \"No halt\"\n",
    "    if \"apply\" in lower:\n",
    "        return \"Apply halt\"\n",
    "    return \"Unknown\"\n",
    "\n",
    "\n",
    "def load_time_series(\n",
    "    csv_path: Path,\n",
    "    time_column: str,\n",
    "    value_column: str,\n",
    "    value_alias: str = \"velocity\",\n",
    ") -> pd.DataFrame:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    if time_column not in df.columns:\n",
    "        raise ValueError(f\"Time column '{time_column}' not found in {csv_path}\")\n",
    "    if value_column not in df.columns:\n",
    "        raise ValueError(f\"Column '{value_column}' not found in {csv_path}\")\n",
    "    subset = df[[time_column, value_column]].copy()\n",
    "    subset = subset.dropna()\n",
    "    subset = subset.rename(columns={time_column: \"time\", value_column: value_alias})\n",
    "    subset[\"time\"] = pd.to_numeric(subset[\"time\"], errors=\"coerce\")\n",
    "    subset[value_alias] = pd.to_numeric(subset[value_alias], errors=\"coerce\")\n",
    "    subset = subset.dropna()\n",
    "    subset = subset.sort_values(\"time\")\n",
    "    return subset\n",
    "\n",
    "\n",
    "def load_motor_velocity(\n",
    "    csv_path: Path,\n",
    "    time_column: str,\n",
    "    velocity_column: str,\n",
    ") -> pd.DataFrame:\n",
    "    return load_time_series(csv_path, time_column, velocity_column, value_alias=\"velocity\")\n",
    "\n",
    "\n",
    "def compute_window_mean(df: pd.DataFrame, window: Tuple[float, float]) -> float:\n",
    "    start, end = window\n",
    "    mask = (df[\"time\"] >= start) & (df[\"time\"] < end if start < end else df[\"time\"] <= end)\n",
    "    if not mask.any():\n",
    "        return float(\"nan\")\n",
    "    return float(df.loc[mask, \"velocity\"].mean())\n",
    "\n",
    "\n",
    "def sign_with_threshold(value: float, threshold: float) -> int:\n",
    "    if not math.isfinite(value) or abs(value) <= threshold:\n",
    "        return 0\n",
    "    return 1 if value > 0 else -1\n",
    "\n",
    "\n",
    "def sem(values) -> float:\n",
    "    arr = np.asarray(values, dtype=float)\n",
    "    arr = arr[np.isfinite(arr)]\n",
    "    n = arr.size\n",
    "    if n <= 1:\n",
    "        return 0.0 if n == 1 else float(\"nan\")\n",
    "    return float(arr.std(ddof=1) / np.sqrt(n))\n",
    "\n",
    "\n",
    "def fit_exponential_decay(time_values: Iterable[float], amplitude_values: Iterable[float], verbose: bool = False) -> float:\n",
    "    \"\"\"\n",
    "    Fit exponential decay: y = A * exp(-t/tau)\n",
    "    \n",
    "    NOTE: This often fails because motor velocity doesn't always show clean exponential decay.\n",
    "    The velocity may:\n",
    "    - Stay elevated (sustained turning)\n",
    "    - Show complex multi-phase dynamics\n",
    "    - Have noise that breaks the exponential assumption\n",
    "    \n",
    "    Consider using alternative metrics like AUC in different time windows instead.\n",
    "    \"\"\"\n",
    "    time_arr = np.asarray(time_values, dtype=float)\n",
    "    amp_arr = np.asarray(amplitude_values, dtype=float)\n",
    "    mask = np.isfinite(time_arr) & np.isfinite(amp_arr)\n",
    "    if mask.sum() < 3:\n",
    "        if verbose:\n",
    "            print(f\"Decay fit failed: Only {mask.sum()} finite points\")\n",
    "        return float(\"nan\")\n",
    "    time_arr = time_arr[mask]\n",
    "    amp_arr = amp_arr[mask]\n",
    "    amp_arr = np.abs(amp_arr)\n",
    "    positive_mask = amp_arr > 0\n",
    "    if positive_mask.sum() < 3:\n",
    "        if verbose:\n",
    "            print(f\"Decay fit failed: Only {positive_mask.sum()} positive amplitude points\")\n",
    "        return float(\"nan\")\n",
    "    time_arr = time_arr[positive_mask]\n",
    "    amp_arr = amp_arr[positive_mask]\n",
    "    time_arr = time_arr - time_arr.min()\n",
    "    if time_arr.ptp() <= 0:\n",
    "        if verbose:\n",
    "            print(\"Decay fit failed: No time variation\")\n",
    "        return float(\"nan\")\n",
    "    log_amp = np.log(amp_arr)\n",
    "    slope, intercept = np.polyfit(time_arr, log_amp, 1)\n",
    "    if slope >= 0:\n",
    "        if verbose:\n",
    "            print(f\"Decay fit failed: Non-decaying (slope={slope:.6f})\")\n",
    "            print(f\"  → Signal is increasing or flat, not exponentially decaying\")\n",
    "            print(f\"  → Consider using AUC or sustained response metrics instead\")\n",
    "        return float(\"nan\")\n",
    "    tau = -1.0 / slope\n",
    "    return float(tau)\n",
    "\n",
    "\n",
    "def assign_mouse_colors_consistent(mouse_ids: Iterable[str]) -> Dict[str, tuple]:\n",
    "    normalized = [str(mouse) for mouse in mouse_ids]\n",
    "    unique_mice = sorted(dict.fromkeys(normalized))\n",
    "    if not unique_mice:\n",
    "        return OrderedDict()\n",
    "    palette = sns.color_palette(\"gnuplot2\", len(unique_mice))\n",
    "    return OrderedDict((mouse, palette[idx]) for idx, mouse in enumerate(unique_mice))\n",
    "\n",
    "\n",
    "def compute_windowed_auc(\n",
    "    df: pd.DataFrame,\n",
    "    windows: List[Tuple[float, float]],\n",
    "    time_col: str = \"time\",\n",
    "    value_col: str = \"velocity\",\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compute area under curve for multiple time windows.\n",
    "    \n",
    "    This is often more robust than decay fitting for characterizing response dynamics.\n",
    "    Different windows capture different phases of the response:\n",
    "    - Early (0-0.5s): Initial response magnitude\n",
    "    - Mid (0.5-1s): Sustained response\n",
    "    - Late (1-2s): Extended dynamics\n",
    "    \"\"\"\n",
    "    auc_results = {}\n",
    "    for start, end in windows:\n",
    "        window_df = df[(df[time_col] >= start) & (df[time_col] <= end)]\n",
    "        if not window_df.empty and len(window_df) > 1:\n",
    "            auc = float(np.trapz(\n",
    "                np.abs(window_df[value_col].to_numpy()),\n",
    "                window_df[time_col].to_numpy()\n",
    "            ))\n",
    "            auc_results[f\"auc_{start}_{end}s\"] = auc\n",
    "        else:\n",
    "            auc_results[f\"auc_{start}_{end}s\"] = float(\"nan\")\n",
    "    return auc_results\n",
    "\n",
    "\n",
    "def compute_sustained_response_ratio(\n",
    "    df: pd.DataFrame,\n",
    "    early_window: Tuple[float, float] = (0.0, 0.5),\n",
    "    late_window: Tuple[float, float] = (1.0, 2.0),\n",
    "    time_col: str = \"time\",\n",
    "    value_col: str = \"velocity\",\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Compute ratio of late to early response magnitude.\n",
    "    \n",
    "    Ratio < 1: Response decays\n",
    "    Ratio ~ 1: Response is sustained\n",
    "    Ratio > 1: Response increases over time\n",
    "    \n",
    "    This can distinguish between transient vs sustained turning responses.\n",
    "    \"\"\"\n",
    "    early_auc = compute_window_mean(df.rename(columns={time_col: \"time\", value_col: \"velocity\"}), early_window)\n",
    "    late_auc = compute_window_mean(df.rename(columns={time_col: \"time\", value_col: \"velocity\"}), late_window)\n",
    "    \n",
    "    if math.isfinite(early_auc) and math.isfinite(late_auc) and abs(early_auc) > 1e-6:\n",
    "        return float(abs(late_auc) / abs(early_auc))\n",
    "    return float(\"nan\")\n",
    "\n",
    "\n",
    "def compute_time_to_baseline(\n",
    "    df: pd.DataFrame,\n",
    "    baseline_window: Tuple[float, float] = (-1.0, 0.0),\n",
    "    post_start: float = 0.0,\n",
    "    n_std: float = 2.0,\n",
    "    time_col: str = \"time\",\n",
    "    value_col: str = \"velocity\",\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Compute time for response to return to baseline level (mean + n_std).\n",
    "    \n",
    "    Returns NaN if response never returns to baseline in the available data.\n",
    "    This is more interpretable than tau for non-exponential dynamics.\n",
    "    \"\"\"\n",
    "    baseline_df = df[(df[time_col] >= baseline_window[0]) & (df[time_col] < baseline_window[1])]\n",
    "    if baseline_df.empty:\n",
    "        return float(\"nan\")\n",
    "    \n",
    "    baseline_mean = float(baseline_df[value_col].abs().mean())\n",
    "    baseline_std = float(baseline_df[value_col].abs().std())\n",
    "    threshold = baseline_mean + n_std * baseline_std\n",
    "    \n",
    "    post_df = df[df[time_col] >= post_start].sort_values(time_col)\n",
    "    if post_df.empty:\n",
    "        return float(\"nan\")\n",
    "    \n",
    "    post_abs = post_df[value_col].abs()\n",
    "    below_threshold = post_abs < threshold\n",
    "    \n",
    "    if not below_threshold.any():\n",
    "        return float(\"nan\")\n",
    "    \n",
    "    # Find first sustained return (at least 3 consecutive points below threshold)\n",
    "    below_indices = post_df.index[below_threshold].tolist()\n",
    "    if len(below_indices) < 3:\n",
    "        return float(\"nan\")\n",
    "    \n",
    "    for i in range(len(below_indices) - 2):\n",
    "        if (below_indices[i+1] == below_indices[i] + 1 and \n",
    "            below_indices[i+2] == below_indices[i] + 2):\n",
    "            return float(post_df.loc[below_indices[i], time_col])\n",
    "    \n",
    "    return float(\"nan\")\n",
    "\n",
    "\n",
    "def diagnostic_exponential_fit(\n",
    "    time_values: Iterable[float],\n",
    "    amplitude_values: Iterable[float],\n",
    "    title: str = \"Decay Fit Diagnostic\",\n",
    "    show_plots: bool = True,\n",
    ") -> Tuple[float, Dict[str, object]]:\n",
    "    \"\"\"\n",
    "    Detailed diagnostic of exponential decay fitting.\n",
    "    \n",
    "    Returns:\n",
    "        tau: The decay time constant (NaN if fit fails)\n",
    "        diagnostics: Dictionary with diagnostic information\n",
    "    \"\"\"\n",
    "    time_arr = np.asarray(time_values, dtype=float)\n",
    "    amp_arr = np.asarray(amplitude_values, dtype=float)\n",
    "    \n",
    "    diagnostics = {\n",
    "        \"title\": title,\n",
    "        \"n_points_initial\": len(time_arr),\n",
    "        \"time_range\": (float(time_arr.min()), float(time_arr.max())) if len(time_arr) > 0 else (np.nan, np.nan),\n",
    "        \"amp_range\": (float(amp_arr.min()), float(amp_arr.max())) if len(amp_arr) > 0 else (np.nan, np.nan),\n",
    "        \"failure_reason\": None,\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{title}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Initial data points: {len(time_arr)}\")\n",
    "    if len(time_arr) > 0:\n",
    "        print(f\"Time range: {time_arr.min():.3f} to {time_arr.max():.3f}\")\n",
    "        print(f\"Amplitude range: {amp_arr.min():.3f} to {amp_arr.max():.3f}\")\n",
    "    \n",
    "    # Check 1: Finite values\n",
    "    mask = np.isfinite(time_arr) & np.isfinite(amp_arr)\n",
    "    if mask.sum() < 3:\n",
    "        diagnostics[\"failure_reason\"] = f\"Only {mask.sum()} finite points (need ≥3)\"\n",
    "        print(f\"❌ FAIL: {diagnostics['failure_reason']}\")\n",
    "        return float(\"nan\"), diagnostics\n",
    "    \n",
    "    time_arr = time_arr[mask]\n",
    "    amp_arr = amp_arr[mask]\n",
    "    diagnostics[\"n_points_finite\"] = int(mask.sum())\n",
    "    print(f\"After finite check: {len(time_arr)} points\")\n",
    "    \n",
    "    # Check 2: Absolute values\n",
    "    amp_arr = np.abs(amp_arr)\n",
    "    positive_mask = amp_arr > 0\n",
    "    if positive_mask.sum() < 3:\n",
    "        diagnostics[\"failure_reason\"] = f\"Only {positive_mask.sum()} positive amplitude points\"\n",
    "        print(f\"❌ FAIL: {diagnostics['failure_reason']}\")\n",
    "        return float(\"nan\"), diagnostics\n",
    "    \n",
    "    time_arr = time_arr[positive_mask]\n",
    "    amp_arr = amp_arr[positive_mask]\n",
    "    diagnostics[\"n_points_positive\"] = int(positive_mask.sum())\n",
    "    print(f\"After positive check: {len(time_arr)} points\")\n",
    "    \n",
    "    # Check 3: Time normalization\n",
    "    time_arr = time_arr - time_arr.min()\n",
    "    if time_arr.ptp() <= 0:\n",
    "        diagnostics[\"failure_reason\"] = f\"No time variation (ptp={time_arr.ptp()})\"\n",
    "        print(f\"❌ FAIL: {diagnostics['failure_reason']}\")\n",
    "        return float(\"nan\"), diagnostics\n",
    "    \n",
    "    diagnostics[\"time_ptp\"] = float(time_arr.ptp())\n",
    "    print(f\"Time variation (ptp): {time_arr.ptp():.3f}\")\n",
    "    \n",
    "    # Check 4: Log transform and linear fit\n",
    "    log_amp = np.log(amp_arr)\n",
    "    slope, intercept = np.polyfit(time_arr, log_amp, 1)\n",
    "    diagnostics[\"slope\"] = float(slope)\n",
    "    diagnostics[\"intercept\"] = float(intercept)\n",
    "    print(f\"Log-linear fit: slope={slope:.6f}, intercept={intercept:.3f}\")\n",
    "    \n",
    "    if slope >= 0:\n",
    "        diagnostics[\"failure_reason\"] = f\"Non-decaying (slope={slope:.6f})\"\n",
    "        diagnostics[\"amp_start\"] = float(amp_arr[0])\n",
    "        diagnostics[\"amp_end\"] = float(amp_arr[-1])\n",
    "        diagnostics[\"amp_mean\"] = float(amp_arr.mean())\n",
    "        diagnostics[\"trend\"] = \"increasing\" if amp_arr[-1] > amp_arr[0] else \"decreasing\"\n",
    "        \n",
    "        print(f\"❌ FAIL: Positive or zero slope ({slope:.6f}) - not decaying!\")\n",
    "        print(f\"   This means the signal is INCREASING or flat, not decaying.\")\n",
    "        print(f\"\\n   Amplitude at start: {amp_arr[0]:.3f}\")\n",
    "        print(f\"   Amplitude at end: {amp_arr[-1]:.3f}\")\n",
    "        print(f\"   Mean amplitude: {amp_arr.mean():.3f}\")\n",
    "        print(f\"   Amplitude trend: {diagnostics['trend']}\")\n",
    "        \n",
    "        if show_plots:\n",
    "            # Create diagnostic plot\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "            \n",
    "            # Plot 1: Actual data\n",
    "            axes[0].plot(time_arr, amp_arr, 'o-', label='Data', color='#1f77b4')\n",
    "            axes[0].set_xlabel('Time (s)')\n",
    "            axes[0].set_ylabel('|Velocity| (deg/s)')\n",
    "            axes[0].set_title('Absolute Velocity vs Time')\n",
    "            axes[0].legend()\n",
    "            axes[0].grid(True, alpha=0.3)\n",
    "            \n",
    "            # Plot 2: Log scale\n",
    "            axes[1].plot(time_arr, log_amp, 'o-', label='Log(amplitude)', color='#1f77b4')\n",
    "            axes[1].plot(time_arr, slope * time_arr + intercept, 'r--', \n",
    "                        label=f'Fit: slope={slope:.4f}', linewidth=2)\n",
    "            axes[1].set_xlabel('Time (s)')\n",
    "            axes[1].set_ylabel('Log(|Velocity|)')\n",
    "            axes[1].set_title('Log Scale (should be linear for exponential decay)')\n",
    "            axes[1].legend()\n",
    "            axes[1].grid(True, alpha=0.3)\n",
    "            \n",
    "            plt.suptitle(f\"{title} - Decay Fit Failed\", fontsize=12)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        return float(\"nan\"), diagnostics\n",
    "    \n",
    "    tau = -1.0 / slope\n",
    "    diagnostics[\"tau\"] = float(tau)\n",
    "    diagnostics[\"failure_reason\"] = None\n",
    "    print(f\"✅ SUCCESS: τ = {tau:.3f} s\")\n",
    "    \n",
    "    if show_plots:\n",
    "        # Create diagnostic plot for successful fit\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "        \n",
    "        # Plot 1: Actual data with exponential fit\n",
    "        fitted_curve = np.exp(intercept) * np.exp(slope * time_arr)\n",
    "        axes[0].plot(time_arr, amp_arr, 'o', label='Data', alpha=0.6, color='#1f77b4')\n",
    "        axes[0].plot(time_arr, fitted_curve, 'r-', label=f'Fit: τ={tau:.3f}s', linewidth=2)\n",
    "        axes[0].set_xlabel('Time (s)')\n",
    "        axes[0].set_ylabel('|Velocity| (deg/s)')\n",
    "        axes[0].set_title('Exponential Decay Fit')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 2: Log scale\n",
    "        axes[1].plot(time_arr, log_amp, 'o', label='Log(amplitude)', color='#1f77b4')\n",
    "        axes[1].plot(time_arr, slope * time_arr + intercept, 'r--', \n",
    "                    label=f'Fit: slope={slope:.4f}', linewidth=2)\n",
    "        axes[1].set_xlabel('Time (s)')\n",
    "        axes[1].set_ylabel('Log(|Velocity|)')\n",
    "        axes[1].set_title('Log Scale')\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 3: Residuals\n",
    "        residuals = log_amp - (slope * time_arr + intercept)\n",
    "        axes[2].plot(time_arr, residuals, 'o', color='#1f77b4')\n",
    "        axes[2].axhline(0, color='r', linestyle='--', linewidth=2)\n",
    "        axes[2].set_xlabel('Time (s)')\n",
    "        axes[2].set_ylabel('Residuals')\n",
    "        axes[2].set_title('Fit Residuals')\n",
    "        axes[2].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.suptitle(f\"{title} - Decay Fit Successful\", fontsize=12)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return float(tau), diagnostics\n",
    "\n",
    "\n",
    "def _sanitize_label(value: str) -> str:\n",
    "    cleaned = re.sub(r\"[^\\w\\-]+\", \"_\", value)\n",
    "    cleaned = re.sub(r\"_+\", \"_\", cleaned)\n",
    "    return cleaned.strip(\"_\")\n",
    "\n",
    "\n",
    "def build_output_folder_name(base_name: str, data_dirs: Iterable[Path]) -> str:\n",
    "    labels: List[str] = []\n",
    "    for directory in data_dirs:\n",
    "        try:\n",
    "            path = Path(directory).expanduser()\n",
    "        except Exception:\n",
    "            continue\n",
    "        if not path.exists():\n",
    "            continue\n",
    "        cohort = path.parent.name if path.parent != path else \"\"\n",
    "        day = path.name\n",
    "        parts = [part for part in (cohort, day) if part]\n",
    "        if not parts:\n",
    "            continue\n",
    "        label = _sanitize_label(\"_\".join(parts))\n",
    "        if not label:\n",
    "            continue\n",
    "        if label not in labels:\n",
    "            labels.append(label)\n",
    "    if not labels:\n",
    "        return base_name\n",
    "    suffix = \"__\".join(labels)\n",
    "    return f\"{base_name}__{suffix}\"\n",
    "\n",
    "\n",
    "def determine_output_directory(data_dirs: Iterable[Path], folder_name: str) -> Optional[Path]:\n",
    "    existing_dirs = []\n",
    "    for directory in data_dirs:\n",
    "        candidate = Path(directory).expanduser()\n",
    "        if candidate.exists():\n",
    "            existing_dirs.append(candidate)\n",
    "    if not existing_dirs:\n",
    "        print(\"⚠️ No existing data directories found; results will not be saved.\")\n",
    "        return None\n",
    "\n",
    "    folder_name = build_output_folder_name(folder_name, existing_dirs)\n",
    "    try:\n",
    "        common_path = Path(os.path.commonpath([str(path) for path in existing_dirs]))\n",
    "    except ValueError:\n",
    "        common_path = existing_dirs[0]\n",
    "\n",
    "    candidate_bases: List[Path] = []\n",
    "    candidate_bases.append(common_path)\n",
    "    for path in existing_dirs:\n",
    "        if path not in candidate_bases:\n",
    "            candidate_bases.append(path)\n",
    "        parent = path.parent\n",
    "        if parent not in candidate_bases:\n",
    "            candidate_bases.append(parent)\n",
    "    cwd_base = Path.cwd()\n",
    "    if cwd_base not in candidate_bases:\n",
    "        candidate_bases.append(cwd_base)\n",
    "\n",
    "    for base in candidate_bases:\n",
    "        target_dir = Path(base) / folder_name\n",
    "        try:\n",
    "            target_dir.mkdir(parents=True, exist_ok=True)\n",
    "            return target_dir\n",
    "        except Exception as exc:  # noqa: BLE001\n",
    "            print(f\"⚠️ Could not create output directory {target_dir}: {exc}\")\n",
    "\n",
    "    print(\"⚠️ Exhausted all fallback locations; results will not be saved.\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def compute_paired_t_test(\n",
    "    pivot_df: pd.DataFrame,\n",
    "    group_a: str,\n",
    "    group_b: str,\n",
    ") -> Dict[str, float]:\n",
    "    result = {\n",
    "        \"n_pairs\": 0,\n",
    "        \"mean_difference\": float(\"nan\"),\n",
    "        \"t_statistic\": float(\"nan\"),\n",
    "        \"p_value\": float(\"nan\"),\n",
    "    }\n",
    "    if group_a not in pivot_df.columns or group_b not in pivot_df.columns:\n",
    "        return result\n",
    "    paired = pivot_df[[group_a, group_b]].dropna()\n",
    "    n_pairs = int(len(paired))\n",
    "    result[\"n_pairs\"] = n_pairs\n",
    "    if n_pairs == 0:\n",
    "        return result\n",
    "\n",
    "    diff = paired[group_b] - paired[group_a]\n",
    "    mean_diff = float(diff.mean())\n",
    "    result[\"mean_difference\"] = mean_diff\n",
    "\n",
    "    if n_pairs < 2:\n",
    "        return result\n",
    "\n",
    "    if stats is not None:\n",
    "        t_stat, p_value = stats.ttest_rel(\n",
    "            paired[group_b].to_numpy(dtype=float),\n",
    "            paired[group_a].to_numpy(dtype=float),\n",
    "            nan_policy=\"omit\",\n",
    "        )\n",
    "        result[\"t_statistic\"] = float(t_stat)\n",
    "        result[\"p_value\"] = float(p_value)\n",
    "    else:\n",
    "        std_diff = diff.std(ddof=1)\n",
    "        if math.isfinite(std_diff) and std_diff > 0:\n",
    "            t_statistic = mean_diff / (std_diff / math.sqrt(n_pairs))\n",
    "            result[\"t_statistic\"] = float(t_statistic)\n",
    "    return result\n",
    "\n",
    "\n",
    "OUTPUT_DIR = determine_output_directory(DATA_DIRS, OUTPUT_SUBDIR_NAME)\n",
    "if OUTPUT_DIR is not None:\n",
    "    print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "\n",
    "\n",
    "def analyse_turn_direction(\n",
    "    df: pd.DataFrame,\n",
    "    pre_window: Tuple[float, float],\n",
    "    post_window: Tuple[float, float],\n",
    "    zero_threshold: float,\n",
    ") -> Dict[str, float]:\n",
    "    pre_mean = compute_window_mean(df, pre_window)\n",
    "    post_mean = compute_window_mean(df, post_window)\n",
    "    pre_sign = sign_with_threshold(pre_mean, zero_threshold)\n",
    "    post_sign = sign_with_threshold(post_mean, zero_threshold)\n",
    "    direction_changed = (pre_sign != 0 and post_sign != 0 and pre_sign != post_sign)\n",
    "    return {\n",
    "        \"pre_mean\": pre_mean,\n",
    "        \"post_mean\": post_mean,\n",
    "        \"pre_sign\": pre_sign,\n",
    "        \"post_sign\": post_sign,\n",
    "        \"direction_changed\": direction_changed,\n",
    "    }\n",
    "\n",
    "\n",
    "def summarise_results(results: pd.DataFrame) -> pd.DataFrame:\n",
    "    if results.empty:\n",
    "        return results\n",
    "    group_cols = [\"group\", \"direction\", \"expected_sign\"]\n",
    "    grouped = (\n",
    "        results.groupby(group_cols, dropna=False)\n",
    "        .agg(\n",
    "            n_files=(\"mouse\", \"count\"),\n",
    "            n_mice=(\"mouse\", \"nunique\"),\n",
    "            mean_post_velocity=(\"post_mean\", \"mean\"),\n",
    "            fraction_direction_change=(\"direction_changed\", \"mean\"),\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "    grouped[\"fraction_direction_change\"] = grouped[\"fraction_direction_change\"].round(3)\n",
    "    return grouped\n",
    "\n",
    "\n",
    "def compute_turn_timing_metrics(\n",
    "    df: pd.DataFrame,\n",
    "    expected_sign: float,\n",
    "    zero_threshold: float,\n",
    "    peak_window: Tuple[float, float],\n",
    "    auc_window: Tuple[float, float],\n",
    "    pre_window: Tuple[float, float],\n",
    "    latency_fraction: float,\n",
    "    decay_fit_window: Tuple[float, float],\n",
    ") -> Dict[str, float]:\n",
    "    sign_used = expected_sign\n",
    "    if not math.isfinite(sign_used) or sign_used == 0:\n",
    "        post_mean = compute_window_mean(df, (0.0, 1.0))\n",
    "        fallback_sign = sign_with_threshold(post_mean, zero_threshold)\n",
    "        sign_used = fallback_sign if fallback_sign != 0 else 1\n",
    "\n",
    "    start_peak, end_peak = peak_window\n",
    "    peak_subset = df[(df[\"time\"] >= start_peak) & (df[\"time\"] <= end_peak)]\n",
    "    timing_metrics = {\n",
    "        \"sign_used\": float(sign_used),\n",
    "        \"time_to_peak\": float(\"nan\"),\n",
    "        \"peak_velocity_signed\": float(\"nan\"),\n",
    "        \"peak_velocity_magnitude\": float(\"nan\"),\n",
    "        \"peak_velocity_abs_1s\": float(\"nan\"),\n",
    "        \"latency_to_fraction_peak\": float(\"nan\"),\n",
    "        \"auc_abs\": float(\"nan\"),\n",
    "        \"decay_tau\": float(\"nan\"),\n",
    "    }\n",
    "    if peak_subset.empty:\n",
    "        return timing_metrics\n",
    "\n",
    "    abs_velocity = peak_subset[\"velocity\"].abs()\n",
    "    if abs_velocity.isna().all():\n",
    "        return timing_metrics\n",
    "\n",
    "    peak_idx = abs_velocity.idxmax()\n",
    "    peak_value_abs = abs_velocity.loc[peak_idx]\n",
    "    if not math.isfinite(peak_value_abs) or peak_value_abs <= 0:\n",
    "        return timing_metrics\n",
    "\n",
    "    peak_time = float(peak_subset.loc[peak_idx, \"time\"])\n",
    "    raw_peak_value = float(peak_subset.loc[peak_idx, \"velocity\"])\n",
    "    timing_metrics[\"time_to_peak\"] = peak_time\n",
    "    timing_metrics[\"peak_velocity_signed\"] = raw_peak_value\n",
    "    timing_metrics[\"peak_velocity_magnitude\"] = float(peak_value_abs)\n",
    "    timing_metrics[\"peak_velocity_abs_1s\"] = float(peak_value_abs)\n",
    "\n",
    "    baseline_mask = (df[\"time\"] >= pre_window[0]) & (df[\"time\"] <= pre_window[1])\n",
    "    baseline_abs = df.loc[baseline_mask, \"velocity\"].abs().mean()\n",
    "    baseline_abs = float(baseline_abs) if math.isfinite(baseline_abs) else 0.0\n",
    "\n",
    "    if 0 < latency_fraction < 1 and peak_value_abs > baseline_abs:\n",
    "        target_value = baseline_abs + (peak_value_abs - baseline_abs) * latency_fraction\n",
    "        time_values = peak_subset[\"time\"].to_numpy(dtype=float)\n",
    "        abs_values = abs_velocity.to_numpy(dtype=float)\n",
    "        above_threshold_idx = np.where(abs_values >= target_value)[0]\n",
    "        if above_threshold_idx.size:\n",
    "            idx = int(above_threshold_idx[0])\n",
    "            if idx == 0:\n",
    "                latency_time = float(time_values[0])\n",
    "            else:\n",
    "                prev_idx = idx - 1\n",
    "                y0 = abs_values[prev_idx]\n",
    "                y1 = abs_values[idx]\n",
    "                t0 = time_values[prev_idx]\n",
    "                t1 = time_values[idx]\n",
    "                if not math.isfinite(y0):\n",
    "                    y0 = 0.0\n",
    "                if not math.isfinite(y1) or math.isclose(y1, y0):\n",
    "                    latency_time = float(t1)\n",
    "                else:\n",
    "                    fraction = (target_value - y0) / (y1 - y0)\n",
    "                    fraction = min(max(fraction, 0.0), 1.0)\n",
    "                    latency_time = float(t0 + fraction * (t1 - t0))\n",
    "            timing_metrics[\"latency_to_fraction_peak\"] = latency_time\n",
    "\n",
    "    start_auc, end_auc = auc_window\n",
    "    auc_subset = df[(df[\"time\"] >= start_auc) & (df[\"time\"] <= end_auc)]\n",
    "    if not auc_subset.empty:\n",
    "        auc_abs = auc_subset[\"velocity\"].abs()\n",
    "        auc_value = float(np.trapz(auc_abs.to_numpy(), auc_subset[\"time\"].to_numpy()))\n",
    "        timing_metrics[\"auc_abs\"] = auc_value\n",
    "\n",
    "    start_decay, end_decay = decay_fit_window\n",
    "    if math.isfinite(peak_time):\n",
    "        start_decay = max(start_decay, peak_time)\n",
    "    decay_mask = (df[\"time\"] >= start_decay) & (df[\"time\"] <= end_decay)\n",
    "    decay_subset = df.loc[decay_mask].copy()\n",
    "    if not decay_subset.empty:\n",
    "        decay_subset[\"abs_velocity\"] = decay_subset[\"velocity\"].abs()\n",
    "        per_time = (\n",
    "            decay_subset.groupby(\"time\", dropna=False)[\"abs_velocity\"]\n",
    "            .mean()\n",
    "            .reset_index()\n",
    "            .sort_values(\"time\")\n",
    "        )\n",
    "        tau = fit_exponential_decay(per_time[\"time\"], per_time[\"abs_velocity\"])\n",
    "        if math.isfinite(tau):\n",
    "            timing_metrics[\"decay_tau\"] = float(tau)\n",
    "\n",
    "    return timing_metrics\n",
    "\n",
    "\n",
    "def plot_motor_velocity(df: pd.DataFrame, title: str, pre_window, post_window) -> None:\n",
    "    fig, ax = plt.subplots(figsize=(6, 3.5))\n",
    "    ax.plot(df[\"time\"], df[\"velocity\"], color=\"#1f77b4\", linewidth=0.9)\n",
    "    ax.axvline(0, color=\"black\", linestyle=\"--\", linewidth=1)\n",
    "    ax.axvspan(pre_window[0], pre_window[1], color=\"#2ca02c\", alpha=0.15, label=\"Pre window\")\n",
    "    ax.axvspan(post_window[0], post_window[1], color=\"#d62728\", alpha=0.15, label=\"Post window\")\n",
    "    ax.set_xlabel(\"Time (s)\")\n",
    "    ax.set_ylabel(\"Motor velocity (deg/s)\")\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 35 turn-specific files\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mouse</th>\n",
       "      <th>direction</th>\n",
       "      <th>group</th>\n",
       "      <th>event_suffix</th>\n",
       "      <th>csv_path</th>\n",
       "      <th>pre_mean</th>\n",
       "      <th>post_mean</th>\n",
       "      <th>pre_sign</th>\n",
       "      <th>post_sign</th>\n",
       "      <th>direction_changed</th>\n",
       "      <th>sign_used</th>\n",
       "      <th>time_to_peak</th>\n",
       "      <th>peak_velocity_signed</th>\n",
       "      <th>peak_velocity_magnitude</th>\n",
       "      <th>peak_velocity_abs_1s</th>\n",
       "      <th>latency_to_fraction_peak</th>\n",
       "      <th>auc_abs</th>\n",
       "      <th>decay_tau</th>\n",
       "      <th>expected_sign</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B6J2718</td>\n",
       "      <td>right</td>\n",
       "      <td>Apply halt</td>\n",
       "      <td>_Apply halt_2s_right_turns_baselined_data.csv</td>\n",
       "      <td>/Volumes/RanczLab2/DATA_NEW/Cohort1_rotation/V...</td>\n",
       "      <td>8.700897</td>\n",
       "      <td>8.593928</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.915</td>\n",
       "      <td>26.8795</td>\n",
       "      <td>26.8795</td>\n",
       "      <td>26.8795</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20.408136</td>\n",
       "      <td>3.449111</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B6J2718</td>\n",
       "      <td>left</td>\n",
       "      <td>Apply halt</td>\n",
       "      <td>_Apply halt_2s_left_turns_baselined_data.csv</td>\n",
       "      <td>/Volumes/RanczLab2/DATA_NEW/Cohort1_rotation/V...</td>\n",
       "      <td>-7.426496</td>\n",
       "      <td>-6.166812</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.159</td>\n",
       "      <td>-23.8115</td>\n",
       "      <td>23.8115</td>\n",
       "      <td>23.8115</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.897289</td>\n",
       "      <td>4.076830</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B6J2718</td>\n",
       "      <td>right</td>\n",
       "      <td>No halt</td>\n",
       "      <td>_No halt_right_turns_baselined_data.csv</td>\n",
       "      <td>/Volumes/RanczLab2/DATA_NEW/Cohort1_rotation/V...</td>\n",
       "      <td>8.075251</td>\n",
       "      <td>10.711407</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.066</td>\n",
       "      <td>26.9865</td>\n",
       "      <td>26.9865</td>\n",
       "      <td>26.9865</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>23.915696</td>\n",
       "      <td>3.170291</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B6J2718</td>\n",
       "      <td>left</td>\n",
       "      <td>No halt</td>\n",
       "      <td>_No halt_left_turns_baselined_data.csv</td>\n",
       "      <td>/Volumes/RanczLab2/DATA_NEW/Cohort1_rotation/V...</td>\n",
       "      <td>-8.373595</td>\n",
       "      <td>-6.164667</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-21.5464</td>\n",
       "      <td>21.5464</td>\n",
       "      <td>21.5464</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>19.303492</td>\n",
       "      <td>3.124289</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B6J2719</td>\n",
       "      <td>right</td>\n",
       "      <td>Apply halt</td>\n",
       "      <td>_Apply halt_2s_right_turns_baselined_data.csv</td>\n",
       "      <td>/Volumes/RanczLab2/DATA_NEW/Cohort1_rotation/V...</td>\n",
       "      <td>8.934042</td>\n",
       "      <td>5.074776</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.151</td>\n",
       "      <td>20.0803</td>\n",
       "      <td>20.0803</td>\n",
       "      <td>20.0803</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.521531</td>\n",
       "      <td>5.012169</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>B6J2719</td>\n",
       "      <td>left</td>\n",
       "      <td>Apply halt</td>\n",
       "      <td>_Apply halt_2s_left_turns_baselined_data.csv</td>\n",
       "      <td>/Volumes/RanczLab2/DATA_NEW/Cohort1_rotation/V...</td>\n",
       "      <td>-7.531351</td>\n",
       "      <td>-5.470690</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.977</td>\n",
       "      <td>-26.2773</td>\n",
       "      <td>26.2773</td>\n",
       "      <td>26.2773</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20.392304</td>\n",
       "      <td>4.808638</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>B6J2719</td>\n",
       "      <td>right</td>\n",
       "      <td>No halt</td>\n",
       "      <td>_No halt_right_turns_baselined_data.csv</td>\n",
       "      <td>/Volumes/RanczLab2/DATA_NEW/Cohort1_rotation/V...</td>\n",
       "      <td>13.667830</td>\n",
       "      <td>10.036068</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>28.3602</td>\n",
       "      <td>28.3602</td>\n",
       "      <td>28.3602</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>22.580627</td>\n",
       "      <td>3.339692</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>B6J2719</td>\n",
       "      <td>left</td>\n",
       "      <td>No halt</td>\n",
       "      <td>_No halt_left_turns_baselined_data.csv</td>\n",
       "      <td>/Volumes/RanczLab2/DATA_NEW/Cohort1_rotation/V...</td>\n",
       "      <td>-10.597337</td>\n",
       "      <td>-6.715189</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.505</td>\n",
       "      <td>26.4874</td>\n",
       "      <td>26.4874</td>\n",
       "      <td>26.4874</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>22.658935</td>\n",
       "      <td>41.443389</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>B6J2721</td>\n",
       "      <td>right</td>\n",
       "      <td>Apply halt</td>\n",
       "      <td>_Apply halt_2s_right_turns_baselined_data.csv</td>\n",
       "      <td>/Volumes/RanczLab2/DATA_NEW/Cohort1_rotation/V...</td>\n",
       "      <td>10.560097</td>\n",
       "      <td>6.334559</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.500</td>\n",
       "      <td>28.1413</td>\n",
       "      <td>28.1413</td>\n",
       "      <td>28.1413</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.256685</td>\n",
       "      <td>43.949530</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>B6J2721</td>\n",
       "      <td>left</td>\n",
       "      <td>Apply halt</td>\n",
       "      <td>_Apply halt_2s_left_turns_baselined_data.csv</td>\n",
       "      <td>/Volumes/RanczLab2/DATA_NEW/Cohort1_rotation/V...</td>\n",
       "      <td>-5.402623</td>\n",
       "      <td>-3.445785</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.105</td>\n",
       "      <td>-20.1320</td>\n",
       "      <td>20.1320</td>\n",
       "      <td>20.1320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.781462</td>\n",
       "      <td>5.154824</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>B6J2721</td>\n",
       "      <td>right</td>\n",
       "      <td>No halt</td>\n",
       "      <td>_No halt_right_turns_baselined_data.csv</td>\n",
       "      <td>/Volumes/RanczLab2/DATA_NEW/Cohort1_rotation/V...</td>\n",
       "      <td>10.923739</td>\n",
       "      <td>10.781615</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.974</td>\n",
       "      <td>28.7013</td>\n",
       "      <td>28.7013</td>\n",
       "      <td>28.7013</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>22.312956</td>\n",
       "      <td>3.828418</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>B6J2721</td>\n",
       "      <td>left</td>\n",
       "      <td>No halt</td>\n",
       "      <td>_No halt_left_turns_baselined_data.csv</td>\n",
       "      <td>/Volumes/RanczLab2/DATA_NEW/Cohort1_rotation/V...</td>\n",
       "      <td>-7.439175</td>\n",
       "      <td>-9.021315</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.262</td>\n",
       "      <td>-24.3416</td>\n",
       "      <td>24.3416</td>\n",
       "      <td>24.3416</td>\n",
       "      <td>0.084000</td>\n",
       "      <td>20.374881</td>\n",
       "      <td>1.431185</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>B6J2722</td>\n",
       "      <td>right</td>\n",
       "      <td>Apply halt</td>\n",
       "      <td>_Apply halt_2s_right_turns_baselined_data.csv</td>\n",
       "      <td>/Volumes/RanczLab2/DATA_NEW/Cohort1_rotation/V...</td>\n",
       "      <td>6.109308</td>\n",
       "      <td>2.305159</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.757</td>\n",
       "      <td>-15.8938</td>\n",
       "      <td>15.8938</td>\n",
       "      <td>15.8938</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.570030</td>\n",
       "      <td>4.035738</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>B6J2722</td>\n",
       "      <td>left</td>\n",
       "      <td>Apply halt</td>\n",
       "      <td>_Apply halt_2s_left_turns_baselined_data.csv</td>\n",
       "      <td>/Volumes/RanczLab2/DATA_NEW/Cohort1_rotation/V...</td>\n",
       "      <td>-9.446984</td>\n",
       "      <td>-9.206770</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-26.5918</td>\n",
       "      <td>26.5918</td>\n",
       "      <td>26.5918</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>22.027632</td>\n",
       "      <td>4.063660</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>B6J2722</td>\n",
       "      <td>right</td>\n",
       "      <td>No halt</td>\n",
       "      <td>_No halt_right_turns_baselined_data.csv</td>\n",
       "      <td>/Volumes/RanczLab2/DATA_NEW/Cohort1_rotation/V...</td>\n",
       "      <td>3.943848</td>\n",
       "      <td>3.392899</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.306</td>\n",
       "      <td>15.3704</td>\n",
       "      <td>15.3704</td>\n",
       "      <td>15.3704</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.766911</td>\n",
       "      <td>15.447194</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>B6J2722</td>\n",
       "      <td>left</td>\n",
       "      <td>No halt</td>\n",
       "      <td>_No halt_left_turns_baselined_data.csv</td>\n",
       "      <td>/Volumes/RanczLab2/DATA_NEW/Cohort1_rotation/V...</td>\n",
       "      <td>-12.151944</td>\n",
       "      <td>-10.434755</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.156</td>\n",
       "      <td>-24.6687</td>\n",
       "      <td>24.6687</td>\n",
       "      <td>24.6687</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>22.208195</td>\n",
       "      <td>4.191272</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>B6J2723</td>\n",
       "      <td>right</td>\n",
       "      <td>Apply halt</td>\n",
       "      <td>_Apply halt_2s_right_turns_baselined_data.csv</td>\n",
       "      <td>/Volumes/RanczLab2/DATA_NEW/Cohort1_rotation/V...</td>\n",
       "      <td>0.985852</td>\n",
       "      <td>-3.217040</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.551</td>\n",
       "      <td>-16.3982</td>\n",
       "      <td>16.3982</td>\n",
       "      <td>16.3982</td>\n",
       "      <td>0.173000</td>\n",
       "      <td>13.919669</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>B6J2723</td>\n",
       "      <td>left</td>\n",
       "      <td>Apply halt</td>\n",
       "      <td>_Apply halt_2s_left_turns_baselined_data.csv</td>\n",
       "      <td>/Volumes/RanczLab2/DATA_NEW/Cohort1_rotation/V...</td>\n",
       "      <td>-9.012121</td>\n",
       "      <td>-8.648950</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-18.7832</td>\n",
       "      <td>18.7832</td>\n",
       "      <td>18.7832</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.061436</td>\n",
       "      <td>6.646003</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>B6J2723</td>\n",
       "      <td>right</td>\n",
       "      <td>No halt</td>\n",
       "      <td>_No halt_right_turns_baselined_data.csv</td>\n",
       "      <td>/Volumes/RanczLab2/DATA_NEW/Cohort1_rotation/V...</td>\n",
       "      <td>1.867592</td>\n",
       "      <td>1.376447</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.217</td>\n",
       "      <td>7.4796</td>\n",
       "      <td>7.4796</td>\n",
       "      <td>7.4796</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.045231</td>\n",
       "      <td>6.247141</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>B6J2723</td>\n",
       "      <td>left</td>\n",
       "      <td>No halt</td>\n",
       "      <td>_No halt_left_turns_baselined_data.csv</td>\n",
       "      <td>/Volumes/RanczLab2/DATA_NEW/Cohort1_rotation/V...</td>\n",
       "      <td>-6.219690</td>\n",
       "      <td>-7.067221</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.289</td>\n",
       "      <td>-21.7282</td>\n",
       "      <td>21.7282</td>\n",
       "      <td>21.7282</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.930709</td>\n",
       "      <td>4.421527</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>B6J2780</td>\n",
       "      <td>right</td>\n",
       "      <td>Apply halt</td>\n",
       "      <td>_Apply halt_2s_right_turns_baselined_data.csv</td>\n",
       "      <td>/Volumes/RanczLab2/DATA_NEW/Cohort3_rotation/V...</td>\n",
       "      <td>4.772713</td>\n",
       "      <td>4.428471</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.022</td>\n",
       "      <td>9.3821</td>\n",
       "      <td>9.3821</td>\n",
       "      <td>9.3821</td>\n",
       "      <td>0.540000</td>\n",
       "      <td>8.848248</td>\n",
       "      <td>5.362025</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>B6J2780</td>\n",
       "      <td>left</td>\n",
       "      <td>Apply halt</td>\n",
       "      <td>_Apply halt_2s_left_turns_baselined_data.csv</td>\n",
       "      <td>/Volumes/RanczLab2/DATA_NEW/Cohort3_rotation/V...</td>\n",
       "      <td>-0.566796</td>\n",
       "      <td>3.784272</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.852</td>\n",
       "      <td>7.3547</td>\n",
       "      <td>7.3547</td>\n",
       "      <td>7.3547</td>\n",
       "      <td>0.472477</td>\n",
       "      <td>7.568219</td>\n",
       "      <td>1.235226</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>B6J2780</td>\n",
       "      <td>right</td>\n",
       "      <td>No halt</td>\n",
       "      <td>_No halt_right_turns_baselined_data.csv</td>\n",
       "      <td>/Volumes/RanczLab2/DATA_NEW/Cohort3_rotation/V...</td>\n",
       "      <td>6.866136</td>\n",
       "      <td>7.080836</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.788</td>\n",
       "      <td>14.1194</td>\n",
       "      <td>14.1194</td>\n",
       "      <td>14.1194</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.136194</td>\n",
       "      <td>5.748712</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>B6J2781</td>\n",
       "      <td>right</td>\n",
       "      <td>Apply halt</td>\n",
       "      <td>_Apply halt_2s_right_turns_baselined_data.csv</td>\n",
       "      <td>/Volumes/RanczLab2/DATA_NEW/Cohort3_rotation/V...</td>\n",
       "      <td>6.120463</td>\n",
       "      <td>4.499280</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.134</td>\n",
       "      <td>15.1304</td>\n",
       "      <td>15.1304</td>\n",
       "      <td>15.1304</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.884107</td>\n",
       "      <td>2.790457</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>B6J2781</td>\n",
       "      <td>left</td>\n",
       "      <td>Apply halt</td>\n",
       "      <td>_Apply halt_2s_left_turns_baselined_data.csv</td>\n",
       "      <td>/Volumes/RanczLab2/DATA_NEW/Cohort3_rotation/V...</td>\n",
       "      <td>-3.849120</td>\n",
       "      <td>-7.980359</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.000</td>\n",
       "      <td>-12.7300</td>\n",
       "      <td>12.7300</td>\n",
       "      <td>12.7300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.967492</td>\n",
       "      <td>39.793745</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>B6J2781</td>\n",
       "      <td>right</td>\n",
       "      <td>No halt</td>\n",
       "      <td>_No halt_right_turns_baselined_data.csv</td>\n",
       "      <td>/Volumes/RanczLab2/DATA_NEW/Cohort3_rotation/V...</td>\n",
       "      <td>5.710246</td>\n",
       "      <td>3.297306</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.029</td>\n",
       "      <td>15.5250</td>\n",
       "      <td>15.5250</td>\n",
       "      <td>15.5250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.534692</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>B6J2781</td>\n",
       "      <td>left</td>\n",
       "      <td>No halt</td>\n",
       "      <td>_No halt_left_turns_baselined_data.csv</td>\n",
       "      <td>/Volumes/RanczLab2/DATA_NEW/Cohort3_rotation/V...</td>\n",
       "      <td>-2.603119</td>\n",
       "      <td>-1.357629</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.153</td>\n",
       "      <td>13.6150</td>\n",
       "      <td>13.6150</td>\n",
       "      <td>13.6150</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.695436</td>\n",
       "      <td>1.264212</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>B6J2782</td>\n",
       "      <td>right</td>\n",
       "      <td>Apply halt</td>\n",
       "      <td>_Apply halt_2s_right_turns_baselined_data.csv</td>\n",
       "      <td>/Volumes/RanczLab2/DATA_NEW/Cohort3_rotation/V...</td>\n",
       "      <td>9.631554</td>\n",
       "      <td>10.556258</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.303</td>\n",
       "      <td>27.1805</td>\n",
       "      <td>27.1805</td>\n",
       "      <td>27.1805</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21.332515</td>\n",
       "      <td>1.131739</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>B6J2782</td>\n",
       "      <td>left</td>\n",
       "      <td>Apply halt</td>\n",
       "      <td>_Apply halt_2s_left_turns_baselined_data.csv</td>\n",
       "      <td>/Volumes/RanczLab2/DATA_NEW/Cohort3_rotation/V...</td>\n",
       "      <td>-2.119390</td>\n",
       "      <td>-0.397812</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-3.5321</td>\n",
       "      <td>3.5321</td>\n",
       "      <td>3.5321</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.708121</td>\n",
       "      <td>2.593148</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>B6J2782</td>\n",
       "      <td>right</td>\n",
       "      <td>No halt</td>\n",
       "      <td>_No halt_right_turns_baselined_data.csv</td>\n",
       "      <td>/Volumes/RanczLab2/DATA_NEW/Cohort3_rotation/V...</td>\n",
       "      <td>11.639326</td>\n",
       "      <td>11.919999</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.043</td>\n",
       "      <td>24.8298</td>\n",
       "      <td>24.8298</td>\n",
       "      <td>24.8298</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24.828369</td>\n",
       "      <td>2.643412</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>B6J2782</td>\n",
       "      <td>left</td>\n",
       "      <td>No halt</td>\n",
       "      <td>_No halt_left_turns_baselined_data.csv</td>\n",
       "      <td>/Volumes/RanczLab2/DATA_NEW/Cohort3_rotation/V...</td>\n",
       "      <td>-12.602863</td>\n",
       "      <td>-9.531000</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.776</td>\n",
       "      <td>-16.7686</td>\n",
       "      <td>16.7686</td>\n",
       "      <td>16.7686</td>\n",
       "      <td>1.496000</td>\n",
       "      <td>19.063818</td>\n",
       "      <td>3.676406</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>B6J2783</td>\n",
       "      <td>right</td>\n",
       "      <td>Apply halt</td>\n",
       "      <td>_Apply halt_2s_right_turns_baselined_data.csv</td>\n",
       "      <td>/Volumes/RanczLab2/DATA_NEW/Cohort3_rotation/V...</td>\n",
       "      <td>0.526897</td>\n",
       "      <td>-2.735657</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.144</td>\n",
       "      <td>18.1079</td>\n",
       "      <td>18.1079</td>\n",
       "      <td>18.1079</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.001816</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>B6J2783</td>\n",
       "      <td>left</td>\n",
       "      <td>Apply halt</td>\n",
       "      <td>_Apply halt_2s_left_turns_baselined_data.csv</td>\n",
       "      <td>/Volumes/RanczLab2/DATA_NEW/Cohort3_rotation/V...</td>\n",
       "      <td>-9.305842</td>\n",
       "      <td>-9.649104</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.000</td>\n",
       "      <td>-20.8758</td>\n",
       "      <td>20.8758</td>\n",
       "      <td>20.8758</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>19.897712</td>\n",
       "      <td>7.689660</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>B6J2783</td>\n",
       "      <td>right</td>\n",
       "      <td>No halt</td>\n",
       "      <td>_No halt_right_turns_baselined_data.csv</td>\n",
       "      <td>/Volumes/RanczLab2/DATA_NEW/Cohort3_rotation/V...</td>\n",
       "      <td>2.417707</td>\n",
       "      <td>-1.020023</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.804</td>\n",
       "      <td>-17.7208</td>\n",
       "      <td>17.7208</td>\n",
       "      <td>17.7208</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.695716</td>\n",
       "      <td>45.999007</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>B6J2783</td>\n",
       "      <td>left</td>\n",
       "      <td>No halt</td>\n",
       "      <td>_No halt_left_turns_baselined_data.csv</td>\n",
       "      <td>/Volumes/RanczLab2/DATA_NEW/Cohort3_rotation/V...</td>\n",
       "      <td>-8.857115</td>\n",
       "      <td>-8.839440</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.441</td>\n",
       "      <td>-25.6498</td>\n",
       "      <td>25.6498</td>\n",
       "      <td>25.6498</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>19.816924</td>\n",
       "      <td>2.866057</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      mouse direction       group  \\\n",
       "0   B6J2718     right  Apply halt   \n",
       "1   B6J2718      left  Apply halt   \n",
       "2   B6J2718     right     No halt   \n",
       "3   B6J2718      left     No halt   \n",
       "4   B6J2719     right  Apply halt   \n",
       "5   B6J2719      left  Apply halt   \n",
       "6   B6J2719     right     No halt   \n",
       "7   B6J2719      left     No halt   \n",
       "8   B6J2721     right  Apply halt   \n",
       "9   B6J2721      left  Apply halt   \n",
       "10  B6J2721     right     No halt   \n",
       "11  B6J2721      left     No halt   \n",
       "12  B6J2722     right  Apply halt   \n",
       "13  B6J2722      left  Apply halt   \n",
       "14  B6J2722     right     No halt   \n",
       "15  B6J2722      left     No halt   \n",
       "16  B6J2723     right  Apply halt   \n",
       "17  B6J2723      left  Apply halt   \n",
       "18  B6J2723     right     No halt   \n",
       "19  B6J2723      left     No halt   \n",
       "20  B6J2780     right  Apply halt   \n",
       "21  B6J2780      left  Apply halt   \n",
       "22  B6J2780     right     No halt   \n",
       "23  B6J2781     right  Apply halt   \n",
       "24  B6J2781      left  Apply halt   \n",
       "25  B6J2781     right     No halt   \n",
       "26  B6J2781      left     No halt   \n",
       "27  B6J2782     right  Apply halt   \n",
       "28  B6J2782      left  Apply halt   \n",
       "29  B6J2782     right     No halt   \n",
       "30  B6J2782      left     No halt   \n",
       "31  B6J2783     right  Apply halt   \n",
       "32  B6J2783      left  Apply halt   \n",
       "33  B6J2783     right     No halt   \n",
       "34  B6J2783      left     No halt   \n",
       "\n",
       "                                     event_suffix  \\\n",
       "0   _Apply halt_2s_right_turns_baselined_data.csv   \n",
       "1    _Apply halt_2s_left_turns_baselined_data.csv   \n",
       "2         _No halt_right_turns_baselined_data.csv   \n",
       "3          _No halt_left_turns_baselined_data.csv   \n",
       "4   _Apply halt_2s_right_turns_baselined_data.csv   \n",
       "5    _Apply halt_2s_left_turns_baselined_data.csv   \n",
       "6         _No halt_right_turns_baselined_data.csv   \n",
       "7          _No halt_left_turns_baselined_data.csv   \n",
       "8   _Apply halt_2s_right_turns_baselined_data.csv   \n",
       "9    _Apply halt_2s_left_turns_baselined_data.csv   \n",
       "10        _No halt_right_turns_baselined_data.csv   \n",
       "11         _No halt_left_turns_baselined_data.csv   \n",
       "12  _Apply halt_2s_right_turns_baselined_data.csv   \n",
       "13   _Apply halt_2s_left_turns_baselined_data.csv   \n",
       "14        _No halt_right_turns_baselined_data.csv   \n",
       "15         _No halt_left_turns_baselined_data.csv   \n",
       "16  _Apply halt_2s_right_turns_baselined_data.csv   \n",
       "17   _Apply halt_2s_left_turns_baselined_data.csv   \n",
       "18        _No halt_right_turns_baselined_data.csv   \n",
       "19         _No halt_left_turns_baselined_data.csv   \n",
       "20  _Apply halt_2s_right_turns_baselined_data.csv   \n",
       "21   _Apply halt_2s_left_turns_baselined_data.csv   \n",
       "22        _No halt_right_turns_baselined_data.csv   \n",
       "23  _Apply halt_2s_right_turns_baselined_data.csv   \n",
       "24   _Apply halt_2s_left_turns_baselined_data.csv   \n",
       "25        _No halt_right_turns_baselined_data.csv   \n",
       "26         _No halt_left_turns_baselined_data.csv   \n",
       "27  _Apply halt_2s_right_turns_baselined_data.csv   \n",
       "28   _Apply halt_2s_left_turns_baselined_data.csv   \n",
       "29        _No halt_right_turns_baselined_data.csv   \n",
       "30         _No halt_left_turns_baselined_data.csv   \n",
       "31  _Apply halt_2s_right_turns_baselined_data.csv   \n",
       "32   _Apply halt_2s_left_turns_baselined_data.csv   \n",
       "33        _No halt_right_turns_baselined_data.csv   \n",
       "34         _No halt_left_turns_baselined_data.csv   \n",
       "\n",
       "                                             csv_path   pre_mean  post_mean  \\\n",
       "0   /Volumes/RanczLab2/DATA_NEW/Cohort1_rotation/V...   8.700897   8.593928   \n",
       "1   /Volumes/RanczLab2/DATA_NEW/Cohort1_rotation/V...  -7.426496  -6.166812   \n",
       "2   /Volumes/RanczLab2/DATA_NEW/Cohort1_rotation/V...   8.075251  10.711407   \n",
       "3   /Volumes/RanczLab2/DATA_NEW/Cohort1_rotation/V...  -8.373595  -6.164667   \n",
       "4   /Volumes/RanczLab2/DATA_NEW/Cohort1_rotation/V...   8.934042   5.074776   \n",
       "5   /Volumes/RanczLab2/DATA_NEW/Cohort1_rotation/V...  -7.531351  -5.470690   \n",
       "6   /Volumes/RanczLab2/DATA_NEW/Cohort1_rotation/V...  13.667830  10.036068   \n",
       "7   /Volumes/RanczLab2/DATA_NEW/Cohort1_rotation/V... -10.597337  -6.715189   \n",
       "8   /Volumes/RanczLab2/DATA_NEW/Cohort1_rotation/V...  10.560097   6.334559   \n",
       "9   /Volumes/RanczLab2/DATA_NEW/Cohort1_rotation/V...  -5.402623  -3.445785   \n",
       "10  /Volumes/RanczLab2/DATA_NEW/Cohort1_rotation/V...  10.923739  10.781615   \n",
       "11  /Volumes/RanczLab2/DATA_NEW/Cohort1_rotation/V...  -7.439175  -9.021315   \n",
       "12  /Volumes/RanczLab2/DATA_NEW/Cohort1_rotation/V...   6.109308   2.305159   \n",
       "13  /Volumes/RanczLab2/DATA_NEW/Cohort1_rotation/V...  -9.446984  -9.206770   \n",
       "14  /Volumes/RanczLab2/DATA_NEW/Cohort1_rotation/V...   3.943848   3.392899   \n",
       "15  /Volumes/RanczLab2/DATA_NEW/Cohort1_rotation/V... -12.151944 -10.434755   \n",
       "16  /Volumes/RanczLab2/DATA_NEW/Cohort1_rotation/V...   0.985852  -3.217040   \n",
       "17  /Volumes/RanczLab2/DATA_NEW/Cohort1_rotation/V...  -9.012121  -8.648950   \n",
       "18  /Volumes/RanczLab2/DATA_NEW/Cohort1_rotation/V...   1.867592   1.376447   \n",
       "19  /Volumes/RanczLab2/DATA_NEW/Cohort1_rotation/V...  -6.219690  -7.067221   \n",
       "20  /Volumes/RanczLab2/DATA_NEW/Cohort3_rotation/V...   4.772713   4.428471   \n",
       "21  /Volumes/RanczLab2/DATA_NEW/Cohort3_rotation/V...  -0.566796   3.784272   \n",
       "22  /Volumes/RanczLab2/DATA_NEW/Cohort3_rotation/V...   6.866136   7.080836   \n",
       "23  /Volumes/RanczLab2/DATA_NEW/Cohort3_rotation/V...   6.120463   4.499280   \n",
       "24  /Volumes/RanczLab2/DATA_NEW/Cohort3_rotation/V...  -3.849120  -7.980359   \n",
       "25  /Volumes/RanczLab2/DATA_NEW/Cohort3_rotation/V...   5.710246   3.297306   \n",
       "26  /Volumes/RanczLab2/DATA_NEW/Cohort3_rotation/V...  -2.603119  -1.357629   \n",
       "27  /Volumes/RanczLab2/DATA_NEW/Cohort3_rotation/V...   9.631554  10.556258   \n",
       "28  /Volumes/RanczLab2/DATA_NEW/Cohort3_rotation/V...  -2.119390  -0.397812   \n",
       "29  /Volumes/RanczLab2/DATA_NEW/Cohort3_rotation/V...  11.639326  11.919999   \n",
       "30  /Volumes/RanczLab2/DATA_NEW/Cohort3_rotation/V... -12.602863  -9.531000   \n",
       "31  /Volumes/RanczLab2/DATA_NEW/Cohort3_rotation/V...   0.526897  -2.735657   \n",
       "32  /Volumes/RanczLab2/DATA_NEW/Cohort3_rotation/V...  -9.305842  -9.649104   \n",
       "33  /Volumes/RanczLab2/DATA_NEW/Cohort3_rotation/V...   2.417707  -1.020023   \n",
       "34  /Volumes/RanczLab2/DATA_NEW/Cohort3_rotation/V...  -8.857115  -8.839440   \n",
       "\n",
       "    pre_sign  post_sign  direction_changed  sign_used  time_to_peak  \\\n",
       "0          1          1              False       -1.0         0.915   \n",
       "1         -1         -1              False        1.0         0.159   \n",
       "2          1          1              False       -1.0         0.066   \n",
       "3         -1         -1              False        1.0         0.000   \n",
       "4          1          1              False       -1.0         0.151   \n",
       "5         -1         -1              False        1.0         0.977   \n",
       "6          1          1              False       -1.0         0.000   \n",
       "7         -1         -1              False        1.0         1.505   \n",
       "8          1          1              False       -1.0         0.500   \n",
       "9         -1         -1              False        1.0         0.105   \n",
       "10         1          1              False       -1.0         0.974   \n",
       "11        -1         -1              False        1.0         1.262   \n",
       "12         1          1              False       -1.0         1.757   \n",
       "13        -1         -1              False        1.0         0.000   \n",
       "14         1          1              False       -1.0         0.306   \n",
       "15        -1         -1              False        1.0         0.156   \n",
       "16         1         -1               True       -1.0         0.551   \n",
       "17        -1         -1              False        1.0         0.000   \n",
       "18         1          1              False       -1.0         0.217   \n",
       "19        -1         -1              False        1.0         0.289   \n",
       "20         1          1              False       -1.0         1.022   \n",
       "21        -1          1               True        1.0         0.852   \n",
       "22         1          1              False       -1.0         0.788   \n",
       "23         1          1              False       -1.0         0.134   \n",
       "24        -1         -1              False        1.0         2.000   \n",
       "25         1          1              False       -1.0         1.029   \n",
       "26        -1         -1              False        1.0         1.153   \n",
       "27         1          1              False       -1.0         0.303   \n",
       "28        -1         -1              False        1.0         0.000   \n",
       "29         1          1              False       -1.0         0.043   \n",
       "30        -1         -1              False        1.0         1.776   \n",
       "31         1         -1               True       -1.0         0.144   \n",
       "32        -1         -1              False        1.0         2.000   \n",
       "33         1         -1               True       -1.0         0.804   \n",
       "34        -1         -1              False        1.0         1.441   \n",
       "\n",
       "    peak_velocity_signed  peak_velocity_magnitude  peak_velocity_abs_1s  \\\n",
       "0                26.8795                  26.8795               26.8795   \n",
       "1               -23.8115                  23.8115               23.8115   \n",
       "2                26.9865                  26.9865               26.9865   \n",
       "3               -21.5464                  21.5464               21.5464   \n",
       "4                20.0803                  20.0803               20.0803   \n",
       "5               -26.2773                  26.2773               26.2773   \n",
       "6                28.3602                  28.3602               28.3602   \n",
       "7                26.4874                  26.4874               26.4874   \n",
       "8                28.1413                  28.1413               28.1413   \n",
       "9               -20.1320                  20.1320               20.1320   \n",
       "10               28.7013                  28.7013               28.7013   \n",
       "11              -24.3416                  24.3416               24.3416   \n",
       "12              -15.8938                  15.8938               15.8938   \n",
       "13              -26.5918                  26.5918               26.5918   \n",
       "14               15.3704                  15.3704               15.3704   \n",
       "15              -24.6687                  24.6687               24.6687   \n",
       "16              -16.3982                  16.3982               16.3982   \n",
       "17              -18.7832                  18.7832               18.7832   \n",
       "18                7.4796                   7.4796                7.4796   \n",
       "19              -21.7282                  21.7282               21.7282   \n",
       "20                9.3821                   9.3821                9.3821   \n",
       "21                7.3547                   7.3547                7.3547   \n",
       "22               14.1194                  14.1194               14.1194   \n",
       "23               15.1304                  15.1304               15.1304   \n",
       "24              -12.7300                  12.7300               12.7300   \n",
       "25               15.5250                  15.5250               15.5250   \n",
       "26               13.6150                  13.6150               13.6150   \n",
       "27               27.1805                  27.1805               27.1805   \n",
       "28               -3.5321                   3.5321                3.5321   \n",
       "29               24.8298                  24.8298               24.8298   \n",
       "30              -16.7686                  16.7686               16.7686   \n",
       "31               18.1079                  18.1079               18.1079   \n",
       "32              -20.8758                  20.8758               20.8758   \n",
       "33              -17.7208                  17.7208               17.7208   \n",
       "34              -25.6498                  25.6498               25.6498   \n",
       "\n",
       "    latency_to_fraction_peak    auc_abs  decay_tau  expected_sign  \n",
       "0                   0.000000  20.408136   3.449111             -1  \n",
       "1                   0.000000  15.897289   4.076830              1  \n",
       "2                   0.000000  23.915696   3.170291             -1  \n",
       "3                   0.000000  19.303492   3.124289              1  \n",
       "4                   0.000000  13.521531   5.012169             -1  \n",
       "5                   0.000000  20.392304   4.808638              1  \n",
       "6                   0.000000  22.580627   3.339692             -1  \n",
       "7                   0.000000  22.658935  41.443389              1  \n",
       "8                   0.000000  15.256685  43.949530             -1  \n",
       "9                   0.000000  15.781462   5.154824              1  \n",
       "10                  0.000000  22.312956   3.828418             -1  \n",
       "11                  0.084000  20.374881   1.431185              1  \n",
       "12                  0.000000  15.570030   4.035738             -1  \n",
       "13                  0.000000  22.027632   4.063660              1  \n",
       "14                  0.000000   7.766911  15.447194             -1  \n",
       "15                  0.000000  22.208195   4.191272              1  \n",
       "16                  0.173000  13.919669        NaN             -1  \n",
       "17                  0.000000  18.061436   6.646003              1  \n",
       "18                  0.000000   6.045231   6.247141             -1  \n",
       "19                  0.000000  14.930709   4.421527              1  \n",
       "20                  0.540000   8.848248   5.362025             -1  \n",
       "21                  0.472477   7.568219   1.235226              1  \n",
       "22                  0.000000  14.136194   5.748712             -1  \n",
       "23                  0.000000  12.884107   2.790457             -1  \n",
       "24                  0.000000  15.967492  39.793745              1  \n",
       "25                  0.000000   9.534692        NaN             -1  \n",
       "26                  0.000000  10.695436   1.264212              1  \n",
       "27                  0.000000  21.332515   1.131739             -1  \n",
       "28                  0.000000   2.708121   2.593148              1  \n",
       "29                  0.000000  24.828369   2.643412             -1  \n",
       "30                  1.496000  19.063818   3.676406              1  \n",
       "31                  0.000000  16.001816        NaN             -1  \n",
       "32                  0.000000  19.897712   7.689660              1  \n",
       "33                  0.000000  15.695716  45.999007             -1  \n",
       "34                  0.000000  19.816924   2.866057              1  "
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data and compute direction metrics\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "turn_event_files = find_turn_event_files(DATA_DIRS, EVENT_SUFFIXES, SELECTED_MICE)\n",
    "print(f\"Found {len(turn_event_files)} turn-specific files\")\n",
    "\n",
    "records: List[Dict[str, object]] = []\n",
    "trace_records: List[pd.DataFrame] = []\n",
    "errors: List[str] = []\n",
    "\n",
    "for entry in turn_event_files:\n",
    "    csv_path = entry[\"csv_path\"]\n",
    "    try:\n",
    "        df = load_motor_velocity(csv_path, TIME_COLUMN, VELOCITY_COLUMN)\n",
    "    except Exception as exc:  # noqa: BLE001\n",
    "        errors.append(f\"{csv_path}: {exc}\")\n",
    "        continue\n",
    "\n",
    "    metrics = analyse_turn_direction(df, PRE_WINDOW, POST_WINDOW, ZERO_THRESHOLD)\n",
    "    expected_sign = EXPECTED_DIRECTION_SIGN.get(entry[\"direction\"], np.nan)\n",
    "    timing_metrics = compute_turn_timing_metrics(\n",
    "        df,\n",
    "        expected_sign,\n",
    "        ZERO_THRESHOLD,\n",
    "        PEAK_WINDOW,\n",
    "        AUC_WINDOW,\n",
    "        PRE_WINDOW,\n",
    "        LATENCY_FRACTION,\n",
    "        DECAY_FIT_WINDOW,\n",
    "    )\n",
    "\n",
    "    enriched_trace = df.copy()\n",
    "    enriched_trace[\"group\"] = entry[\"group\"]\n",
    "    enriched_trace[\"mouse\"] = entry[\"mouse\"]\n",
    "    enriched_trace[\"direction\"] = entry[\"direction\"]\n",
    "    enriched_trace[\"csv_path\"] = str(csv_path)\n",
    "    enriched_trace[\"sign_used\"] = timing_metrics.get(\"sign_used\", expected_sign)\n",
    "    trace_records.append(enriched_trace)\n",
    "\n",
    "    records.append(\n",
    "        {\n",
    "            **entry,\n",
    "            **metrics,\n",
    "            **timing_metrics,\n",
    "            \"expected_sign\": expected_sign,\n",
    "            \"csv_path\": str(csv_path),\n",
    "        }\n",
    "    )\n",
    "\n",
    "results_df = pd.DataFrame(records)\n",
    "trace_samples_df = pd.concat(trace_records, ignore_index=True) if trace_records else pd.DataFrame()\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect any files that could not be processed\n",
    "errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59fdb99",
   "metadata": {},
   "source": [
    "### Decay fit diagnostics (optional)\n",
    "If enabled, diagnose why exponential decay fitting fails for specific examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "8cc63b3c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Run detailed diagnostics on failed decay fits\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "if ENABLE_DECAY_DIAGNOSTICS and not results_df.empty:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"DECAY FIT DIAGNOSTICS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Find files where decay fit failed (tau is NaN)\n",
    "    failed_fits = results_df[results_df[\"decay_tau\"].isna()].copy()\n",
    "    \n",
    "    if failed_fits.empty:\n",
    "        print(\"✅ All decay fits succeeded (no NaN values)\")\n",
    "    else:\n",
    "        n_failed = len(failed_fits)\n",
    "        print(f\"⚠️ Found {n_failed} failed decay fits out of {len(results_df)} total\")\n",
    "        print(f\"   ({100 * n_failed / len(results_df):.1f}% failure rate)\\n\")\n",
    "        \n",
    "        # Analyze distribution of failures\n",
    "        print(\"Failure distribution:\")\n",
    "        for group_name, group_df in failed_fits.groupby(\"group\", dropna=False):\n",
    "            n_group = len(group_df)\n",
    "            n_group_total = len(results_df[results_df[\"group\"] == group_name])\n",
    "            print(f\"  {group_name}: {n_group}/{n_group_total} failed ({100*n_group/n_group_total:.1f}%)\")\n",
    "        \n",
    "        # Run detailed diagnostics on a few examples\n",
    "        n_to_diagnose = min(MAX_DIAGNOSTIC_EXAMPLES, len(failed_fits))\n",
    "        print(f\"\\nRunning detailed diagnostics on {n_to_diagnose} examples...\")\n",
    "        \n",
    "        diagnostic_results = []\n",
    "        \n",
    "        for idx, row in failed_fits.head(n_to_diagnose).iterrows():\n",
    "            csv_path = Path(row[\"csv_path\"])\n",
    "            try:\n",
    "                df = load_motor_velocity(csv_path, TIME_COLUMN, VELOCITY_COLUMN)\n",
    "            except Exception as exc:\n",
    "                print(f\"⚠️ Could not load {csv_path}: {exc}\")\n",
    "                continue\n",
    "            \n",
    "            # Extract decay window data\n",
    "            decay_mask = (df[\"time\"] >= DECAY_FIT_WINDOW[0]) & (df[\"time\"] <= DECAY_FIT_WINDOW[1])\n",
    "            decay_data = df.loc[decay_mask].copy()\n",
    "            \n",
    "            if decay_data.empty:\n",
    "                print(f\"⚠️ No data in decay window for {row['mouse']} - {row['group']}\")\n",
    "                continue\n",
    "            \n",
    "            # Group by time and average (same as original analysis)\n",
    "            decay_data[\"abs_velocity\"] = decay_data[\"velocity\"].abs()\n",
    "            per_time = (\n",
    "                decay_data.groupby(\"time\", dropna=False)[\"abs_velocity\"]\n",
    "                .mean()\n",
    "                .reset_index()\n",
    "                .sort_values(\"time\")\n",
    "            )\n",
    "            \n",
    "            # Run diagnostic\n",
    "            title = f\"{row['mouse']} | {row['group']} | {row['direction']} turn\"\n",
    "            tau, diag_info = diagnostic_exponential_fit(\n",
    "                per_time[\"time\"].values,\n",
    "                per_time[\"abs_velocity\"].values,\n",
    "                title=title,\n",
    "                show_plots=True,\n",
    "            )\n",
    "            \n",
    "            diagnostic_results.append({\n",
    "                \"mouse\": row[\"mouse\"],\n",
    "                \"group\": row[\"group\"],\n",
    "                \"direction\": row[\"direction\"],\n",
    "                \"csv_path\": str(csv_path),\n",
    "                **diag_info,\n",
    "            })\n",
    "        \n",
    "        # Save diagnostic results\n",
    "        if diagnostic_results:\n",
    "            diagnostic_df = pd.DataFrame(diagnostic_results)\n",
    "            if OUTPUT_DIR is not None:\n",
    "                diagnostic_df.to_csv(OUTPUT_DIR / \"decay_fit_diagnostics.csv\", index=False)\n",
    "                print(f\"\\n✅ Saved diagnostic results to: {OUTPUT_DIR / 'decay_fit_diagnostics.csv'}\")\n",
    "            \n",
    "            # Summary of failure reasons\n",
    "            print(\"\\n\" + \"-\"*60)\n",
    "            print(\"Summary of failure reasons:\")\n",
    "            if \"failure_reason\" in diagnostic_df.columns:\n",
    "                for reason, count_df in diagnostic_df.groupby(\"failure_reason\"):\n",
    "                    if pd.notna(reason):\n",
    "                        print(f\"  {reason}: {len(count_df)} cases\")\n",
    "elif ENABLE_DECAY_DIAGNOSTICS:\n",
    "    print(\"⚠️ Decay diagnostics enabled but no results available to diagnose\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running velocity summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Running velocity (Velocity_0X_Baseline) analysis\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "running_records: List[Dict[str, object]] = []\n",
    "running_trace_records: List[pd.DataFrame] = []\n",
    "running_errors: List[str] = []\n",
    "running_peak_diagnostics: List[Dict[str, object]] = []\n",
    "\n",
    "if not results_df.empty:\n",
    "    for _, row in results_df.iterrows():\n",
    "        csv_path = Path(row[\"csv_path\"])\n",
    "        try:\n",
    "            running_df = load_time_series(csv_path, TIME_COLUMN, RUNNING_COLUMN, value_alias=\"velocity\")\n",
    "        except Exception as exc:  # noqa: BLE001\n",
    "            running_errors.append(f\"{csv_path}: {exc}\")\n",
    "            continue\n",
    "\n",
    "        peak_window_mask = (running_df[\"time\"] >= 0.0) & (running_df[\"time\"] <= 1.0)\n",
    "        if peak_window_mask.any():\n",
    "            raw_peak_abs_mps = float(running_df.loc[peak_window_mask, \"velocity\"].abs().max())\n",
    "        else:\n",
    "            raw_peak_abs_mps = float(\"nan\")\n",
    "\n",
    "        running_df[\"velocity\"] = running_df[\"velocity\"] * 100.0\n",
    "\n",
    "        running_metrics = analyse_turn_direction(running_df, PRE_WINDOW, POST_WINDOW, ZERO_THRESHOLD)\n",
    "        expected_sign = EXPECTED_DIRECTION_SIGN.get(row[\"direction\"], np.nan)\n",
    "\n",
    "        running_trace = running_df.copy()\n",
    "        running_trace[\"group\"] = row[\"group\"]\n",
    "        running_trace[\"mouse\"] = row[\"mouse\"]\n",
    "        running_trace[\"turn_label\"] = row[\"direction\"]\n",
    "        running_trace[\"csv_path\"] = row[\"csv_path\"]\n",
    "        running_trace_records.append(running_trace)\n",
    "\n",
    "        running_records.append(\n",
    "            {\n",
    "                \"mouse\": row[\"mouse\"],\n",
    "                \"turn_label\": row[\"direction\"],\n",
    "                \"group\": row[\"group\"],\n",
    "                \"event_suffix\": row[\"event_suffix\"],\n",
    "                \"csv_path\": row[\"csv_path\"],\n",
    "                **running_metrics,\n",
    "                \"expected_sign\": expected_sign,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        running_peak_diagnostics.append(\n",
    "            {\n",
    "                \"mouse\": row[\"mouse\"],\n",
    "                \"group\": row[\"group\"],\n",
    "                \"turn_label\": row[\"direction\"],\n",
    "                \"csv_path\": row[\"csv_path\"],\n",
    "                \"peak_abs_velocity_mps\": raw_peak_abs_mps,\n",
    "                \"peak_abs_velocity_cmps\": raw_peak_abs_mps * 100.0 if math.isfinite(raw_peak_abs_mps) else float(\"nan\"),\n",
    "            }\n",
    "        )\n",
    "\n",
    "running_results_df = pd.DataFrame(running_records)\n",
    "if not running_results_df.empty:\n",
    "    running_results_df[\"direction\"] = \"All turns\"\n",
    "running_trace_samples_df = pd.concat(running_trace_records, ignore_index=True) if running_trace_records else pd.DataFrame()\n",
    "if not running_trace_samples_df.empty:\n",
    "    running_trace_samples_df[\"direction\"] = \"All turns\"\n",
    "\n",
    "running_peak_diag_df = pd.DataFrame(running_peak_diagnostics)\n",
    "\n",
    "# Save running results\n",
    "if OUTPUT_DIR is not None and not running_results_df.empty:\n",
    "    running_results_df.to_csv(OUTPUT_DIR / \"running_velocity_per_file.csv\", index=False)\n",
    "    print(f\"✅ Saved: running_velocity_per_file.csv\")\n",
    "\n",
    "running_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f545f36c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "if running_peak_diag_df.empty:\n",
    "    print(\"⚠️ No running peak diagnostics available\")\n",
    "else:\n",
    "    display(Markdown(\"#### Running peak velocity diagnostics (per file)\"))\n",
    "    display(\n",
    "        running_peak_diag_df.sort_values(\"peak_abs_velocity_cmps\", ascending=False)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    if OUTPUT_DIR is not None:\n",
    "        running_peak_diag_df.to_csv(OUTPUT_DIR / \"running_peak_velocity_diagnostic.csv\", index=False)\n",
    "        print(f\"✅ Saved: running_peak_velocity_diagnostic.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "running_summary_df = summarise_results(running_results_df)\n",
    "if running_summary_df.empty:\n",
    "    print(\"⚠️ No running velocity statistics available\")\n",
    "else:\n",
    "    for group_name, subdf in running_summary_df.groupby(\"group\", dropna=False):\n",
    "        title = group_name if isinstance(group_name, str) else \"Unknown\"\n",
    "        display(Markdown(f\"#### {title}\"))\n",
    "        display(subdf.drop(columns=[\"group\"], errors=\"ignore\").reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "if running_results_df.empty:\n",
    "    running_mouse_means = pd.DataFrame()\n",
    "else:\n",
    "    running_mouse_means = (\n",
    "        running_results_df.groupby([\"group\", \"mouse\"], dropna=False)\n",
    "        .agg(\n",
    "            post_mean=(\"post_mean\", \"mean\"),\n",
    "            pre_mean=(\"pre_mean\", \"mean\"),\n",
    "            n_files=(\"csv_path\", \"count\"),\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "    running_mouse_means[\"expected_sign\"] = np.nan\n",
    "    running_mouse_means[\"post_sign\"] = running_mouse_means[\"post_mean\"].apply(lambda v: sign_with_threshold(v, ZERO_THRESHOLD))\n",
    "    running_mouse_means[\"direction\"] = \"All turns\"\n",
    "\n",
    "running_mouse_means\n",
    "\n",
    "# Save running per-mouse metrics\n",
    "if OUTPUT_DIR is not None and not running_mouse_means.empty:\n",
    "    running_mouse_means.to_csv(OUTPUT_DIR / \"running_velocity_per_mouse.csv\", index=False)\n",
    "    print(f\"✅ Saved: running_velocity_per_mouse.csv\")\n",
    "\n",
    "if running_trace_samples_df.empty:\n",
    "    running_peak_abs_df = pd.DataFrame()\n",
    "else:\n",
    "    running_peak_mask = (running_trace_samples_df[\"time\"] >= 0.0) & (running_trace_samples_df[\"time\"] <= 1.0)\n",
    "    running_peak_subset = running_trace_samples_df.loc[running_peak_mask].copy()\n",
    "    if running_peak_subset.empty:\n",
    "        running_peak_abs_df = pd.DataFrame()\n",
    "    else:\n",
    "        running_peak_subset[\"abs_velocity\"] = running_peak_subset[\"velocity\"].abs()\n",
    "        running_per_record_peak = (\n",
    "            running_peak_subset.groupby([\"mouse\", \"group\", \"csv_path\"], dropna=False)[\"abs_velocity\"]\n",
    "            .max()\n",
    "            .reset_index(name=\"peak_velocity_abs_1s\")\n",
    "        )\n",
    "        running_peak_abs_df = (\n",
    "            running_per_record_peak.groupby([\"mouse\", \"group\"], dropna=False)[\"peak_velocity_abs_1s\"]\n",
    "            .mean()\n",
    "            .reset_index()\n",
    "        )\n",
    "\n",
    "if running_mouse_means.empty and running_peak_abs_df.empty:\n",
    "    running_metrics_combined = pd.DataFrame()\n",
    "else:\n",
    "    if running_mouse_means.empty:\n",
    "        running_metrics_combined = running_peak_abs_df.copy()\n",
    "    else:\n",
    "        running_metrics_combined = running_mouse_means[[\"group\", \"mouse\", \"post_mean\"]].copy()\n",
    "        if \"direction\" in running_mouse_means.columns and \"direction\" not in running_metrics_combined.columns:\n",
    "            running_metrics_combined[\"direction\"] = running_mouse_means[\"direction\"]\n",
    "        if \"n_files\" in running_mouse_means.columns and \"n_files\" not in running_metrics_combined.columns:\n",
    "            running_metrics_combined[\"n_files\"] = running_mouse_means[\"n_files\"]\n",
    "        if \"pre_mean\" in running_mouse_means.columns and \"pre_mean\" not in running_metrics_combined.columns:\n",
    "            running_metrics_combined[\"pre_mean\"] = running_mouse_means[\"pre_mean\"]\n",
    "        if \"expected_sign\" in running_mouse_means.columns and \"expected_sign\" not in running_metrics_combined.columns:\n",
    "            running_metrics_combined[\"expected_sign\"] = running_mouse_means[\"expected_sign\"]\n",
    "        if \"post_sign\" in running_mouse_means.columns and \"post_sign\" not in running_metrics_combined.columns:\n",
    "            running_metrics_combined[\"post_sign\"] = running_mouse_means[\"post_sign\"]\n",
    "        if running_peak_abs_df.empty:\n",
    "            pass\n",
    "        else:\n",
    "            running_metrics_combined = pd.merge(\n",
    "                running_metrics_combined,\n",
    "                running_peak_abs_df,\n",
    "                on=[\"mouse\", \"group\"],\n",
    "                how=\"outer\",\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "if running_trace_samples_df.empty:\n",
    "    running_per_mouse_traces = pd.DataFrame()\n",
    "    running_avg_traces = pd.DataFrame()\n",
    "    print(\"⚠️ No running traces available. Ensure the running velocity analysis cell completed successfully.\")\n",
    "else:\n",
    "    running_per_mouse_traces = (\n",
    "        running_trace_samples_df\n",
    "        .groupby([\"group\", \"mouse\", \"time\"], dropna=False)[\"velocity\"]\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "    )\n",
    "    running_per_mouse_traces[\"direction\"] = \"All turns\"\n",
    "    running_avg_traces = (\n",
    "        running_per_mouse_traces\n",
    "        .groupby([\"group\", \"direction\", \"time\"], dropna=False)\n",
    "        .agg(\n",
    "            mean_velocity=(\"velocity\", \"mean\"),\n",
    "            sem_velocity=(\"velocity\", lambda x: sem(x) if len(x.dropna()) > 1 else 0.0),\n",
    "            n_mice=(\"mouse\", \"nunique\"),\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    n_traces = running_trace_samples_df[\"csv_path\"].nunique()\n",
    "    n_mice_traces = running_trace_samples_df[\"mouse\"].nunique()\n",
    "    print(f\"✅ Running traces aggregated: {n_traces} files across {n_mice_traces} mice.\")\n",
    "    \n",
    "    # Save running average traces\n",
    "    if OUTPUT_DIR is not None:\n",
    "        running_avg_traces.to_csv(OUTPUT_DIR / \"running_velocity_avg_traces.csv\", index=False)\n",
    "        print(f\"✅ Saved: running_velocity_avg_traces.csv\")\n",
    "\n",
    "running_avg_traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "if running_avg_traces.empty:\n",
    "    print(\"⚠️ Skipping running trace plots because no averaged traces are available.\")\n",
    "else:\n",
    "    plot_groups = [\"Apply halt\", \"No halt\"]\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4), sharey=True)\n",
    "\n",
    "    for ax, group_name in zip(axes, plot_groups):\n",
    "        subset = running_avg_traces[running_avg_traces[\"group\"] == group_name]\n",
    "        if subset.empty:\n",
    "            ax.text(0.5, 0.5, \"No data\", ha=\"center\", va=\"center\")\n",
    "            ax.axis(\"off\")\n",
    "            continue\n",
    "\n",
    "        ax.plot(subset[\"time\"], subset[\"mean_velocity\"], color=\"#ff7f0e\", linewidth=1.5)\n",
    "        if \"sem_velocity\" in subset.columns:\n",
    "            upper = subset[\"mean_velocity\"] + subset[\"sem_velocity\"].fillna(0)\n",
    "            lower = subset[\"mean_velocity\"] - subset[\"sem_velocity\"].fillna(0)\n",
    "            ax.fill_between(subset[\"time\"], lower, upper, color=\"#ff7f0e\", alpha=0.2)\n",
    "\n",
    "        ax.axvline(0, color=\"black\", linestyle=\"--\", linewidth=1)\n",
    "        ax.axhline(0, color=\"grey\", linestyle=\":\", linewidth=1)\n",
    "        ax.set_title(f\"Running | {group_name}\")\n",
    "        ax.set_xlabel(\"Time (s)\")\n",
    "        ax.set_ylabel(\"Running velocity (cm/s)\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db1d544",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Running velocity comparisons (Apply halt vs No halt)\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "running_metric_specs = [\n",
    "    (\"post_mean\", f\"Post-halt mean velocity {ANALYSIS_WINDOW_MEAN_PEAK[0]}-{ANALYSIS_WINDOW_MEAN_PEAK[1]}s (cm/s)\"),\n",
    "    (\"peak_velocity_abs_1s\", f\"Peak running velocity {ANALYSIS_WINDOW_MEAN_PEAK[0]}-{ANALYSIS_WINDOW_MEAN_PEAK[1]}s (cm/s)\"),\n",
    "]\n",
    "running_plot_groups = [\"No halt\", \"Apply halt\"]\n",
    "running_stats_df = pd.DataFrame()\n",
    "\n",
    "if running_metrics_combined.empty:\n",
    "    print(\"⚠️ No per-mouse running velocity metrics available for comparison\")\n",
    "else:\n",
    "    running_mouse_colors = assign_mouse_colors_consistent(running_metrics_combined[\"mouse\"].dropna().unique())\n",
    "    available_running_specs = [\n",
    "        (metric, label)\n",
    "        for metric, label in running_metric_specs\n",
    "        if metric in running_metrics_combined.columns\n",
    "    ]\n",
    "\n",
    "    if not available_running_specs:\n",
    "        print(\"⚠️ Running metrics not found in aggregated data\")\n",
    "    else:\n",
    "        subplot_width_cm = 7.5\n",
    "        subplot_height_cm = 7\n",
    "        fig_width = len(available_running_specs) * subplot_width_cm / 2.54\n",
    "        fig_height = subplot_height_cm / 2.54\n",
    "        fig, axes = plt.subplots(\n",
    "            1,\n",
    "            len(available_running_specs),\n",
    "            figsize=(fig_width, fig_height),\n",
    "            sharey=False,\n",
    "        )\n",
    "        if len(available_running_specs) == 1:\n",
    "            axes = [axes]\n",
    "\n",
    "        mean_handles = []\n",
    "        running_mouse_handles: Dict[str, object] = {}\n",
    "        running_stats_records: List[Dict[str, object]] = []\n",
    "\n",
    "        for ax, (metric, label) in zip(axes, available_running_specs):\n",
    "            pivot = running_metrics_combined.pivot(index=\"mouse\", columns=\"group\", values=metric)\n",
    "            groups_present = [group for group in running_plot_groups if group in pivot.columns]\n",
    "            if len(groups_present) < 2:\n",
    "                ax.text(0.5, 0.5, \"Insufficient data\", ha=\"center\", va=\"center\")\n",
    "                ax.axis(\"off\")\n",
    "                continue\n",
    "\n",
    "            group_a, group_b = groups_present[0], groups_present[1]\n",
    "            stats_result = compute_paired_t_test(pivot, group_a, group_b)\n",
    "            stats_result.update(\n",
    "                {\n",
    "                    \"metric\": metric,\n",
    "                    \"metric_label\": label,\n",
    "                    \"group_a\": group_a,\n",
    "                    \"group_b\": group_b,\n",
    "                }\n",
    "            )\n",
    "            running_stats_records.append(stats_result)\n",
    "\n",
    "            x_positions = np.arange(len(groups_present), dtype=float)\n",
    "\n",
    "            for mouse in pivot.index:\n",
    "                values = pivot.loc[mouse, groups_present]\n",
    "                if values.isna().all():\n",
    "                    continue\n",
    "                line, = ax.plot(\n",
    "                    x_positions,\n",
    "                    values.to_numpy(dtype=float),\n",
    "                    marker=\"o\",\n",
    "                    linewidth=1.1,\n",
    "                    alpha=0.65,\n",
    "                    color=running_mouse_colors.get(mouse, \"#1f77b4\"),\n",
    "                    zorder=2,\n",
    "                )\n",
    "                running_mouse_handles.setdefault(mouse, line)\n",
    "\n",
    "            group_means = pivot[groups_present].mean(axis=0)\n",
    "            group_sems = pivot[groups_present].apply(lambda col: sem(col.dropna()), axis=0)\n",
    "            mean_values = group_means.to_numpy(dtype=float)\n",
    "            sem_values = group_sems.to_numpy(dtype=float)\n",
    "            valid_mask = np.isfinite(mean_values) & np.isfinite(sem_values)\n",
    "            if valid_mask.any():\n",
    "                x_valid = x_positions[valid_mask]\n",
    "                mean_valid = mean_values[valid_mask]\n",
    "                sem_valid = sem_values[valid_mask]\n",
    "                ax.fill_between(\n",
    "                    x_valid,\n",
    "                    mean_valid - sem_valid,\n",
    "                    mean_valid + sem_valid,\n",
    "                    color=\"#b3b3b3\",\n",
    "                    alpha=0.3,\n",
    "                    zorder=1,\n",
    "                    linewidth=0,\n",
    "                )\n",
    "\n",
    "            error_container = ax.errorbar(\n",
    "                x_positions,\n",
    "                mean_values,\n",
    "                yerr=sem_values,\n",
    "                fmt=\"o-\",\n",
    "                color=\"#333333\",\n",
    "                linewidth=2.1,\n",
    "                capsize=4,\n",
    "                label=\"Mean ± SEM\",\n",
    "                zorder=3,\n",
    "            )\n",
    "            mean_handles.append(error_container)\n",
    "\n",
    "            ax.set_xticks(x_positions)\n",
    "            ax.set_xticklabels(groups_present)\n",
    "            ax.set_xlim(-0.3, len(groups_present) - 1 + 0.31)\n",
    "            ax.set_title(label)\n",
    "            ax.set_ylabel(label)\n",
    "            ax.grid(True, which=\"both\", axis=\"y\", linestyle=\":\", linewidth=0.7)\n",
    "\n",
    "        legend_handles = list(running_mouse_handles.values())\n",
    "        legend_labels = list(running_mouse_handles.keys())\n",
    "        if mean_handles:\n",
    "            legend_handles.append(mean_handles[0].lines[0])\n",
    "            legend_labels.append(\"Mean ± SEM\")\n",
    "        if legend_handles:\n",
    "            fig.legend(\n",
    "                legend_handles,\n",
    "                legend_labels,\n",
    "                loc=\"center left\",\n",
    "                bbox_to_anchor=(0.85, 0.5),\n",
    "                borderaxespad=0.0,\n",
    "                frameon=True,\n",
    "                fontsize=8,\n",
    "            )\n",
    "\n",
    "        fig.subplots_adjust(left=0, right=0.84, bottom=0.18, top=0.92, wspace=0.4)\n",
    "        if OUTPUT_DIR is not None:\n",
    "            fig.savefig(OUTPUT_DIR / \"running_metric_comparisons.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "            print(f\"✅ Saved: running_metric_comparisons.pdf\")\n",
    "        plt.show()\n",
    "        plt.close(fig)\n",
    "\n",
    "        if running_stats_records:\n",
    "            running_stats_df = pd.DataFrame(running_stats_records)\n",
    "            display(Markdown(\"#### Running metrics paired t-tests\"))\n",
    "            display(running_stats_df)\n",
    "            if OUTPUT_DIR is not None:\n",
    "                running_stats_df.to_csv(OUTPUT_DIR / \"running_metrics_ttests.csv\", index=False)\n",
    "                print(f\"✅ Saved: running_metrics_ttests.csv\")\n",
    "        else:\n",
    "            running_stats_df = pd.DataFrame()\n",
    "        \n",
    "        # Save running combined metrics\n",
    "        if OUTPUT_DIR is not None and not running_metrics_combined.empty:\n",
    "            running_metrics_combined.to_csv(OUTPUT_DIR / \"running_metrics_combined.csv\", index=False)\n",
    "            print(f\"✅ Saved: running_metrics_combined.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f965ee",
   "metadata": {},
   "source": [
    "### Eye tracking metrics (Saccade, Pupil, Eye Position) - Unified Analysis\n",
    "\n",
    "**NOTE**: Eye position analysis keeps left/right turns separate because eye position \n",
    "direction matters. Saccade and pupil metrics are averaged across turn directions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f0e0d2",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Eye tracking analysis - Load all metrics together\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "eye_tracking_records: List[Dict[str, object]] = []\n",
    "eye_tracking_errors: Dict[str, List[str]] = {\n",
    "    \"saccade\": [],\n",
    "    \"pupil\": [],\n",
    "    \"eye_position\": [],\n",
    "}\n",
    "\n",
    "# Define eye tracking metrics to load\n",
    "# Format: (metric_name, column_name, display_name, direction_matters, is_absolute)\n",
    "# - direction_matters: whether to separate left/right turns\n",
    "# - is_absolute: whether values should be treated as absolute (probability, diameter) vs signed (position, velocity)\n",
    "eye_metrics_config = [\n",
    "    (\"saccade\", SACCADE_COLUMN, \"Saccade Probability\", False, True),  # ✓ COMBINED, ABSOLUTE (probability 0-1)\n",
    "    (\"pupil\", PUPIL_COLUMN, \"Pupil Diameter\", False, True),           # ✓ COMBINED, ABSOLUTE (size always positive)\n",
    "    (\"eye_position\", EYE_POSITION_COLUMN, \"Eye Position (X)\", True, False),  # × SEPARATED, SIGNED (position can be + or -)\n",
    "]\n",
    "\n",
    "# Optional: Check available columns in first CSV file (for debugging)\n",
    "SHOW_AVAILABLE_COLUMNS = False  # Set to True to see what columns are in your CSV files\n",
    "\n",
    "if not results_df.empty:\n",
    "    n_files_processed = 0\n",
    "    n_files_with_data = {\"saccade\": 0, \"pupil\": 0, \"eye_position\": 0}\n",
    "    \n",
    "    # Show available columns from first file (if requested)\n",
    "    if SHOW_AVAILABLE_COLUMNS:\n",
    "        first_csv = Path(results_df.iloc[0][\"csv_path\"])\n",
    "        if first_csv.exists():\n",
    "            first_df = pd.read_csv(first_csv)\n",
    "            print(f\"\\n📋 Available columns in {first_csv.name}:\")\n",
    "            eye_related_cols = [col for col in first_df.columns if any(\n",
    "                keyword in col.lower() for keyword in [\"saccade\", \"pupil\", \"eye\", \"ellipse\"]\n",
    "            )]\n",
    "            if eye_related_cols:\n",
    "                print(f\"   Eye-related columns found:\")\n",
    "                for col in eye_related_cols:\n",
    "                    sample_vals = first_df[col].dropna().head(5)\n",
    "                    if len(sample_vals) > 0:\n",
    "                        print(f\"      • {col}: range {sample_vals.min():.6f} to {sample_vals.max():.6f}\")\n",
    "            else:\n",
    "                print(f\"   No eye-related columns found. All columns:\")\n",
    "                for col in first_df.columns[:20]:  # Show first 20\n",
    "                    print(f\"      • {col}\")\n",
    "    \n",
    "    for _, row in results_df.iterrows():\n",
    "        csv_path = Path(row[\"csv_path\"])\n",
    "        n_files_processed += 1\n",
    "        \n",
    "        record = {\n",
    "            \"mouse\": row[\"mouse\"],\n",
    "            \"turn_label\": row[\"direction\"],\n",
    "            \"direction\": row[\"direction\"],  # Keep original direction\n",
    "            \"group\": row[\"group\"],\n",
    "            \"event_suffix\": row[\"event_suffix\"],\n",
    "            \"csv_path\": row[\"csv_path\"],\n",
    "        }\n",
    "        \n",
    "        # Load all eye tracking metrics for this file\n",
    "        for metric_name, column_name, display_name, direction_matters, is_absolute in eye_metrics_config:\n",
    "            try:\n",
    "                df = load_time_series(csv_path, TIME_COLUMN, column_name, value_alias=\"value\")\n",
    "                \n",
    "                if df.empty:\n",
    "                    raise ValueError(f\"Empty dataframe after loading {column_name}\")\n",
    "                \n",
    "                # For absolute metrics (probability, diameter), use absolute values\n",
    "                # - Probability: Always 0-1, no negative values (treat as absolute magnitude)\n",
    "                # - Diameter: Always positive (size measure)\n",
    "                # For signed metrics (position), keep original sign\n",
    "                # - Position: Can be left (-) or right (+) of center\n",
    "                if is_absolute:\n",
    "                    df[\"value\"] = df[\"value\"].abs()\n",
    "                \n",
    "                # Calculate pre and post means\n",
    "                pre_mask = (df[\"time\"] >= PRE_WINDOW[0]) & (df[\"time\"] < PRE_WINDOW[1])\n",
    "                post_mask = (df[\"time\"] >= POST_WINDOW[0]) & (df[\"time\"] < POST_WINDOW[1])\n",
    "                \n",
    "                pre_mean = float(df.loc[pre_mask, \"value\"].mean()) if pre_mask.any() else float(\"nan\")\n",
    "                post_mean = float(df.loc[post_mask, \"value\"].mean()) if post_mask.any() else float(\"nan\")\n",
    "                \n",
    "                # Calculate peak value in analysis window\n",
    "                peak_mask = (df[\"time\"] >= ANALYSIS_WINDOW_MEAN_PEAK[0]) & (df[\"time\"] <= ANALYSIS_WINDOW_MEAN_PEAK[1])\n",
    "                if peak_mask.any():\n",
    "                    if is_absolute:\n",
    "                        peak_val = float(df.loc[peak_mask, \"value\"].max())  # Already absolute\n",
    "                    else:\n",
    "                        peak_val = float(df.loc[peak_mask, \"value\"].abs().max())  # Take abs for signed\n",
    "                else:\n",
    "                    peak_val = float(\"nan\")\n",
    "                \n",
    "                # Add metrics with prefixes\n",
    "                record[f\"{metric_name}_pre_mean\"] = pre_mean\n",
    "                record[f\"{metric_name}_post_mean\"] = post_mean\n",
    "                record[f\"{metric_name}_peak\"] = peak_val\n",
    "                n_files_with_data[metric_name] += 1\n",
    "                \n",
    "            except Exception as exc:  # noqa: BLE001\n",
    "                error_msg = f\"{csv_path.name}: {exc}\"\n",
    "                eye_tracking_errors[metric_name].append(error_msg)\n",
    "                record[f\"{metric_name}_pre_mean\"] = float(\"nan\")\n",
    "                record[f\"{metric_name}_post_mean\"] = float(\"nan\")\n",
    "                record[f\"{metric_name}_peak\"] = float(\"nan\")\n",
    "        \n",
    "        eye_tracking_records.append(record)\n",
    "    \n",
    "    print(f\"\\n📊 Eye tracking data loading summary:\")\n",
    "    print(f\"   Total files processed: {n_files_processed}\")\n",
    "    for metric_name in [\"saccade\", \"pupil\", \"eye_position\"]:\n",
    "        n_success = n_files_with_data[metric_name]\n",
    "        n_failed = len(eye_tracking_errors[metric_name])\n",
    "        print(f\"   {metric_name}: {n_success} succeeded, {n_failed} failed\")\n",
    "\n",
    "eye_tracking_df = pd.DataFrame(eye_tracking_records)\n",
    "\n",
    "# Data range diagnostics - check if values are in expected ranges\n",
    "if not eye_tracking_df.empty:\n",
    "    print(f\"\\n📊 Data range diagnostics (raw values from CSV):\")\n",
    "    for metric_name in [\"saccade\", \"pupil\", \"eye_position\"]:\n",
    "        post_col = f\"{metric_name}_post_mean\"\n",
    "        if post_col in eye_tracking_df.columns:\n",
    "            values = eye_tracking_df[post_col].dropna()\n",
    "            if len(values) > 0:\n",
    "                print(f\"   {metric_name}_post_mean: min={values.min():.6f}, max={values.max():.6f}, mean={values.mean():.6f}\")\n",
    "                if metric_name == \"saccade\" and values.max() < 0.01:\n",
    "                    print(f\"      ⚠️ WARNING: Saccade probability values are very small (< 0.01)\")\n",
    "                    print(f\"      ⚠️ This might be per-frame probability, or the data may need different column\")\n",
    "                    print(f\"      ⚠️ Expected range: 0-1 for probability, but found: {values.min():.6f}-{values.max():.6f}\")\n",
    "\n",
    "# Save combined eye tracking results\n",
    "if OUTPUT_DIR is not None and not eye_tracking_df.empty:\n",
    "    eye_tracking_df.to_csv(OUTPUT_DIR / \"eye_tracking_metrics_per_file.csv\", index=False)\n",
    "    print(f\"✅ Saved: eye_tracking_metrics_per_file.csv\")\n",
    "\n",
    "eye_tracking_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e64a07",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Report eye tracking loading errors\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "total_errors = sum(len(errors) for errors in eye_tracking_errors.values())\n",
    "if total_errors > 0:\n",
    "    print(f\"\\n⚠️ Eye tracking loading errors details:\")\n",
    "    for metric_name, errors in eye_tracking_errors.items():\n",
    "        if errors:\n",
    "            print(f\"\\n  {metric_name.upper()}: {len(errors)} files failed\")\n",
    "            # Show first 3 errors\n",
    "            for i, error in enumerate(errors[:3]):\n",
    "                print(f\"    {i+1}. {error}\")\n",
    "            if len(errors) > 3:\n",
    "                print(f\"    ... and {len(errors)-3} more\")\n",
    "else:\n",
    "    print(f\"✅ All eye tracking metrics loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab21056",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Aggregate eye tracking metrics per mouse\n",
    "# ----------------------------------------------------------------------\n",
    "# For saccade and pupil: average across all turns\n",
    "# For eye_position: keep left and right turns SEPARATE\n",
    "\n",
    "if eye_tracking_df.empty:\n",
    "    eye_tracking_per_mouse = pd.DataFrame()\n",
    "    eye_position_per_mouse = pd.DataFrame()\n",
    "else:\n",
    "    # Saccade and Pupil: aggregate across ALL turns (direction doesn't matter)\n",
    "    saccade_pupil_cols = {\n",
    "        \"saccade_post_mean\": \"mean\",\n",
    "        \"saccade_peak\": \"mean\",\n",
    "        \"pupil_post_mean\": \"mean\",\n",
    "        \"pupil_peak\": \"mean\",\n",
    "    }\n",
    "    # Only aggregate columns that exist\n",
    "    saccade_pupil_cols = {k: v for k, v in saccade_pupil_cols.items() if k in eye_tracking_df.columns}\n",
    "    \n",
    "    if saccade_pupil_cols:\n",
    "        eye_tracking_per_mouse = eye_tracking_df.groupby([\"group\", \"mouse\"], dropna=False).agg(\n",
    "            saccade_pupil_cols\n",
    "        ).reset_index()\n",
    "    else:\n",
    "        eye_tracking_per_mouse = pd.DataFrame()\n",
    "    \n",
    "    # Eye Position: keep direction SEPARATE (left vs right turns matter!)\n",
    "    eye_position_cols = {\n",
    "        \"eye_position_post_mean\": \"mean\",\n",
    "        \"eye_position_peak\": \"mean\",\n",
    "    }\n",
    "    eye_position_cols = {k: v for k, v in eye_position_cols.items() if k in eye_tracking_df.columns}\n",
    "    \n",
    "    if eye_position_cols:\n",
    "        eye_position_per_mouse = eye_tracking_df.groupby(\n",
    "            [\"group\", \"mouse\", \"direction\"], dropna=False\n",
    "        ).agg(eye_position_cols).reset_index()\n",
    "    else:\n",
    "        eye_position_per_mouse = pd.DataFrame()\n",
    "    \n",
    "    # Save per-mouse aggregated metrics\n",
    "    if OUTPUT_DIR is not None:\n",
    "        if not eye_tracking_per_mouse.empty:\n",
    "            eye_tracking_per_mouse.to_csv(OUTPUT_DIR / \"saccade_pupil_metrics_per_mouse.csv\", index=False)\n",
    "            print(f\"✅ Saved: saccade_pupil_metrics_per_mouse.csv\")\n",
    "        if not eye_position_per_mouse.empty:\n",
    "            eye_position_per_mouse.to_csv(OUTPUT_DIR / \"eye_position_metrics_per_mouse.csv\", index=False)\n",
    "            print(f\"✅ Saved: eye_position_metrics_per_mouse.csv\")\n",
    "\n",
    "print(\"\\n📊 Saccade & Pupil metrics (averaged across turn directions):\")\n",
    "display(eye_tracking_per_mouse if not eye_tracking_per_mouse.empty else \"No data\")\n",
    "\n",
    "print(\"\\n📊 Eye Position metrics (separated by turn direction):\")\n",
    "display(eye_position_per_mouse if not eye_position_per_mouse.empty else \"No data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5694dcc",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Load time traces for all eye tracking metrics for plotting\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "eye_trace_records = {\n",
    "    \"saccade\": [],\n",
    "    \"pupil\": [],\n",
    "    \"eye_position\": [],\n",
    "}\n",
    "\n",
    "if not results_df.empty:\n",
    "    n_traces_loaded = {\"saccade\": 0, \"pupil\": 0, \"eye_position\": 0}\n",
    "    \n",
    "    for _, row in results_df.iterrows():\n",
    "        csv_path = Path(row[\"csv_path\"])\n",
    "        \n",
    "        for metric_name, column_name, display_name, direction_matters, is_absolute in eye_metrics_config:\n",
    "            try:\n",
    "                df = load_time_series(csv_path, TIME_COLUMN, column_name, value_alias=\"value\")\n",
    "                \n",
    "                if df.empty:\n",
    "                    continue\n",
    "                \n",
    "                # For absolute metrics (probability, diameter), use absolute values\n",
    "                if is_absolute:\n",
    "                    df[\"value\"] = df[\"value\"].abs()\n",
    "                \n",
    "                df[\"group\"] = row[\"group\"]\n",
    "                df[\"mouse\"] = row[\"mouse\"]\n",
    "                df[\"direction\"] = row[\"direction\"]  # Keep direction info\n",
    "                df[\"metric\"] = display_name\n",
    "                eye_trace_records[metric_name].append(df)\n",
    "                n_traces_loaded[metric_name] += 1\n",
    "                \n",
    "            except Exception as exc:  # noqa: BLE001\n",
    "                # Silently skip - errors already reported in the loading section\n",
    "                continue\n",
    "    \n",
    "    print(f\"\\n📊 Time traces loaded:\")\n",
    "    for metric_name in [\"saccade\", \"pupil\", \"eye_position\"]:\n",
    "        print(f\"   {metric_name}: {n_traces_loaded[metric_name]} traces\")\n",
    "\n",
    "# Create trace dataframes\n",
    "eye_traces = {}\n",
    "for metric_name in [\"saccade\", \"pupil\", \"eye_position\"]:\n",
    "    if eye_trace_records[metric_name]:\n",
    "        eye_traces[metric_name] = pd.concat(eye_trace_records[metric_name], ignore_index=True)\n",
    "        print(f\"   ✅ {metric_name}: {len(eye_traces[metric_name])} total data points\")\n",
    "    else:\n",
    "        eye_traces[metric_name] = pd.DataFrame()\n",
    "        print(f\"   ⚠️ {metric_name}: No data available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51797bd6",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Compute averaged traces for each eye metric\n",
    "# ----------------------------------------------------------------------\n",
    "# NOTE: Saccade and Pupil average across ALL turns (left + right)\n",
    "#       Eye Position keeps left and right turns SEPARATE\n",
    "\n",
    "eye_avg_traces = {}\n",
    "\n",
    "for metric_name, column_name, display_name, direction_matters, is_absolute in eye_metrics_config:\n",
    "    if not eye_traces[metric_name].empty:\n",
    "        if direction_matters:\n",
    "            # Eye position: keep direction separate\n",
    "            # Per-mouse average first (within each direction)\n",
    "            per_mouse = (\n",
    "                eye_traces[metric_name]\n",
    "                .groupby([\"group\", \"mouse\", \"direction\", \"time\"], dropna=False)[\"value\"]\n",
    "                .mean()\n",
    "                .reset_index()\n",
    "            )\n",
    "            \n",
    "            # Then average across mice (within each direction)\n",
    "            avg_trace = (\n",
    "                per_mouse\n",
    "                .groupby([\"group\", \"direction\", \"time\"], dropna=False)\n",
    "                .agg(\n",
    "                    mean_value=(\"value\", \"mean\"),\n",
    "                    sem_value=(\"value\", lambda x: sem(x) if len(x.dropna()) > 1 else 0.0),\n",
    "                    n_mice=(\"mouse\", \"nunique\"),\n",
    "                )\n",
    "                .reset_index()\n",
    "            )\n",
    "        else:\n",
    "            # Saccade and Pupil: average across ALL turns\n",
    "            # Per-mouse average first\n",
    "            per_mouse = (\n",
    "                eye_traces[metric_name]\n",
    "                .groupby([\"group\", \"mouse\", \"time\"], dropna=False)[\"value\"]\n",
    "                .mean()\n",
    "                .reset_index()\n",
    "            )\n",
    "            \n",
    "            # Then average across mice\n",
    "            avg_trace = (\n",
    "                per_mouse\n",
    "                .groupby([\"group\", \"time\"], dropna=False)\n",
    "                .agg(\n",
    "                    mean_value=(\"value\", \"mean\"),\n",
    "                    sem_value=(\"value\", lambda x: sem(x) if len(x.dropna()) > 1 else 0.0),\n",
    "                    n_mice=(\"mouse\", \"nunique\"),\n",
    "                )\n",
    "                .reset_index()\n",
    "            )\n",
    "        \n",
    "        avg_trace[\"metric\"] = display_name\n",
    "        avg_trace[\"is_absolute\"] = is_absolute  # Store for reference\n",
    "        eye_avg_traces[metric_name] = avg_trace\n",
    "        print(f\"   ✅ Computed avg traces for {metric_name}: {len(avg_trace)} time points\")\n",
    "    else:\n",
    "        eye_avg_traces[metric_name] = pd.DataFrame()\n",
    "        print(f\"   ⚠️ No traces to average for {metric_name}\")\n",
    "\n",
    "# Save averaged traces with more descriptive column names\n",
    "if OUTPUT_DIR is not None:\n",
    "    n_saved = 0\n",
    "    for metric_name, df in eye_avg_traces.items():\n",
    "        if not df.empty:\n",
    "            # Rename columns for clarity in CSV output\n",
    "            df_save = df.copy()\n",
    "            \n",
    "            # Rename based on metric type for better readability\n",
    "            if metric_name == \"saccade\":\n",
    "                df_save = df_save.rename(columns={\n",
    "                    \"mean_value\": \"mean_probability\",\n",
    "                    \"sem_value\": \"sem_probability\"\n",
    "                })\n",
    "            elif metric_name == \"pupil\":\n",
    "                df_save = df_save.rename(columns={\n",
    "                    \"mean_value\": \"mean_diameter\",\n",
    "                    \"sem_value\": \"sem_diameter\"\n",
    "                })\n",
    "            elif metric_name == \"eye_position\":\n",
    "                df_save = df_save.rename(columns={\n",
    "                    \"mean_value\": \"mean_position\",\n",
    "                    \"sem_value\": \"sem_position\"\n",
    "                })\n",
    "            \n",
    "            df_save.to_csv(OUTPUT_DIR / f\"eye_{metric_name}_avg_traces.csv\", index=False)\n",
    "            print(f\"✅ Saved: eye_{metric_name}_avg_traces.csv ({len(df_save)} rows)\")\n",
    "            n_saved += 1\n",
    "    if n_saved == 0:\n",
    "        print(\"⚠️ No averaged traces to save (all dataframes empty)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e5af9d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Plot averaged traces for all eye metrics\n",
    "# ----------------------------------------------------------------------\n",
    "# Saccade and Pupil: 1 panel each (averaged across turns)\n",
    "# Eye Position: 2 panels (left and right turns separate)\n",
    "\n",
    "plot_colors = {\n",
    "    \"saccade\": \"#2ca02c\",      # Green\n",
    "    \"pupil\": \"#9467bd\",        # Purple  \n",
    "    \"eye_position_left\": \"#d62728\",   # Red\n",
    "    \"eye_position_right\": \"#ff7f0e\",  # Orange\n",
    "}\n",
    "\n",
    "plot_labels = {\n",
    "    \"saccade\": \"Saccade Probability\",\n",
    "    \"pupil\": \"Pupil Diameter\",\n",
    "    \"eye_position\": \"Eye Position (X)\",\n",
    "}\n",
    "\n",
    "if any(not df.empty for df in eye_avg_traces.values()):\n",
    "    # Create figure with 4 subplots: saccade, pupil, eye_pos_left, eye_pos_right\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(18, 4), sharey=False)\n",
    "    \n",
    "    plot_idx = 0\n",
    "    \n",
    "    # Plot Saccade and Pupil (averaged across turns)\n",
    "    for metric_name in [\"saccade\", \"pupil\"]:\n",
    "        ax = axes[plot_idx]\n",
    "        avg_df = eye_avg_traces[metric_name]\n",
    "        \n",
    "        if avg_df.empty:\n",
    "            ax.text(0.5, 0.5, \"No data\", ha=\"center\", va=\"center\", transform=ax.transAxes)\n",
    "            ax.set_title(plot_labels[metric_name])\n",
    "            ax.axis(\"off\")\n",
    "            plot_idx += 1\n",
    "            continue\n",
    "        \n",
    "        # Plot for each group\n",
    "        for group_name in [\"Apply halt\", \"No halt\"]:\n",
    "            subset = avg_df[avg_df[\"group\"] == group_name]\n",
    "            if subset.empty:\n",
    "                continue\n",
    "            \n",
    "            color = plot_colors[metric_name]\n",
    "            alpha = 1.0 if group_name == \"Apply halt\" else 0.6\n",
    "            linestyle = \"-\" if group_name == \"Apply halt\" else \"--\"\n",
    "            \n",
    "            ax.plot(subset[\"time\"], subset[\"mean_value\"], \n",
    "                   color=color, linewidth=1.5, alpha=alpha, linestyle=linestyle,\n",
    "                   label=group_name)\n",
    "            \n",
    "            if \"sem_value\" in subset.columns:\n",
    "                upper = subset[\"mean_value\"] + subset[\"sem_value\"].fillna(0)\n",
    "                lower = subset[\"mean_value\"] - subset[\"sem_value\"].fillna(0)\n",
    "                ax.fill_between(subset[\"time\"], lower, upper, \n",
    "                               color=color, alpha=0.15)\n",
    "        \n",
    "        ax.axvline(0, color=\"black\", linestyle=\"--\", linewidth=1)\n",
    "        ax.axhline(0, color=\"grey\", linestyle=\":\", linewidth=0.8)\n",
    "        ax.set_title(plot_labels[metric_name], fontsize=11)\n",
    "        ax.set_xlabel(\"Time (s)\")\n",
    "        ax.set_ylabel(plot_labels[metric_name])\n",
    "        ax.legend(fontsize=9, loc='best')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        plot_idx += 1\n",
    "    \n",
    "    # Plot Eye Position (separate for left and right turns)\n",
    "    eye_pos_df = eye_avg_traces.get(\"eye_position\", pd.DataFrame())\n",
    "    \n",
    "    for turn_direction in [\"left\", \"right\"]:\n",
    "        ax = axes[plot_idx]\n",
    "        \n",
    "        if eye_pos_df.empty or \"direction\" not in eye_pos_df.columns:\n",
    "            ax.text(0.5, 0.5, \"No data\", ha=\"center\", va=\"center\", transform=ax.transAxes)\n",
    "            ax.set_title(f\"Eye Position | {turn_direction.capitalize()}\")\n",
    "            ax.axis(\"off\")\n",
    "            plot_idx += 1\n",
    "            continue\n",
    "        \n",
    "        direction_data = eye_pos_df[eye_pos_df[\"direction\"] == turn_direction]\n",
    "        \n",
    "        if direction_data.empty:\n",
    "            ax.text(0.5, 0.5, \"No data\", ha=\"center\", va=\"center\", transform=ax.transAxes)\n",
    "            ax.set_title(f\"Eye Position | {turn_direction.capitalize()}\")\n",
    "            ax.axis(\"off\")\n",
    "            plot_idx += 1\n",
    "            continue\n",
    "        \n",
    "        # Plot for each group\n",
    "        for group_name in [\"Apply halt\", \"No halt\"]:\n",
    "            subset = direction_data[direction_data[\"group\"] == group_name]\n",
    "            if subset.empty:\n",
    "                continue\n",
    "            \n",
    "            color = plot_colors[f\"eye_position_{turn_direction}\"]\n",
    "            alpha = 1.0 if group_name == \"Apply halt\" else 0.6\n",
    "            linestyle = \"-\" if group_name == \"Apply halt\" else \"--\"\n",
    "            \n",
    "            ax.plot(subset[\"time\"], subset[\"mean_value\"], \n",
    "                   color=color, linewidth=1.5, alpha=alpha, linestyle=linestyle,\n",
    "                   label=group_name)\n",
    "            \n",
    "            if \"sem_value\" in subset.columns:\n",
    "                upper = subset[\"mean_value\"] + subset[\"sem_value\"].fillna(0)\n",
    "                lower = subset[\"mean_value\"] - subset[\"sem_value\"].fillna(0)\n",
    "                ax.fill_between(subset[\"time\"], lower, upper, \n",
    "                               color=color, alpha=0.15)\n",
    "        \n",
    "        ax.axvline(0, color=\"black\", linestyle=\"--\", linewidth=1)\n",
    "        ax.axhline(0, color=\"grey\", linestyle=\":\", linewidth=0.8)\n",
    "        ax.set_title(f\"Eye Position (X) | {turn_direction.capitalize()} turns\", fontsize=11)\n",
    "        ax.set_xlabel(\"Time (s)\")\n",
    "        ax.set_ylabel(\"Eye Position (X)\")\n",
    "        ax.legend(fontsize=9, loc='best')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        plot_idx += 1\n",
    "    \n",
    "    plt.suptitle(\"Eye Tracking Metrics - Averaged Traces\", fontsize=13, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if OUTPUT_DIR is not None:\n",
    "        output_path = OUTPUT_DIR / \"eye_tracking_avg_traces.pdf\"\n",
    "        fig.savefig(output_path, format=\"pdf\", bbox_inches=\"tight\")\n",
    "        print(f\"✅ Saved: {output_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "else:\n",
    "    print(\"⚠️ No eye tracking traces available for plotting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bcba40",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Combined comparison plot: Saccade & Pupil (Apply halt vs No halt)\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "eye_metric_specs = [\n",
    "    (\"saccade_post_mean\", f\"Saccade probability\\n{ANALYSIS_WINDOW_MEAN_PEAK[0]}-{ANALYSIS_WINDOW_MEAN_PEAK[1]}s\"),\n",
    "    (\"pupil_post_mean\", f\"Pupil diameter\\n{ANALYSIS_WINDOW_MEAN_PEAK[0]}-{ANALYSIS_WINDOW_MEAN_PEAK[1]}s\"),\n",
    "]\n",
    "eye_plot_groups = [\"No halt\", \"Apply halt\"]\n",
    "eye_tracking_stats_df = pd.DataFrame()\n",
    "\n",
    "if eye_tracking_per_mouse.empty:\n",
    "    print(\"⚠️ No per-mouse saccade/pupil metrics available for comparison\")\n",
    "else:\n",
    "    eye_mouse_colors = assign_mouse_colors_consistent(eye_tracking_per_mouse[\"mouse\"].dropna().unique())\n",
    "    \n",
    "    subplot_width_cm = 6.5\n",
    "    subplot_height_cm = 7\n",
    "    fig_width = len(eye_metric_specs) * subplot_width_cm / 2.54\n",
    "    fig_height = subplot_height_cm / 2.54\n",
    "    fig, axes = plt.subplots(1, len(eye_metric_specs), figsize=(fig_width, fig_height), sharey=False)\n",
    "    \n",
    "    mean_handles = []\n",
    "    eye_mouse_handles: Dict[str, object] = {}\n",
    "    eye_stats_records: List[Dict[str, object]] = []\n",
    "    \n",
    "    for ax, (metric, label) in zip(axes, eye_metric_specs):\n",
    "        pivot = eye_tracking_per_mouse.pivot(index=\"mouse\", columns=\"group\", values=metric)\n",
    "        groups_present = [group for group in eye_plot_groups if group in pivot.columns]\n",
    "        \n",
    "        if len(groups_present) < 2:\n",
    "            ax.text(0.5, 0.5, \"Insufficient data\", ha=\"center\", va=\"center\")\n",
    "            ax.axis(\"off\")\n",
    "            continue\n",
    "        \n",
    "        group_a, group_b = groups_present[0], groups_present[1]\n",
    "        stats_result = compute_paired_t_test(pivot, group_a, group_b)\n",
    "        stats_result.update({\n",
    "            \"metric\": metric,\n",
    "            \"metric_label\": label.replace('\\n', ' '),\n",
    "            \"group_a\": group_a,\n",
    "            \"group_b\": group_b,\n",
    "        })\n",
    "        eye_stats_records.append(stats_result)\n",
    "        \n",
    "        x_positions = np.arange(len(groups_present), dtype=float)\n",
    "        \n",
    "        # Plot individual mice\n",
    "        for mouse in pivot.index:\n",
    "            values = pivot.loc[mouse, groups_present]\n",
    "            if values.isna().all():\n",
    "                continue\n",
    "            line, = ax.plot(\n",
    "                x_positions,\n",
    "                values.to_numpy(dtype=float),\n",
    "                marker=\"o\",\n",
    "                linewidth=1.1,\n",
    "                alpha=0.65,\n",
    "                color=eye_mouse_colors.get(mouse, \"#1f77b4\"),\n",
    "                zorder=2,\n",
    "            )\n",
    "            eye_mouse_handles.setdefault(mouse, line)\n",
    "        \n",
    "        # Plot group means\n",
    "        group_means = pivot[groups_present].mean(axis=0)\n",
    "        group_sems = pivot[groups_present].apply(lambda col: sem(col.dropna()), axis=0)\n",
    "        mean_values = group_means.to_numpy(dtype=float)\n",
    "        sem_values = group_sems.to_numpy(dtype=float)\n",
    "        \n",
    "        valid_mask = np.isfinite(mean_values) & np.isfinite(sem_values)\n",
    "        if valid_mask.any():\n",
    "            x_valid = x_positions[valid_mask]\n",
    "            mean_valid = mean_values[valid_mask]\n",
    "            sem_valid = sem_values[valid_mask]\n",
    "            ax.fill_between(\n",
    "                x_valid,\n",
    "                mean_valid - sem_valid,\n",
    "                mean_valid + sem_valid,\n",
    "                color=\"#b3b3b3\",\n",
    "                alpha=0.3,\n",
    "                zorder=1,\n",
    "                linewidth=0,\n",
    "            )\n",
    "        \n",
    "        error_container = ax.errorbar(\n",
    "            x_positions,\n",
    "            mean_values,\n",
    "            yerr=sem_values,\n",
    "            fmt=\"o-\",\n",
    "            color=\"#333333\",\n",
    "            linewidth=2.1,\n",
    "            capsize=4,\n",
    "            label=\"Mean ± SEM\",\n",
    "            zorder=3,\n",
    "        )\n",
    "        mean_handles.append(error_container)\n",
    "        \n",
    "        ax.set_xticks(x_positions)\n",
    "        ax.set_xticklabels(groups_present)\n",
    "        ax.set_xlim(-0.3, len(groups_present) - 1 + 0.31)\n",
    "        ax.set_title(label)\n",
    "        ax.set_ylabel(label.replace('\\n', ' '))\n",
    "        ax.grid(True, which=\"both\", axis=\"y\", linestyle=\":\", linewidth=0.7)\n",
    "    \n",
    "    # Add legend\n",
    "    legend_handles = list(eye_mouse_handles.values())\n",
    "    legend_labels = list(eye_mouse_handles.keys())\n",
    "    if mean_handles:\n",
    "        legend_handles.append(mean_handles[0].lines[0])\n",
    "        legend_labels.append(\"Mean ± SEM\")\n",
    "    if legend_handles:\n",
    "        fig.legend(\n",
    "            legend_handles,\n",
    "            legend_labels,\n",
    "            loc=\"center left\",\n",
    "            bbox_to_anchor=(0.85, 0.5),\n",
    "            borderaxespad=0.0,\n",
    "            frameon=True,\n",
    "            fontsize=8,\n",
    "        )\n",
    "    \n",
    "    fig.subplots_adjust(left=0, right=0.84, bottom=0.18, top=0.92, wspace=0.4)\n",
    "    if OUTPUT_DIR is not None:\n",
    "        fig.savefig(OUTPUT_DIR / \"saccade_pupil_metric_comparisons.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "        print(f\"✅ Saved: saccade_pupil_metric_comparisons.pdf\")\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "    \n",
    "    # Save statistics\n",
    "    if eye_stats_records:\n",
    "        eye_tracking_stats_df = pd.DataFrame(eye_stats_records)\n",
    "        display(Markdown(\"#### Saccade & Pupil metrics paired t-tests\"))\n",
    "        display(eye_tracking_stats_df)\n",
    "        if OUTPUT_DIR is not None:\n",
    "            eye_tracking_stats_df.to_csv(OUTPUT_DIR / \"saccade_pupil_metrics_ttests.csv\", index=False)\n",
    "            print(f\"✅ Saved: saccade_pupil_metrics_ttests.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9136fb",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Eye Position comparison plot (Apply halt vs No halt, separated by turn direction)\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "eye_position_metric_specs = [\n",
    "    (\"eye_position_post_mean\", f\"Eye position (X)\\n{ANALYSIS_WINDOW_MEAN_PEAK[0]}-{ANALYSIS_WINDOW_MEAN_PEAK[1]}s\"),\n",
    "]\n",
    "eye_position_stats_df = pd.DataFrame()\n",
    "\n",
    "if eye_position_per_mouse.empty:\n",
    "    print(\"⚠️ No per-mouse eye position metrics available for comparison\")\n",
    "else:\n",
    "    print(f\"\\n📊 Eye position comparison (separated by turn direction)\")\n",
    "    \n",
    "    # Create separate plots for left and right turns\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 4.5), sharey=False)\n",
    "    \n",
    "    eye_pos_mouse_colors = assign_mouse_colors_consistent(eye_position_per_mouse[\"mouse\"].dropna().unique())\n",
    "    eye_pos_stats_records = []\n",
    "    \n",
    "    for ax, turn_direction in zip(axes, [\"left\", \"right\"]):\n",
    "        direction_data = eye_position_per_mouse[eye_position_per_mouse[\"direction\"] == turn_direction]\n",
    "        \n",
    "        if direction_data.empty:\n",
    "            ax.text(0.5, 0.5, f\"No data for {turn_direction} turns\", ha=\"center\", va=\"center\")\n",
    "            ax.axis(\"off\")\n",
    "            continue\n",
    "        \n",
    "        metric = \"eye_position_post_mean\"\n",
    "        pivot = direction_data.pivot(index=\"mouse\", columns=\"group\", values=metric)\n",
    "        groups_present = [group for group in eye_plot_groups if group in pivot.columns]\n",
    "        \n",
    "        if len(groups_present) < 2:\n",
    "            ax.text(0.5, 0.5, \"Insufficient data\", ha=\"center\", va=\"center\")\n",
    "            ax.axis(\"off\")\n",
    "            continue\n",
    "        \n",
    "        # Compute statistics\n",
    "        group_a, group_b = groups_present[0], groups_present[1]\n",
    "        stats_result = compute_paired_t_test(pivot, group_a, group_b)\n",
    "        stats_result.update({\n",
    "            \"metric\": f\"eye_position_post_mean_{turn_direction}\",\n",
    "            \"metric_label\": f\"Eye position (X) | {turn_direction} turns\",\n",
    "            \"direction\": turn_direction,\n",
    "            \"group_a\": group_a,\n",
    "            \"group_b\": group_b,\n",
    "        })\n",
    "        eye_pos_stats_records.append(stats_result)\n",
    "        \n",
    "        x_positions = np.arange(len(groups_present), dtype=float)\n",
    "        \n",
    "        # Plot individual mice\n",
    "        for mouse in pivot.index:\n",
    "            values = pivot.loc[mouse, groups_present]\n",
    "            if values.isna().all():\n",
    "                continue\n",
    "            ax.plot(\n",
    "                x_positions,\n",
    "                values.to_numpy(dtype=float),\n",
    "                marker=\"o\",\n",
    "                linewidth=1.1,\n",
    "                alpha=0.65,\n",
    "                color=eye_pos_mouse_colors.get(mouse, \"#1f77b4\"),\n",
    "                zorder=2,\n",
    "            )\n",
    "        \n",
    "        # Plot group means\n",
    "        group_means = pivot[groups_present].mean(axis=0)\n",
    "        group_sems = pivot[groups_present].apply(lambda col: sem(col.dropna()), axis=0)\n",
    "        mean_values = group_means.to_numpy(dtype=float)\n",
    "        sem_values = group_sems.to_numpy(dtype=float)\n",
    "        \n",
    "        valid_mask = np.isfinite(mean_values) & np.isfinite(sem_values)\n",
    "        if valid_mask.any():\n",
    "            x_valid = x_positions[valid_mask]\n",
    "            mean_valid = mean_values[valid_mask]\n",
    "            sem_valid = sem_values[valid_mask]\n",
    "            ax.fill_between(\n",
    "                x_valid,\n",
    "                mean_valid - sem_valid,\n",
    "                mean_valid + sem_valid,\n",
    "                color=\"#b3b3b3\",\n",
    "                alpha=0.3,\n",
    "                zorder=1,\n",
    "                linewidth=0,\n",
    "            )\n",
    "        \n",
    "        ax.errorbar(\n",
    "            x_positions,\n",
    "            mean_values,\n",
    "            yerr=sem_values,\n",
    "            fmt=\"o-\",\n",
    "            color=\"#333333\",\n",
    "            linewidth=2.1,\n",
    "            capsize=4,\n",
    "            label=\"Mean ± SEM\",\n",
    "            zorder=3,\n",
    "        )\n",
    "        \n",
    "        ax.set_xticks(x_positions)\n",
    "        ax.set_xticklabels(groups_present)\n",
    "        ax.set_xlim(-0.3, len(groups_present) - 1 + 0.31)\n",
    "        ax.set_title(f\"Eye Position (X) | {turn_direction.capitalize()} turns\")\n",
    "        ax.set_ylabel(\"Eye Position (X)\")\n",
    "        ax.grid(True, which=\"both\", axis=\"y\", linestyle=\":\", linewidth=0.7)\n",
    "    \n",
    "    plt.suptitle(f\"Eye Position Comparison\\n{ANALYSIS_WINDOW_MEAN_PEAK[0]}-{ANALYSIS_WINDOW_MEAN_PEAK[1]}s post-turn\", \n",
    "                 fontsize=12, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if OUTPUT_DIR is not None:\n",
    "        fig.savefig(OUTPUT_DIR / \"eye_position_metric_comparisons.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "        print(f\"✅ Saved: eye_position_metric_comparisons.pdf\")\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "    \n",
    "    # Save statistics\n",
    "    if eye_pos_stats_records:\n",
    "        eye_position_stats_df = pd.DataFrame(eye_pos_stats_records)\n",
    "        display(Markdown(\"#### Eye position metrics paired t-tests (by turn direction)\"))\n",
    "        display(eye_position_stats_df)\n",
    "        if OUTPUT_DIR is not None:\n",
    "            eye_position_stats_df.to_csv(OUTPUT_DIR / \"eye_position_metrics_ttests.csv\", index=False)\n",
    "            print(f\"✅ Saved: eye_position_metrics_ttests.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79d2fd4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Baseline Saccade Density Analysis\n",
    "# ----------------------------------------------------------------------\n",
    "# Quantify saccade probability during baseline window (-1 to 0s)\n",
    "\n",
    "baseline_saccade_df = pd.DataFrame()\n",
    "baseline_saccade_stats_df = pd.DataFrame()\n",
    "\n",
    "if not eye_tracking_df.empty and \"saccade_pre_mean\" in eye_tracking_df.columns:\n",
    "    print(f\"\\n📊 Analyzing baseline saccade density (window: {BASELINE_WINDOW[0]} to {BASELINE_WINDOW[1]}s)\")\n",
    "    \n",
    "    # Aggregate baseline saccade probability per mouse (average across all turns)\n",
    "    baseline_saccade_per_mouse = (\n",
    "        eye_tracking_df.groupby([\"group\", \"mouse\"], dropna=False)\n",
    "        .agg({\"saccade_pre_mean\": \"mean\"})\n",
    "        .reset_index()\n",
    "        .rename(columns={\"saccade_pre_mean\": \"baseline_saccade_probability\"})\n",
    "    )\n",
    "    \n",
    "    baseline_saccade_df = baseline_saccade_per_mouse.copy()\n",
    "    \n",
    "    # Display per-mouse results\n",
    "    display(Markdown(\"#### Baseline saccade probability per mouse\"))\n",
    "    display(baseline_saccade_df)\n",
    "    \n",
    "    # Save per-mouse baseline saccade data\n",
    "    if OUTPUT_DIR is not None:\n",
    "        baseline_saccade_df.to_csv(OUTPUT_DIR / \"baseline_saccade_density_per_mouse.csv\", index=False)\n",
    "        print(f\"✅ Saved: baseline_saccade_density_per_mouse.csv\")\n",
    "    \n",
    "    # Statistical comparison: Apply halt vs No halt\n",
    "    pivot_baseline_saccade = baseline_saccade_df.pivot(\n",
    "        index=\"mouse\", \n",
    "        columns=\"group\", \n",
    "        values=\"baseline_saccade_probability\"\n",
    "    )\n",
    "    \n",
    "    if \"Apply halt\" in pivot_baseline_saccade.columns and \"No halt\" in pivot_baseline_saccade.columns:\n",
    "        stats_result = compute_paired_t_test(pivot_baseline_saccade, \"No halt\", \"Apply halt\")\n",
    "        \n",
    "        # Add descriptive statistics\n",
    "        no_halt_vals = pivot_baseline_saccade[\"No halt\"].dropna()\n",
    "        apply_halt_vals = pivot_baseline_saccade[\"Apply halt\"].dropna()\n",
    "        \n",
    "        baseline_saccade_stats_df = pd.DataFrame([{\n",
    "            \"metric\": \"baseline_saccade_probability\",\n",
    "            \"metric_label\": f\"Baseline saccade probability ({BASELINE_WINDOW[0]} to {BASELINE_WINDOW[1]}s)\",\n",
    "            \"no_halt_mean\": float(no_halt_vals.mean()) if len(no_halt_vals) > 0 else float(\"nan\"),\n",
    "            \"no_halt_sem\": float(sem(no_halt_vals)) if len(no_halt_vals) > 1 else float(\"nan\"),\n",
    "            \"apply_halt_mean\": float(apply_halt_vals.mean()) if len(apply_halt_vals) > 0 else float(\"nan\"),\n",
    "            \"apply_halt_sem\": float(sem(apply_halt_vals)) if len(apply_halt_vals) > 1 else float(\"nan\"),\n",
    "            \"mean_difference\": stats_result[\"mean_difference\"],\n",
    "            \"t_statistic\": stats_result[\"t_statistic\"],\n",
    "            \"p_value\": stats_result[\"p_value\"],\n",
    "            \"n_pairs\": stats_result[\"n_pairs\"],\n",
    "        }])\n",
    "        \n",
    "        display(Markdown(\"#### Baseline saccade probability: Paired t-test\"))\n",
    "        display(baseline_saccade_stats_df)\n",
    "        \n",
    "        if OUTPUT_DIR is not None:\n",
    "            baseline_saccade_stats_df.to_csv(OUTPUT_DIR / \"baseline_saccade_density_ttest.csv\", index=False)\n",
    "            print(f\"✅ Saved: baseline_saccade_density_ttest.csv\")\n",
    "        \n",
    "        # Create comparison plot\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(5, 5.5), sharey=False)\n",
    "        \n",
    "        groups_present = [\"No halt\", \"Apply halt\"]\n",
    "        x_positions = np.arange(len(groups_present), dtype=float)\n",
    "        \n",
    "        # Assign consistent mouse colors\n",
    "        baseline_mouse_colors = assign_mouse_colors_consistent(baseline_saccade_df[\"mouse\"].dropna().unique())\n",
    "        \n",
    "        # Plot individual mice\n",
    "        for mouse in pivot_baseline_saccade.index:\n",
    "            values = pivot_baseline_saccade.loc[mouse, groups_present]\n",
    "            if values.isna().all():\n",
    "                continue\n",
    "            ax.plot(\n",
    "                x_positions,\n",
    "                values.to_numpy(dtype=float),\n",
    "                marker=\"o\",\n",
    "                linewidth=1.1,\n",
    "                alpha=0.65,\n",
    "                color=baseline_mouse_colors.get(mouse, \"#1f77b4\"),\n",
    "                zorder=2,\n",
    "            )\n",
    "        \n",
    "        # Plot group means with SEM\n",
    "        group_means = pivot_baseline_saccade[groups_present].mean(axis=0)\n",
    "        group_sems = pivot_baseline_saccade[groups_present].apply(lambda col: sem(col.dropna()), axis=0)\n",
    "        mean_values = group_means.to_numpy(dtype=float)\n",
    "        sem_values = group_sems.to_numpy(dtype=float)\n",
    "        \n",
    "        # Add SEM shading\n",
    "        valid_mask = np.isfinite(mean_values) & np.isfinite(sem_values)\n",
    "        if valid_mask.any():\n",
    "            x_valid = x_positions[valid_mask]\n",
    "            mean_valid = mean_values[valid_mask]\n",
    "            sem_valid = sem_values[valid_mask]\n",
    "            ax.fill_between(\n",
    "                x_valid,\n",
    "                mean_valid - sem_valid,\n",
    "                mean_valid + sem_valid,\n",
    "                color=\"#b3b3b3\",\n",
    "                alpha=0.3,\n",
    "                zorder=1,\n",
    "                linewidth=0,\n",
    "            )\n",
    "        \n",
    "        # Plot mean line\n",
    "        ax.errorbar(\n",
    "            x_positions,\n",
    "            mean_values,\n",
    "            yerr=sem_values,\n",
    "            fmt=\"o-\",\n",
    "            color=\"#333333\",\n",
    "            linewidth=2.1,\n",
    "            capsize=4,\n",
    "            label=\"Mean ± SEM\",\n",
    "            zorder=3,\n",
    "        )\n",
    "        \n",
    "        # Formatting\n",
    "        ax.set_xticks(x_positions)\n",
    "        ax.set_xticklabels(groups_present)\n",
    "        ax.set_xlim(-0.3, len(groups_present) - 1 + 0.31)\n",
    "        ax.set_ylabel(f\"Baseline saccade probability\\n({BASELINE_WINDOW[0]} to {BASELINE_WINDOW[1]}s)\")\n",
    "        ax.set_title(f\"Baseline Saccade Density Comparison\")\n",
    "        ax.grid(True, which=\"both\", axis=\"y\", linestyle=\":\", linewidth=0.7)\n",
    "        \n",
    "        # Add statistics annotation\n",
    "        p_val = stats_result[\"p_value\"]\n",
    "        if p_val < 0.001:\n",
    "            sig_text = \"***\"\n",
    "            p_text = \"p < 0.001\"\n",
    "        elif p_val < 0.01:\n",
    "            sig_text = \"**\"\n",
    "            p_text = f\"p = {p_val:.3f}\"\n",
    "        elif p_val < 0.05:\n",
    "            sig_text = \"*\"\n",
    "            p_text = f\"p = {p_val:.3f}\"\n",
    "        else:\n",
    "            sig_text = \"ns\"\n",
    "            p_text = f\"p = {p_val:.3f}\"\n",
    "        \n",
    "        # Add text box with statistics\n",
    "        stats_text = f\"n = {stats_result['n_pairs']} mice\\n{p_text} {sig_text}\"\n",
    "        ax.text(\n",
    "            0.98, 0.98,\n",
    "            stats_text,\n",
    "            transform=ax.transAxes,\n",
    "            ha=\"right\",\n",
    "            va=\"top\",\n",
    "            fontsize=9,\n",
    "            bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"white\", edgecolor=\"grey\", alpha=0.8),\n",
    "        )\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if OUTPUT_DIR is not None:\n",
    "            fig.savefig(OUTPUT_DIR / \"baseline_saccade_density_comparison.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "            print(f\"✅ Saved: baseline_saccade_density_comparison.pdf\")\n",
    "        \n",
    "        plt.show()\n",
    "        plt.close(fig)\n",
    "    else:\n",
    "        print(\"⚠️ Cannot perform statistical comparison - missing Apply halt or No halt data\")\n",
    "else:\n",
    "    print(\"⚠️ No saccade data available for baseline analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turning velocity summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# TURNING Summary statistics\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "summary_df = summarise_results(results_df)\n",
    "if summary_df.empty:\n",
    "    print(\"⚠️ No summary statistics available\")\n",
    "else:\n",
    "    for group_name, subdf in summary_df.groupby(\"group\", dropna=False):\n",
    "        title = group_name if isinstance(group_name, str) else \"Unknown\"\n",
    "        display(Markdown(f\"### {title}\"))\n",
    "        display(subdf.drop(columns=[\"group\"], errors=\"ignore\").reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bdff62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn timing and magnitude metrics\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "if results_df.empty:\n",
    "    timing_summary_df = pd.DataFrame()\n",
    "    timing_mouse_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn timing and magnitude metrics\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "if results_df.empty:\n",
    "    timing_summary_df = pd.DataFrame()\n",
    "    timing_mouse_df = pd.DataFrame()\n",
    "else:\n",
    "    timing_columns = [\n",
    "        \"time_to_peak\",\n",
    "        \"latency_to_fraction_peak\",\n",
    "        \"peak_velocity_magnitude\",\n",
    "        \"peak_velocity_abs_1s\",\n",
    "        \"auc_abs\",\n",
    "        \"decay_tau\",\n",
    "    ]\n",
    "    timing_columns = [column for column in timing_columns if column in results_df.columns]\n",
    "    if timing_columns:\n",
    "        timing_summary_df = (\n",
    "            results_df.groupby([\"group\", \"direction\"], dropna=False)[timing_columns]\n",
    "            .agg([\"mean\", \"sem\"])\n",
    "        )\n",
    "        timing_summary_df.columns = [f\"{metric}_{stat}\" for metric, stat in timing_summary_df.columns]\n",
    "        timing_summary_df = timing_summary_df.reset_index()\n",
    "\n",
    "        timing_mouse_df = (\n",
    "            results_df.groupby([\"group\", \"mouse\", \"direction\"], dropna=False)[timing_columns]\n",
    "            .mean()\n",
    "            .reset_index()\n",
    "        )\n",
    "    else:\n",
    "        timing_summary_df = pd.DataFrame()\n",
    "        timing_mouse_df = pd.DataFrame()\n",
    "\n",
    "timing_summary_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if timing_summary_df.empty:\n",
    "    print(\"⚠️ No timing metrics available\")\n",
    "else:\n",
    "    for group_name, subset in timing_summary_df.groupby(\"group\", dropna=False):\n",
    "        title = group_name if isinstance(group_name, str) else \"Unknown\"\n",
    "        display(Markdown(f\"### Turn timing | {title}\"))\n",
    "        display(subset.drop(columns=[\"group\"], errors=\"ignore\").reset_index(drop=True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Compute mean absolute velocity 0-2s (for combining left and right turns)\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "if trace_samples_df.empty:\n",
    "    mean_abs_velocity_by_mouse = pd.DataFrame()\n",
    "else:\n",
    "    mean_abs_records = []\n",
    "    \n",
    "    for (group, mouse), group_df in trace_samples_df.groupby([\"group\", \"mouse\"]):\n",
    "        window_mask = (group_df[\"time\"] >= EXTENDED_RESPONSE_WINDOW[0]) & (group_df[\"time\"] <= EXTENDED_RESPONSE_WINDOW[1])\n",
    "        window_df = group_df.loc[window_mask].copy()\n",
    "        \n",
    "        if not window_df.empty:\n",
    "            mean_abs_vel = float(window_df[\"velocity\"].abs().mean())\n",
    "            mean_abs_records.append({\n",
    "                \"group\": group,\n",
    "                \"mouse\": mouse,\n",
    "                \"mean_abs_velocity_0_2s\": mean_abs_vel,\n",
    "            })\n",
    "    \n",
    "    mean_abs_velocity_by_mouse = pd.DataFrame(mean_abs_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6eda4a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Turning metric comparisons in a single figure (Apply halt vs No halt)\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "# Build metric labels with actual window durations from configuration\n",
    "analysis_window_duration = ANALYSIS_WINDOW_MEAN_PEAK[1] - ANALYSIS_WINDOW_MEAN_PEAK[0]\n",
    "extended_window_duration = EXTENDED_RESPONSE_WINDOW[1] - EXTENDED_RESPONSE_WINDOW[0]\n",
    "\n",
    "metric_specs = [\n",
    "    (\"peak_velocity_abs_1s\", f\"Peak velocity {ANALYSIS_WINDOW_MEAN_PEAK[0]}-{ANALYSIS_WINDOW_MEAN_PEAK[1]}s (deg/s)\"),\n",
    "    (\"mean_abs_velocity_0_2s\", f\"Mean |velocity| {EXTENDED_RESPONSE_WINDOW[0]}-{EXTENDED_RESPONSE_WINDOW[1]}s (deg/s)\"),\n",
    "    (\"auc_abs\", f\"AUC {ANALYSIS_WINDOW_MEAN_PEAK[0]}-{ANALYSIS_WINDOW_MEAN_PEAK[1]}s (deg·s)\"),\n",
    "    (\"decay_tau\", \"Decay time constant τ (s)\"),\n",
    "]\n",
    "plot_groups = [\"No halt\", \"Apply halt\"]\n",
    "turning_stats_df = pd.DataFrame()\n",
    "\n",
    "# Aggregate timing metrics directly from results_df\n",
    "if results_df.empty:\n",
    "    combined = pd.DataFrame()\n",
    "else:\n",
    "    metric_columns = [metric for metric, _ in metric_specs if metric in results_df.columns]\n",
    "    if metric_columns:\n",
    "        combined = (\n",
    "            results_df.groupby([\"mouse\", \"group\"], dropna=False)[metric_columns]\n",
    "            .mean()\n",
    "            .reset_index()\n",
    "        )\n",
    "    else:\n",
    "        combined = pd.DataFrame()\n",
    "    \n",
    "    # Merge in mean absolute velocity 0-2s\n",
    "    if not mean_abs_velocity_by_mouse.empty and not combined.empty:\n",
    "        combined = pd.merge(\n",
    "            combined,\n",
    "            mean_abs_velocity_by_mouse,\n",
    "            on=[\"mouse\", \"group\"],\n",
    "            how=\"outer\",\n",
    "        )\n",
    "    elif not mean_abs_velocity_by_mouse.empty:\n",
    "        combined = mean_abs_velocity_by_mouse.copy()\n",
    "\n",
    "if combined.empty:\n",
    "    print(\"⚠️ No turning metrics available for comparison\")\n",
    "else:\n",
    "    # Save turning metrics combined (only mean absolute velocity)\n",
    "    if OUTPUT_DIR is not None and \"mean_abs_velocity_0_2s\" in combined.columns:\n",
    "        combined[[\"mouse\", \"group\", \"mean_abs_velocity_0_2s\"]].to_csv(\n",
    "            OUTPUT_DIR / \"turning_metrics_combined.csv\", index=False\n",
    "        )\n",
    "        print(f\"✅ Saved: turning_metrics_combined.csv\")\n",
    "    \n",
    "    available_specs = [(metric, label) for metric, label in metric_specs if metric in combined.columns]\n",
    "    if not available_specs:\n",
    "        print(\"⚠️ Metrics not found in aggregated data\")\n",
    "    else:\n",
    "        subplot_width_cm = 6.5\n",
    "        subplot_height_cm = 7\n",
    "        fig_width = len(available_specs) * subplot_width_cm / 2.54\n",
    "        fig_height = subplot_height_cm / 2.54\n",
    "        fig, axes = plt.subplots(1, len(available_specs), figsize=(fig_width, fig_height), sharey=False)\n",
    "        if len(available_specs) == 1:\n",
    "            axes = [axes]\n",
    "\n",
    "        mouse_colors = assign_mouse_colors_consistent(combined[\"mouse\"].dropna().unique())\n",
    "        mean_handles = []\n",
    "        mouse_handles: Dict[str, object] = {}\n",
    "        turning_stats_records: List[Dict[str, object]] = []\n",
    "\n",
    "        for ax, (metric, label) in zip(axes, available_specs):\n",
    "            pivot = combined.pivot(index=\"mouse\", columns=\"group\", values=metric)\n",
    "            groups_present = [group for group in plot_groups if group in pivot.columns]\n",
    "            if len(groups_present) < 2:\n",
    "                ax.text(0.5, 0.5, \"Insufficient data\", ha=\"center\", va=\"center\")\n",
    "                ax.axis(\"off\")\n",
    "                continue\n",
    "\n",
    "            group_a, group_b = groups_present[0], groups_present[1]\n",
    "            stats_result = compute_paired_t_test(pivot, group_a, group_b)\n",
    "            stats_result.update(\n",
    "                {\n",
    "                    \"metric\": metric,\n",
    "                    \"metric_label\": label,\n",
    "                    \"group_a\": group_a,\n",
    "                    \"group_b\": group_b,\n",
    "                }\n",
    "            )\n",
    "            turning_stats_records.append(stats_result)\n",
    "\n",
    "            x_positions = np.arange(len(groups_present))\n",
    "\n",
    "            for mouse in pivot.index:\n",
    "                values = pivot.loc[mouse, groups_present]\n",
    "                if values.isna().all():\n",
    "                    continue\n",
    "                valid_mask = values.notna()\n",
    "                if not valid_mask.any():\n",
    "                    continue\n",
    "                x_mouse = x_positions[valid_mask.to_numpy(dtype=bool)]\n",
    "                y_mouse = values[valid_mask].to_numpy(dtype=float)\n",
    "                color = mouse_colors.get(mouse, \"#1f77b4\")\n",
    "                if x_mouse.size == 1:\n",
    "                    line, = ax.plot(\n",
    "                        x_mouse,\n",
    "                        y_mouse,\n",
    "                        marker=\"o\",\n",
    "                        linestyle=\"none\",\n",
    "                        markersize=5,\n",
    "                        alpha=0.8,\n",
    "                        color=color,\n",
    "                        zorder=2,\n",
    "                    )\n",
    "                else:\n",
    "                    line, = ax.plot(\n",
    "                        x_mouse,\n",
    "                        y_mouse,\n",
    "                        marker=\"o\",\n",
    "                        linewidth=1.1,\n",
    "                        alpha=0.65,\n",
    "                        color=color,\n",
    "                        zorder=2,\n",
    "                    )\n",
    "                mouse_handles.setdefault(mouse, line)\n",
    "\n",
    "            group_means = pivot[groups_present].mean(axis=0)\n",
    "            group_sems = pivot[groups_present].apply(lambda col: sem(col.dropna()), axis=0)\n",
    "            mean_values = group_means.to_numpy(dtype=float)\n",
    "            sem_values = group_sems.to_numpy(dtype=float)\n",
    "            valid_mask = np.isfinite(mean_values) & np.isfinite(sem_values)\n",
    "            if valid_mask.any():\n",
    "                x_valid = x_positions[valid_mask]\n",
    "                mean_valid = mean_values[valid_mask]\n",
    "                sem_valid = sem_values[valid_mask]\n",
    "                ax.fill_between(\n",
    "                    x_valid,\n",
    "                    mean_valid - sem_valid,\n",
    "                    mean_valid + sem_valid,\n",
    "                    color=\"#b3b3b3\",\n",
    "                    alpha=0.3,\n",
    "                    zorder=1,\n",
    "                    linewidth=0,\n",
    "                )\n",
    "\n",
    "            error_container = ax.errorbar(\n",
    "                x_positions,\n",
    "                mean_values,\n",
    "                yerr=sem_values,\n",
    "                fmt=\"o-\",\n",
    "                color=\"#333333\",\n",
    "                linewidth=2.1,\n",
    "                capsize=4,\n",
    "                label=\"Mean ± SEM\",\n",
    "                zorder=3,\n",
    "            )\n",
    "            mean_handles.append(error_container)\n",
    "\n",
    "            ax.set_xticks(x_positions)\n",
    "            ax.set_xticklabels(groups_present)\n",
    "            ax.set_xlim(-0.3, len(groups_present) - 1 + 0.31)\n",
    "            ax.set_title(label)\n",
    "            ax.set_ylabel(label)\n",
    "            ax.grid(True, which=\"both\", axis=\"y\", linestyle=\":\", linewidth=0.7)\n",
    "\n",
    "        legend_handles = list(mouse_handles.values())\n",
    "        legend_labels = list(mouse_handles.keys())\n",
    "        if mean_handles:\n",
    "            legend_handles.append(mean_handles[0].lines[0])\n",
    "            legend_labels.append(\"Mean ± SEM\")\n",
    "        if legend_handles:\n",
    "            fig.legend(\n",
    "                legend_handles,\n",
    "                legend_labels,\n",
    "                loc=\"center left\",\n",
    "                bbox_to_anchor=(0.85, 0.5),\n",
    "                borderaxespad=0.0,\n",
    "                frameon=True,\n",
    "                fontsize=8,\n",
    "            )\n",
    "\n",
    "        fig.subplots_adjust(left=0, right=0.84, bottom=0.18, top=0.92, wspace=0.4)\n",
    "        if OUTPUT_DIR is not None:\n",
    "            fig.savefig(OUTPUT_DIR / \"turning_metric_comparisons.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "            print(f\"✅ Saved: turning_metric_comparisons.pdf\")\n",
    "        plt.show()\n",
    "        plt.close(fig)\n",
    "\n",
    "        if turning_stats_records:\n",
    "            turning_stats_df = pd.DataFrame(turning_stats_records)\n",
    "            display(Markdown(\"#### Turning metrics paired t-tests\"))\n",
    "            display(turning_stats_df)\n",
    "            if OUTPUT_DIR is not None:\n",
    "                turning_stats_df.to_csv(OUTPUT_DIR / \"turning_metrics_ttests.csv\", index=False)\n",
    "                print(f\"✅ Saved: turning_metrics_ttests.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Average motor velocity traces by condition and turn direction\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "if trace_samples_df.empty:\n",
    "    per_mouse_traces = pd.DataFrame()\n",
    "    avg_traces = pd.DataFrame()\n",
    "else:\n",
    "    per_mouse_traces = (\n",
    "        trace_samples_df\n",
    "        .groupby([\"group\", \"direction\", \"mouse\", \"time\"], dropna=False)[\"velocity\"]\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "    )\n",
    "    avg_traces = (\n",
    "        per_mouse_traces\n",
    "        .groupby([\"group\", \"direction\", \"time\"], dropna=False)\n",
    "        .agg(\n",
    "            mean_velocity=(\"velocity\", \"mean\"),\n",
    "            sem_velocity=(\"velocity\", lambda x: sem(x) if len(x.dropna()) > 1 else 0.0),\n",
    "            n_mice=(\"mouse\", \"nunique\"),\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "avg_traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# 2x2 average motor velocity figure (Apply halt / No halt x Left / Right)\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "if avg_traces.empty:\n",
    "    print(\"⚠️ No averaged traces available for plotting\")\n",
    "else:\n",
    "    plot_order = [\n",
    "        (\"Apply halt\", \"left\"),\n",
    "        (\"Apply halt\", \"right\"),\n",
    "        (\"No halt\", \"left\"),\n",
    "        (\"No halt\", \"right\"),\n",
    "    ]\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(10, 8), sharex=True, sharey=True)\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for ax, (group_name, direction) in zip(axes, plot_order):\n",
    "        subset = avg_traces[\n",
    "            (avg_traces[\"group\"] == group_name)\n",
    "            & (avg_traces[\"direction\"] == direction)\n",
    "        ]\n",
    "        if subset.empty:\n",
    "            ax.text(0.5, 0.5, \"No data\", ha=\"center\", va=\"center\")\n",
    "            ax.axis(\"off\")\n",
    "            continue\n",
    "\n",
    "        ax.plot(subset[\"time\"], subset[\"mean_velocity\"], color=\"#1f77b4\", linewidth=1.5)\n",
    "        if \"sem_velocity\" in subset.columns:\n",
    "            upper = subset[\"mean_velocity\"] + subset[\"sem_velocity\"].fillna(0)\n",
    "            lower = subset[\"mean_velocity\"] - subset[\"sem_velocity\"].fillna(0)\n",
    "            ax.fill_between(subset[\"time\"], lower, upper, color=\"#1f77b4\", alpha=0.2)\n",
    "\n",
    "        ax.axvline(0, color=\"black\", linestyle=\"--\", linewidth=1)\n",
    "        ax.axhline(0, color=\"grey\", linestyle=\":\", linewidth=1)\n",
    "        ax.set_title(f\"{group_name} | {direction.capitalize()} turns\")\n",
    "        ax.set_xlabel(\"Time (s)\")\n",
    "        ax.set_ylabel(\"Motor velocity (deg/s)\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a34a22",
   "metadata": {},
   "source": [
    "### Alternative temporal dynamics analysis (better than decay fitting)\n",
    "\n",
    "The exponential decay fit often fails because motor velocity shows complex, \n",
    "non-exponential dynamics. Better approaches:\n",
    "1. **Windowed AUC**: Quantify response in different time phases\n",
    "2. **Sustained response ratio**: Compare early vs late response magnitude  \n",
    "3. **Time to baseline**: When does response return to pre-turn levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e6625c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Compute alternative temporal metrics for each mouse\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "if not trace_samples_df.empty:\n",
    "    print(\"Computing alternative temporal dynamics metrics...\")\n",
    "    \n",
    "    # Define time windows for AUC analysis\n",
    "    AUC_WINDOWS = [\n",
    "        (0.0, 0.5),   # Early response\n",
    "        (0.5, 1.0),   # Mid response  \n",
    "        (1.0, 2.0),   # Late response\n",
    "        (0.0, 2.0),   # Total response\n",
    "    ]\n",
    "    \n",
    "    alternative_metrics_records = []\n",
    "    \n",
    "    for (group, mouse, direction), group_df in trace_samples_df.groupby([\"group\", \"mouse\", \"direction\"]):\n",
    "        # Compute windowed AUC\n",
    "        auc_metrics = compute_windowed_auc(group_df, AUC_WINDOWS)\n",
    "        \n",
    "        # Compute sustained response ratio\n",
    "        sustained_ratio = compute_sustained_response_ratio(group_df)\n",
    "        \n",
    "        # Compute time to baseline\n",
    "        time_to_baseline = compute_time_to_baseline(group_df)\n",
    "        \n",
    "        # Compile all metrics\n",
    "        alternative_metrics_records.append({\n",
    "            \"group\": group,\n",
    "            \"mouse\": mouse,\n",
    "            \"direction\": direction,\n",
    "            **auc_metrics,\n",
    "            \"sustained_ratio\": sustained_ratio,\n",
    "            \"time_to_baseline\": time_to_baseline,\n",
    "        })\n",
    "    \n",
    "    alternative_metrics_df = pd.DataFrame(alternative_metrics_records)\n",
    "    \n",
    "    # Average across turn directions per mouse\n",
    "    alt_metrics_by_mouse = (\n",
    "        alternative_metrics_df.groupby([\"group\", \"mouse\"], dropna=False)\n",
    "        .mean(numeric_only=True)\n",
    "        .reset_index()\n",
    "    )\n",
    "    \n",
    "    print(f\"✅ Computed alternative metrics for {len(alt_metrics_by_mouse)} mouse-group combinations\")\n",
    "    display(Markdown(\"#### Alternative temporal dynamics metrics (per mouse)\"))\n",
    "    display(alt_metrics_by_mouse)\n",
    "    \n",
    "    if OUTPUT_DIR is not None:\n",
    "        alt_metrics_by_mouse.to_csv(OUTPUT_DIR / \"alternative_temporal_metrics.csv\", index=False)\n",
    "else:\n",
    "    alternative_metrics_df = pd.DataFrame()\n",
    "    alt_metrics_by_mouse = pd.DataFrame()\n",
    "    print(\"⚠️ No trace data available for alternative metrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fadaf3f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Statistical comparison of alternative metrics: Apply halt vs No halt\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "if not alt_metrics_by_mouse.empty:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STATISTICAL COMPARISON: Apply halt vs No halt\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Define metrics to compare\n",
    "    alt_metrics_to_compare = [\n",
    "        (\"auc_0.0_0.5s\", \"Early AUC 0-0.5s (deg)\"),\n",
    "        (\"auc_0.5_1.0s\", \"Mid AUC 0.5-1s (deg)\"),\n",
    "        (\"auc_1.0_2.0s\", \"Late AUC 1-2s (deg)\"),\n",
    "        (\"auc_0.0_2.0s\", \"Total AUC 0-2s (deg)\"),\n",
    "        (\"sustained_ratio\", \"Sustained response ratio (late/early)\"),\n",
    "        (\"time_to_baseline\", \"Time to baseline (s)\"),\n",
    "    ]\n",
    "    \n",
    "    alt_stats_records = []\n",
    "    \n",
    "    for metric, label in alt_metrics_to_compare:\n",
    "        if metric not in alt_metrics_by_mouse.columns:\n",
    "            continue\n",
    "        \n",
    "        # Create pivot table\n",
    "        pivot = alt_metrics_by_mouse.pivot(index=\"mouse\", columns=\"group\", values=metric)\n",
    "        \n",
    "        if \"No halt\" in pivot.columns and \"Apply halt\" in pivot.columns:\n",
    "            stats_result = compute_paired_t_test(pivot, \"No halt\", \"Apply halt\")\n",
    "            \n",
    "            # Add descriptive statistics\n",
    "            no_halt_vals = pivot[\"No halt\"].dropna()\n",
    "            apply_halt_vals = pivot[\"Apply halt\"].dropna()\n",
    "            \n",
    "            stats_result.update({\n",
    "                \"metric\": metric,\n",
    "                \"metric_label\": label,\n",
    "                \"no_halt_mean\": float(no_halt_vals.mean()) if len(no_halt_vals) > 0 else float(\"nan\"),\n",
    "                \"no_halt_sem\": float(sem(no_halt_vals)) if len(no_halt_vals) > 1 else float(\"nan\"),\n",
    "                \"apply_halt_mean\": float(apply_halt_vals.mean()) if len(apply_halt_vals) > 0 else float(\"nan\"),\n",
    "                \"apply_halt_sem\": float(sem(apply_halt_vals)) if len(apply_halt_vals) > 1 else float(\"nan\"),\n",
    "            })\n",
    "            \n",
    "            alt_stats_records.append(stats_result)\n",
    "    \n",
    "    if alt_stats_records:\n",
    "        alt_stats_df = pd.DataFrame(alt_stats_records)\n",
    "        \n",
    "        # Reorder columns for clarity\n",
    "        col_order = [\n",
    "            \"metric_label\", \n",
    "            \"no_halt_mean\", \"no_halt_sem\",\n",
    "            \"apply_halt_mean\", \"apply_halt_sem\",\n",
    "            \"mean_difference\", \"t_statistic\", \"p_value\", \"n_pairs\"\n",
    "        ]\n",
    "        alt_stats_df = alt_stats_df[[col for col in col_order if col in alt_stats_df.columns]]\n",
    "        \n",
    "        display(Markdown(\"#### Paired t-tests for alternative temporal metrics\"))\n",
    "        display(alt_stats_df)\n",
    "        \n",
    "        if OUTPUT_DIR is not None:\n",
    "            alt_stats_df.to_csv(OUTPUT_DIR / \"alternative_metrics_ttests.csv\", index=False)\n",
    "        \n",
    "        # Highlight significant results\n",
    "        if \"p_value\" in alt_stats_df.columns:\n",
    "            sig_results = alt_stats_df[alt_stats_df[\"p_value\"] < 0.05]\n",
    "            if not sig_results.empty:\n",
    "                display(Markdown(\"#### Significant differences (p < 0.05)\"))\n",
    "                display(sig_results)\n",
    "    else:\n",
    "        alt_stats_df = pd.DataFrame()\n",
    "        print(\"⚠️ Could not perform statistical comparisons\")\n",
    "else:\n",
    "    alt_stats_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9142d1c9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Visualization: Alternative metrics comparison\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "if not alt_metrics_by_mouse.empty:\n",
    "    # Select key metrics to visualize\n",
    "    viz_metrics = [\n",
    "        (\"auc_0.0_0.5s\", \"Early AUC 0-0.5s\"),\n",
    "        (\"auc_1.0_2.0s\", \"Late AUC 1-2s\"),  \n",
    "        (\"sustained_ratio\", \"Sustained ratio\\n(late/early)\"),\n",
    "    ]\n",
    "    \n",
    "    # Filter to metrics that exist\n",
    "    viz_metrics = [(m, l) for m, l in viz_metrics if m in alt_metrics_by_mouse.columns]\n",
    "    \n",
    "    if viz_metrics:\n",
    "        fig, axes = plt.subplots(1, len(viz_metrics), figsize=(4*len(viz_metrics), 4.5))\n",
    "        if len(viz_metrics) == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        mouse_colors = assign_mouse_colors_consistent(alt_metrics_by_mouse[\"mouse\"].unique())\n",
    "        \n",
    "        for ax, (metric, label) in zip(axes, viz_metrics):\n",
    "            pivot = alt_metrics_by_mouse.pivot(index=\"mouse\", columns=\"group\", values=metric)\n",
    "            groups_present = [g for g in [\"No halt\", \"Apply halt\"] if g in pivot.columns]\n",
    "            \n",
    "            if len(groups_present) < 2:\n",
    "                ax.text(0.5, 0.5, \"Insufficient data\", ha=\"center\", va=\"center\")\n",
    "                ax.axis(\"off\")\n",
    "                continue\n",
    "            \n",
    "            x_positions = np.arange(len(groups_present))\n",
    "            \n",
    "            # Plot individual mice\n",
    "            for mouse in pivot.index:\n",
    "                values = pivot.loc[mouse, groups_present]\n",
    "                if values.isna().all():\n",
    "                    continue\n",
    "                ax.plot(\n",
    "                    x_positions,\n",
    "                    values.to_numpy(dtype=float),\n",
    "                    marker=\"o\",\n",
    "                    linewidth=1.1,\n",
    "                    alpha=0.65,\n",
    "                    color=mouse_colors.get(mouse, \"#1f77b4\"),\n",
    "                    zorder=2,\n",
    "                )\n",
    "            \n",
    "            # Plot group means\n",
    "            group_means = pivot[groups_present].mean(axis=0)\n",
    "            group_sems = pivot[groups_present].apply(lambda col: sem(col.dropna()), axis=0)\n",
    "            \n",
    "            ax.errorbar(\n",
    "                x_positions,\n",
    "                group_means.to_numpy(dtype=float),\n",
    "                yerr=group_sems.to_numpy(dtype=float),\n",
    "                fmt=\"o-\",\n",
    "                color=\"#333333\",\n",
    "                linewidth=2.1,\n",
    "                capsize=4,\n",
    "                label=\"Mean ± SEM\",\n",
    "                zorder=3,\n",
    "            )\n",
    "            \n",
    "            ax.set_xticks(x_positions)\n",
    "            ax.set_xticklabels(groups_present)\n",
    "            ax.set_ylabel(label)\n",
    "            ax.set_title(label)\n",
    "            ax.grid(True, which=\"both\", axis=\"y\", linestyle=\":\", linewidth=0.7)\n",
    "        \n",
    "        plt.suptitle(\"Alternative Temporal Dynamics Metrics\", fontsize=12, y=1.02)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if OUTPUT_DIR is not None:\n",
    "            fig.savefig(OUTPUT_DIR / \"alternative_metrics_comparison.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "        \n",
    "        plt.show()\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07c8e80",
   "metadata": {},
   "source": [
    "### Single Mouse Plots - Eye Tracking, Turning, and Running\n",
    "Plot individual mouse data (comparing Apply halt vs No halt conditions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522a5128",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Single Mouse Eye Tracking Plot\n",
    "# ----------------------------------------------------------------------\n",
    "# NOTE: This uses the already-loaded trace data (eye_traces dictionary)\n",
    "# which contains the same data as the saved CSV files, just in memory.\n",
    "# Mean and SEM are calculated across trials for the specified mouse.\n",
    "\n",
    "# Configure which mouse to plot\n",
    "SINGLE_MOUSE_ID = \"B6J2718\"  # Change this to plot a different mouse\n",
    "\n",
    "if not eye_traces:\n",
    "    print(f\"⚠️ No eye tracking traces available for plotting\")\n",
    "else:\n",
    "    print(f\"\\n📊 Creating single-mouse eye tracking plot for: {SINGLE_MOUSE_ID}\")\n",
    "    \n",
    "    # Check if mouse exists in the data\n",
    "    available_mice = set()\n",
    "    for metric_name, df in eye_traces.items():\n",
    "        if not df.empty and \"mouse\" in df.columns:\n",
    "            available_mice.update(df[\"mouse\"].unique())\n",
    "    \n",
    "    if SINGLE_MOUSE_ID not in available_mice:\n",
    "        print(f\"⚠️ Mouse {SINGLE_MOUSE_ID} not found in eye tracking data\")\n",
    "        print(f\"   Available mice: {sorted(available_mice)}\")\n",
    "    else:\n",
    "        # Create 4-panel figure (saccade, pupil, eye_pos_left, eye_pos_right)\n",
    "        fig, axes = plt.subplots(1, 4, figsize=(18, 4), sharey=False)\n",
    "        \n",
    "        plot_idx = 0\n",
    "        \n",
    "        # Plot Saccade and Pupil (averaged across turns for this mouse)\n",
    "        for metric_name in [\"saccade\", \"pupil\"]:\n",
    "            ax = axes[plot_idx]\n",
    "            metric_traces = eye_traces.get(metric_name, pd.DataFrame())\n",
    "            \n",
    "            if metric_traces.empty:\n",
    "                ax.text(0.5, 0.5, \"No data\", ha=\"center\", va=\"center\", transform=ax.transAxes)\n",
    "                ax.set_title(f\"{plot_labels[metric_name]} | {SINGLE_MOUSE_ID}\")\n",
    "                ax.axis(\"off\")\n",
    "                plot_idx += 1\n",
    "                continue\n",
    "            \n",
    "            # Filter for this mouse\n",
    "            mouse_data = metric_traces[metric_traces[\"mouse\"] == SINGLE_MOUSE_ID]\n",
    "            \n",
    "            if mouse_data.empty:\n",
    "                ax.text(0.5, 0.5, \"No data\", ha=\"center\", va=\"center\", transform=ax.transAxes)\n",
    "                ax.set_title(f\"{plot_labels[metric_name]} | {SINGLE_MOUSE_ID}\")\n",
    "                ax.axis(\"off\")\n",
    "                plot_idx += 1\n",
    "                continue\n",
    "            \n",
    "            # Diagnostic: show how many unique trials/files per group\n",
    "            if metric_name == \"saccade\" and plot_idx == 0:  # Print once\n",
    "                for grp in mouse_data[\"group\"].unique():\n",
    "                    grp_data = mouse_data[mouse_data[\"group\"] == grp]\n",
    "                    # Count unique csv files if available\n",
    "                    if \"csv_path\" in grp_data.columns:\n",
    "                        n_trials = grp_data[\"csv_path\"].nunique()\n",
    "                    else:\n",
    "                        # Estimate from number of rows at a single time point\n",
    "                        sample_time = grp_data[\"time\"].iloc[0] if not grp_data.empty else None\n",
    "                        if sample_time is not None:\n",
    "                            n_trials = len(grp_data[grp_data[\"time\"] == sample_time])\n",
    "                        else:\n",
    "                            n_trials = \"unknown\"\n",
    "                    print(f\"   {metric_name} | {grp}: {n_trials} trials for {SINGLE_MOUSE_ID}\")\n",
    "            \n",
    "            # Average across trials for each group with SEM\n",
    "            # This averages all trials (different turning events) for this mouse\n",
    "            mouse_avg = (\n",
    "                mouse_data.groupby([\"group\", \"time\"], dropna=False)\n",
    "                .agg(\n",
    "                    mean_value=(\"value\", \"mean\"),\n",
    "                    sem_value=(\"value\", lambda x: sem(x) if len(x.dropna()) > 1 else 0.0),\n",
    "                    n_trials=(\"value\", \"count\"),\n",
    "                )\n",
    "                .reset_index()\n",
    "            )\n",
    "            \n",
    "            # Plot for each group\n",
    "            for group_name in [\"Apply halt\", \"No halt\"]:\n",
    "                subset = mouse_avg[mouse_avg[\"group\"] == group_name]\n",
    "                if subset.empty:\n",
    "                    continue\n",
    "                \n",
    "                color = plot_colors[metric_name]\n",
    "                alpha = 1.0 if group_name == \"Apply halt\" else 0.6\n",
    "                linestyle = \"-\" if group_name == \"Apply halt\" else \"--\"\n",
    "                \n",
    "                ax.plot(subset[\"time\"], subset[\"mean_value\"], \n",
    "                       color=color, linewidth=1.5, alpha=alpha, linestyle=linestyle,\n",
    "                       label=group_name)\n",
    "                \n",
    "                # Add SEM shading\n",
    "                if \"sem_value\" in subset.columns:\n",
    "                    upper = subset[\"mean_value\"] + subset[\"sem_value\"].fillna(0)\n",
    "                    lower = subset[\"mean_value\"] - subset[\"sem_value\"].fillna(0)\n",
    "                    ax.fill_between(subset[\"time\"], lower, upper, \n",
    "                                   color=color, alpha=0.15)\n",
    "            \n",
    "            ax.axvline(0, color=\"black\", linestyle=\"--\", linewidth=1)\n",
    "            ax.axhline(0, color=\"grey\", linestyle=\":\", linewidth=0.8)\n",
    "            ax.set_title(f\"{plot_labels[metric_name]}\", fontsize=11)\n",
    "            ax.set_xlabel(\"Time (s)\")\n",
    "            ax.set_ylabel(plot_labels[metric_name])\n",
    "            ax.legend(fontsize=9, loc='best')\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            plot_idx += 1\n",
    "        \n",
    "        # Plot Eye Position (separate for left and right turns)\n",
    "        eye_pos_traces = eye_traces.get(\"eye_position\", pd.DataFrame())\n",
    "        \n",
    "        for turn_direction in [\"left\", \"right\"]:\n",
    "            ax = axes[plot_idx]\n",
    "            \n",
    "            if eye_pos_traces.empty:\n",
    "                ax.text(0.5, 0.5, \"No data\", ha=\"center\", va=\"center\", transform=ax.transAxes)\n",
    "                ax.set_title(f\"Eye Position | {turn_direction.capitalize()}\")\n",
    "                ax.axis(\"off\")\n",
    "                plot_idx += 1\n",
    "                continue\n",
    "            \n",
    "            # Filter for this mouse and direction\n",
    "            mouse_direction_data = eye_pos_traces[\n",
    "                (eye_pos_traces[\"mouse\"] == SINGLE_MOUSE_ID) & \n",
    "                (eye_pos_traces[\"direction\"] == turn_direction)\n",
    "            ]\n",
    "            \n",
    "            if mouse_direction_data.empty:\n",
    "                ax.text(0.5, 0.5, \"No data\", ha=\"center\", va=\"center\", transform=ax.transAxes)\n",
    "                ax.set_title(f\"Eye Position | {turn_direction.capitalize()}\")\n",
    "                ax.axis(\"off\")\n",
    "                plot_idx += 1\n",
    "                continue\n",
    "            \n",
    "            # Average across trials for each group with SEM\n",
    "            mouse_avg = (\n",
    "                mouse_direction_data.groupby([\"group\", \"time\"], dropna=False)\n",
    "                .agg(\n",
    "                    mean_value=(\"value\", \"mean\"),\n",
    "                    sem_value=(\"value\", lambda x: sem(x) if len(x.dropna()) > 1 else 0.0),\n",
    "                )\n",
    "                .reset_index()\n",
    "            )\n",
    "            \n",
    "            # Plot for each group\n",
    "            for group_name in [\"Apply halt\", \"No halt\"]:\n",
    "                subset = mouse_avg[mouse_avg[\"group\"] == group_name]\n",
    "                if subset.empty:\n",
    "                    continue\n",
    "                \n",
    "                color = plot_colors[f\"eye_position_{turn_direction}\"]\n",
    "                alpha = 1.0 if group_name == \"Apply halt\" else 0.6\n",
    "                linestyle = \"-\" if group_name == \"Apply halt\" else \"--\"\n",
    "                \n",
    "                ax.plot(subset[\"time\"], subset[\"mean_value\"], \n",
    "                       color=color, linewidth=1.5, alpha=alpha, linestyle=linestyle,\n",
    "                       label=group_name)\n",
    "                \n",
    "                # Add SEM shading\n",
    "                if \"sem_value\" in subset.columns:\n",
    "                    upper = subset[\"mean_value\"] + subset[\"sem_value\"].fillna(0)\n",
    "                    lower = subset[\"mean_value\"] - subset[\"sem_value\"].fillna(0)\n",
    "                    ax.fill_between(subset[\"time\"], lower, upper, \n",
    "                                   color=color, alpha=0.15)\n",
    "            \n",
    "            ax.axvline(0, color=\"black\", linestyle=\"--\", linewidth=1)\n",
    "            ax.axhline(0, color=\"grey\", linestyle=\":\", linewidth=0.8)\n",
    "            ax.set_title(f\"Eye Position (X) | {turn_direction.capitalize()}\", fontsize=11)\n",
    "            ax.set_xlabel(\"Time (s)\")\n",
    "            ax.set_ylabel(\"Eye Position (X)\")\n",
    "            ax.legend(fontsize=9, loc='best')\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            plot_idx += 1\n",
    "        \n",
    "        plt.suptitle(f\"Eye Tracking - Mouse {SINGLE_MOUSE_ID}\", fontsize=13, y=1.02)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if OUTPUT_DIR is not None:\n",
    "            output_path = OUTPUT_DIR / f\"eye_tracking_{SINGLE_MOUSE_ID}.pdf\"\n",
    "            fig.savefig(output_path, format=\"pdf\", bbox_inches=\"tight\")\n",
    "            print(f\"✅ Saved: {output_path}\")\n",
    "        \n",
    "        plt.show()\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffdbe34",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Single Mouse Turning Velocity Plot\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "if trace_samples_df.empty:\n",
    "    print(f\"⚠️ No turning traces available for plotting\")\n",
    "else:\n",
    "    print(f\"\\n📊 Creating single-mouse turning velocity plot for: {SINGLE_MOUSE_ID}\")\n",
    "    \n",
    "    # Check if mouse exists\n",
    "    if SINGLE_MOUSE_ID not in trace_samples_df[\"mouse\"].unique():\n",
    "        print(f\"⚠️ Mouse {SINGLE_MOUSE_ID} not found in turning data\")\n",
    "        print(f\"   Available mice: {sorted(trace_samples_df['mouse'].unique())}\")\n",
    "    else:\n",
    "        # Filter for this mouse\n",
    "        mouse_data = trace_samples_df[trace_samples_df[\"mouse\"] == SINGLE_MOUSE_ID]\n",
    "        \n",
    "        # Average across trials for each group and direction with SEM\n",
    "        mouse_avg = (\n",
    "            mouse_data.groupby([\"group\", \"direction\", \"time\"], dropna=False)\n",
    "            .agg(\n",
    "                mean_velocity=(\"velocity\", \"mean\"),\n",
    "                sem_velocity=(\"velocity\", lambda x: sem(x) if len(x.dropna()) > 1 else 0.0),\n",
    "            )\n",
    "            .reset_index()\n",
    "        )\n",
    "        \n",
    "        # Create 2x2 plot (left/right x halt/no-halt)\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(12, 8), sharex=True, sharey=True)\n",
    "        \n",
    "        plot_order = [\n",
    "            (\"Apply halt\", \"left\"),\n",
    "            (\"Apply halt\", \"right\"),\n",
    "            (\"No halt\", \"left\"),\n",
    "            (\"No halt\", \"right\"),\n",
    "        ]\n",
    "        \n",
    "        axes_flat = axes.flatten()\n",
    "        \n",
    "        for ax, (group_name, direction) in zip(axes_flat, plot_order):\n",
    "            subset = mouse_avg[\n",
    "                (mouse_avg[\"group\"] == group_name) & \n",
    "                (mouse_avg[\"direction\"] == direction)\n",
    "            ]\n",
    "            \n",
    "            if subset.empty:\n",
    "                ax.text(0.5, 0.5, \"No data\", ha=\"center\", va=\"center\", transform=ax.transAxes)\n",
    "                ax.set_title(f\"{group_name} | {direction.capitalize()} turns\")\n",
    "                ax.axis(\"off\")\n",
    "                continue\n",
    "            \n",
    "            # Plot mean line\n",
    "            ax.plot(subset[\"time\"], subset[\"mean_velocity\"], color=\"#1f77b4\", linewidth=1.5)\n",
    "            \n",
    "            # Add SEM shading\n",
    "            if \"sem_velocity\" in subset.columns:\n",
    "                upper = subset[\"mean_velocity\"] + subset[\"sem_velocity\"].fillna(0)\n",
    "                lower = subset[\"mean_velocity\"] - subset[\"sem_velocity\"].fillna(0)\n",
    "                ax.fill_between(subset[\"time\"], lower, upper, color=\"#1f77b4\", alpha=0.2)\n",
    "            \n",
    "            ax.axvline(0, color=\"black\", linestyle=\"--\", linewidth=1)\n",
    "            ax.axhline(0, color=\"grey\", linestyle=\":\", linewidth=1)\n",
    "            ax.set_title(f\"{group_name} | {direction.capitalize()} turns\")\n",
    "            ax.set_xlabel(\"Time (s)\")\n",
    "            ax.set_ylabel(\"Motor velocity (deg/s)\")\n",
    "            ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.suptitle(f\"Turning Velocity - Mouse {SINGLE_MOUSE_ID}\", fontsize=13, y=0.995)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if OUTPUT_DIR is not None:\n",
    "            output_path = OUTPUT_DIR / f\"turning_velocity_{SINGLE_MOUSE_ID}.pdf\"\n",
    "            fig.savefig(output_path, format=\"pdf\", bbox_inches=\"tight\")\n",
    "            print(f\"✅ Saved: {output_path}\")\n",
    "        \n",
    "        plt.show()\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712dfe9b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Single Mouse Running Velocity Plot  \n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "if running_trace_samples_df.empty:\n",
    "    print(f\"⚠️ No running traces available for plotting\")\n",
    "else:\n",
    "    print(f\"\\n📊 Creating single-mouse running velocity plot for: {SINGLE_MOUSE_ID}\")\n",
    "    \n",
    "    # Check if mouse exists\n",
    "    if SINGLE_MOUSE_ID not in running_trace_samples_df[\"mouse\"].unique():\n",
    "        print(f\"⚠️ Mouse {SINGLE_MOUSE_ID} not found in running data\")\n",
    "        print(f\"   Available mice: {sorted(running_trace_samples_df['mouse'].unique())}\")\n",
    "    else:\n",
    "        # Filter for this mouse\n",
    "        mouse_data = running_trace_samples_df[running_trace_samples_df[\"mouse\"] == SINGLE_MOUSE_ID]\n",
    "        \n",
    "        # Average across all trials for each group with SEM (combining left and right turns)\n",
    "        mouse_avg = (\n",
    "            mouse_data.groupby([\"group\", \"time\"], dropna=False)\n",
    "            .agg(\n",
    "                mean_velocity=(\"velocity\", \"mean\"),\n",
    "                sem_velocity=(\"velocity\", lambda x: sem(x) if len(x.dropna()) > 1 else 0.0),\n",
    "            )\n",
    "            .reset_index()\n",
    "        )\n",
    "        \n",
    "        # Create single plot comparing halt vs no-halt\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
    "        \n",
    "        for group_name in [\"Apply halt\", \"No halt\"]:\n",
    "            subset = mouse_avg[mouse_avg[\"group\"] == group_name]\n",
    "            \n",
    "            if subset.empty:\n",
    "                continue\n",
    "            \n",
    "            color = \"#ff7f0e\" if group_name == \"Apply halt\" else \"#2ca02c\"\n",
    "            linestyle = \"-\" if group_name == \"Apply halt\" else \"--\"\n",
    "            \n",
    "            # Plot mean line\n",
    "            ax.plot(subset[\"time\"], subset[\"mean_velocity\"], \n",
    "                   color=color, linewidth=2, linestyle=linestyle, label=group_name)\n",
    "            \n",
    "            # Add SEM shading\n",
    "            if \"sem_velocity\" in subset.columns:\n",
    "                upper = subset[\"mean_velocity\"] + subset[\"sem_velocity\"].fillna(0)\n",
    "                lower = subset[\"mean_velocity\"] - subset[\"sem_velocity\"].fillna(0)\n",
    "                ax.fill_between(subset[\"time\"], lower, upper, color=color, alpha=0.2)\n",
    "        \n",
    "        ax.axvline(0, color=\"black\", linestyle=\"--\", linewidth=1)\n",
    "        ax.axhline(0, color=\"grey\", linestyle=\":\", linewidth=1)\n",
    "        ax.set_title(f\"Running Velocity - Mouse {SINGLE_MOUSE_ID}\", fontsize=12)\n",
    "        ax.set_xlabel(\"Time (s)\")\n",
    "        ax.set_ylabel(\"Running velocity (cm/s)\")\n",
    "        ax.legend(fontsize=10)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if OUTPUT_DIR is not None:\n",
    "            output_path = OUTPUT_DIR / f\"running_velocity_{SINGLE_MOUSE_ID}.pdf\"\n",
    "            fig.savefig(output_path, format=\"pdf\", bbox_inches=\"tight\")\n",
    "            print(f\"✅ Saved: {output_path}\")\n",
    "        \n",
    "        plt.show()\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0600ecec",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Summary: Single mouse plots saved\n",
    "# ----------------------------------------------------------------------\n",
    "if OUTPUT_DIR is not None:\n",
    "    print(f\"\\n✅ Single-mouse plots for {SINGLE_MOUSE_ID} saved to:\")\n",
    "    print(f\"   {OUTPUT_DIR / f'eye_tracking_{SINGLE_MOUSE_ID}.pdf'}\")\n",
    "    print(f\"   {OUTPUT_DIR / f'turning_velocity_{SINGLE_MOUSE_ID}.pdf'}\")\n",
    "    print(f\"   {OUTPUT_DIR / f'running_velocity_{SINGLE_MOUSE_ID}.pdf'}\")\n",
    "    print(f\"\\n💡 To plot a different mouse, change SINGLE_MOUSE_ID at the top of the eye tracking cell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc6fb2e",
   "metadata": {},
   "source": [
    "### Difference in Turning Velocity: Apply halt vs No halt\n",
    "Compute the difference between Apply halt and No halt turning velocity averaged over 0-2s post-halt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Compute turning velocity difference (Apply halt - No halt) for 0-2s window\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "if trace_samples_df.empty:\n",
    "    print(\"⚠️ No turning traces available for difference analysis\")\n",
    "    turning_velocity_diff_df = pd.DataFrame()\n",
    "else:\n",
    "    print(f\"\\n📊 Computing turning velocity difference (Apply halt - No halt) over 0-2s window\")\n",
    "    \n",
    "    # Define analysis window (0-2s post-halt)\n",
    "    DIFF_WINDOW = (0.0, 2.0)\n",
    "    \n",
    "    # Compute mean absolute velocity in the window for each mouse and group\n",
    "    diff_analysis_records = []\n",
    "    \n",
    "    for (group, mouse), group_df in trace_samples_df.groupby([\"group\", \"mouse\"]):\n",
    "        # Filter to analysis window\n",
    "        window_mask = (group_df[\"time\"] >= DIFF_WINDOW[0]) & (group_df[\"time\"] <= DIFF_WINDOW[1])\n",
    "        window_df = group_df.loc[window_mask].copy()\n",
    "        \n",
    "        if not window_df.empty:\n",
    "            # Use absolute velocity to combine left and right turns\n",
    "            mean_abs_vel = float(window_df[\"velocity\"].abs().mean())\n",
    "            diff_analysis_records.append({\n",
    "                \"group\": group,\n",
    "                \"mouse\": mouse,\n",
    "                \"mean_abs_velocity_0_2s\": mean_abs_vel,\n",
    "            })\n",
    "    \n",
    "    if not diff_analysis_records:\n",
    "        print(\"⚠️ No data available for difference analysis\")\n",
    "        turning_velocity_diff_df = pd.DataFrame()\n",
    "    else:\n",
    "        diff_analysis_df = pd.DataFrame(diff_analysis_records)\n",
    "        \n",
    "        # Pivot to get Apply halt and No halt as columns\n",
    "        pivot_diff = diff_analysis_df.pivot(\n",
    "            index=\"mouse\", \n",
    "            columns=\"group\", \n",
    "            values=\"mean_abs_velocity_0_2s\"\n",
    "        )\n",
    "        \n",
    "        # Check if both groups are present\n",
    "        if \"Apply halt\" not in pivot_diff.columns or \"No halt\" not in pivot_diff.columns:\n",
    "            print(\"⚠️ Missing Apply halt or No halt data\")\n",
    "            turning_velocity_diff_df = pd.DataFrame()\n",
    "        else:\n",
    "            # Compute difference (Apply halt - No halt)\n",
    "            pivot_diff[\"velocity_difference\"] = pivot_diff[\"Apply halt\"] - pivot_diff[\"No halt\"]\n",
    "            \n",
    "            # Create results dataframe\n",
    "            turning_velocity_diff_df = pivot_diff.reset_index()\n",
    "            \n",
    "            # Compute statistics\n",
    "            valid_diffs = turning_velocity_diff_df[\"velocity_difference\"].dropna()\n",
    "            \n",
    "            if len(valid_diffs) > 0:\n",
    "                mean_diff = float(valid_diffs.mean())\n",
    "                sem_diff = float(sem(valid_diffs))\n",
    "                n_mice = len(valid_diffs)\n",
    "                \n",
    "                print(f\"\\n✅ Turning velocity difference (Apply halt - No halt):\")\n",
    "                print(f\"   Analysis window: {DIFF_WINDOW[0]}-{DIFF_WINDOW[1]}s\")\n",
    "                print(f\"   N mice: {n_mice}\")\n",
    "                print(f\"   Mean difference: {mean_diff:.3f} ± {sem_diff:.3f} deg/s (mean ± SEM)\")\n",
    "                \n",
    "                # Paired t-test\n",
    "                if \"Apply halt\" in pivot_diff.columns and \"No halt\" in pivot_diff.columns:\n",
    "                    stats_result = compute_paired_t_test(pivot_diff, \"No halt\", \"Apply halt\")\n",
    "                    print(f\"   Paired t-test: t = {stats_result['t_statistic']:.3f}, p = {stats_result['p_value']:.4f}\")\n",
    "                    \n",
    "                    # Add to dataframe\n",
    "                    turning_velocity_diff_df[\"mean_difference\"] = mean_diff\n",
    "                    turning_velocity_diff_df[\"sem_difference\"] = sem_diff\n",
    "                    turning_velocity_diff_df[\"t_statistic\"] = stats_result[\"t_statistic\"]\n",
    "                    turning_velocity_diff_df[\"p_value\"] = stats_result[\"p_value\"]\n",
    "                \n",
    "                # Display results\n",
    "                display(Markdown(\"#### Turning velocity difference per mouse\"))\n",
    "                display(turning_velocity_diff_df[[\"mouse\", \"No halt\", \"Apply halt\", \"velocity_difference\"]])\n",
    "                \n",
    "                # Save to CSV\n",
    "                if OUTPUT_DIR is not None:\n",
    "                    turning_velocity_diff_df.to_csv(OUTPUT_DIR / \"turning_velocity_difference.csv\", index=False)\n",
    "                    print(f\"✅ Saved: turning_velocity_difference.csv\")\n",
    "                \n",
    "                # Create point plot\n",
    "                fig, ax = plt.subplots(1, 1, figsize=(4, 5))\n",
    "                \n",
    "                # Assign consistent colors\n",
    "                mouse_colors = assign_mouse_colors_consistent(turning_velocity_diff_df[\"mouse\"].unique())\n",
    "                \n",
    "                # Plot individual mice\n",
    "                x_pos = 0\n",
    "                for idx, row in turning_velocity_diff_df.iterrows():\n",
    "                    if pd.notna(row[\"velocity_difference\"]):\n",
    "                        ax.plot(\n",
    "                            x_pos,\n",
    "                            row[\"velocity_difference\"],\n",
    "                            marker=\"o\",\n",
    "                            markersize=8,\n",
    "                            color=mouse_colors.get(row[\"mouse\"], \"#1f77b4\"),\n",
    "                            alpha=0.7,\n",
    "                            zorder=2,\n",
    "                        )\n",
    "                \n",
    "                # Plot mean ± SEM\n",
    "                ax.errorbar(\n",
    "                    x_pos,\n",
    "                    mean_diff,\n",
    "                    yerr=sem_diff,\n",
    "                    fmt=\"o\",\n",
    "                    color=\"#333333\",\n",
    "                    markersize=12,\n",
    "                    linewidth=2.5,\n",
    "                    capsize=8,\n",
    "                    capthick=2.5,\n",
    "                    label=f\"Mean ± SEM\",\n",
    "                    zorder=3,\n",
    "                )\n",
    "                \n",
    "                # Add horizontal line at zero\n",
    "                ax.axhline(0, color=\"grey\", linestyle=\"--\", linewidth=1, alpha=0.5)\n",
    "                \n",
    "                # Formatting\n",
    "                ax.set_xlim(-0.5, 0.5)\n",
    "                ax.set_xticks([])\n",
    "                ax.set_ylabel(\"Velocity difference (deg/s)\\n(Apply halt - No halt)\", fontsize=11)\n",
    "                ax.set_title(f\"Turning Velocity Difference\\n{DIFF_WINDOW[0]}-{DIFF_WINDOW[1]}s post-halt\", fontsize=12)\n",
    "                ax.grid(True, axis=\"y\", linestyle=\":\", linewidth=0.7, alpha=0.5)\n",
    "                \n",
    "                # Add statistics text\n",
    "                if stats_result[\"p_value\"] < 0.001:\n",
    "                    p_text = \"p < 0.001***\"\n",
    "                elif stats_result[\"p_value\"] < 0.01:\n",
    "                    p_text = f\"p = {stats_result['p_value']:.3f}**\"\n",
    "                elif stats_result[\"p_value\"] < 0.05:\n",
    "                    p_text = f\"p = {stats_result['p_value']:.3f}*\"\n",
    "                else:\n",
    "                    p_text = f\"p = {stats_result['p_value']:.3f} ns\"\n",
    "                \n",
    "                ax.text(\n",
    "                    0.5, 0.98,\n",
    "                    f\"n = {n_mice} mice\\n{p_text}\",\n",
    "                    transform=ax.transAxes,\n",
    "                    ha=\"center\",\n",
    "                    va=\"top\",\n",
    "                    fontsize=10,\n",
    "                    bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"white\", edgecolor=\"grey\", alpha=0.8),\n",
    "                )\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                \n",
    "                if OUTPUT_DIR is not None:\n",
    "                    fig.savefig(OUTPUT_DIR / \"turning_velocity_difference_plot.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "                    print(f\"✅ Saved: turning_velocity_difference_plot.pdf\")\n",
    "                \n",
    "                plt.show()\n",
    "                plt.close(fig)\n",
    "            else:\n",
    "                print(\"⚠️ No valid differences computed\")\n",
    "                turning_velocity_diff_df = pd.DataFrame()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "aeon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
