{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa411362",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------------------------------#\n",
    "# IMPORTS\n",
    "#---------------------------------------------------------------------------------------------------#\n",
    "from pathlib import Path\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from matplotlib import cm\n",
    "import plotly.io as pio\n",
    "import plotly.express as px\n",
    "import plotly.subplots as sp\n",
    "import math\n",
    "import re\n",
    "from pprint import pprint\n",
    "import pickle\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy.stats import mode, pearsonr, norm, ttest_rel, ttest_ind, friedmanchisquare, wilcoxon\n",
    "from scipy.integrate import cumulative_trapezoid\n",
    "from scipy.signal import correlate, find_peaks\n",
    "from scipy.optimize import curve_fit\n",
    "import gc  # garbage collector for removing large variables from memory instantly \n",
    "import importlib  # for force updating changed packages \n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from collections import OrderedDict, defaultdict\n",
    "from typing import Dict, Optional, Tuple, Iterable, List\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "try:\n",
    "    from statsmodels.stats.anova import AnovaRM, anova_lm\n",
    "    from statsmodels.formula.api import ols\n",
    "    from statsmodels.stats.multitest import multipletests\n",
    "    STATS_MODELS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    STATS_MODELS_AVAILABLE = False\n",
    "    AnovaRM = None  # type: ignore\n",
    "    anova_lm = None  # type: ignore\n",
    "    ols = None  # type: ignore\n",
    "    multipletests = None  # type: ignore\n",
    "    print(\"‚ö†Ô∏è statsmodels not available. Repeated measures ANOVA will fall back to non-parametric tests.\")\n",
    "\n",
    "# Interactive widgets for dropdowns\n",
    "try:\n",
    "    import ipywidgets as widgets\n",
    "    from IPython.display import display, clear_output\n",
    "    WIDGETS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è ipywidgets not available. Install with: pip install ipywidgets\")\n",
    "    WIDGETS_AVAILABLE = False\n",
    "%config Completer.use_jedi = False  # Fixes autocomplete issues\n",
    "%config InlineBackend.figure_format = 'retina'  # Improves plot resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65c45d6",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# Aligned Data Analysis Notebook\n",
    "\n",
    "Analyze aligned `.csv` files generated in **SANDBOX_2** to produce mouse-level averages, grand averages with SEM, and a suite of post-alignment calcium signal metrics. The notebook also supports interactive plotting and the new halt vs no-halt comparison workflow.\n",
    "\n",
    "## What this notebook does\n",
    "- Loads aligned data for your selected cohort(s) and mice\n",
    "- Computes per-mouse mean ¬± SEM traces for key columns\n",
    "- Builds grand averages with SEM across mice\n",
    "- Extracts signal features (peak, onset, decay tau) for `z_470_Baseline` and `z_560_Baseline`\n",
    "- Calculates the 2‚Äì8 s post-alignment mean for `z_560_Baseline`\n",
    "- Optionally saves per-mouse and cohort-level summary CSVs\n",
    "- Provides interactive grand-average plotting widgets (when `ipywidgets` is available)\n",
    "- Compares halt vs no-halt conditions with paired statistics and plotting\n",
    "\n",
    "## How to use it\n",
    "1. Review **Configuration** and update cohort, mouse selection, event names, and data directories.\n",
    "2. Run the **Data Loading** cell to load aligned data.\n",
    "3. Execute the **Analysis** cells to compute metrics and grand averages.\n",
    "4. (Optional) Enable saving flags to export CSVs with per-mouse metrics and grand averages.\n",
    "5. Use the **Interactive Grand Average Plotting** cell to explore previously saved grand-average CSVs.\n",
    "6. Run the **Halt vs No Halt Condition Comparison** section to generate paired plots, statistics, and export CSV summaries per metric.\n",
    "\n",
    "## Outputs\n",
    "- **Grand averages CSV:** time-point means plus SEM for each selected column.\n",
    "- **Signal metric CSVs:** per-mouse feature metrics (peak, onset, decay tau, 2‚Äì8 s mean) saved in each experiment folder.\n",
    "- **Comparison exports:** per-metric statistics, plot data, and figures saved to `calcium_analysis/` within the source cohort directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7c0181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cohort: Cohort3\n",
      "‚úÖ Available mice: ['B6J2780', 'B6J2781', 'B6J2783', 'B6J2782']\n",
      "‚úÖ Selected mice: ['B6J2780', 'B6J2781', 'B6J2783', 'B6J2782']\n"
     ]
    }
   ],
   "source": [
    "#CONFIGURATION SECTION\n",
    "# ---------------------------------------------------------------------------------------------------#\n",
    "# Configure all settings here before running the analysis.\n",
    "\n",
    "# Cohort selection\n",
    "COHORT_OPTIONS = {\n",
    "    \"Cohort1\": {\n",
    "        \"mice\": ['B6J2717', 'B6J2718', 'B6J2719', 'B6J2721', 'B6J2722'],\n",
    "        \"identifier\": \"Cohort1\"\n",
    "    },\n",
    "    \"Cohort3\": {\n",
    "        \"mice\": [\"B6J2780\", \"B6J2781\", \"B6J2783\", \"B6J2782\"],\n",
    "        \"identifier\": \"Cohort3\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Select cohort\n",
    "cohort_identifier = \"Cohort3\"  # Options: \"Cohort1\", \"Cohort3\n",
    "\n",
    "# which part of the notebook to run \n",
    "# 1st run the save_signal_metrics first for \"Apple_halt_2s\", \n",
    "# 2nd run the for \"No_halt\" - \n",
    "# NB: input  files may need to be rename, i.e. removing spurious / or ; characters\n",
    "# 2nd run the generate_condition_comparison, then change cohorts \n",
    "SAVE_SIGNAL_METRICS = False\n",
    "GENERATE_CONDITION_COMPARISON = False\n",
    "\n",
    "# Select which animals to process (subset of the cohort's available mice).\n",
    "# Leave empty list [] to process all mice in the cohort.\n",
    "selected_mice: List[str] = []  # Example: ['B6J2717', 'B6J2718'] or [] for all\n",
    "\n",
    "# Data columns to analyze\n",
    "selected_columns = [\n",
    "    'z_560_Baseline', 'Motor_Velocity'\n",
    "]\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# SINGLE-CONDITION ANALYSIS SETUP\n",
    "# ------------------------------------------------------------------\n",
    "# Configure the 1Ô∏è‚É£ primary dataset whose aligned traces will be loaded and\n",
    "# summarised into metrics/grand averages in the first half of the notebook.\n",
    "COMPUTE_PER_MOUSE_ANALYSIS_OF_CALCIUM_DATA = {\n",
    "    # Friendly name used in status prints / saved filenames.\n",
    "    \"label\": \"no halt_motor\", # change for Apply halt 2s / No halt \n",
    "    # Suffix appended to each mouse ID to locate aligned CSVs inside `aligned_data/`.\n",
    "    \"event_suffix\": \"_No halt_right_turns_baselined_data.csv\", # change for halt / nohalt  \"_Apply halt_2s_baselined_data.csv\" \"_No halt_baselined_data.csv\"\n",
    "    # One or more experiment-day directories that contain the aligned traces.\n",
    "    \"data_dirs\": [\n",
    "        Path('/Volumes/RanczLab2/DATA_NEW/Cohort3_rotation/Visual_mismatch_day3').expanduser(), # change per cohort\n",
    "        Path('/Volumes/RanczLab2/DATA_NEW/Cohort3_rotation/Visual_mismatch_day4').expanduser(),\n",
    "    ],\n",
    "}\n",
    "\n",
    "event_name: str = COMPUTE_PER_MOUSE_ANALYSIS_OF_CALCIUM_DATA[\"event_suffix\"]\n",
    "PRIMARY_ANALYSIS_LABEL: str = COMPUTE_PER_MOUSE_ANALYSIS_OF_CALCIUM_DATA[\"label\"]\n",
    "DATA_DIRS: List[Path] = [\n",
    "    Path(p).expanduser() for p in COMPUTE_PER_MOUSE_ANALYSIS_OF_CALCIUM_DATA[\"data_dirs\"]\n",
    "]\n",
    "if not DATA_DIRS:\n",
    "    raise ValueError(\"COMPUTE_PER_MOUSE_ANALYSIS_OF_CALCIUM_DATA must provide at least one data directory.\")\n",
    "\n",
    "SAVE_CSV = True      # Save grand averages with SEM as CSV file for plotting traces\n",
    "\n",
    "SIGNAL_METRICS_OUTPUT_SUBDIR = DATA_DIRS[0].parent.parent\n",
    "\n",
    "# Signal feature metric settings (incorporated from SANDBOX 4 workflow)\n",
    "SIGNAL_METRIC_COLUMNS = [\n",
    "    'z_560_Baseline',\n",
    "]\n",
    "SIGNAL_METRIC_FEATURES = (\n",
    "    'peak',\n",
    "    'onset_time',\n",
    "    'decay_tau1',\n",
    "    'mean_fluorescence_2_to_8s',\n",
    "    'main_peak_residual_auc',\n",
    "    'offset_residual_auc',\n",
    "    'offset_peak_amplitude'\n",
    ")\n",
    "\n",
    "# Configure the post-alignment analysis window (seconds relative to alignment time).\n",
    "# Adjust POST_ALIGNMENT_WINDOW_DURATION to control the length of the comparison interval.\n",
    "POST_ALIGNMENT_WINDOW_START = 0.0 #aligment means halt time here  \n",
    "POST_ALIGNMENT_WINDOW_DURATION = 2.0\n",
    "POST_ALIGNMENT_WINDOW = (\n",
    "    POST_ALIGNMENT_WINDOW_START,\n",
    "    POST_ALIGNMENT_WINDOW_START + POST_ALIGNMENT_WINDOW_DURATION,\n",
    ")\n",
    "\n",
    "# Offset response detection settings (Strategy 3: Hybrid residual + peak detection)\n",
    "OFFSET_BASELINE_WINDOW = (1.9, 2.0)  # Baseline window for direct peak amplitude calculation\n",
    "OFFSET_ANALYSIS_START = 2.0          # Start analyzing offset response after main peak\n",
    "OFFSET_ANALYSIS_END = 8.0            # End of offset analysis window\n",
    "OFFSET_AUC_WINDOW = (2.0, 6.0)       # Window for area-under-curve calculation on residuals\n",
    "DECAY_FIT_START_OFFSET = 0.0         # Extra time (seconds) added to peak time before starting exponential fit (0.0 = fit from peak)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# CONDITION COMPARISON SETUP\n",
    "# ------------------------------------------------------------------\n",
    "# Configure the 2Ô∏è‚É£ cross-condition comparison module (halt vs no-halt, etc.).\n",
    "# Each entry stands on its own so it is easy to enable/disable or point to\n",
    "# different folders without touching the single-condition settings above.  \n",
    "# Each key defines one condition: give it a unique name, the event suffix to load,\n",
    "# the directories the aligned traces live in, and an optional list of already\n",
    "# computed metrics CSVs (leave empty to recompute on the fly).\n",
    "COMPARE_CALCIUM_METRICS_ACROSS_CONDITIONS = OrderedDict({    # CHANGE according to cohort \n",
    "    \"Apply_halt_day3\": {\n",
    "        \"event_name\": \"_Apply halt_2s_baselined_data.csv\",\n",
    "        \"data_dirs\": [\n",
    "            Path('/Volumes/RanczLab/Cohort3_rotation/Visual_mismatch_day3').expanduser(),\n",
    "        ],\n",
    "        \"label\": \"Apply halt day 3\",\n",
    "        \"metrics_csvs\": [],\n",
    "    },\n",
    "    \"Apply_halt_day4\": {\n",
    "        \"event_name\": \"_Apply halt_2s_baselined_data.csv\",\n",
    "        \"data_dirs\": [\n",
    "            Path('/Volumes/RanczLab/Cohort3_rotation/Visual_mismatch_day4').expanduser(),\n",
    "        ],\n",
    "        \"label\": \"Apply halt day 4\",\n",
    "        \"metrics_csvs\": [],\n",
    "    },\n",
    "    \"No_halt\": {\n",
    "        \"event_name\": \"_No halt_baselined_data.csv\",\n",
    "        \"data_dirs\": [\n",
    "            Path('/Volumes/RanczLab/Cohort3_rotation/Visual_mismatch_day3').expanduser(),\n",
    "            Path('/Volumes/RanczLab/Cohort3_rotation/Visual_mismatch_day4').expanduser(),\n",
    "        ],\n",
    "        \"label\": \"No halt\",\n",
    "        \"metrics_csvs\": [],\n",
    "    },\n",
    "})\n",
    "\n",
    "COMPARISON_METRICS = ( # FIXME EDE TO ADD EXTRA FEATURES (aad function cell ANALYSIS functions  )\n",
    "    \"peak\",\n",
    "    \"onset_time\",\n",
    "    \"decay_tau1\",\n",
    "    \"mean_fluorescence_2_to_8s\",\n",
    "    \"main_peak_residual_auc\",\n",
    "    \"offset_residual_auc\",\n",
    "    \"offset_peak_amplitude\"\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------#\n",
    "# # Auto-configure based on cohort selection\n",
    "# ---------------------------------------------------------------------------------------------------#\n",
    "if cohort_identifier in COHORT_OPTIONS:\n",
    "    cohort_info = COHORT_OPTIONS[cohort_identifier]\n",
    "    available_mice = cohort_info[\"mice\"]\n",
    "    if not selected_mice:  # If empty, use all mice\n",
    "        selected_mice = available_mice\n",
    "    else:  # Filter to only include valid mice for the selected cohort\n",
    "        filtered_mice = [m for m in selected_mice if m in available_mice]\n",
    "        ignored_mice = [m for m in selected_mice if m not in available_mice]\n",
    "        selected_mice = filtered_mice\n",
    "        if ignored_mice:\n",
    "            print(f\"‚è≠Ô∏è Ignoring mice not in cohort {cohort_identifier}: {ignored_mice}\")\n",
    "    print(f\"‚úÖ Cohort: {cohort_identifier}\")\n",
    "    print(f\"‚úÖ Available mice: {available_mice}\")\n",
    "    print(f\"‚úÖ Selected mice: {selected_mice}\")\n",
    "else:\n",
    "    raise ValueError(f\"Invalid cohort_identifier: {cohort_identifier}. Must be one of {list(COHORT_OPTIONS.keys())}\")\n",
    "\n",
    "# Save options\n",
    "# SAVE_PICKLE = False  # Save results as pickle file (deprecated - use SAVE_ANIMAL_CSV instead)\n",
    "# FIXME THIS IS REGENERATING SOMETHING BUT IT DOES NOT MAKE SENSE! SAVE_ANIMAL_CSV = False  # Save averaged mismatch aligned data for each animal as CSV\n",
    "# FIXME WE WILL NOT USE THESE PLOTS FOR NOW!!! GENERATE_PLOTS = True  # Generate plots\n",
    "# # # Columns to plot\n",
    "# columns_to_plot = [\n",
    "#     'Velocity_0X_Baseline', 'Motor_Velocity_Baseline', \n",
    "#     'z_470_Baseline', 'z_560_Baseline'\n",
    "# ]\n",
    "# # Pre/post comparison plotting options\n",
    "# FIXME WE WILL NOT USE THESE PLOTS FOR NOW!!! PLOT_PREPOST_FROM_RESULTS = True  # Generate pre/post plots from freshly computed results\n",
    "# FIXME NEED TO VERIFY WHAT IS GOING ON LOAD_EXISTING_PREPOST_CSV = False  # Load a previously created cohort_aligned_data_analysis.csv\n",
    "# FIXME VERIFY WHAT IS GOING ONEXISTING_PREPOST_CSV_PATH = Path('/Users/nora/Desktop/for_poster/cohort_3/cohort_aligned_data_analysis.csv').expanduser()\n",
    "# PREPOST_SAVE_DIR = None  # Optional custom directory to save pre/post plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5f6f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded B6J2781: /Volumes/RanczLab2/DATA_NEW/Cohort3_rotation/Visual_mismatch_day3/B6J2781-2025-04-25T12-30-52_processedData/aligned_data/B6J2781_No halt_right_turns_baselined_data.csv\n",
      "‚úÖ Loaded B6J2780: /Volumes/RanczLab2/DATA_NEW/Cohort3_rotation/Visual_mismatch_day3/B6J2780-2025-04-25T11-51-53_processedData/aligned_data/B6J2780_No halt_right_turns_baselined_data.csv\n",
      "‚úÖ Loaded B6J2783: /Volumes/RanczLab2/DATA_NEW/Cohort3_rotation/Visual_mismatch_day3/B6J2783-2025-04-25T13-41-53_processedData/aligned_data/B6J2783_No halt_right_turns_baselined_data.csv\n",
      "‚úÖ Loaded B6J2781: /Volumes/RanczLab2/DATA_NEW/Cohort3_rotation/Visual_mismatch_day4/B6J2781-2025-04-28T13-45-40_processedData/aligned_data/B6J2781_No halt_right_turns_baselined_data.csv\n",
      "‚úÖ Loaded B6J2782: /Volumes/RanczLab2/DATA_NEW/Cohort3_rotation/Visual_mismatch_day4/B6J2782-2025-04-28T14-22-03_processedData/aligned_data/B6J2782_No halt_right_turns_baselined_data.csv\n",
      "‚úÖ Loaded B6J2783: /Volumes/RanczLab2/DATA_NEW/Cohort3_rotation/Visual_mismatch_day4/B6J2783-2025-04-28T14-57-30_processedData/aligned_data/B6J2783_No halt_right_turns_baselined_data.csv\n",
      "‚úÖ Loaded B6J2780: /Volumes/RanczLab2/DATA_NEW/Cohort3_rotation/Visual_mismatch_day4/B6J2780-2025-04-28T13-10-18_processedData/aligned_data/B6J2780_No halt_right_turns_baselined_data.csv\n"
     ]
    }
   ],
   "source": [
    "# DATA LOADING\n",
    "#---------------------------------------------------------------------------------------------------#\n",
    "def load_aligned_data(data_dirs, event_name, selected_mice, allowed_mice=None):\n",
    "    \"\"\"\n",
    "    Load aligned data from every CSV that matches the requested event suffix across\n",
    "    all provided directories. Multiple matching files per mouse (and per experiment\n",
    "    day) are preserved so downstream steps can aggregate or analyse them per day.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_dirs : list\n",
    "        Directories that should be searched (recursively) for aligned CSV files.\n",
    "    event_name : str\n",
    "        Event name suffix (e.g. \"_No halt_baselined_data.csv\").\n",
    "    selected_mice : list[str]\n",
    "        Explicit list of mice to include. If empty, all cohort mice are considered.\n",
    "    allowed_mice : list[str] | None\n",
    "        Full set of cohort-allowed mice (used when selected_mice is empty).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dictionary keyed by mouse name containing loaded dataframes and metadata.\n",
    "    \"\"\"\n",
    "    if selected_mice:\n",
    "        selected_mice_set: Optional[set[str]] = set(selected_mice)\n",
    "    elif allowed_mice:\n",
    "        selected_mice_set = set(allowed_mice)\n",
    "    else:\n",
    "        selected_mice_set = None\n",
    "\n",
    "    event_suffix = event_name or \"\"\n",
    "    if event_suffix and not event_suffix.lower().endswith(\".csv\"):\n",
    "        print(f\"‚ö†Ô∏è Event suffix '{event_suffix}' does not end with '.csv'; loading may fail.\")\n",
    "\n",
    "    data_dir_paths: List[Path] = [Path(p).expanduser().resolve() for p in data_dirs]\n",
    "    per_mouse_records: Dict[str, List[Dict[str, object]]] = {}\n",
    "    discovered_mice: set[str] = set()\n",
    "\n",
    "    for base_dir in data_dir_paths:\n",
    "        if not base_dir.exists():\n",
    "            print(f\"‚ö†Ô∏è Data directory not found: {base_dir}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            csv_candidates = list(base_dir.rglob(\"*.csv\"))\n",
    "        except Exception as exc:  # noqa: BLE001\n",
    "            print(f\"‚ö†Ô∏è Failed to scan directory {base_dir}: {exc}\")\n",
    "            continue\n",
    "\n",
    "        for csv_path in csv_candidates:\n",
    "            if not csv_path.is_file() or csv_path.name.startswith(\"._\"):\n",
    "                continue\n",
    "            if event_suffix and not csv_path.name.endswith(event_suffix):\n",
    "                continue\n",
    "\n",
    "            resolved_csv = csv_path.resolve()\n",
    "            mouse_id = (\n",
    "                resolved_csv.name[:-len(event_suffix)] if event_suffix else resolved_csv.stem\n",
    "            )\n",
    "            if not mouse_id:\n",
    "                continue\n",
    "\n",
    "            discovered_mice.add(mouse_id)\n",
    "            if selected_mice_set and mouse_id not in selected_mice_set:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                aligned_df = pd.read_csv(resolved_csv)\n",
    "            except FileNotFoundError:\n",
    "                continue\n",
    "            except Exception as exc:  # noqa: BLE001\n",
    "                print(f\"‚ö†Ô∏è Failed to read {resolved_csv}: {exc}\")\n",
    "                continue\n",
    "\n",
    "            if aligned_df.empty:\n",
    "                print(f\"‚ö†Ô∏è Empty CSV skipped: {resolved_csv}\")\n",
    "                continue\n",
    "\n",
    "            experiment_dir = next(\n",
    "                (candidate for candidate in data_dir_paths if candidate in resolved_csv.parents),\n",
    "                None,\n",
    "            )\n",
    "            experiment_day = experiment_dir.name if experiment_dir else resolved_csv.parent.name\n",
    "\n",
    "            per_mouse_records.setdefault(mouse_id, []).append(\n",
    "                {\n",
    "                    \"dataframe\": aligned_df,\n",
    "                    \"csv_path\": resolved_csv,\n",
    "                    \"aligned_dir\": resolved_csv.parent,\n",
    "                    \"experiment_dir\": experiment_dir,\n",
    "                    \"experiment_day\": experiment_day,\n",
    "                }\n",
    "            )\n",
    "            print(f\"‚úÖ Loaded {mouse_id}: {resolved_csv}\")\n",
    "\n",
    "    if selected_mice_set:\n",
    "        missing = sorted(selected_mice_set - discovered_mice)\n",
    "        if missing:\n",
    "            print(f\"‚è≠Ô∏è No matching files found for selected mice: {missing}\")\n",
    "\n",
    "    loaded_data: Dict[str, Dict[str, object]] = {}\n",
    "    for mouse_name, records in per_mouse_records.items():\n",
    "        aligned_dirs = [record[\"aligned_dir\"] for record in records]\n",
    "        source_csvs = [record[\"csv_path\"] for record in records]\n",
    "        experiment_dirs = [\n",
    "            record[\"experiment_dir\"] for record in records if record.get(\"experiment_dir\") is not None\n",
    "        ]\n",
    "        loaded_data[mouse_name] = {\n",
    "            \"mouse_name\": mouse_name,\n",
    "            \"records\": records,\n",
    "            \"dataframes\": [record[\"dataframe\"] for record in records],\n",
    "            \"data_path\": aligned_dirs[0] if aligned_dirs else None,\n",
    "            \"aligned_dirs\": aligned_dirs,\n",
    "            \"source_csvs\": source_csvs,\n",
    "            \"experiment_dirs\": experiment_dirs,\n",
    "        }\n",
    "\n",
    "    if not loaded_data:\n",
    "        print(\"‚ö†Ô∏è No aligned CSV files were loaded. Please verify the configuration.\")\n",
    "\n",
    "    return loaded_data\n",
    "\n",
    "# Load the data\n",
    "loaded_data = load_aligned_data(DATA_DIRS, event_name, selected_mice, available_mice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dab6881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OUTPUT DIRECTORY HELPERS\n",
    "#---------------------------------------------------------------------------------------------------#\n",
    "\n",
    "def determine_main_data_dir(loaded_data, data_dirs, cohort_identifier):\n",
    "    \"\"\"Select the base data directory corresponding to the loaded cohort data.\"\"\"\n",
    "    data_dir_paths = [Path(p).expanduser().resolve() for p in data_dirs]\n",
    "\n",
    "    loaded_base_dirs = []\n",
    "    for entry in loaded_data.values():\n",
    "        if not isinstance(entry, dict):\n",
    "            continue\n",
    "        candidate_paths: List[Path] = []\n",
    "        records = entry.get(\"records\", [])\n",
    "        for record in records:\n",
    "            experiment_dir = record.get(\"experiment_dir\")\n",
    "            if experiment_dir:\n",
    "                candidate_paths.append(Path(experiment_dir).resolve())\n",
    "            else:\n",
    "                aligned_dir = record.get(\"aligned_dir\")\n",
    "                if aligned_dir:\n",
    "                    candidate_paths.append(Path(aligned_dir).resolve().parent.parent)\n",
    "        if not candidate_paths and entry.get(\"data_path\"):\n",
    "            candidate_paths.append(Path(entry[\"data_path\"]).resolve())\n",
    "        for candidate in candidate_paths:\n",
    "            if candidate not in loaded_base_dirs:\n",
    "                loaded_base_dirs.append(candidate)\n",
    "\n",
    "    if not loaded_base_dirs:\n",
    "        for candidate in data_dir_paths:\n",
    "            if candidate.exists():\n",
    "                return candidate\n",
    "        return Path.cwd()\n",
    "\n",
    "    cohort_id = (cohort_identifier or \"\").lower()\n",
    "    if cohort_id:\n",
    "        for candidate in data_dir_paths:\n",
    "            candidate_resolved = Path(candidate).resolve()\n",
    "            if candidate_resolved in loaded_base_dirs and cohort_id in str(candidate_resolved).lower():\n",
    "                return candidate_resolved\n",
    "\n",
    "    for candidate in data_dir_paths:\n",
    "        candidate_resolved = Path(candidate).resolve()\n",
    "        if candidate_resolved in loaded_base_dirs:\n",
    "            return candidate_resolved\n",
    "\n",
    "    return loaded_base_dirs[0]\n",
    "\n",
    "def extract_experiment_day(base_dir: Path) -> str:\n",
    "    \"\"\"Derive experiment day label from the selected base directory.\"\"\"\n",
    "    if not base_dir or not isinstance(base_dir, Path):\n",
    "        return \"unknown\"\n",
    "    return base_dir.name or \"unknown\"\n",
    "\n",
    "\n",
    "def sanitize_for_filename(value: Optional[str], fallback: str = \"unknown\") -> str:\n",
    "    \"\"\"Convert a string into a filesystem-friendly fragment.\"\"\"\n",
    "    if value is None:\n",
    "        return fallback\n",
    "    cleaned = re.sub(r\"[^A-Za-z0-9_-]+\", \"_\", str(value)).strip(\"_\")\n",
    "    return cleaned or fallback\n",
    "\n",
    "\n",
    "def build_grand_average_filename(cohort_id: str, experiment_day: str, event_name: str) -> str:\n",
    "    \"\"\"Compose the output filename for grand average CSV exports.\"\"\"\n",
    "    cohort_part = sanitize_for_filename(cohort_id, \"cohort\")\n",
    "    day_part = sanitize_for_filename(experiment_day, \"day\")\n",
    "    event_label = sanitize_for_filename(event_name.replace(\".csv\", \"\"), \"event\")\n",
    "    return f\"{cohort_part}_{day_part}_grand_averages_with_sem_{event_label}.csv\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315472fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANALYSIS FUNCTIONS\n",
    "#---------------------------------------------------------------------------------------------------#\n",
    "\n",
    "def compute_mouse_means_and_grand_average(loaded_data, selected_columns, main_data_dir, selected_mice):\n",
    "    \"\"\"\n",
    "    Compute means per mouse and grand averages across selected mice for selected columns.\n",
    "    \n",
    "    Parameters:\n",
    "    loaded_data (dict): Dictionary with data paths as keys and mouse data as values\n",
    "    selected_columns (list): List of column names to analyze\n",
    "    main_data_dir (str/Path): Main directory to save results\n",
    "    selected_mice (list): List of mouse names to include in the grand average\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (mean_data_per_mouse, sem_data_per_mouse, grand_averages, grand_sems, mouse_to_data_path)\n",
    "        mouse_to_data_path: Dictionary mapping mouse names to their data_path (aligned_data folder)\n",
    "    \"\"\"\n",
    "    \n",
    "    main_data_dir = Path(main_data_dir)\n",
    "    \n",
    "    loaded_mouse_names = [data['mouse_name'] for data in loaded_data.values()]\n",
    "    loaded_mouse_set = set(loaded_mouse_names)\n",
    "\n",
    "    if selected_mice:\n",
    "        filtered_selection = [m for m in selected_mice if m in loaded_mouse_set]\n",
    "        ignored_selection = [m for m in selected_mice if m not in loaded_mouse_set]\n",
    "        if ignored_selection:\n",
    "            print(f\"‚è≠Ô∏è Ignoring selected mice without loaded data: {ignored_selection}\")\n",
    "        selected_mice = filtered_selection\n",
    "    else:\n",
    "        selected_mice = loaded_mouse_names\n",
    "\n",
    "    if not selected_mice:\n",
    "        print(\"‚ö†Ô∏è No mice available for grand average computation.\")\n",
    "        return {}, {}, pd.DataFrame(), pd.DataFrame(), {}\n",
    "    \n",
    "    print(f\"Processing selected columns: {selected_columns}\")\n",
    "    \n",
    "    # Step 1: Compute mean and SEM for each mouse\n",
    "    mean_data_per_mouse = {}\n",
    "    sem_data_per_mouse = {}\n",
    "    mouse_to_data_path = {}  # Track data_path for each mouse\n",
    "    \n",
    "    for mouse_name, entry in loaded_data.items():\n",
    "        if mouse_name not in selected_mice:\n",
    "            continue\n",
    "\n",
    "        frames: List[pd.DataFrame] = []\n",
    "        if isinstance(entry, dict):\n",
    "            records = entry.get(\"records\", [])\n",
    "            if records:\n",
    "                for record in records:\n",
    "                    df = record.get(\"dataframe\")\n",
    "                    if isinstance(df, pd.DataFrame):\n",
    "                        frames.append(df.copy())\n",
    "            elif entry.get(\"dataframes\"):\n",
    "                frames = [\n",
    "                    df.copy()\n",
    "                    for df in entry.get(\"dataframes\", [])\n",
    "                    if isinstance(df, pd.DataFrame)\n",
    "                ]\n",
    "\n",
    "        if not frames:\n",
    "            print(f\"‚ö†Ô∏è No data frames available for {mouse_name}, skipping...\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Processing mouse: {mouse_name} ({len(frames)} file(s))\")\n",
    "\n",
    "        combined_df = pd.concat(frames, ignore_index=True, sort=False)\n",
    "        if combined_df.empty:\n",
    "            print(f\"‚ö†Ô∏è Combined dataframe empty for {mouse_name}, skipping...\")\n",
    "            continue\n",
    "\n",
    "        if \"Time (s)\" not in combined_df.columns:\n",
    "            print(f\"‚ö†Ô∏è  'Time (s)' column not found for {mouse_name}, skipping...\")\n",
    "            continue\n",
    "\n",
    "        available_columns = [col for col in selected_columns if col in combined_df.columns]\n",
    "        missing_columns = [col for col in selected_columns if col not in combined_df.columns]\n",
    "        if missing_columns:\n",
    "            print(f\"‚ö†Ô∏è  Missing columns for {mouse_name}: {missing_columns}\")\n",
    "        if not available_columns:\n",
    "            print(f\"‚ö†Ô∏è  No requested columns available for {mouse_name}, skipping...\")\n",
    "            continue\n",
    "\n",
    "        numeric_selected = []\n",
    "        for col in available_columns:\n",
    "            combined_df[col] = pd.to_numeric(combined_df[col], errors=\"coerce\")\n",
    "            if col != \"Time (s)\" and pd.api.types.is_numeric_dtype(combined_df[col]):\n",
    "                numeric_selected.append(col)\n",
    "        combined_df[\"Time (s)\"] = pd.to_numeric(combined_df[\"Time (s)\"], errors=\"coerce\")\n",
    "        combined_df = combined_df.dropna(subset=[\"Time (s)\"])\n",
    "\n",
    "        if not numeric_selected:\n",
    "            print(f\"‚ö†Ô∏è  No numeric columns found for {mouse_name}\")\n",
    "            continue\n",
    "\n",
    "        grouped = combined_df.groupby(\"Time (s)\")\n",
    "        mean_df = grouped[numeric_selected].mean()\n",
    "        sem_df = grouped[numeric_selected].sem()\n",
    "\n",
    "        mean_data_per_mouse[mouse_name] = mean_df\n",
    "        sem_data_per_mouse[mouse_name] = sem_df\n",
    "\n",
    "        data_path_value = None\n",
    "        if isinstance(entry, dict):\n",
    "            if entry.get(\"records\"):\n",
    "                first_record = next(\n",
    "                    (record for record in entry[\"records\"] if record.get(\"aligned_dir\")), None\n",
    "                )\n",
    "                if first_record:\n",
    "                    data_path_value = Path(first_record[\"aligned_dir\"])\n",
    "            if data_path_value is None and entry.get(\"data_path\"):\n",
    "                data_path_value = Path(entry[\"data_path\"])\n",
    "\n",
    "        mouse_to_data_path[mouse_name] = data_path_value\n",
    "\n",
    "        print(f\"‚úÖ Processed {len(numeric_selected)} columns for {mouse_name}\")\n",
    "    \n",
    "    # Step 2: Compute grand averages across selected mice\n",
    "    print(f\"\\nüìä Computing grand averages across {len(mean_data_per_mouse)} selected mice...\")\n",
    "    \n",
    "    # Get all unique time points\n",
    "    all_time_points = set()\n",
    "    for mouse_data in mean_data_per_mouse.values():\n",
    "        all_time_points.update(mouse_data.index)\n",
    "    all_time_points = sorted(list(all_time_points))\n",
    "    \n",
    "    # Get all columns that were successfully processed, plus any selected columns that might be missing\n",
    "    # This ensures all selected columns are included in the grand average, even if missing from all mice\n",
    "    all_processed_columns = set()\n",
    "    for mouse_data in mean_data_per_mouse.values():\n",
    "        all_processed_columns.update(mouse_data.columns)\n",
    "    # Also include all selected columns (excluding 'Time (s)') to ensure they're in the output\n",
    "    for col in selected_columns:\n",
    "        if col != \"Time (s)\":\n",
    "            all_processed_columns.add(col)\n",
    "    all_processed_columns = sorted(list(all_processed_columns))\n",
    "    \n",
    "    print(f\"Time points: {len(all_time_points)} from {min(all_time_points):.2f}s to {max(all_time_points):.2f}s\")\n",
    "    print(f\"Processed columns: {all_processed_columns}\")\n",
    "    \n",
    "    # Create grand average DataFrame\n",
    "    grand_averages = pd.DataFrame(index=all_time_points, columns=all_processed_columns)\n",
    "    grand_averages.index.name = 'Time (s)'\n",
    "    \n",
    "    grand_sems = pd.DataFrame(index=all_time_points, columns=all_processed_columns)\n",
    "    grand_sems.index.name = 'Time (s)'\n",
    "    \n",
    "    # Compute grand averages for each column and time point\n",
    "    for col in all_processed_columns:\n",
    "        for time_point in all_time_points:\n",
    "            # Collect data from all selected mice for this time point and column\n",
    "            mouse_values = []\n",
    "            for mouse_name, mouse_data in mean_data_per_mouse.items():\n",
    "                if time_point in mouse_data.index and col in mouse_data.columns:\n",
    "                    value = mouse_data.loc[time_point, col]\n",
    "                    if not pd.isna(value):\n",
    "                        mouse_values.append(value)\n",
    "            \n",
    "            if len(mouse_values) > 0:\n",
    "                grand_averages.loc[time_point, col] = np.mean(mouse_values)\n",
    "                if len(mouse_values) > 1:\n",
    "                    grand_sems.loc[time_point, col] = np.std(mouse_values) / np.sqrt(len(mouse_values))\n",
    "                else:\n",
    "                    grand_sems.loc[time_point, col] = 0\n",
    "    \n",
    "    return mean_data_per_mouse, sem_data_per_mouse, grand_averages, grand_sems, mouse_to_data_path\n",
    "\n",
    "def analyze_mice_data(loaded_data, selected_columns, main_data_dir):\n",
    "    \"\"\"\n",
    "    Complete analysis workflow: compute means, grand averages, save CSV, and create plots.\n",
    "    \n",
    "    Parameters:\n",
    "    loaded_data (dict): Your loaded_data dictionary\n",
    "    selected_columns (list): List of column names to analyze (including 'Time (s)')\n",
    "    main_data_dir (str/Path): Main directory to save results\n",
    "    \n",
    "    Returns:\n",
    "    dict: Complete results including individual and grand averages\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"MOUSE DATA ANALYSIS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Use the selected_mice from the configuration (defined in Cell 2)\n",
    "    # Compute means and grand averages\n",
    "    mean_data_per_mouse, sem_data_per_mouse, grand_averages, grand_sems, mouse_to_data_path = compute_mouse_means_and_grand_average(\n",
    "        loaded_data, selected_columns, main_data_dir, selected_mice\n",
    "    )\n",
    "\n",
    "    # Print summary\n",
    "    print(f\"\\nüìä ANALYSIS COMPLETE:\")\n",
    "    print(f\"   ‚Ä¢ Number of mice analyzed: {len(mean_data_per_mouse)}\")\n",
    "    print(f\"   ‚Ä¢ Mouse names: {list(mean_data_per_mouse.keys())}\")\n",
    "    print(f\"   ‚Ä¢ Columns processed: {list(grand_averages.columns)}\")\n",
    "    print(f\"   ‚Ä¢ Time range: {grand_averages.index.min():.2f}s to {grand_averages.index.max():.2f}s\")\n",
    "    print(f\"   ‚Ä¢ Files saved in: {main_data_dir}\")\n",
    "    \n",
    "    # Return all results\n",
    "    results = {\n",
    "        'mean_data_per_mouse': mean_data_per_mouse,\n",
    "        'sem_data_per_mouse': sem_data_per_mouse,\n",
    "        'grand_averages': grand_averages,\n",
    "        'grand_sems': grand_sems,\n",
    "        'mouse_to_data_path': mouse_to_data_path,\n",
    "    }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810f08ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "MOUSE DATA ANALYSIS\n",
      "============================================================\n",
      "Processing selected columns: ['z_560_Baseline', 'Motor_Velocity']\n",
      "Processing mouse: B6J2781 (2 file(s))\n",
      "‚úÖ Processed 2 columns for B6J2781\n",
      "Processing mouse: B6J2780 (2 file(s))\n",
      "‚úÖ Processed 2 columns for B6J2780\n",
      "Processing mouse: B6J2783 (2 file(s))\n",
      "‚úÖ Processed 2 columns for B6J2783\n",
      "Processing mouse: B6J2782 (1 file(s))\n",
      "‚úÖ Processed 2 columns for B6J2782\n",
      "\n",
      "üìä Computing grand averages across 4 selected mice...\n",
      "Time points: 16001 from -6.00s to 10.00s\n",
      "Processed columns: ['Motor_Velocity', 'z_560_Baseline']\n",
      "\n",
      "üìä ANALYSIS COMPLETE:\n",
      "   ‚Ä¢ Number of mice analyzed: 4\n",
      "   ‚Ä¢ Mouse names: ['B6J2781', 'B6J2780', 'B6J2783', 'B6J2782']\n",
      "   ‚Ä¢ Columns processed: ['Motor_Velocity', 'z_560_Baseline']\n",
      "   ‚Ä¢ Time range: -6.00s to 10.00s\n",
      "   ‚Ä¢ Files saved in: /Volumes/RanczLab2/DATA_NEW/Cohort3_rotation/Visual_mismatch_day3\n",
      "üìä Columns in grand_averages: ['Motor_Velocity', 'z_560_Baseline']\n",
      "üìä Columns being saved to CSV: ['Motor_Velocity', 'z_560_Baseline', 'Motor_Velocity_SEM', 'z_560_Baseline_SEM']\n",
      "‚úÖ Grand averages with SEM saved to: /Volumes/RanczLab2/DATA_NEW/Cohort3_rotation/Visual_mismatch_day3/Cohort3_Visual_mismatch_day3_grand_averages_with_sem_No_halt_right_turns_baselined_data.csv\n"
     ]
    }
   ],
   "source": [
    "# SAVE GRAND AVERAGES CSV\n",
    "# Determine main data directory (first existing entry in DATA_DIRS, otherwise current working dir)\n",
    "_existing_data_dirs = [Path(p).expanduser() for p in DATA_DIRS if Path(p).expanduser().exists()]\n",
    "if 'loaded_data' in locals():\n",
    "    main_data_dir = determine_main_data_dir(loaded_data, DATA_DIRS, cohort_identifier)\n",
    "else:\n",
    "    main_data_dir = _existing_data_dirs[0] if _existing_data_dirs else Path.cwd()\n",
    "main_data_dir = Path(main_data_dir).resolve()\n",
    "experiment_day = extract_experiment_day(main_data_dir)\n",
    "\n",
    "# Run analysis and keep results available for later cells/plots\n",
    "if loaded_data:\n",
    "    results = analyze_mice_data(loaded_data, selected_columns, main_data_dir)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No data loaded. Please check your configuration and data paths.\")\n",
    "    results = None\n",
    "if results and SAVE_CSV:\n",
    "    # Create a DataFrame combining grand averages and SEMs\n",
    "    grand_avg_with_sem = results['grand_averages'].copy()\n",
    "    for col in results['grand_sems'].columns:\n",
    "        grand_avg_with_sem[f'{col}_SEM'] = results['grand_sems'][col]\n",
    "\n",
    "    # Generate filename with cohort and experiment day context\n",
    "    csv_filename = main_data_dir / build_grand_average_filename(\n",
    "        cohort_identifier,\n",
    "        experiment_day,\n",
    "        event_name,\n",
    "    )\n",
    "\n",
    "    # Debug: print columns being saved\n",
    "    print(f\"üìä Columns in grand_averages: {list(results['grand_averages'].columns)}\")\n",
    "    print(f\"üìä Columns being saved to CSV: {list(grand_avg_with_sem.columns)}\")\n",
    "    \n",
    "    # Save the DataFrame to a CSV file (retain Time (s) as index)\n",
    "    grand_avg_with_sem.to_csv(csv_filename)\n",
    "    print(f\"‚úÖ Grand averages with SEM saved to: {csv_filename}\")\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è  Skipping CSV save (SAVE_CSV=False or no results)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1dd45c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "404ccbe905e04db98e5f475a03fa5f1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h3>Select multiple files and columns to plot:</h3>'), HTML(value='<p><i>Add multip‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# INTERACTIVE GRAND AVERAGE PLOTTING\n",
    "#---------------------------------------------------------------------------------------------------#\n",
    "\n",
    "def find_csv_files(base_paths):\n",
    "    \"\"\"Find all CSV files containing 'grand_averages' in the given paths.\"\"\"\n",
    "    csv_files = set()\n",
    "    for base_path in base_paths:\n",
    "        path = Path(base_path)\n",
    "        if path.exists():\n",
    "            for csv_file in path.rglob('*.csv'):\n",
    "                name = csv_file.name\n",
    "                if name.startswith('._'):\n",
    "                    continue\n",
    "                if 'grand_averages' in name:\n",
    "                    csv_files.add(str(csv_file))\n",
    "    return sorted(csv_files)\n",
    "\n",
    "def get_available_columns(df):\n",
    "    \"\"\"Get available data columns (excluding Time and SEM columns).\"\"\"\n",
    "    # Filter out Time column and SEM columns\n",
    "    data_cols = [col for col in df.columns \n",
    "                 if col != 'Time (s)' and not col.endswith('_SEM')]\n",
    "    return sorted(data_cols)\n",
    "\n",
    "def get_column_label(column_name):\n",
    "    \"\"\"Generate a readable label for a column.\"\"\"\n",
    "    # Map common column names to readable labels\n",
    "    label_map = {\n",
    "        'z_470': 'GRAB-5HT3.0 (z-score)',\n",
    "        'z_470_Baseline': 'GRAB-5HT3.0 (z-score)',\n",
    "        'z_560': 'RGeco1a (z-score)',\n",
    "        'z_560_Baseline': 'RGeco1a (z-score)',\n",
    "        'Velocity_0X': 'Running speed',\n",
    "        'Velocity_0X_Baseline': 'Running speed',\n",
    "        'Motor_Velocity': 'Motor Velocity',\n",
    "        'Motor_Velocity_Baseline': 'Motor Velocity',\n",
    "    }\n",
    "    return label_map.get(column_name, column_name)\n",
    "\n",
    "def get_axis_label(column_name):\n",
    "    \"\"\"Generate axis label based on column name.\"\"\"\n",
    "    if 'z_' in column_name or 'z-score' in column_name.lower():\n",
    "        return 'z-score'\n",
    "    elif 'Velocity' in column_name or 'velocity' in column_name.lower():\n",
    "        return 'Running speed (m/s)'\n",
    "    elif 'Motor' in column_name:\n",
    "        return 'Motor velocity (m/s)'\n",
    "    else:\n",
    "        return column_name\n",
    "\n",
    "def should_use_right_axis(column_name):\n",
    "    \"\"\"Determine if a column should be plotted on the right axis.\"\"\"\n",
    "    # Velocity and Motor columns go on right axis\n",
    "    return 'Velocity' in column_name or 'Motor' in column_name\n",
    "\n",
    "\n",
    "def infer_series_metadata(file_path: str) -> dict:\n",
    "    \"\"\"Extract cohort and experiment day heuristically from a CSV path.\"\"\"\n",
    "    path = Path(file_path)\n",
    "    parts = [p for p in path.parts]\n",
    "\n",
    "    cohort = next((p for p in parts if 'Cohort' in p), 'Unknown')\n",
    "    experiment_day = next((p for p in parts if 'Visual_mismatch' in p or 'Vestibular_mismatch' in p), 'UnknownDay')\n",
    "\n",
    "    # Normalise formatting\n",
    "    cohort_clean = cohort.replace('_', ' ')\n",
    "    day_clean = experiment_day.replace('_', ' ')\n",
    "    return {'cohort': cohort_clean, 'experiment_day': day_clean}\n",
    "\n",
    "\n",
    "def select_series_color(cohort: str) -> str:\n",
    "    \"\"\"Choose a colour based on cohort name.\"\"\"\n",
    "    cohort_lower = cohort.lower()\n",
    "    if 'cohort3' in cohort_lower:\n",
    "        return '#8B0000'  # dark red\n",
    "    if 'cohort1' in cohort_lower:\n",
    "        return '#FF0000'  # red\n",
    "    return 'slategray'\n",
    "\n",
    "\n",
    "def plot_grand_averages_multi_series(series_list):\n",
    "    \"\"\"Plot multiple series from selected grand-average CSV files on a shared figure.\"\"\"\n",
    "    plt.rcParams.update({\n",
    "        'font.size': 9,\n",
    "        'font.family': 'sans-serif',\n",
    "        'font.sans-serif': ['Arial'],\n",
    "        'axes.titlesize': 9,\n",
    "        'axes.labelsize': 9,\n",
    "        'legend.fontsize': 7,\n",
    "        'xtick.labelsize': 8,\n",
    "        'ytick.labelsize': 8\n",
    "    })\n",
    "\n",
    "    if not series_list:\n",
    "        print(\"‚ö†Ô∏è No series selected for plotting.\")\n",
    "        return\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax2 = None\n",
    "\n",
    "    left_axis_series = []\n",
    "    right_axis_series = []\n",
    "\n",
    "    for series in series_list:\n",
    "        file_path = series.get('file')\n",
    "        column = series.get('column')\n",
    "        label = series.get('label')\n",
    "\n",
    "        if not file_path or not column:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            abs_file_path = Path(file_path).resolve()\n",
    "            # Read the CSV file normally - Time (s) is already a column\n",
    "            df = pd.read_csv(abs_file_path)\n",
    "            \n",
    "            # If first column is unnamed (from index), rename it to 'Time (s)'\n",
    "            if len(df.columns) > 0 and df.columns[0].startswith('Unnamed') and 'Time (s)' not in df.columns:\n",
    "                df = df.rename(columns={df.columns[0]: 'Time (s)'})\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error reading CSV {file_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "        if column not in df.columns:\n",
    "            continue\n",
    "\n",
    "        metadata = infer_series_metadata(file_path)\n",
    "        colour = series.get('color') or select_series_color(metadata['cohort'])\n",
    "\n",
    "        label_parts = [get_column_label(column)]\n",
    "        if metadata['cohort'] != 'Unknown':\n",
    "            label_parts.append(metadata['cohort'])\n",
    "        if metadata['experiment_day'] != 'UnknownDay':\n",
    "            label_parts.append(metadata['experiment_day'])\n",
    "        plot_label = label if label else ' | '.join(label_parts)\n",
    "\n",
    "        use_right = should_use_right_axis(column)\n",
    "\n",
    "        if use_right:\n",
    "            if ax2 is None:\n",
    "                ax2 = ax.twinx()\n",
    "            ax_handle = ax2\n",
    "            collection = right_axis_series\n",
    "        else:\n",
    "            ax_handle = ax\n",
    "            collection = left_axis_series\n",
    "\n",
    "        ax_handle.plot(\n",
    "            df['Time (s)'],\n",
    "            df[column],\n",
    "            label=plot_label,\n",
    "            color=colour,\n",
    "            linestyle=series.get('linestyle', '-'),\n",
    "            alpha=1,\n",
    "        )\n",
    "\n",
    "        sem_column = f'{column}_SEM'\n",
    "        if sem_column in df.columns:\n",
    "            ax_handle.fill_between(\n",
    "                df['Time (s)'],\n",
    "                df[column] - df[sem_column],\n",
    "                df[column] + df[sem_column],\n",
    "                color=colour,\n",
    "                alpha=0.1 if not use_right else 0.2,\n",
    "            )\n",
    "\n",
    "        collection.append(series)\n",
    "\n",
    "    if left_axis_series:\n",
    "        first_col = left_axis_series[0]['column']\n",
    "        ax.set_ylabel(get_axis_label(first_col), fontname='Arial', fontsize=9, color='black')\n",
    "        ax.tick_params(axis='y', labelcolor='black')\n",
    "\n",
    "    if right_axis_series and ax2:\n",
    "        first_col = right_axis_series[0]['column']\n",
    "        ax2.set_ylabel(get_axis_label(first_col), fontname='Arial', fontsize=9, color='slategray')\n",
    "        ax2.tick_params(axis='y', labelcolor='slategray')\n",
    "\n",
    "    ax.axvspan(0, 2, color='gray', alpha=0.2, label='Visual mismatch (0-2s)')\n",
    "    ax.set_title('Grand Averages Comparison', fontname='Arial', fontsize=10)\n",
    "    ax.set_xlabel('Time (s)', fontname='Arial', fontsize=9)\n",
    "\n",
    "    lines, labels = ax.get_legend_handles_labels()\n",
    "    if ax2:\n",
    "        lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "        ax.legend(lines + lines2, labels + labels2, loc='upper right',\n",
    "                  prop={'family': 'Arial', 'size': 8})\n",
    "    else:\n",
    "        ax.legend(lines, labels, loc='upper right', prop={'family': 'Arial', 'size': 8})\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    return fig\n",
    "\n",
    "def plot_grand_averages_single(df, label, y1_col=None, y2_col=None):\n",
    "    \"\"\"Plot grand averages from a single CSV file with selected columns.\"\"\"\n",
    "    plt.rcParams.update({\n",
    "        'font.size': 10,\n",
    "        'font.family': 'sans-serif',\n",
    "        'font.sans-serif': ['DejaVu Sans'],\n",
    "        'axes.titlesize': 10,\n",
    "        'axes.labelsize': 10,\n",
    "        'legend.fontsize': 8,\n",
    "        'xtick.labelsize': 10,\n",
    "        'ytick.labelsize': 10\n",
    "    })\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax2 = None\n",
    "    \n",
    "    # Plot y1 column on left axis\n",
    "    if y1_col and y1_col in df.columns:\n",
    "        ax.plot(df['Time (s)'], df[y1_col], label=get_column_label(y1_col), \n",
    "                color='green', alpha=1)\n",
    "        if f'{y1_col}_SEM' in df.columns:\n",
    "            ax.fill_between(df['Time (s)'],\n",
    "                           df[y1_col] - df[f'{y1_col}_SEM'],\n",
    "                           df[y1_col] + df[f'{y1_col}_SEM'],\n",
    "                           color='green', alpha=0.1)\n",
    "        ax.set_ylabel(get_axis_label(y1_col), fontname='DejaVu Sans', fontsize=10, color='black')\n",
    "        ax.tick_params(axis='y', labelcolor='black')\n",
    "    \n",
    "    # Plot y2 column on right axis (if different from y1)\n",
    "    if y2_col and y2_col in df.columns and y2_col != y1_col:\n",
    "        ax2 = ax.twinx()\n",
    "        ax2.plot(df['Time (s)'], df[y2_col], label=get_column_label(y2_col), \n",
    "                color='slategray', alpha=1)\n",
    "        if f'{y2_col}_SEM' in df.columns:\n",
    "            ax2.fill_between(df['Time (s)'],\n",
    "                            df[y2_col] - df[f'{y2_col}_SEM'],\n",
    "                            df[y2_col] + df[f'{y2_col}_SEM'],\n",
    "                            color='slategray', alpha=0.2)\n",
    "        ax2.set_ylabel(get_axis_label(y2_col), fontname='DejaVu Sans', fontsize=10, color='slategray')\n",
    "        ax2.tick_params(axis='y', labelcolor='slategray')\n",
    "    \n",
    "    ax.axvspan(0, 2, color='gray', alpha=0.2, label='Visual mismatch (0-2s)')\n",
    "    ax.set_title(f'Grand Averages: {label}', fontname='DejaVu Sans', fontsize=12)\n",
    "    ax.set_xlabel('Time (s)', fontname='DejaVu Sans', fontsize=10)\n",
    "    \n",
    "    # Combine legends\n",
    "    lines, labels = ax.get_legend_handles_labels()\n",
    "    if ax2:\n",
    "        lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "        ax.legend(lines + lines2, labels + labels2, loc='upper right', \n",
    "                 prop={'family': 'DejaVu Sans', 'size': 10})\n",
    "    else:\n",
    "        ax.legend(lines, labels, loc='upper right', prop={'family': 'DejaVu Sans', 'size': 10})\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_grand_averages_comparison(df1, df2, label1, label2, y1_col=None, y2_col=None):\n",
    "    \"\"\"Plot grand averages comparing two CSV files with selected columns.\"\"\"\n",
    "    plt.rcParams.update({\n",
    "        'font.size': 10,\n",
    "        'font.family': 'sans-serif',\n",
    "        'font.sans-serif': ['DejaVu Sans'],\n",
    "        'axes.titlesize': 10,\n",
    "        'axes.labelsize': 10,\n",
    "        'legend.fontsize': 8,\n",
    "        'xtick.labelsize': 10,\n",
    "        'ytick.labelsize': 10\n",
    "    })\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax2 = None\n",
    "    \n",
    "    # Plot y1 column on left axis\n",
    "    if y1_col:\n",
    "        if y1_col in df1.columns:\n",
    "            ax.plot(df1['Time (s)'], df1[y1_col], label=f\"{get_column_label(y1_col)} ({label1})\", \n",
    "                    color='green', alpha=1)\n",
    "            if f'{y1_col}_SEM' in df1.columns:\n",
    "                ax.fill_between(df1['Time (s)'],\n",
    "                               df1[y1_col] - df1[f'{y1_col}_SEM'],\n",
    "                               df1[y1_col] + df1[f'{y1_col}_SEM'],\n",
    "                               color='green', alpha=0.1)\n",
    "        \n",
    "        if y1_col in df2.columns:\n",
    "            ax.plot(df2['Time (s)'], df2[y1_col], label=f\"{get_column_label(y1_col)} ({label2})\", \n",
    "                    color='orange', alpha=1, linestyle='--')\n",
    "            if f'{y1_col}_SEM' in df2.columns:\n",
    "                ax.fill_between(df2['Time (s)'],\n",
    "                               df2[y1_col] - df2[f'{y1_col}_SEM'],\n",
    "                               df2[y1_col] + df2[f'{y1_col}_SEM'],\n",
    "                               color='orange', alpha=0.1)\n",
    "        \n",
    "        ax.set_ylabel(get_axis_label(y1_col), fontname='DejaVu Sans', fontsize=10, color='black')\n",
    "        ax.tick_params(axis='y', labelcolor='black')\n",
    "    \n",
    "    # Plot y2 column on right axis (if different from y1)\n",
    "    if y2_col and y2_col != y1_col:\n",
    "        ax2 = ax.twinx()\n",
    "        \n",
    "        if y2_col in df1.columns:\n",
    "            ax2.plot(df1['Time (s)'], df1[y2_col], label=f\"{get_column_label(y2_col)} ({label1})\", \n",
    "                    color='slategray', alpha=1)\n",
    "            if f'{y2_col}_SEM' in df1.columns:\n",
    "                ax2.fill_between(df1['Time (s)'],\n",
    "                                df1[y2_col] - df1[f'{y2_col}_SEM'],\n",
    "                                df1[y2_col] + df1[f'{y2_col}_SEM'],\n",
    "                                color='slategray', alpha=0.2)\n",
    "        \n",
    "        if y2_col in df2.columns:\n",
    "            ax2.plot(df2['Time (s)'], df2[y2_col], label=f\"{get_column_label(y2_col)} ({label2})\", \n",
    "                    color='slategray', alpha=1, linestyle='--')\n",
    "            if f'{y2_col}_SEM' in df2.columns:\n",
    "                ax2.fill_between(df2['Time (s)'],\n",
    "                                df2[y2_col] - df2[f'{y2_col}_SEM'],\n",
    "                                df2[y2_col] + df2[f'{y2_col}_SEM'],\n",
    "                                color='slategray', alpha=0.2)\n",
    "        \n",
    "        ax2.set_ylabel(get_axis_label(y2_col), fontname='DejaVu Sans', fontsize=10, color='slategray')\n",
    "        ax2.tick_params(axis='y', labelcolor='slategray')\n",
    "    \n",
    "    ax.axvspan(0, 2, color='gray', alpha=0.2, label='Visual mismatch (0-2s)')\n",
    "    ax.set_title('Grand Averages with SEM', fontname='DejaVu Sans', fontsize=12)\n",
    "    ax.set_xlabel('Time (s)', fontname='DejaVu Sans', fontsize=10)\n",
    "    \n",
    "    # Combine legends\n",
    "    lines, labels = ax.get_legend_handles_labels()\n",
    "    if ax2:\n",
    "        lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "        ax.legend(lines + lines2, labels + labels2, loc='upper right', \n",
    "                 prop={'family': 'DejaVu Sans', 'size': 10})\n",
    "    else:\n",
    "        ax.legend(lines, labels, loc='upper right', prop={'family': 'DejaVu Sans', 'size': 10})\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_grand_averages_interactive():\n",
    "    \"\"\"Create interactive plot with dropdowns for multiple CSV file and column selection.\"\"\"\n",
    "    \n",
    "    # Find available CSV files using the same paths as DATA_DIRS\n",
    "    base_paths = DATA_DIRS if DATA_DIRS else []\n",
    "    \n",
    "    csv_files = find_csv_files(base_paths)\n",
    "    \n",
    "    if not csv_files:\n",
    "        return\n",
    "    \n",
    "    if WIDGETS_AVAILABLE:\n",
    "        # Store series widgets\n",
    "        series_widgets = []\n",
    "        series_container = widgets.VBox([])\n",
    "        \n",
    "        # Store current figure for saving\n",
    "        current_fig = [None]  # Use list to allow modification in nested functions\n",
    "        \n",
    "        # Color options for different series\n",
    "        color_options = ['green', 'blue', 'red', 'orange', 'purple', 'brown', 'pink', 'gray', 'olive', 'cyan']\n",
    "        linestyle_options = ['-', '--', '-.', ':']\n",
    "        \n",
    "        def create_series_row(series_id):\n",
    "            \"\"\"Create a row of widgets for selecting file and column.\"\"\"\n",
    "            file_dropdown = widgets.Dropdown(\n",
    "                options=['None'] + csv_files,\n",
    "                value='None',\n",
    "                description=f'File {series_id}:',\n",
    "                style={'description_width': 'initial'},\n",
    "                layout=widgets.Layout(width='400px')\n",
    "            )\n",
    "            \n",
    "            col_dropdown = widgets.Dropdown(\n",
    "                options=[''],\n",
    "                value='',\n",
    "                description='Column:',\n",
    "                style={'description_width': 'initial'},\n",
    "                layout=widgets.Layout(width='250px')\n",
    "            )\n",
    "            \n",
    "            remove_btn = widgets.Button(\n",
    "                description='Remove',\n",
    "                button_style='danger',\n",
    "                layout=widgets.Layout(width='80px', height='30px')\n",
    "            )\n",
    "            \n",
    "            def update_columns(change):\n",
    "                \"\"\"Update column dropdown when file changes.\"\"\"\n",
    "                if change['new'] and change['new'] != 'None':\n",
    "                    try:\n",
    "                        # Clear the current column value first\n",
    "                        col_dropdown.value = ''\n",
    "                        \n",
    "                        # Read the CSV file normally - Time (s) is already a column\n",
    "                        df = pd.read_csv(change['new'])\n",
    "                        \n",
    "                        # If first column is unnamed (from index), rename it to 'Time (s)'\n",
    "                        if len(df.columns) > 0 and df.columns[0].startswith('Unnamed') and 'Time (s)' not in df.columns:\n",
    "                            df = df.rename(columns={df.columns[0]: 'Time (s)'})\n",
    "                        \n",
    "                        # Debug: print all columns found\n",
    "                        print(f\"üìã All columns in CSV: {list(df.columns)}\")\n",
    "                        available_cols = get_available_columns(df)\n",
    "                        print(f\"üìä Available columns (after filtering): {available_cols}\")\n",
    "                        \n",
    "                        # Update options and set to first available column\n",
    "                        col_dropdown.options = available_cols\n",
    "                        if available_cols:\n",
    "                            col_dropdown.value = available_cols[0]\n",
    "                        else:\n",
    "                            col_dropdown.value = ''\n",
    "                    except Exception:\n",
    "                        col_dropdown.options = ['']\n",
    "                        col_dropdown.value = ''\n",
    "                else:\n",
    "                    col_dropdown.options = ['']\n",
    "                    col_dropdown.value = ''\n",
    "            \n",
    "            def remove_series(b):\n",
    "                \"\"\"Remove this series row.\"\"\"\n",
    "                for i, (f, c, r, _) in enumerate(series_widgets):\n",
    "                    if f == file_dropdown:\n",
    "                        series_widgets.pop(i)\n",
    "                        update_series_display()\n",
    "                        break\n",
    "            \n",
    "            file_dropdown.observe(update_columns, names='value')\n",
    "            remove_btn.on_click(remove_series)\n",
    "            \n",
    "            return file_dropdown, col_dropdown, remove_btn, widgets.HBox([file_dropdown, col_dropdown, remove_btn])\n",
    "        \n",
    "        def update_series_display():\n",
    "            \"\"\"Update the display of all series rows.\"\"\"\n",
    "            children = [row[3] for row in series_widgets]\n",
    "            series_container.children = children\n",
    "        \n",
    "        def add_series(b):\n",
    "            \"\"\"Add a new series row.\"\"\"\n",
    "            series_id = len(series_widgets) + 1\n",
    "            widgets_row = create_series_row(series_id)\n",
    "            series_widgets.append(widgets_row)\n",
    "            update_series_display()\n",
    "        \n",
    "        def plot_all_series(b):\n",
    "            \"\"\"Plot all selected series.\"\"\"\n",
    "            with output:\n",
    "                clear_output(wait=True)\n",
    "                series_list = []\n",
    "                seen = set()\n",
    "\n",
    "                for file_dropdown, col_dropdown, _, _ in series_widgets:\n",
    "                    file_path = file_dropdown.value\n",
    "                    column = col_dropdown.value\n",
    "\n",
    "                    key = (file_path, column)\n",
    "                    if (\n",
    "                        file_path\n",
    "                        and file_path != 'None'\n",
    "                        and column\n",
    "                        and column != ''\n",
    "                        and key not in seen\n",
    "                    ):\n",
    "                        seen.add(key)\n",
    "                        series_list.append({\n",
    "                            'file': file_path,\n",
    "                            'column': column,\n",
    "                            'linestyle': linestyle_options[(len(series_list) // len(color_options)) % len(linestyle_options)]\n",
    "                        })\n",
    "\n",
    "                if not series_list:\n",
    "                    return\n",
    "\n",
    "                try:\n",
    "                    fig = plot_grand_averages_multi_series(series_list)\n",
    "                    current_fig[0] = fig  # Store figure for saving\n",
    "                    if fig is not None:\n",
    "                        display(fig)\n",
    "                        plt.close(fig)\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "        def save_current_plot(b):\n",
    "            \"\"\"Save the current plot to a file.\"\"\"\n",
    "            if current_fig[0] is None:\n",
    "                return\n",
    "\n",
    "            # Generate filename based on selected series\n",
    "            series_names = []\n",
    "            first_file_path = None\n",
    "\n",
    "            for file_dropdown, col_dropdown, _, _ in series_widgets:\n",
    "                file_path = file_dropdown.value\n",
    "                column = col_dropdown.value\n",
    "                if file_path and file_path != 'None' and column and column != '':\n",
    "                    if first_file_path is None:\n",
    "                        first_file_path = file_path\n",
    "                    file_label = Path(file_path).parent.name\n",
    "                    series_names.append(f\"{file_label}_{column}\")\n",
    "\n",
    "            if series_names:\n",
    "                filename_base = \"_vs_\".join(series_names[:3])  # Limit filename length\n",
    "                if len(series_names) > 3:\n",
    "                    filename_base += f\"_and_{len(series_names)-3}_more\"\n",
    "            else:\n",
    "                filename_base = \"grand_averages_plot\"\n",
    "\n",
    "            # Get save directory from the first CSV file being plotted\n",
    "            if first_file_path:\n",
    "                save_dir = Path(first_file_path).parent\n",
    "            else:\n",
    "                save_dir = Path(DATA_DIRS[0]) if DATA_DIRS else Path.cwd()\n",
    "\n",
    "            # Save as PDF and PNG\n",
    "            pdf_path = save_dir / f\"{filename_base}.pdf\"\n",
    "            png_path = save_dir / f\"{filename_base}.png\"\n",
    "\n",
    "            try:\n",
    "                current_fig[0].savefig(pdf_path, dpi=300, bbox_inches='tight')\n",
    "                current_fig[0].savefig(png_path, dpi=300, bbox_inches='tight')\n",
    "            except Exception:\n",
    "                pass\n",
    "        \n",
    "        # Create initial series row\n",
    "        add_series(None)\n",
    "        \n",
    "        # Buttons\n",
    "        add_btn = widgets.Button(\n",
    "            description='+ Add Series',\n",
    "            button_style='info',\n",
    "            layout=widgets.Layout(width='150px')\n",
    "        )\n",
    "        add_btn.on_click(add_series)\n",
    "        \n",
    "        plot_button = widgets.Button(\n",
    "            description='Plot All Series',\n",
    "            button_style='success',\n",
    "            layout=widgets.Layout(width='200px')\n",
    "        )\n",
    "\n",
    "        def plot_and_show(_):\n",
    "            plot_all_series(_)\n",
    "\n",
    "        plot_button.on_click(plot_and_show)\n",
    "        \n",
    "        save_button = widgets.Button(\n",
    "            description='Save Plot',\n",
    "            button_style='warning',\n",
    "            layout=widgets.Layout(width='150px')\n",
    "        )\n",
    "        save_button.on_click(save_current_plot)\n",
    "        \n",
    "        output = widgets.Output()\n",
    "        \n",
    "        display(widgets.VBox([\n",
    "            widgets.HTML(\"<h3>Select multiple files and columns to plot:</h3>\"),\n",
    "            widgets.HTML(\"<p><i>Add multiple series to compare columns from different files on the same graph.</i></p>\"),\n",
    "            series_container,\n",
    "            widgets.HBox([add_btn, plot_button, save_button]),\n",
    "            output\n",
    "        ]))\n",
    "    else:\n",
    "        return\n",
    "\n",
    "# Run interactive plotting\n",
    "plot_grand_averages_interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a93f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIGNAL METRIC ANALYSIS UTILITIES\n",
    "#---------------------------------------------------------------------------------------------------#\n",
    "\n",
    "def _estimate_baseline(times: np.ndarray, values: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Estimate the baseline from late post-stimulus period.\n",
    "    \n",
    "    Uses the median of the last 2 seconds of data to robustly estimate baseline.\n",
    "    Falls back to pre-stimulus period if late data is unavailable.\n",
    "    \"\"\"\n",
    "    if times.size == 0 or values.size == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # Try late post-stimulus period (last 2 seconds of data)\n",
    "    time_range = times[-1] - times[0]\n",
    "    if time_range >= 2.0:\n",
    "        late_mask = times >= (times[-1] - 2.0)\n",
    "        if late_mask.any():\n",
    "            late_values = values[late_mask]\n",
    "            late_values = late_values[np.isfinite(late_values)]\n",
    "            if late_values.size > 0:\n",
    "                return float(np.median(late_values))\n",
    "    \n",
    "    # Fallback: use pre-stimulus period if available (times < 0)\n",
    "    pre_mask = times < 0.0\n",
    "    if pre_mask.any():\n",
    "        pre_values = values[pre_mask]\n",
    "        pre_values = pre_values[np.isfinite(pre_values)]\n",
    "        if pre_values.size > 0:\n",
    "            return float(np.median(pre_values))\n",
    "    \n",
    "    # Last resort: use median of first 10% of data\n",
    "    n_baseline = max(3, int(0.1 * times.size))\n",
    "    baseline_values = values[:n_baseline]\n",
    "    baseline_values = baseline_values[np.isfinite(baseline_values)]\n",
    "    if baseline_values.size > 0:\n",
    "        return float(np.median(baseline_values))\n",
    "    \n",
    "    return 0.0\n",
    "\n",
    "\n",
    "def _double_exponential_model(t: np.ndarray, A1: float, tau1: float, A2: float, tau2: float) -> np.ndarray:\n",
    "    \"\"\"Double exponential decay model (without baseline, added separately).\"\"\"\n",
    "    return A1 * np.exp(-t / tau1) + A2 * np.exp(-t / tau2)\n",
    "\n",
    "\n",
    "def _estimate_double_exponential_decay(\n",
    "    times: np.ndarray,\n",
    "    values: np.ndarray,\n",
    "    peak_index: int,\n",
    "    baseline: float,\n",
    "    fit_start_offset: float = 0.0,\n",
    ") -> Tuple[float, float, float, float]:\n",
    "    \"\"\"\n",
    "    Fit a double-exponential decay with baseline:\n",
    "    y(t) = baseline + A1*exp(-t/tau1) + A2*exp(-t/tau2)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    times : np.ndarray\n",
    "        Time points\n",
    "    values : np.ndarray\n",
    "        Signal values\n",
    "    peak_index : int\n",
    "        Index of the peak in the arrays\n",
    "    baseline : float\n",
    "        Baseline level\n",
    "    fit_start_offset : float\n",
    "        Additional time (seconds) to add to peak time before starting fit (default: 0.0)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[float, float, float, float]\n",
    "        (tau1, A1, tau2, A2) or (nan, nan, nan, nan) if fitting fails\n",
    "    \"\"\"\n",
    "    if times.size == 0 or values.size == 0:\n",
    "        return float(np.nan), float(np.nan), float(np.nan), float(np.nan)\n",
    "    \n",
    "    peak_value = values[peak_index]\n",
    "    if np.isnan(peak_value):\n",
    "        return float(np.nan), float(np.nan), float(np.nan), float(np.nan)\n",
    "    \n",
    "    # Use post-peak data for fitting, optionally offset from peak time\n",
    "    peak_time = times[peak_index]\n",
    "    fit_start_time = peak_time + fit_start_offset\n",
    "    \n",
    "    # Find data points at or after the fit start time\n",
    "    fit_mask = times >= fit_start_time\n",
    "    if not fit_mask.any():\n",
    "        return float(np.nan), float(np.nan), float(np.nan), float(np.nan)\n",
    "    \n",
    "    post_times = times[fit_mask]\n",
    "    post_values = values[fit_mask]\n",
    "    \n",
    "    if post_times.size < 6:  # Need at least 6 points for 4-parameter fit\n",
    "        return float(np.nan), float(np.nan), float(np.nan), float(np.nan)\n",
    "    \n",
    "    # Subtract baseline\n",
    "    baseline_corrected = post_values - baseline\n",
    "    total_amplitude = peak_value - baseline\n",
    "    \n",
    "    if total_amplitude <= 0:\n",
    "        return float(np.nan), float(np.nan), float(np.nan), float(np.nan)\n",
    "    \n",
    "    # Relative times from fit start time\n",
    "    rel_times = post_times - fit_start_time\n",
    "    \n",
    "    # Initial parameter guesses\n",
    "    # Fast component: ~70% of amplitude, tau ~0.3-0.5s\n",
    "    # Slow component: ~30% of amplitude, tau ~2-4s\n",
    "    A1_guess = 0.7 * total_amplitude\n",
    "    tau1_guess = 0.4\n",
    "    A2_guess = 0.3 * total_amplitude\n",
    "    tau2_guess = 2.5\n",
    "    \n",
    "    p0 = [A1_guess, tau1_guess, A2_guess, tau2_guess]\n",
    "    \n",
    "    # Bounds: amplitudes positive, tau1 < tau2 (fast < slow)\n",
    "    bounds_lower = [0.0, 0.05, 0.0, 0.5]  # tau1 > 0.05s, tau2 > 0.5s\n",
    "    bounds_upper = [total_amplitude * 2, 2.0, total_amplitude * 2, 10.0]  # tau1 < 2s, tau2 < 10s\n",
    "    \n",
    "    try:\n",
    "        params, _ = curve_fit(\n",
    "            _double_exponential_model,\n",
    "            rel_times,\n",
    "            baseline_corrected,\n",
    "            p0=p0,\n",
    "            bounds=(bounds_lower, bounds_upper),\n",
    "            maxfev=5000,\n",
    "        )\n",
    "        A1, tau1, A2, tau2 = params\n",
    "        \n",
    "        # Ensure tau1 < tau2 (fast < slow)\n",
    "        if tau1 > tau2:\n",
    "            tau1, tau2 = tau2, tau1\n",
    "            A1, A2 = A2, A1\n",
    "        \n",
    "        return float(tau1), float(A1), float(tau2), float(A2)\n",
    "    \n",
    "    except (RuntimeError, ValueError):\n",
    "        # If double-exponential fails, return NaN\n",
    "        return float(np.nan), float(np.nan), float(np.nan), float(np.nan)\n",
    "\n",
    "\n",
    "def _estimate_decay_tau_with_baseline(\n",
    "    times: np.ndarray, \n",
    "    values: np.ndarray, \n",
    "    peak_index: int,\n",
    "    baseline: float,\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Fit a single exponential decay with baseline: y(t) = baseline + amplitude * exp(-t/tau)\n",
    "    \n",
    "    This is kept for backward compatibility but is now deprecated in favor of\n",
    "    _estimate_double_exponential_decay.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[float, float]\n",
    "        (tau, amplitude) or (nan, nan) if fitting fails\n",
    "    \"\"\"\n",
    "    if times.size == 0 or values.size == 0:\n",
    "        return float(np.nan), float(np.nan)\n",
    "\n",
    "    peak_value = values[peak_index]\n",
    "    if np.isnan(peak_value):\n",
    "        return float(np.nan), float(np.nan)\n",
    "\n",
    "    post_times = times[peak_index:]\n",
    "    post_values = values[peak_index:]\n",
    "    if post_times.size < 3:\n",
    "        return float(np.nan), float(np.nan)\n",
    "\n",
    "    # Subtract baseline to get amplitude decay\n",
    "    baseline_corrected = post_values - baseline\n",
    "    \n",
    "    # Initial amplitude is peak minus baseline\n",
    "    initial_amplitude = peak_value - baseline\n",
    "    if initial_amplitude <= 0:\n",
    "        return float(np.nan), float(np.nan)\n",
    "\n",
    "    rel_times = post_times - post_times[0]\n",
    "    normalized = baseline_corrected / initial_amplitude\n",
    "\n",
    "    # Only use positive normalized values for log fitting\n",
    "    mask = (\n",
    "        np.isfinite(rel_times)\n",
    "        & np.isfinite(normalized)\n",
    "        & (rel_times >= 0.0)\n",
    "        & (normalized > 0.0)\n",
    "    )\n",
    "    mask[0] = False  # exclude the peak itself to avoid log(1)=0 dominating\n",
    "\n",
    "    if mask.sum() < 2:\n",
    "        return float(np.nan), float(np.nan)\n",
    "\n",
    "    try:\n",
    "        slope, intercept = np.polyfit(rel_times[mask], np.log(normalized[mask]), 1)\n",
    "    except np.linalg.LinAlgError:\n",
    "        return float(np.nan), float(np.nan)\n",
    "\n",
    "    if slope >= 0:\n",
    "        return float(np.nan), float(np.nan)\n",
    "\n",
    "    tau = float(-1.0 / slope)\n",
    "    # Recalculate amplitude from intercept: log(normalized) = log(amp_factor) + slope*t\n",
    "    amplitude = float(initial_amplitude * np.exp(intercept))\n",
    "    \n",
    "    return tau, amplitude\n",
    "\n",
    "\n",
    "def _estimate_decay_tau(times: np.ndarray, values: np.ndarray, peak_index: int) -> float:\n",
    "    \"\"\"\n",
    "    Legacy function for backward compatibility.\n",
    "    Estimates tau assuming baseline=0 (for main peak analysis in 0-2s window).\n",
    "    \"\"\"\n",
    "    if times.size == 0 or values.size == 0:\n",
    "        return float(np.nan)\n",
    "\n",
    "    peak_value = values[peak_index]\n",
    "    if np.isnan(peak_value) or peak_value == 0.0:\n",
    "        return float(np.nan)\n",
    "\n",
    "    post_times = times[peak_index:]\n",
    "    post_values = values[peak_index:]\n",
    "    if post_times.size < 3:\n",
    "        return float(np.nan)\n",
    "\n",
    "    rel_times = post_times - post_times[0]\n",
    "    normalized = post_values / peak_value\n",
    "\n",
    "    mask = (\n",
    "        np.isfinite(rel_times)\n",
    "        & np.isfinite(normalized)\n",
    "        & (rel_times >= 0.0)\n",
    "        & (normalized > 0.0)\n",
    "    )\n",
    "    mask[0] = False  # exclude the peak itself to avoid log(1)=0 dominating\n",
    "\n",
    "    if mask.sum() < 2:\n",
    "        return float(np.nan)\n",
    "\n",
    "    try:\n",
    "        slope, intercept = np.polyfit(rel_times[mask], np.log(normalized[mask]), 1)\n",
    "    except np.linalg.LinAlgError:\n",
    "        return float(np.nan)\n",
    "\n",
    "    if slope >= 0:\n",
    "        return float(np.nan)\n",
    "\n",
    "    return float(-1.0 / slope)\n",
    "\n",
    "\n",
    "def _compute_offset_residual_metrics(\n",
    "    times: np.ndarray,\n",
    "    values: np.ndarray,\n",
    "    peak_index: int,\n",
    "    peak_value: float,\n",
    "    tau1: float,\n",
    "    A1: float,\n",
    "    tau2: float,\n",
    "    A2: float,\n",
    "    baseline: float,\n",
    "    offset_start: float,\n",
    "    offset_end: float,\n",
    "    auc_window: Tuple[float, float],\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Strategy 3: Compute offset response metrics using double-exponential residual analysis.\n",
    "    \n",
    "    This function implements double-exponential decay subtraction to detect offset responses\n",
    "    (late peaks or shoulders) that occur after the main calcium transient.\n",
    "    \n",
    "    Algorithm:\n",
    "    1. Reconstruct expected double-exponential decay from peak using fitted parameters\n",
    "    2. Subtract fitted decay from observed trace to get residuals\n",
    "    3. Compute area under curve for positive residuals only (removes negative fluctuations)\n",
    "    \n",
    "    Model: y(t) = baseline + A1*exp(-(t-t_peak)/tau1) + A2*exp(-(t-t_peak)/tau2)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    times : np.ndarray\n",
    "        Time points (relative to alignment)\n",
    "    values : np.ndarray\n",
    "        Signal values (z-scores)\n",
    "    peak_index : int\n",
    "        Index of the main peak\n",
    "    peak_value : float\n",
    "        Amplitude of the main peak\n",
    "    tau1 : float\n",
    "        Fast decay time constant (seconds)\n",
    "    A1 : float\n",
    "        Fast component amplitude\n",
    "    tau2 : float\n",
    "        Slow decay time constant (seconds)\n",
    "    A2 : float\n",
    "        Slow component amplitude\n",
    "    baseline : float\n",
    "        Baseline level (pre-stimulus)\n",
    "    offset_start : float\n",
    "        Start time for offset analysis (typically 2.0s)\n",
    "    offset_end : float\n",
    "        End time for offset analysis (typically 8.0s)\n",
    "    auc_window : Tuple[float, float]\n",
    "        Time window for AUC calculation (typically 2.0-3.0s)\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, float]\n",
    "        Dictionary with keys:\n",
    "        - 'residual_auc': Area under positive residuals\n",
    "    \"\"\"\n",
    "    # Initialize return values\n",
    "    result = {\n",
    "        \"residual_auc\": np.nan,\n",
    "    }\n",
    "    \n",
    "    # Check if double-exponential parameters are valid\n",
    "    if not np.isfinite(tau1) or tau1 <= 0:\n",
    "        return result\n",
    "    if not np.isfinite(A1):\n",
    "        return result\n",
    "    \n",
    "    # Restrict to offset analysis window\n",
    "    offset_mask = (times >= offset_start) & (times <= offset_end)\n",
    "    if not offset_mask.any():\n",
    "        return result\n",
    "    \n",
    "    offset_times = times[offset_mask]\n",
    "    offset_values = values[offset_mask]\n",
    "    \n",
    "    # Reconstruct baseline-corrected double-exponential decay:\n",
    "    # y(t) = baseline + A1*exp(-(t - t_peak)/tau1) + A2*exp(-(t - t_peak)/tau2)\n",
    "    peak_time = times[peak_index]\n",
    "    time_from_peak = offset_times - peak_time\n",
    "    expected_decay = baseline + A1 * np.exp(-time_from_peak / tau1) + A2 * np.exp(-time_from_peak / tau2)\n",
    "    \n",
    "    # Compute residuals (observed - expected)\n",
    "    residuals = offset_values - expected_decay\n",
    "    \n",
    "    # Compute area under curve for positive residuals only\n",
    "    auc_mask = (offset_times >= auc_window[0]) & (offset_times <= auc_window[1])\n",
    "    if auc_mask.any():\n",
    "        auc_times = offset_times[auc_mask]\n",
    "        auc_residuals = residuals[auc_mask]\n",
    "        \n",
    "        # Keep only positive residuals\n",
    "        positive_residuals = np.maximum(auc_residuals, 0.0)\n",
    "        \n",
    "        # Integrate using trapezoidal rule\n",
    "        if positive_residuals.size > 1:\n",
    "            if np.any(positive_residuals > 0):\n",
    "                auc = float(np.trapz(positive_residuals, auc_times))\n",
    "                result[\"residual_auc\"] = auc\n",
    "            else:\n",
    "                # No positive residuals found - set to 0 instead of NaN\n",
    "                result[\"residual_auc\"] = 0.0\n",
    "        else:\n",
    "            # Not enough points for integration - set to 0\n",
    "            result[\"residual_auc\"] = 0.0\n",
    "    else:\n",
    "        # No data in AUC window - set to 0\n",
    "        result[\"residual_auc\"] = 0.0\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def _detect_offset_peak_direct(\n",
    "    times: np.ndarray,\n",
    "    values: np.ndarray,\n",
    "    baseline_window: Tuple[float, float],\n",
    "    detection_start: float,\n",
    "    detection_end: float,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Strategy 1: Direct peak detection with baseline subtraction.\n",
    "    \n",
    "    This is a fallback method when exponential fitting fails. It detects\n",
    "    the maximum value after detection_start and subtracts a baseline.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    times : np.ndarray\n",
    "        Time points\n",
    "    values : np.ndarray\n",
    "        Signal values\n",
    "    baseline_window : Tuple[float, float]\n",
    "        Time window for baseline calculation (e.g., 1.9-2.0s)\n",
    "    detection_start : float\n",
    "        Start time for peak detection\n",
    "    detection_end : float\n",
    "        End time for peak detection\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Peak amplitude relative to baseline (NaN if detection fails)\n",
    "    \"\"\"\n",
    "    # Compute baseline\n",
    "    baseline_mask = (times >= baseline_window[0]) & (times <= baseline_window[1])\n",
    "    if not baseline_mask.any():\n",
    "        return np.nan\n",
    "    baseline_value = float(np.nanmean(values[baseline_mask]))\n",
    "    \n",
    "    # Find peak in detection window\n",
    "    detection_mask = (times >= detection_start) & (times <= detection_end)\n",
    "    if not detection_mask.any():\n",
    "        return np.nan\n",
    "    \n",
    "    detection_values = values[detection_mask]\n",
    "    peak_value = float(np.nanmax(detection_values))\n",
    "    \n",
    "    # Return amplitude above baseline\n",
    "    return peak_value - baseline_value\n",
    "\n",
    "\n",
    "def _logistic_rise_model(t: np.ndarray, baseline: float, amplitude: float, t_half: float, tau: float) -> np.ndarray:\n",
    "    return baseline + amplitude / (1.0 + np.exp(-(t - t_half) / tau))\n",
    "\n",
    "\n",
    "def _estimate_logistic_onset(times: np.ndarray, values: np.ndarray) -> Optional[float]:\n",
    "    if times.size < 6:\n",
    "        return None\n",
    "\n",
    "    sorted_idx = np.argsort(times)\n",
    "    times_sorted = times[sorted_idx]\n",
    "    values_sorted = values[sorted_idx]\n",
    "\n",
    "    amplitude_guess = float(np.nanmax(values_sorted) - np.nanmin(values_sorted))\n",
    "    baseline_guess = float(np.nanpercentile(values_sorted, 5))\n",
    "    amplitude_guess = float(np.nanmax(values_sorted) - baseline_guess)\n",
    "    if not np.isfinite(amplitude_guess) or amplitude_guess <= 0:\n",
    "        return None\n",
    "\n",
    "    above_half = np.where(values_sorted >= baseline_guess + 0.5 * amplitude_guess)[0]\n",
    "    if above_half.size:\n",
    "        t_half_guess = float(times_sorted[above_half[0]])\n",
    "    else:\n",
    "        t_half_guess = float(times_sorted[len(times_sorted) // 2])\n",
    "\n",
    "    tau_guess = max((times_sorted[-1] - times_sorted[0]) / 4.0, 1e-3)\n",
    "\n",
    "    lower_bounds = [-np.inf, 0.0, times_sorted[0] - 1.0, 1e-3]\n",
    "    upper_bounds = [np.inf, amplitude_guess * 10.0, times_sorted[-1] + 1.0, max((times_sorted[-1] - times_sorted[0]) * 2.0, 1e-2)]\n",
    "\n",
    "    try:\n",
    "        params, _ = curve_fit(\n",
    "            _logistic_rise_model,\n",
    "            times_sorted,\n",
    "            values_sorted,\n",
    "            p0=[baseline_guess, amplitude_guess, t_half_guess, tau_guess],\n",
    "            bounds=(lower_bounds, upper_bounds),\n",
    "            maxfev=10000,\n",
    "        )\n",
    "    except (RuntimeError, ValueError):\n",
    "        return None\n",
    "\n",
    "    baseline_fit, amplitude_fit, t_half_fit, tau_fit = params\n",
    "    if amplitude_fit <= 0 or tau_fit <= 0:\n",
    "        return None\n",
    "\n",
    "    onset_time = float(t_half_fit - tau_fit * np.log(9.0))\n",
    "    if not np.isfinite(onset_time):\n",
    "        return None\n",
    "\n",
    "    if onset_time < times_sorted[0] - 0.5:\n",
    "        onset_time = times_sorted[0]\n",
    "    if onset_time > times_sorted[-1]:\n",
    "        onset_time = times_sorted[-1]\n",
    "\n",
    "    return onset_time\n",
    "\n",
    "\n",
    "def _prepare_mouse_signal_frame(mouse_df: pd.DataFrame, signal_column: str) -> pd.DataFrame:\n",
    "    \"\"\"Return tidy DataFrame with time/value pairs for a given signal.\"\"\"\n",
    "    if signal_column not in mouse_df.columns:\n",
    "        return pd.DataFrame(columns=[\"time\", \"value\"])\n",
    "\n",
    "    times = pd.to_numeric(mouse_df.index, errors=\"coerce\")\n",
    "    values = pd.to_numeric(mouse_df[signal_column], errors=\"coerce\")\n",
    "    df_signal = pd.DataFrame({\"time\": times, \"value\": values}).dropna()\n",
    "\n",
    "    if df_signal.empty:\n",
    "        return df_signal\n",
    "\n",
    "    return df_signal.sort_values(\"time\")\n",
    "\n",
    "\n",
    "def _compute_signal_metrics_for_mouse(\n",
    "    mouse_df: pd.DataFrame,\n",
    "    signal_column: str,\n",
    "    window: Tuple[float, float],\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Compute peak, logistic 10% onset time, and decay tau for a mouse signal.\n",
    "    \n",
    "    Note: window should extend through offset analysis period (e.g., 0-8s),\n",
    "    but main peak detection is restricted to the early period (0-2s).\n",
    "    \"\"\"\n",
    "    df_signal = _prepare_mouse_signal_frame(mouse_df, signal_column)\n",
    "\n",
    "    if df_signal.empty:\n",
    "        return {\"peak\": np.nan, \"onset_time\": np.nan, \"decay_tau1\": np.nan}\n",
    "\n",
    "    window_mask = (df_signal[\"time\"] >= window[0]) & (df_signal[\"time\"] <= window[1])\n",
    "    window_df = df_signal.loc[window_mask]\n",
    "\n",
    "    if window_df.empty:\n",
    "        return {\"peak\": np.nan, \"onset_time\": np.nan, \"decay_tau1\": np.nan}\n",
    "\n",
    "    times = window_df[\"time\"].to_numpy(dtype=float)\n",
    "    values = window_df[\"value\"].to_numpy(dtype=float)\n",
    "\n",
    "    finite_mask = np.isfinite(times) & np.isfinite(values)\n",
    "    times = times[finite_mask]\n",
    "    values = values[finite_mask]\n",
    "\n",
    "    if values.size == 0:\n",
    "        return {\"peak\": np.nan, \"onset_time\": np.nan, \"decay_tau1\": np.nan}\n",
    "\n",
    "    smooth_window = min(7, values.size)\n",
    "    if smooth_window > 1:\n",
    "        kernel = np.ones(smooth_window, dtype=float) / smooth_window\n",
    "        smoothed = np.convolve(values, kernel, mode=\"same\")\n",
    "    else:\n",
    "        smoothed = values.copy()\n",
    "\n",
    "    candidate = smoothed\n",
    "    \n",
    "    # Restrict peak detection to main peak window (0 to POST_ALIGNMENT_WINDOW_DURATION)\n",
    "    # to avoid detecting offset responses as the main peak\n",
    "    main_peak_window_end = POST_ALIGNMENT_WINDOW_START + POST_ALIGNMENT_WINDOW_DURATION\n",
    "    main_peak_mask = times <= main_peak_window_end\n",
    "    candidate_peak_search = candidate[main_peak_mask]\n",
    "    times_peak_search = times[main_peak_mask]\n",
    "    \n",
    "    if candidate_peak_search.size == 0:\n",
    "        return {\"peak\": np.nan, \"onset_time\": np.nan, \"decay_tau1\": np.nan}\n",
    "    \n",
    "    candidate_std = float(np.nanstd(candidate_peak_search)) if candidate_peak_search.size else 0.0\n",
    "    prominence = max(0.05, 0.15 * candidate_std) if candidate_std > 0 else 0.05\n",
    "\n",
    "    peaks, _ = find_peaks(candidate_peak_search, prominence=prominence)\n",
    "    if peaks.size == 0:\n",
    "        peaks, _ = find_peaks(candidate_peak_search)\n",
    "\n",
    "    if peaks.size == 0:\n",
    "        peak_index_in_search = int(np.nanargmax(candidate_peak_search))\n",
    "    else:\n",
    "        best_idx = int(np.argmax(candidate_peak_search[peaks]))\n",
    "        peak_index_in_search = int(peaks[best_idx])\n",
    "    \n",
    "    # Convert back to index in full array\n",
    "    peak_index = peak_index_in_search\n",
    "\n",
    "    peak_index = max(0, min(peak_index, candidate_peak_search.size - 1))\n",
    "\n",
    "    peak_value_raw = values[peak_index]\n",
    "    peak_value_smooth = candidate_peak_search[peak_index]\n",
    "    effective_peak_value = max(peak_value_raw, peak_value_smooth)\n",
    "\n",
    "    if not np.isfinite(effective_peak_value) or effective_peak_value <= 0:\n",
    "        return {\"peak\": 0.0, \"onset_time\": np.nan, \"decay_tau1\": np.nan}\n",
    "\n",
    "    peak_value = float(effective_peak_value)\n",
    "\n",
    "    # Estimate baseline from full data (for offset analysis)\n",
    "    baseline = _estimate_baseline(times, candidate)\n",
    "    \n",
    "    # Fit double-exponential decay using full data (for offset analysis)\n",
    "    tau1, A1, tau2, A2 = _estimate_double_exponential_decay(\n",
    "        times, candidate, peak_index, baseline, DECAY_FIT_START_OFFSET\n",
    "    )\n",
    "    \n",
    "    # For main peak metrics, report only tau1 (fast component) as decay_tau1\n",
    "    decay_tau1 = tau1\n",
    "\n",
    "    # Use only main peak window for onset detection\n",
    "    logistic_onset = _estimate_logistic_onset(times_peak_search[: peak_index + 1], candidate_peak_search[: peak_index + 1])\n",
    "\n",
    "    if logistic_onset is not None:\n",
    "        onset_time = float(logistic_onset)\n",
    "    else:\n",
    "        onset_time = np.nan\n",
    "        onset_threshold = peak_value * 0.1\n",
    "\n",
    "        for idx in range(0, peak_index + 1):\n",
    "            current_value = candidate_peak_search[idx]\n",
    "            if not np.isfinite(current_value):\n",
    "                continue\n",
    "            if current_value >= onset_threshold:\n",
    "                if idx == 0:\n",
    "                    onset_time = times_peak_search[idx]\n",
    "                else:\n",
    "                    prev_time = times_peak_search[idx - 1]\n",
    "                    prev_value = candidate_peak_search[idx - 1]\n",
    "                    if not np.isfinite(prev_value) or prev_value == current_value:\n",
    "                        onset_time = times_peak_search[idx]\n",
    "                    else:\n",
    "                        fraction = (onset_threshold - prev_value) / (current_value - prev_value)\n",
    "                        onset_time = prev_time + fraction * (times_peak_search[idx] - prev_time)\n",
    "                break\n",
    "\n",
    "    # Compute offset response metrics (Strategy 3: Double-exponential residual analysis)\n",
    "    offset_metrics = _compute_offset_residual_metrics(\n",
    "        times,\n",
    "        candidate,\n",
    "        peak_index,\n",
    "        peak_value,\n",
    "        tau1,\n",
    "        A1,\n",
    "        tau2,\n",
    "        A2,\n",
    "        baseline,\n",
    "        OFFSET_ANALYSIS_START,\n",
    "        OFFSET_ANALYSIS_END,\n",
    "        OFFSET_AUC_WINDOW,\n",
    "    )\n",
    "    \n",
    "    # Fallback to direct peak detection if residual analysis fails\n",
    "    offset_peak_amplitude = np.nan\n",
    "    if not np.isfinite(offset_metrics.get(\"residual_max\", np.nan)):\n",
    "        offset_peak_amplitude = _detect_offset_peak_direct(\n",
    "            times,\n",
    "            values,\n",
    "            OFFSET_BASELINE_WINDOW,\n",
    "            OFFSET_ANALYSIS_START,\n",
    "            OFFSET_ANALYSIS_END,\n",
    "        )\n",
    "    \n",
    "    # Compute main peak residual AUC (residuals in the fitting window)\n",
    "    main_peak_residual_auc = np.nan\n",
    "    if np.isfinite(decay_tau1) and decay_tau1 > 0 and np.isfinite(peak_value):\n",
    "        main_peak_start = POST_ALIGNMENT_WINDOW_START\n",
    "        main_peak_end = POST_ALIGNMENT_WINDOW_START + POST_ALIGNMENT_WINDOW_DURATION\n",
    "        main_peak_mask = (times >= main_peak_start) & (times <= main_peak_end) & (times >= times[peak_index])\n",
    "        \n",
    "        if main_peak_mask.any():\n",
    "            main_peak_times = times[main_peak_mask]\n",
    "            main_peak_values = values[main_peak_mask]\n",
    "            \n",
    "            # Reconstruct exponential fit for main peak window (using fast tau)\n",
    "            peak_time = times[peak_index]\n",
    "            time_from_peak = main_peak_times - peak_time\n",
    "            expected_decay = peak_value * np.exp(-time_from_peak / decay_tau1)\n",
    "            \n",
    "            # Compute residuals\n",
    "            main_peak_residuals = main_peak_values - expected_decay\n",
    "            \n",
    "            # Integrate positive residuals only (like offset_residual_auc)\n",
    "            positive_residuals = np.maximum(main_peak_residuals, 0.0)\n",
    "            \n",
    "            if main_peak_times.size > 1:\n",
    "                if np.any(positive_residuals > 0):\n",
    "                    main_peak_residual_auc = float(np.trapz(positive_residuals, main_peak_times))\n",
    "                else:\n",
    "                    # No positive residuals - set to 0 instead of NaN\n",
    "                    main_peak_residual_auc = 0.0\n",
    "            else:\n",
    "                # Not enough points - set to 0\n",
    "                main_peak_residual_auc = 0.0\n",
    "        else:\n",
    "            # No data in window - set to 0\n",
    "            main_peak_residual_auc = 0.0\n",
    "    \n",
    "    return {\n",
    "        \"peak\": peak_value,\n",
    "        \"onset_time\": float(onset_time),\n",
    "        \"decay_tau1\": float(decay_tau1),\n",
    "        \"main_peak_residual_auc\": main_peak_residual_auc,\n",
    "        \"offset_residual_auc\": offset_metrics.get(\"residual_auc\", np.nan),\n",
    "        \"offset_peak_amplitude\": offset_peak_amplitude,\n",
    "    }\n",
    "\n",
    "\n",
    "def sem(values) -> float:\n",
    "    \"\"\"Compute standard error of the mean, ignoring NaNs.\"\"\"\n",
    "    arr = np.asarray(values, dtype=float)\n",
    "    arr = arr[np.isfinite(arr)]\n",
    "    n = arr.size\n",
    "    if n <= 1:\n",
    "        return float(np.nan)\n",
    "    return float(arr.std(ddof=1) / np.sqrt(n))\n",
    "\n",
    "def compute_signal_metrics_from_results(\n",
    "    mean_data_per_mouse: Dict[str, pd.DataFrame],\n",
    "    signal_columns: list[str],\n",
    "    window: Tuple[float, float],\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Compute per-mouse and summary signal metrics directly from loaded results.\"\"\"\n",
    "    records = []\n",
    "\n",
    "    for mouse_id, mouse_df in mean_data_per_mouse.items():\n",
    "        if mouse_df is None or mouse_df.empty:\n",
    "            continue\n",
    "\n",
    "        for signal in signal_columns:\n",
    "            metrics = _compute_signal_metrics_for_mouse(mouse_df, signal, window)\n",
    "            records.append({\"mouse\": mouse_id, \"signal\": signal, **metrics})\n",
    "\n",
    "    metrics_df = pd.DataFrame(records)\n",
    "    if metrics_df.empty:\n",
    "        return metrics_df, pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "    for feature_name in SIGNAL_METRIC_FEATURES:\n",
    "        if feature_name not in metrics_df.columns:\n",
    "            metrics_df[feature_name] = np.nan\n",
    "\n",
    "    summary_records = []\n",
    "    for signal_name, group in metrics_df.groupby(\"signal\"):\n",
    "        for metric_name in SIGNAL_METRIC_FEATURES:\n",
    "            values = pd.to_numeric(group[metric_name], errors=\"coerce\").dropna()\n",
    "            summary_records.append(\n",
    "                {\n",
    "                    \"signal\": signal_name,\n",
    "                    \"metric\": metric_name,\n",
    "                    \"n\": int(values.size),\n",
    "                    \"mean\": float(values.mean()) if values.size else np.nan,\n",
    "                    \"sem\": sem(values) if values.size else np.nan,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    summary_df = pd.DataFrame(summary_records)\n",
    "    if not summary_df.empty:\n",
    "        summary_df = summary_df.sort_values([\"signal\", \"metric\"]).reset_index(drop=True)\n",
    "        summary_pivot = summary_df.pivot(index=\"metric\", columns=\"signal\", values=[\"mean\", \"sem\"])\n",
    "    else:\n",
    "        summary_pivot = pd.DataFrame()\n",
    "\n",
    "    return metrics_df, summary_df, summary_pivot\n",
    "\n",
    "def save_signal_metrics_to_csv(\n",
    "    metrics: pd.DataFrame,\n",
    "    summary: pd.DataFrame,\n",
    "    output_dir: Path,\n",
    "    file_prefix: str,\n",
    "    cohort: Optional[str] = None,\n",
    "    experiment_day: Optional[str] = None,\n",
    ") -> Path:\n",
    "    \"\"\"Save per-mouse signal metrics to disk (summary is not written).\"\"\"\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    metrics_to_save = metrics.copy()\n",
    "    if cohort:\n",
    "        metrics_to_save.insert(0, \"Cohort\", cohort)\n",
    "    if experiment_day:\n",
    "        insert_idx = 1 if cohort else 0\n",
    "        metrics_to_save.insert(insert_idx, \"Experiment_Day\", experiment_day)\n",
    "\n",
    "    per_mouse_path = output_dir / f\"{file_prefix}_calcium_analysis_per_mouse_metrics.csv\"\n",
    "    metrics_to_save.to_csv(per_mouse_path, index=False)\n",
    "\n",
    "    return per_mouse_path\n",
    "\n",
    "\n",
    "def _make_signal_metrics_file_prefix(event_name: str) -> str:\n",
    "    \"\"\"Create a filesystem-friendly prefix from the configured event name.\"\"\"\n",
    "    if not event_name:\n",
    "        return \"signal_metrics\"\n",
    "    label = event_name.replace(\".csv\", \"\")\n",
    "    label = label.strip(\"_\")\n",
    "    label = label.replace(\" \", \"_\")\n",
    "    return label or \"signal_metrics\"\n",
    "\n",
    "def compute_signal_window_mean_per_mouse(\n",
    "    mean_data_per_mouse: Dict[str, pd.DataFrame],\n",
    "    signal_column: str,\n",
    "    window_start: float,\n",
    "    window_end: float,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Compute per-mouse mean of a signal within a specified time window.\"\"\"\n",
    "    records = []\n",
    "    for mouse_id, mouse_df in mean_data_per_mouse.items():\n",
    "        if mouse_df is None or signal_column not in mouse_df.columns:\n",
    "            continue\n",
    "\n",
    "        times = pd.to_numeric(mouse_df.index, errors=\"coerce\")\n",
    "        if np.all(np.isnan(times)):\n",
    "            continue\n",
    "\n",
    "        mask = (times >= window_start) & (times <= window_end)\n",
    "        if not mask.any():\n",
    "            continue\n",
    "\n",
    "        window_values = pd.to_numeric(mouse_df.loc[mask, signal_column], errors=\"coerce\").dropna()\n",
    "        if window_values.empty:\n",
    "            continue\n",
    "\n",
    "        records.append(\n",
    "            {\n",
    "                \"mouse\": mouse_id,\n",
    "                \"mean\": float(window_values.mean()),\n",
    "                \"n_samples\": int(window_values.size),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "\n",
    "def extract_mouse_dataframes_from_loaded(\n",
    "    loaded_data: Dict[object, Dict[str, object]],\n",
    "    signal_columns: Optional[Iterable[str]] = None,\n",
    "    time_column: str = \"Time (s)\",\n",
    ") -> Tuple[Dict[str, pd.DataFrame], Dict[str, Path]]:\n",
    "    \"\"\"Convert the raw loaded data into per-mouse DataFrames and track their origins.\"\"\"\n",
    "    mouse_frames: Dict[str, list[pd.DataFrame]] = {}\n",
    "    mouse_sources: Dict[str, Path] = {}\n",
    "\n",
    "    for entry in loaded_data.values():\n",
    "        mouse_name = entry.get(\"mouse_name\") if isinstance(entry, dict) else None\n",
    "        if not mouse_name:\n",
    "            continue\n",
    "\n",
    "        frames: List[pd.DataFrame] = []\n",
    "        if isinstance(entry, dict):\n",
    "            records = entry.get(\"records\", [])\n",
    "            if records:\n",
    "                for record in records:\n",
    "                    df = record.get(\"dataframe\")\n",
    "                    if isinstance(df, pd.DataFrame):\n",
    "                        frames.append(df.copy())\n",
    "            elif entry.get(\"dataframes\"):\n",
    "                raw_frames = entry.get(\"dataframes\", [])\n",
    "                for df in raw_frames:\n",
    "                    if isinstance(df, pd.DataFrame):\n",
    "                        frames.append(df.copy())\n",
    "        else:\n",
    "            columns = [key for key in entry.keys() if key not in {\"mouse_name\", \"data_path\"}]  # type: ignore[union-attr]\n",
    "            if time_column in entry:\n",
    "                columns = [time_column] + [col for col in columns if col != time_column]\n",
    "            if signal_columns is not None:\n",
    "                allowed = set(signal_columns)\n",
    "                columns = [col for col in columns if col == time_column or col in allowed]\n",
    "            data = {col: entry[col] for col in columns if col in entry}\n",
    "            if data:\n",
    "                frames = [pd.DataFrame(data)]\n",
    "\n",
    "        if not frames:\n",
    "            continue\n",
    "\n",
    "        cleaned_frames: List[pd.DataFrame] = []\n",
    "        for df in frames:\n",
    "            if time_column in df.columns:\n",
    "                df = df.set_index(time_column)\n",
    "                df.index.name = time_column\n",
    "            df = df.apply(pd.to_numeric, errors=\"coerce\").dropna(how=\"all\")\n",
    "            if df.empty:\n",
    "                continue\n",
    "            cleaned_frames.append(df.sort_index())\n",
    "\n",
    "        if not cleaned_frames:\n",
    "            continue\n",
    "\n",
    "        mouse_frames.setdefault(mouse_name, []).extend(cleaned_frames)\n",
    "\n",
    "        data_paths = []\n",
    "        if isinstance(entry, dict):\n",
    "            records = entry.get(\"records\", [])\n",
    "            if records:\n",
    "                for record in records:\n",
    "                    aligned_dir = record.get(\"aligned_dir\")\n",
    "                    if aligned_dir:\n",
    "                        data_paths.append(Path(aligned_dir))\n",
    "            if not data_paths:\n",
    "                data_paths = entry.get(\"aligned_dirs\") or []\n",
    "            if not data_paths and entry.get(\"data_path\"):\n",
    "                data_paths = [entry[\"data_path\"]]\n",
    "        if data_paths and mouse_name not in mouse_sources:\n",
    "            mouse_sources[mouse_name] = Path(data_paths[0]).resolve()\n",
    "\n",
    "    consolidated: Dict[str, pd.DataFrame] = {}\n",
    "    for mouse, frames in mouse_frames.items():\n",
    "        if len(frames) == 1:\n",
    "            consolidated[mouse] = frames[0]\n",
    "        else:\n",
    "            combined = pd.concat(frames)\n",
    "            combined = combined.groupby(combined.index).mean().sort_index()\n",
    "            consolidated[mouse] = combined\n",
    "\n",
    "    return consolidated, mouse_sources\n",
    "\n",
    "\n",
    "\n",
    "def assign_mouse_colors_consistent(mouse_ids: Iterable[str]) -> Dict[str, tuple]:\n",
    "    \"\"\"Assign consistent colors to mice using the gnuplot2 palette.\"\"\"\n",
    "    normalized_ids = [str(mouse) for mouse in mouse_ids]\n",
    "    mouse_list = sorted(dict.fromkeys(normalized_ids))\n",
    "    if not mouse_list:\n",
    "        return OrderedDict()\n",
    "    count = len(mouse_list)\n",
    "    if count == 1:\n",
    "        sample_points = np.array([0.6])\n",
    "    else:\n",
    "        sample_points = np.linspace(0.12, 0.95, count)\n",
    "    color_map = plt.cm.gnuplot2\n",
    "    colors: list[tuple] = []\n",
    "    for point in sample_points:\n",
    "        candidate = color_map(point)\n",
    "        attempts = 0\n",
    "        while np.all(np.asarray(candidate[:3]) <= 0.15) and attempts < 5:\n",
    "            point = min(point + 0.08, 0.99)\n",
    "            candidate = color_map(point)\n",
    "            attempts += 1\n",
    "        colors.append(candidate)\n",
    "    return OrderedDict((mouse, colors[idx]) for idx, mouse in enumerate(mouse_list))\n",
    "\n",
    "\n",
    "def cm_to_inches(*dimensions_cm: float) -> Tuple[float, ...]:\n",
    "    \"\"\"Convert one or more centimetre measurements to inches.\"\"\"\n",
    "    return tuple(dim / 2.54 for dim in dimensions_cm)\n",
    "\n",
    "\n",
    "def holm_adjust(p_values: List[float]) -> List[float]:\n",
    "    \"\"\"Perform Holm-Bonferroni correction without statsmodels.\"\"\"\n",
    "    if not p_values:\n",
    "        return []\n",
    "    p_values = np.asarray(p_values, dtype=float)\n",
    "    order = np.argsort(p_values)\n",
    "    m = len(p_values)\n",
    "    adjusted = np.empty(m, dtype=float)\n",
    "    for rank, idx in enumerate(order):\n",
    "        adjusted[idx] = min(1.0, (m - rank) * p_values[idx])\n",
    "    # Ensure monotonicity\n",
    "    for i in range(m - 2, -1, -1):\n",
    "        adjusted[order[i]] = min(adjusted[order[i]], adjusted[order[i + 1]])\n",
    "    return adjusted.tolist()\n",
    "\n",
    "\n",
    "def _discover_metric_csvs(\n",
    "    data_dirs: Iterable[Path],\n",
    "    event_name: Optional[str],\n",
    ") -> list[Path]:\n",
    "    \"\"\"Search data directories for per-mouse metrics CSVs.\"\"\"\n",
    "    discovered: list[Path] = []\n",
    "    candidate_names: list[str] = []\n",
    "    event_label = None\n",
    "    if event_name:\n",
    "        prefix = _make_signal_metrics_file_prefix(event_name)\n",
    "        candidate_names.append(f\"{prefix}_calcium_analysis_per_mouse_metrics.csv\")\n",
    "        event_label = prefix.lower()\n",
    "    candidate_names.append(\"calcium_analysis_per_mouse_metrics.csv\")\n",
    "\n",
    "    for base in data_dirs:\n",
    "        base_path = Path(base).expanduser()\n",
    "        if not base_path.exists():\n",
    "            continue\n",
    "        # Direct name matches first\n",
    "        for name in candidate_names:\n",
    "            direct = base_path / name\n",
    "            if direct.name.startswith(\"._\"):\n",
    "                continue\n",
    "            if event_label and event_label not in direct.stem.lower():\n",
    "                continue\n",
    "            if direct.exists():\n",
    "                discovered.append(direct.resolve())\n",
    "        # Recursive search (limit depth to avoid huge scans)\n",
    "        for csv_file in base_path.rglob(\"*_calcium_analysis_per_mouse_metrics.csv\"):\n",
    "            if csv_file.name.startswith(\"._\"):\n",
    "                continue\n",
    "            if event_label and event_label not in csv_file.stem.lower():\n",
    "                continue\n",
    "            discovered.append(csv_file.resolve())\n",
    "    # Remove duplicates while preserving order\n",
    "    unique: list[Path] = []\n",
    "    seen = set()\n",
    "    for path in discovered:\n",
    "        if path in seen:\n",
    "            continue\n",
    "        seen.add(path)\n",
    "        unique.append(path)\n",
    "    return unique\n",
    "\n",
    "\n",
    "def compute_condition_signal_metrics(\n",
    "    condition_label: str,\n",
    "    condition_cfg: Dict[str, object],\n",
    "    selected_mice: List[str],\n",
    "    allowed_mice: List[str],\n",
    ") -> Tuple[pd.DataFrame, Dict[str, Path]]:\n",
    "    \"\"\"Compute signal metrics for a given condition (event + data directories).\"\"\"\n",
    "    event_name_cfg = condition_cfg.get(\"event_name\")\n",
    "    metrics_csvs_cfg = condition_cfg.get(\"metrics_csvs\", [])\n",
    "    data_dirs_cfg = condition_cfg.get(\"data_dirs\", [])\n",
    "\n",
    "    metrics_csv_paths: list[Path] = []\n",
    "    for csv_entry in metrics_csvs_cfg:\n",
    "        csv_path = Path(csv_entry).expanduser()\n",
    "        if csv_path.exists():\n",
    "            metrics_csv_paths.append(csv_path.resolve())\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Metrics CSV not found for condition '{condition_label}': {csv_path}\")\n",
    "\n",
    "    if not metrics_csv_paths and data_dirs_cfg:\n",
    "        discovered = _discover_metric_csvs(data_dirs_cfg, event_name_cfg)\n",
    "        if discovered:\n",
    "            metrics_csv_paths.extend(discovered)\n",
    "\n",
    "    if metrics_csv_paths:\n",
    "        frames: list[pd.DataFrame] = []\n",
    "        sources: Dict[str, Path] = {}\n",
    "        for csv_path in metrics_csv_paths:\n",
    "            try:\n",
    "                df = pd.read_csv(csv_path)\n",
    "            except Exception as exc:  # noqa: BLE001\n",
    "                print(f\"‚ö†Ô∏è Failed to read metrics CSV '{csv_path}': {exc}\")\n",
    "                continue\n",
    "            if \"mouse\" not in df.columns or \"signal\" not in df.columns:\n",
    "                print(f\"‚ö†Ô∏è CSV '{csv_path}' missing required columns ('mouse', 'signal'). Skipping.\")\n",
    "                continue\n",
    "            df[\"mouse\"] = df[\"mouse\"].astype(str)\n",
    "            if selected_mice:\n",
    "                df = df[df[\"mouse\"].isin(selected_mice)]\n",
    "            elif allowed_mice:\n",
    "                df = df[df[\"mouse\"].isin(allowed_mice)]\n",
    "            if df.empty:\n",
    "                continue\n",
    "            numeric_cols = [col for col in df.columns if col not in {\"mouse\", \"signal\", \"Cohort\", \"Experiment_Day\"}]\n",
    "            df_filtered = df[[\"mouse\", \"signal\"] + numeric_cols].copy()\n",
    "            df_filtered[\"condition\"] = condition_label\n",
    "            df_filtered[\"condition_label\"] = condition_cfg.get(\"label\", condition_label)\n",
    "            df_filtered[\"event_name\"] = event_name_cfg or csv_path.stem\n",
    "            df_filtered[\"source_csv\"] = str(csv_path)\n",
    "            frames.append(df_filtered)\n",
    "            csv_parent = csv_path.parent\n",
    "            for mouse_name in df_filtered[\"mouse\"].unique():\n",
    "                sources.setdefault(mouse_name, csv_parent)\n",
    "        if not frames:\n",
    "            print(f\"‚ö†Ô∏è No usable entries found in metrics CSVs for condition '{condition_label}'.\")\n",
    "            return pd.DataFrame(), {}\n",
    "\n",
    "        metrics_df = pd.concat(frames, ignore_index=True)\n",
    "        value_columns = [\n",
    "            col for col in metrics_df.columns\n",
    "            if col not in {\"mouse\", \"signal\", \"condition\", \"condition_label\", \"event_name\", \"source_csv\"}\n",
    "        ]\n",
    "        grouped = (\n",
    "            metrics_df.groupby(\n",
    "                [\"mouse\", \"signal\", \"condition\", \"condition_label\", \"event_name\"],\n",
    "                as_index=False,\n",
    "            )[value_columns]\n",
    "            .mean(numeric_only=True)\n",
    "        )\n",
    "        return grouped, sources\n",
    "\n",
    "    # Fall back to loading aligned data and computing metrics if CSVs not available\n",
    "    if not data_dirs_cfg:\n",
    "        print(f\"‚ö†Ô∏è No data directories configured for condition '{condition_label}'.\")\n",
    "        return pd.DataFrame(), {}\n",
    "\n",
    "    loaded = load_aligned_data(\n",
    "        data_dirs_cfg,\n",
    "        event_name_cfg,\n",
    "        selected_mice,\n",
    "        allowed_mice,\n",
    "    )\n",
    "    if not loaded:\n",
    "        print(f\"‚ö†Ô∏è No data loaded for condition '{condition_label}'.\")\n",
    "        return pd.DataFrame(), {}\n",
    "\n",
    "    frames, sources = extract_mouse_dataframes_from_loaded(\n",
    "        loaded,\n",
    "        signal_columns=SIGNAL_METRIC_COLUMNS,\n",
    "    )\n",
    "    if not frames:\n",
    "        print(f\"‚ö†Ô∏è No signal frames available for condition '{condition_label}'.\")\n",
    "        return pd.DataFrame(), sources\n",
    "\n",
    "    # Analysis window must extend through offset analysis period to capture all data\n",
    "    metrics_df, _, _ = compute_signal_metrics_from_results(\n",
    "        frames,\n",
    "        SIGNAL_METRIC_COLUMNS,\n",
    "        (POST_ALIGNMENT_WINDOW_START, OFFSET_ANALYSIS_END),\n",
    "    )\n",
    "    if metrics_df.empty:\n",
    "        print(f\"‚ö†Ô∏è No signal metrics computed for condition '{condition_label}'.\")\n",
    "        return metrics_df, sources\n",
    "\n",
    "    window_start = POST_ALIGNMENT_WINDOW_START + 2.0\n",
    "    window_end = POST_ALIGNMENT_WINDOW_START + 8.0\n",
    "    z560_window_stats = compute_signal_window_mean_per_mouse(\n",
    "        frames,\n",
    "        \"z_560_Baseline\",\n",
    "        window_start,\n",
    "        window_end,\n",
    "    )\n",
    "    if not z560_window_stats.empty and \"z_560_Baseline\" in metrics_df[\"signal\"].unique():\n",
    "        mean_map = z560_window_stats.set_index(\"mouse\")[\"mean\"]\n",
    "        mask = metrics_df[\"signal\"] == \"z_560_Baseline\"\n",
    "        metrics_df.loc[mask, \"mean_fluorescence_2_to_8s\"] = metrics_df.loc[\n",
    "            mask, \"mouse\"\n",
    "        ].map(mean_map)\n",
    "\n",
    "    metrics_df[\"condition\"] = condition_label\n",
    "    metrics_df[\"condition_label\"] = condition_cfg.get(\"label\", condition_label)\n",
    "    metrics_df[\"event_name\"] = event_name_cfg\n",
    "\n",
    "    return metrics_df, sources\n",
    "\n",
    "\n",
    "def _paired_sem(values: pd.Series) -> float:\n",
    "    \"\"\"Compute SEM for paired samples.\"\"\"\n",
    "    arr = pd.to_numeric(values, errors=\"coerce\").to_numpy()\n",
    "    arr = arr[np.isfinite(arr)]\n",
    "    if arr.size <= 1:\n",
    "        return 0.0\n",
    "    return float(arr.std(ddof=1) / np.sqrt(arr.size))\n",
    "\n",
    "\n",
    "def compute_repeated_measures_stats(\n",
    "    metric_df: pd.DataFrame,\n",
    "    condition_order: List[str],\n",
    "    signal_order: List[str],\n",
    "    metric_name: str,\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, List[str]]:\n",
    "    \"\"\"Run repeated or non-paired ANOVA depending on data completeness.\"\"\"\n",
    "    notes: List[str] = []\n",
    "    summary_records: List[Dict[str, object]] = []\n",
    "    pairwise_records: List[Dict[str, object]] = []\n",
    "\n",
    "    clean_df = metric_df.dropna(subset=[\"value\"]).copy()\n",
    "    if clean_df.empty:\n",
    "        notes.append(f\"No valid data available for metric '{metric_name}'.\")\n",
    "        return pd.DataFrame(), pd.DataFrame(), notes\n",
    "\n",
    "    condition_levels_present = [\n",
    "        cond for cond in condition_order if cond in clean_df[\"condition\"].unique()\n",
    "    ]\n",
    "    if not condition_levels_present:\n",
    "        condition_levels_present = sorted(clean_df[\"condition\"].unique())\n",
    "\n",
    "    signal_levels_present = [\n",
    "        sig for sig in signal_order if sig in clean_df[\"signal\"].unique()\n",
    "    ]\n",
    "    if not signal_levels_present:\n",
    "        signal_levels_present = sorted(clean_df[\"signal\"].unique())\n",
    "\n",
    "    clean_df[\"Condition\"] = pd.Categorical(\n",
    "        clean_df[\"condition\"], categories=condition_levels_present, ordered=True\n",
    "    )\n",
    "    clean_df[\"Signal\"] = pd.Categorical(\n",
    "        clean_df[\"signal\"], categories=signal_levels_present, ordered=True\n",
    "    )\n",
    "    clean_df[\"mouse\"] = clean_df[\"mouse\"].astype(str)\n",
    "    clean_df[\"Mouse\"] = clean_df[\"mouse\"]\n",
    "    clean_df[\"Value\"] = clean_df[\"value\"]\n",
    "\n",
    "    mice_per_condition = {\n",
    "        cond: sorted(set(clean_df.loc[clean_df[\"condition\"] == cond, \"Mouse\"]))\n",
    "        for cond in condition_levels_present\n",
    "    }\n",
    "    unique_counts = {cond: len(mice) for cond, mice in mice_per_condition.items()}\n",
    "    same_count = bool(unique_counts) and len(set(unique_counts.values())) == 1\n",
    "\n",
    "    if same_count:\n",
    "        common_mice = set.intersection(\n",
    "            *(set(mice) for mice in mice_per_condition.values())\n",
    "        )\n",
    "        expected_count = next(iter(unique_counts.values()))\n",
    "        paired_design = len(common_mice) == expected_count and expected_count > 0\n",
    "        if not paired_design:\n",
    "            notes.append(\n",
    "                \"Mice counts match across conditions, but identities differ; \"\n",
    "                \"using non-paired analyses.\"\n",
    "            )\n",
    "    else:\n",
    "        common_mice = set()\n",
    "        paired_design = False\n",
    "        notes.append(\n",
    "            \"Detected unequal mouse counts across conditions; switching to non-paired analyses.\"\n",
    "        )\n",
    "\n",
    "    anova_performed = False\n",
    "\n",
    "    if paired_design and STATS_MODELS_AVAILABLE:\n",
    "        try:\n",
    "            anova_model = AnovaRM(\n",
    "                clean_df,\n",
    "                depvar=\"Value\",\n",
    "                subject=\"Mouse\",\n",
    "                within=[\"Condition\", \"Signal\"],\n",
    "            )\n",
    "            anova_result = anova_model.fit()\n",
    "            anova_table = anova_result.anova_table.reset_index().rename(\n",
    "                columns={\"index\": \"effect\"}\n",
    "            )\n",
    "            for _, row in anova_table.iterrows():\n",
    "                summary_records.append(\n",
    "                    {\n",
    "                        \"analysis\": \"AnovaRM\",\n",
    "                        \"metric\": metric_name,\n",
    "                        \"effect\": row[\"effect\"],\n",
    "                        \"num_df\": row.get(\"Num DF\"),\n",
    "                        \"den_df\": row.get(\"Den DF\"),\n",
    "                        \"F\": row.get(\"F Value\"),\n",
    "                        \"p_value\": row.get(\"Pr > F\"),\n",
    "                    }\n",
    "                )\n",
    "            anova_performed = True\n",
    "        except Exception as exc:  # noqa: BLE001\n",
    "            notes.append(\n",
    "                f\"Repeated measures ANOVA failed for metric '{metric_name}': {exc}\"\n",
    "            )\n",
    "\n",
    "    if paired_design and not anova_performed:\n",
    "        pivot = clean_df.pivot_table(\n",
    "            index=\"mouse\", columns=[\"signal\", \"condition\"], values=\"value\"\n",
    "        )\n",
    "        pivot = pivot.dropna()\n",
    "        if pivot.shape[0] >= 3:\n",
    "            try:\n",
    "                fried_stat, fried_p = friedmanchisquare(\n",
    "                    *[pivot[col] for col in pivot.columns]\n",
    "                )\n",
    "                summary_records.append(\n",
    "                    {\n",
    "                        \"analysis\": \"Friedman\",\n",
    "                        \"metric\": metric_name,\n",
    "                        \"effect\": \"condition_x_signal\",\n",
    "                        \"df\": pivot.shape[1] - 1,\n",
    "                        \"statistic\": fried_stat,\n",
    "                        \"p_value\": fried_p,\n",
    "                    }\n",
    "                )\n",
    "                notes.append(\n",
    "                    \"Used Friedman test because repeated measures ANOVA was unavailable.\"\n",
    "                )\n",
    "                anova_performed = True\n",
    "            except Exception as exc:  # noqa: BLE001\n",
    "                notes.append(\n",
    "                    f\"Friedman test failed for metric '{metric_name}': {exc}\"\n",
    "                )\n",
    "        else:\n",
    "            notes.append(\n",
    "                \"Not enough paired samples for Friedman test \"\n",
    "                f\"({pivot.shape[0]} mice with complete data).\"\n",
    "            )\n",
    "\n",
    "    if not paired_design and STATS_MODELS_AVAILABLE and not anova_performed:\n",
    "        try:\n",
    "            between_df = clean_df.copy()\n",
    "            between_df[\"Condition\"] = between_df[\"Condition\"].astype(str)\n",
    "            between_df[\"Signal\"] = between_df[\"Signal\"].astype(str)\n",
    "            n_signals = between_df[\"Signal\"].nunique()\n",
    "            n_conditions = between_df[\"Condition\"].nunique()\n",
    "            if n_conditions < 2 and n_signals < 2:\n",
    "                notes.append(\n",
    "                    \"Insufficient variability across conditions/signals for non-paired ANOVA.\"\n",
    "                )\n",
    "            else:\n",
    "                if n_signals > 1 and n_conditions > 1:\n",
    "                    formula = \"Value ~ C(Condition) * C(Signal)\"\n",
    "                    analysis_label = \"TwoWay_ANOVA\"\n",
    "                elif n_signals > 1:\n",
    "                    formula = \"Value ~ C(Signal)\"\n",
    "                    analysis_label = \"OneWay_ANOVA_Signal\"\n",
    "                else:\n",
    "                    formula = \"Value ~ C(Condition)\"\n",
    "                    analysis_label = \"OneWay_ANOVA_Condition\"\n",
    "                model = ols(formula, data=between_df).fit()\n",
    "                anova_table = anova_lm(model, typ=2).reset_index().rename(\n",
    "                    columns={\"index\": \"effect\", \"df\": \"df\", \"PR(>F)\": \"p_value\"}\n",
    "                )\n",
    "                for _, row in anova_table.iterrows():\n",
    "                    summary_records.append(\n",
    "                        {\n",
    "                            \"analysis\": analysis_label,\n",
    "                            \"metric\": metric_name,\n",
    "                            \"effect\": row[\"effect\"],\n",
    "                            \"df\": row.get(\"df\"),\n",
    "                            \"F\": row.get(\"F\"),\n",
    "                            \"p_value\": row.get(\"p_value\"),\n",
    "                        }\n",
    "                    )\n",
    "                anova_performed = True\n",
    "        except Exception as exc:  # noqa: BLE001\n",
    "            notes.append(\n",
    "                f\"Non-paired ANOVA failed for metric '{metric_name}': {exc}\"\n",
    "            )\n",
    "\n",
    "    if not anova_performed:\n",
    "        notes.append(\"No omnibus statistical test could be completed for this metric.\")\n",
    "\n",
    "    if paired_design:\n",
    "        for signal in signal_levels_present:\n",
    "            subset = clean_df[clean_df[\"signal\"] == signal]\n",
    "            pivot = subset.pivot(index=\"mouse\", columns=\"condition\", values=\"value\")\n",
    "            pivot = pivot.dropna()\n",
    "            if len(condition_levels_present) >= 2:\n",
    "                cond_pairs = [\n",
    "                    (condition_levels_present[i], condition_levels_present[j])\n",
    "                    for i in range(len(condition_levels_present))\n",
    "                    for j in range(i + 1, len(condition_levels_present))\n",
    "                ]\n",
    "            else:\n",
    "                cond_pairs = []\n",
    "            for cond_a, cond_b in cond_pairs:\n",
    "                if cond_a not in pivot.columns or cond_b not in pivot.columns:\n",
    "                    continue\n",
    "                paired = pivot[[cond_a, cond_b]].dropna()\n",
    "                if paired.shape[0] < 2:\n",
    "                    notes.append(\n",
    "                        f\"Not enough data for paired comparison between {cond_a} and {cond_b} \"\n",
    "                        f\"in signal {signal} (n={paired.shape[0]}).\"\n",
    "                    )\n",
    "                    continue\n",
    "                diff = paired[cond_a] - paired[cond_b]\n",
    "                try:\n",
    "                    t_stat, p_val = ttest_rel(paired[cond_a], paired[cond_b])\n",
    "                    test_used = \"paired_ttest\"\n",
    "                except Exception:  # noqa: BLE001\n",
    "                    try:\n",
    "                        t_stat, p_val = wilcoxon(paired[cond_a], paired[cond_b])\n",
    "                        test_used = \"wilcoxon\"\n",
    "                    except Exception as exc:  # noqa: BLE001\n",
    "                        notes.append(\n",
    "                            f\"Failed to compute paired test for {cond_a} vs {cond_b} \"\n",
    "                            f\"in signal {signal}: {exc}\"\n",
    "                        )\n",
    "                        continue\n",
    "                effect = (\n",
    "                    diff.mean() / diff.std(ddof=1)\n",
    "                    if diff.std(ddof=1) not in (0, np.nan)\n",
    "                    else np.nan\n",
    "                )\n",
    "                pairwise_records.append(\n",
    "                    {\n",
    "                        \"comparison\": \"condition_within_signal\",\n",
    "                        \"metric\": metric_name,\n",
    "                        \"signal\": signal,\n",
    "                        \"level_a\": cond_a,\n",
    "                        \"level_b\": cond_b,\n",
    "                        \"statistic\": t_stat,\n",
    "                        \"p_value\": p_val,\n",
    "                        \"n\": paired.shape[0],\n",
    "                        \"effect_size_cohens_d\": effect,\n",
    "                        \"test\": test_used,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        if len(signal_levels_present) >= 2:\n",
    "            signal_pairs = [\n",
    "                (signal_levels_present[i], signal_levels_present[j])\n",
    "                for i in range(len(signal_levels_present))\n",
    "                for j in range(i + 1, len(signal_levels_present))\n",
    "            ]\n",
    "        else:\n",
    "            signal_pairs = []\n",
    "\n",
    "        for condition in condition_levels_present:\n",
    "            subset = clean_df[clean_df[\"condition\"] == condition]\n",
    "            pivot = subset.pivot(index=\"mouse\", columns=\"signal\", values=\"value\").dropna()\n",
    "            for signal_a, signal_b in signal_pairs:\n",
    "                if signal_a not in pivot.columns or signal_b not in pivot.columns:\n",
    "                    continue\n",
    "                paired = pivot[[signal_a, signal_b]].dropna()\n",
    "                if paired.shape[0] < 2:\n",
    "                    notes.append(\n",
    "                        f\"Not enough data for paired comparison between {signal_a} and \"\n",
    "                        f\"{signal_b} within condition {condition} (n={paired.shape[0]}).\"\n",
    "                    )\n",
    "                    continue\n",
    "                diff = paired[signal_a] - paired[signal_b]\n",
    "                try:\n",
    "                    t_stat, p_val = ttest_rel(paired[signal_a], paired[signal_b])\n",
    "                    test_used = \"paired_ttest\"\n",
    "                except Exception:  # noqa: BLE001\n",
    "                    try:\n",
    "                        t_stat, p_val = wilcoxon(paired[signal_a], paired[signal_b])\n",
    "                        test_used = \"wilcoxon\"\n",
    "                    except Exception as exc:  # noqa: BLE001\n",
    "                        notes.append(\n",
    "                            f\"Failed to compute signal comparison within {condition}: {exc}\"\n",
    "                        )\n",
    "                        continue\n",
    "                effect = (\n",
    "                    diff.mean() / diff.std(ddof=1)\n",
    "                    if diff.std(ddof=1) not in (0, np.nan)\n",
    "                    else np.nan\n",
    "                )\n",
    "                pairwise_records.append(\n",
    "                    {\n",
    "                        \"comparison\": \"signal_within_condition\",\n",
    "                        \"metric\": metric_name,\n",
    "                        \"condition\": condition,\n",
    "                        \"level_a\": signal_a,\n",
    "                        \"level_b\": signal_b,\n",
    "                        \"statistic\": t_stat,\n",
    "                        \"p_value\": p_val,\n",
    "                        \"n\": paired.shape[0],\n",
    "                        \"effect_size_cohens_d\": effect,\n",
    "                        \"test\": test_used,\n",
    "                    }\n",
    "                )\n",
    "    else:\n",
    "        for signal in signal_levels_present:\n",
    "            subset = clean_df[clean_df[\"signal\"] == signal]\n",
    "            if len(condition_levels_present) >= 2:\n",
    "                cond_pairs = [\n",
    "                    (condition_levels_present[i], condition_levels_present[j])\n",
    "                    for i in range(len(condition_levels_present))\n",
    "                    for j in range(i + 1, len(condition_levels_present))\n",
    "                ]\n",
    "            else:\n",
    "                cond_pairs = []\n",
    "            for cond_a, cond_b in cond_pairs:\n",
    "                group_a = subset[subset[\"condition\"] == cond_a][\"value\"].dropna().to_numpy(dtype=float)\n",
    "                group_b = subset[subset[\"condition\"] == cond_b][\"value\"].dropna().to_numpy(dtype=float)\n",
    "                if group_a.size < 2 or group_b.size < 2:\n",
    "                    notes.append(\n",
    "                        f\"Not enough data for unpaired comparison between {cond_a} and {cond_b} \"\n",
    "                        f\"in signal {signal} (n={group_a.size} vs {group_b.size}).\"\n",
    "                    )\n",
    "                    continue\n",
    "                t_stat, p_val = ttest_ind(group_a, group_b, equal_var=False)\n",
    "                var_a = group_a.var(ddof=1)\n",
    "                var_b = group_b.var(ddof=1)\n",
    "                pooled_denom = ((group_a.size - 1) * var_a + (group_b.size - 1) * var_b)\n",
    "                pooled_denom = pooled_denom / (group_a.size + group_b.size - 2) if (group_a.size + group_b.size - 2) > 0 else np.nan\n",
    "                pooled_sd = np.sqrt(pooled_denom) if pooled_denom > 0 else np.nan\n",
    "                effect = (\n",
    "                    (group_a.mean() - group_b.mean()) / pooled_sd\n",
    "                    if pooled_sd not in (0, np.nan)\n",
    "                    else np.nan\n",
    "                )\n",
    "                pairwise_records.append(\n",
    "                    {\n",
    "                        \"comparison\": \"condition_within_signal\",\n",
    "                        \"metric\": metric_name,\n",
    "                        \"signal\": signal,\n",
    "                        \"level_a\": cond_a,\n",
    "                        \"level_b\": cond_b,\n",
    "                        \"statistic\": t_stat,\n",
    "                        \"p_value\": p_val,\n",
    "                        \"n\": int(group_a.size + group_b.size),\n",
    "                        \"effect_size_cohens_d\": effect,\n",
    "                        \"test\": \"unpaired_ttest\",\n",
    "                    }\n",
    "                )\n",
    "        notes.append(\n",
    "            \"Pairwise comparisons performed with independent-samples tests due to missing mice across conditions.\"\n",
    "        )\n",
    "\n",
    "    if pairwise_records:\n",
    "        p_vals = [rec[\"p_value\"] for rec in pairwise_records]\n",
    "        if multipletests is not None:\n",
    "            _, p_adj, _, _ = multipletests(p_vals, method=\"holm\")\n",
    "        else:\n",
    "            p_adj = holm_adjust(p_vals)\n",
    "            notes.append(\"Applied manual Holm correction (statsmodels not available).\")\n",
    "        for rec, adj in zip(pairwise_records, p_adj):\n",
    "            rec[\"p_value_holm\"] = adj\n",
    "            rec[\"significant_0_05\"] = adj < 0.05\n",
    "\n",
    "    summary_df = pd.DataFrame(summary_records)\n",
    "    pairwise_df = pd.DataFrame(pairwise_records)\n",
    "    return summary_df, pairwise_df, notes\n",
    "\n",
    "\n",
    "def plot_condition_metric(\n",
    "    metric_name: str,\n",
    "    metric_df: pd.DataFrame,\n",
    "    condition_order: List[str],\n",
    "    signal_order: List[str],\n",
    "    mouse_colors: Dict[str, tuple],\n",
    "    connect_lines: bool = True,\n",
    ") -> plt.Figure:\n",
    "    \"\"\"Generate paired plots for a given metric across conditions.\"\"\"\n",
    "    signals_present = [sig for sig in signal_order if sig in metric_df[\"signal\"].unique()]\n",
    "    if not signals_present:\n",
    "        fig, ax = plt.subplots(figsize=(6, 4))\n",
    "        ax.text(\n",
    "            0.5,\n",
    "            0.5,\n",
    "            f\"No data available for metric {metric_name}\",\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "        )\n",
    "        ax.axis(\"off\")\n",
    "        return fig\n",
    "\n",
    "    fig, axes = plt.subplots(\n",
    "        1,\n",
    "        len(signals_present),\n",
    "        figsize=(6 * len(signals_present), 5),\n",
    "        sharey=True,\n",
    "    )\n",
    "    if len(signals_present) == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for ax, signal in zip(axes, signals_present):\n",
    "        subset = metric_df[metric_df[\"signal\"] == signal]\n",
    "        for mouse, color in mouse_colors.items():\n",
    "            mouse_data = (\n",
    "                subset[subset[\"mouse\"] == mouse]\n",
    "                .set_index(\"condition\")\n",
    "                .reindex(condition_order)\n",
    "            )\n",
    "            mouse_series = mouse_data[\"value\"]\n",
    "            if connect_lines and mouse_series.isna().any():\n",
    "                continue\n",
    "            valid_series = mouse_series.dropna()\n",
    "            if valid_series.empty:\n",
    "                continue\n",
    "            x_indices = np.array([condition_order.index(cond) for cond in valid_series.index])\n",
    "            y_values = valid_series.values\n",
    "            if connect_lines:\n",
    "                ax.plot(\n",
    "                    x_indices,\n",
    "                    y_values,\n",
    "                    marker=\"o\",\n",
    "                    linewidth=1.5,\n",
    "                    alpha=0.8,\n",
    "                    color=color,\n",
    "                    label=mouse,\n",
    "                )\n",
    "            else:\n",
    "                jitter_seed = (hash((mouse, signal)) % 11) - 5\n",
    "                jitter = jitter_seed * 0.03\n",
    "                ax.scatter(\n",
    "                    x_indices + jitter,\n",
    "                    y_values,\n",
    "                    marker=\"o\",\n",
    "                    s=55,\n",
    "                    alpha=0.85,\n",
    "                    color=color,\n",
    "                    label=mouse,\n",
    "                )\n",
    "        means = subset.groupby(\"condition\")[\"value\"].mean().reindex(condition_order)\n",
    "        sems = subset.groupby(\"condition\")[\"value\"].apply(_paired_sem).reindex(\n",
    "            condition_order\n",
    "        )\n",
    "        x_axis = np.arange(len(condition_order))\n",
    "        errorbar_common = dict(\n",
    "            yerr=sems.values,\n",
    "            color=\"black\",\n",
    "            linewidth=2.5,\n",
    "            capsize=6,\n",
    "            capthick=2,\n",
    "            markersize=10,\n",
    "            label=\"Mean ¬± SEM\",\n",
    "        )\n",
    "        if connect_lines:\n",
    "            mean_container = ax.errorbar(\n",
    "                x_axis,\n",
    "                means.values,\n",
    "                fmt=\"-o\",\n",
    "                **errorbar_common,\n",
    "            )\n",
    "        else:\n",
    "            mean_container = ax.errorbar(\n",
    "                x_axis,\n",
    "                means.values,\n",
    "                fmt=\"o\",\n",
    "                linestyle=\"none\",\n",
    "                **errorbar_common,\n",
    "            )\n",
    "        ax.set_xticks(np.arange(len(condition_order)))\n",
    "        ax.set_xticklabels(\n",
    "            [subset[subset[\"condition\"] == cond][\"condition_label\"].iloc[0]\n",
    "             if not subset[subset[\"condition\"] == cond].empty else cond\n",
    "             for cond in condition_order],\n",
    "            rotation=15,\n",
    "            ha=\"right\",\n",
    "        )\n",
    "        ax.set_title(signal.replace(\"_Baseline\", \"\").replace(\"_\", \" \"))\n",
    "        ax.grid(True, axis=\"y\", alpha=0.2)\n",
    "        ax.set_xlabel(\"Condition\")\n",
    "    axes[0].set_ylabel(metric_name.replace(\"_\", \" \").title())\n",
    "\n",
    "    handles, labels = axes[0].get_legend_handles_labels()\n",
    "    unique = dict(zip(labels, handles))\n",
    "    fig.legend(\n",
    "        unique.values(),\n",
    "        unique.keys(),\n",
    "        loc=\"upper center\",\n",
    "        ncol=min(len(unique), 6),\n",
    "        frameon=False,\n",
    "        bbox_to_anchor=(0.5, 1.05),\n",
    "    )\n",
    "    fig.tight_layout(rect=(0, 0, 1, 0.95))\n",
    "    fig.suptitle(f\"{metric_name.replace('_', ' ').title()} Comparison\", y=1.02)\n",
    "    return fig\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab90aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN SIGNAL METRIC ANALYSIS\n",
    "#---------------------------------------------------------------------------------------------------#\n",
    "\n",
    "mouse_signal_frames_by_day: Dict[str, Dict[str, pd.DataFrame]] = {}\n",
    "signal_metrics_by_day: Dict[str, pd.DataFrame] = {}\n",
    "signal_metrics_summary_by_day: Dict[str, pd.DataFrame] = {}\n",
    "signal_metrics_summary_pivot_by_day: Dict[str, pd.DataFrame] = {}\n",
    "per_day_output_dirs: Dict[str, Path] = {}\n",
    "\n",
    "def _prepare_frame_for_signals(df: pd.DataFrame) -> Optional[pd.DataFrame]:\n",
    "    if \"Time (s)\" not in df.columns:\n",
    "        return None\n",
    "    working = df.copy()\n",
    "    working[\"Time (s)\"] = pd.to_numeric(working[\"Time (s)\"], errors=\"coerce\")\n",
    "    working = working.dropna(subset=[\"Time (s)\"])\n",
    "    if working.empty:\n",
    "        return None\n",
    "    working = working.sort_values(\"Time (s)\")\n",
    "    working = working.set_index(\"Time (s)\")\n",
    "    working.index.name = \"Time (s)\"\n",
    "    working = working.apply(pd.to_numeric, errors=\"coerce\").dropna(how=\"all\")\n",
    "    if working.empty:\n",
    "        return None\n",
    "    return working\n",
    "\n",
    "if 'loaded_data' in locals() and loaded_data:\n",
    "    per_day_frames = defaultdict(lambda: defaultdict(list))\n",
    "    per_day_sources: Dict[str, set[Path]] = defaultdict(set)\n",
    "\n",
    "    for mouse_name, entry in loaded_data.items():\n",
    "        if not isinstance(entry, dict):\n",
    "            continue\n",
    "        records = entry.get(\"records\", [])\n",
    "        for record in records:\n",
    "            df = record.get(\"dataframe\")\n",
    "            if not isinstance(df, pd.DataFrame):\n",
    "                continue\n",
    "            prepared = _prepare_frame_for_signals(df)\n",
    "            if prepared is None:\n",
    "                continue\n",
    "            day_label = record.get(\"experiment_day\") or extract_experiment_day(\n",
    "                record.get(\"experiment_dir\")\n",
    "            )\n",
    "            per_day_frames[day_label][mouse_name].append(prepared)\n",
    "\n",
    "            experiment_dir = record.get(\"experiment_dir\")\n",
    "            aligned_dir = record.get(\"aligned_dir\")\n",
    "            if experiment_dir:\n",
    "                per_day_sources[day_label].add(Path(experiment_dir))\n",
    "            elif aligned_dir:\n",
    "                per_day_sources[day_label].add(Path(aligned_dir).parent.parent)\n",
    "\n",
    "    for day_label, mouse_map in per_day_frames.items():\n",
    "        consolidated: Dict[str, pd.DataFrame] = {}\n",
    "        for mouse, frames in mouse_map.items():\n",
    "            if not frames:\n",
    "                continue\n",
    "            combined = pd.concat(frames)\n",
    "            combined = combined.groupby(combined.index).mean().sort_index()\n",
    "            consolidated[mouse] = combined\n",
    "        if consolidated:\n",
    "            mouse_signal_frames_by_day[day_label] = consolidated\n",
    "            if per_day_sources.get(day_label):\n",
    "                per_day_output_dirs[day_label] = next(iter(per_day_sources[day_label]))\n",
    "\n",
    "results_available = locals().get(\"results\")\n",
    "if not mouse_signal_frames_by_day and isinstance(results_available, dict):\n",
    "    fallback_frames = results_available.get(\"mean_data_per_mouse\", {})\n",
    "    if fallback_frames:\n",
    "        mouse_signal_frames_by_day[\"combined\"] = fallback_frames\n",
    "        source_map = results_available.get(\"mouse_to_data_path\", {})\n",
    "        if isinstance(source_map, dict):\n",
    "            for mouse, path in source_map.items():\n",
    "                if path:\n",
    "                    per_day_output_dirs.setdefault(\"combined\", Path(path).parent.parent)\n",
    "\n",
    "if not mouse_signal_frames_by_day:\n",
    "    print(\"‚è≠Ô∏è  Skipping signal metric analysis (no loaded data available).\")\n",
    "else:\n",
    "    # Analysis window must extend through offset analysis period to capture all data\n",
    "    analysis_window = (POST_ALIGNMENT_WINDOW_START, OFFSET_ANALYSIS_END)\n",
    "    window_str = f\"[{analysis_window[0]:.2f}, {analysis_window[1]:.2f}] s\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"SIGNAL METRIC ANALYSIS (window = {window_str})\")\n",
    "    print(f\"Note: Main peak detection uses 0-{POST_ALIGNMENT_WINDOW_DURATION}s, offset analysis uses {OFFSET_ANALYSIS_START}-{OFFSET_ANALYSIS_END}s\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    file_prefix = _make_signal_metrics_file_prefix(event_name)\n",
    "    window_start = POST_ALIGNMENT_WINDOW_START + 2.0\n",
    "    window_end = POST_ALIGNMENT_WINDOW_START + 6.0\n",
    "\n",
    "    for day_label, mouse_frames in mouse_signal_frames_by_day.items():\n",
    "        if not mouse_frames:\n",
    "            continue\n",
    "\n",
    "        metrics_df, summary_df, summary_pivot = compute_signal_metrics_from_results(\n",
    "            mouse_frames,\n",
    "            SIGNAL_METRIC_COLUMNS,\n",
    "            analysis_window,\n",
    "        )\n",
    "\n",
    "        z560_window_stats = compute_signal_window_mean_per_mouse(\n",
    "            mouse_frames,\n",
    "            \"z_560_Baseline\",\n",
    "            window_start,\n",
    "            window_end,\n",
    "        )\n",
    "\n",
    "        if not z560_window_stats.empty:\n",
    "            z560_summary = pd.DataFrame(\n",
    "                [\n",
    "                    {\n",
    "                        \"signal\": \"z_560_Baseline\",\n",
    "                        \"metric\": \"mean_fluorescence_2_to_8s\",\n",
    "                        \"n\": int(z560_window_stats[\"mean\"].count()),\n",
    "                        \"mean\": float(z560_window_stats[\"mean\"].mean()),\n",
    "                        \"sem\": sem(z560_window_stats[\"mean\"]),\n",
    "                    }\n",
    "                ]\n",
    "            )\n",
    "            summary_df = pd.concat([summary_df, z560_summary], ignore_index=True)\n",
    "            mask = metrics_df[\"signal\"] == \"z_560_Baseline\"\n",
    "            metrics_df.loc[mask, \"mean_fluorescence_2_to_8s\"] = metrics_df.loc[\n",
    "                mask, \"mouse\"\n",
    "            ].map(z560_window_stats.set_index(\"mouse\")[\"mean\"])\n",
    "            print(\n",
    "                f\"{day_label}: z_560_Baseline mean fluorescence (2-8s post) = \"\n",
    "                f\"{z560_summary.at[0, 'mean']:.6f} ¬± {z560_summary.at[0, 'sem']:.6f} \"\n",
    "                f\"(SEM, n={int(z560_summary.at[0, 'n'])})\"\n",
    "            )\n",
    "        else:\n",
    "            print(f\"{day_label}: ‚ö†Ô∏è No z_560_Baseline data between 2 and 8 seconds post alignment.\")\n",
    "\n",
    "        if not summary_df.empty:\n",
    "            summary_df = summary_df.sort_values([\"signal\", \"metric\"]).drop_duplicates(\n",
    "                subset=[\"signal\", \"metric\"], keep=\"last\"\n",
    "            )\n",
    "            summary_pivot = summary_df.pivot(index=\"metric\", columns=\"signal\", values=[\"mean\", \"sem\"])\n",
    "\n",
    "        signal_metrics_by_day[day_label] = metrics_df\n",
    "        signal_metrics_summary_by_day[day_label] = summary_df\n",
    "        signal_metrics_summary_pivot_by_day[day_label] = summary_pivot\n",
    "\n",
    "        if metrics_df.empty:\n",
    "            print(f\"{day_label}: ‚ö†Ô∏è No signal metrics were computed. Check signal names and analysis window.\")\n",
    "            continue\n",
    "\n",
    "        n_mice = metrics_df[\"mouse\"].nunique()\n",
    "        n_signals = metrics_df[\"signal\"].nunique()\n",
    "        print(f\"{day_label}: ‚úÖ Computed signal metrics for {n_mice} mice across {n_signals} signals.\")\n",
    "\n",
    "        output_dir = per_day_output_dirs.get(day_label, main_data_dir)\n",
    "        experiment_day_label = day_label or extract_experiment_day(output_dir)\n",
    "\n",
    "        if SAVE_SIGNAL_METRICS:\n",
    "            per_mouse_path = save_signal_metrics_to_csv(\n",
    "                metrics_df,\n",
    "                summary_df,\n",
    "                output_dir,\n",
    "                file_prefix,\n",
    "                cohort_identifier,\n",
    "                experiment_day_label,\n",
    "            )\n",
    "            print(f\"{day_label}: Saved per-mouse metrics to {per_mouse_path}\")\n",
    "\n",
    "        if not summary_pivot.empty:\n",
    "            print(f\"\\n{day_label}: Signal metric summary (mean/sem):\")\n",
    "            display(summary_pivot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d9c42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SANITY CHECK: VISUALISE ONSET DETECTION FOR ONE MOUSE\n",
    "#---------------------------------------------------------------------------------------------------#\n",
    "def plot_onset_detection_sanity_check(\n",
    "    mouse_id: str,\n",
    "    signal_column: str,\n",
    "    window: Tuple[float, float],\n",
    "    mouse_frames: Dict[str, pd.DataFrame],\n",
    "    reference_label: str,\n",
    ") -> None:\n",
    "    if mouse_id not in mouse_frames:\n",
    "        print(f\"‚ö†Ô∏è Mouse '{mouse_id}' not found in the available frames: {sorted(mouse_frames.keys())}\")\n",
    "        return\n",
    "\n",
    "    mouse_df = mouse_frames[mouse_id]\n",
    "    if signal_column not in mouse_df.columns:\n",
    "        print(f\"‚ö†Ô∏è Signal '{signal_column}' not present for mouse '{mouse_id}'.\")\n",
    "        return\n",
    "\n",
    "    df_signal = _prepare_mouse_signal_frame(mouse_df, signal_column)\n",
    "    if df_signal.empty:\n",
    "        print(f\"‚ö†Ô∏è No data available for mouse '{mouse_id}' and signal '{signal_column}'.\")\n",
    "        return\n",
    "\n",
    "    window_mask = (df_signal[\"time\"] >= window[0]) & (df_signal[\"time\"] <= window[1])\n",
    "    window_df = df_signal.loc[window_mask]\n",
    "    if window_df.empty:\n",
    "        print(f\"‚ö†Ô∏è Window {window} yielded no data points for mouse '{mouse_id}'.\")\n",
    "        return\n",
    "\n",
    "    times = window_df[\"time\"].to_numpy(dtype=float)\n",
    "    values = window_df[\"value\"].to_numpy(dtype=float)\n",
    "    finite_mask = np.isfinite(times) & np.isfinite(values)\n",
    "    times = times[finite_mask]\n",
    "    values = values[finite_mask]\n",
    "    if values.size == 0:\n",
    "        print(f\"‚ö†Ô∏è No finite values available for plotting (mouse '{mouse_id}').\")\n",
    "        return\n",
    "\n",
    "    smooth_window = min(7, values.size)\n",
    "    if smooth_window > 1:\n",
    "        kernel = np.ones(smooth_window, dtype=float) / smooth_window\n",
    "        smoothed = np.convolve(values, kernel, mode=\"same\")\n",
    "    else:\n",
    "        smoothed = values.copy()\n",
    "\n",
    "    candidate = smoothed\n",
    "    candidate_std = float(np.nanstd(candidate)) if candidate.size else 0.0\n",
    "    prominence = max(0.05, 0.15 * candidate_std) if candidate_std > 0 else 0.05\n",
    "\n",
    "    peaks, _ = find_peaks(candidate, prominence=prominence)\n",
    "    if peaks.size == 0:\n",
    "        peaks, _ = find_peaks(candidate)\n",
    "    if peaks.size == 0:\n",
    "        peak_index = int(np.nanargmax(candidate))\n",
    "    else:\n",
    "        best_idx = int(np.argmax(candidate[peaks]))\n",
    "        peak_index = int(peaks[best_idx])\n",
    "    peak_index = max(0, min(peak_index, values.size - 1))\n",
    "\n",
    "    peak_value_raw = values[peak_index]\n",
    "    peak_value_smooth = candidate[peak_index]\n",
    "    peak_value = max(peak_value_raw, peak_value_smooth)\n",
    "\n",
    "    logistic_onset = _estimate_logistic_onset(times[: peak_index + 1], candidate[: peak_index + 1])\n",
    "    if logistic_onset is not None:\n",
    "        onset_time = float(logistic_onset)\n",
    "        onset_threshold = peak_value * 0.1\n",
    "    else:\n",
    "        onset_time = np.nan\n",
    "        onset_threshold = peak_value * 0.1\n",
    "        for idx in range(0, peak_index + 1):\n",
    "            current_value = candidate[idx]\n",
    "            if not np.isfinite(current_value):\n",
    "                continue\n",
    "            if current_value >= onset_threshold:\n",
    "                if idx == 0:\n",
    "                    onset_time = times[idx]\n",
    "                else:\n",
    "                    prev_time = times[idx - 1]\n",
    "                    prev_value = candidate[idx - 1]\n",
    "                    if not np.isfinite(prev_value) or prev_value == current_value:\n",
    "                        onset_time = times[idx]\n",
    "                    else:\n",
    "                        fraction = (onset_threshold - prev_value) / (current_value - prev_value)\n",
    "                        onset_time = prev_time + fraction * (times[idx] - prev_time)\n",
    "                break\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "    ax.plot(times, values, color=\"#1f77b4\", alpha=0.35, label=\"Raw\")\n",
    "    ax.plot(times, candidate, color=\"#1f77b4\", linewidth=2, label=\"Smoothed\")\n",
    "    ax.axhline(onset_threshold, color=\"orange\", linestyle=\"--\", label=\"10% peak threshold\")\n",
    "    ax.scatter(times[peak_index], candidate[peak_index], color=\"crimson\", marker=\"*\", s=120, label=\"Peak\")\n",
    "    if np.isfinite(onset_time):\n",
    "        ax.axvline(onset_time, color=\"green\", linestyle=\"--\", label=f\"Onset {onset_time:.3f}s\")\n",
    "    else:\n",
    "        ax.text(0.05, 0.9, \"Onset not found\", transform=ax.transAxes, color=\"red\", fontsize=12)\n",
    "\n",
    "    ax.set_title(f\"Onset sanity check | {reference_label}\\nMouse: {mouse_id} | Signal: {signal_column}\")\n",
    "    ax.set_xlabel(\"Time (s)\")\n",
    "    ax.set_ylabel(\"Baseline z-score\")\n",
    "    ax.grid(True, alpha=0.2)\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    unique = dict(zip(labels, handles))\n",
    "    ax.legend(unique.values(), unique.keys(), loc=\"upper right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if 'mouse_signal_frames_by_day' in locals() and mouse_signal_frames_by_day:\n",
    "    first_day_label = next(iter(mouse_signal_frames_by_day.keys()))\n",
    "    frames_for_sanity = mouse_signal_frames_by_day[first_day_label]\n",
    "    sanity_mouse = selected_mice[0] if selected_mice else next(iter(frames_for_sanity.keys()))\n",
    "    plot_onset_detection_sanity_check(\n",
    "        sanity_mouse,\n",
    "        'z_560_Baseline',\n",
    "        POST_ALIGNMENT_WINDOW,\n",
    "        frames_for_sanity,\n",
    "        reference_label=f\"{cohort_identifier} | {first_day_label}\",\n",
    "    )\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No mouse signal frames available for onset sanity check.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aaab5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIAGNOSTIC: EXPONENTIAL FIT AND RESIDUAL VISUALIZATION\n",
    "#---------------------------------------------------------------------------------------------------#\n",
    "def plot_exponential_fit_and_residuals_per_mouse(\n",
    "    mouse_id: str,\n",
    "    mouse_df: pd.DataFrame,\n",
    "    signal_column: str,\n",
    "    metrics: Dict[str, float],\n",
    "    cohort_id: str,\n",
    "    day_label: str,\n",
    ") -> go.Figure:\n",
    "    \"\"\"\n",
    "    Create diagnostic plot showing calcium trace, exponential fit, and residuals.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    mouse_id : str\n",
    "        Mouse identifier\n",
    "    mouse_df : pd.DataFrame\n",
    "        Mouse data with signal column (indexed by time)\n",
    "    signal_column : str\n",
    "        Signal column name (e.g., 'z_560_Baseline')\n",
    "    metrics : Dict[str, float]\n",
    "        Computed metrics containing peak, decay_tau1, etc.\n",
    "    cohort_id : str\n",
    "        Cohort identifier for title\n",
    "    day_label : str\n",
    "        Experiment day label for title\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    go.Figure\n",
    "        Plotly figure with calcium trace, fit, and residuals\n",
    "    \"\"\"\n",
    "    # Extract time and signal\n",
    "    if signal_column not in mouse_df.columns:\n",
    "        print(f\"‚ö†Ô∏è Signal '{signal_column}' not found for mouse {mouse_id}\")\n",
    "        return None\n",
    "    \n",
    "    times = pd.to_numeric(mouse_df.index, errors=\"coerce\")\n",
    "    values = pd.to_numeric(mouse_df[signal_column], errors=\"coerce\")\n",
    "    \n",
    "    # Create clean dataframe\n",
    "    df_clean = pd.DataFrame({\"time\": times, \"value\": values}).dropna()\n",
    "    if df_clean.empty:\n",
    "        print(f\"‚ö†Ô∏è No valid data for mouse {mouse_id}\")\n",
    "        return None\n",
    "    \n",
    "    df_clean = df_clean.sort_values(\"time\")\n",
    "    times_clean = df_clean[\"time\"].to_numpy()\n",
    "    values_clean = df_clean[\"value\"].to_numpy()\n",
    "    \n",
    "    # Get metrics\n",
    "    peak_value = metrics.get(\"peak\", np.nan)\n",
    "    decay_tau1 = metrics.get(\"decay_tau1\", np.nan)\n",
    "    onset_time = metrics.get(\"onset_time\", np.nan)\n",
    "    main_peak_residual_auc = metrics.get(\"main_peak_residual_auc\", np.nan)\n",
    "    offset_residual_auc = metrics.get(\"offset_residual_auc\", np.nan)\n",
    "    \n",
    "    # Find peak index\n",
    "    peak_mask = (times_clean >= POST_ALIGNMENT_WINDOW[0]) & (times_clean <= POST_ALIGNMENT_WINDOW[1])\n",
    "    if not peak_mask.any():\n",
    "        print(f\"‚ö†Ô∏è No data in analysis window for mouse {mouse_id}\")\n",
    "        return None\n",
    "    \n",
    "    window_values = values_clean[peak_mask]\n",
    "    window_times = times_clean[peak_mask]\n",
    "    peak_idx_window = np.argmax(window_values)\n",
    "    peak_time = window_times[peak_idx_window]\n",
    "    \n",
    "    # Estimate baseline and fit double-exponential\n",
    "    baseline = _estimate_baseline(times_clean, values_clean)\n",
    "    \n",
    "    # Find peak index in full data\n",
    "    peak_idx_full = np.argmin(np.abs(times_clean - peak_time))\n",
    "    \n",
    "    # Fit double-exponential decay\n",
    "    tau1, A1, tau2, A2 = _estimate_double_exponential_decay(\n",
    "        times_clean, values_clean, peak_idx_full, baseline, DECAY_FIT_START_OFFSET\n",
    "    )\n",
    "    \n",
    "    # Reconstruct double-exponential decay (from fit start time onward)\n",
    "    exponential_fit = np.full_like(times_clean, np.nan)\n",
    "    residuals = np.full_like(times_clean, np.nan)\n",
    "    \n",
    "    if np.isfinite(tau1) and tau1 > 0 and np.isfinite(A1):\n",
    "        # Only compute exponential from fit start time onward (peak + offset)\n",
    "        fit_start_time = peak_time + DECAY_FIT_START_OFFSET\n",
    "        post_fit_mask = times_clean >= fit_start_time\n",
    "        time_from_fit_start = times_clean[post_fit_mask] - fit_start_time\n",
    "        exponential_fit[post_fit_mask] = (\n",
    "            baseline + \n",
    "            A1 * np.exp(-time_from_fit_start / tau1) + \n",
    "            A2 * np.exp(-time_from_fit_start / tau2)\n",
    "        )\n",
    "        residuals[post_fit_mask] = values_clean[post_fit_mask] - exponential_fit[post_fit_mask]\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Invalid tau1 ({tau1}) or A1 ({A1}) for mouse {mouse_id}\")\n",
    "    \n",
    "    # Create figure with secondary y-axis\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=1,\n",
    "        shared_xaxes=True,\n",
    "        vertical_spacing=0.1,\n",
    "        subplot_titles=(\n",
    "            f\"Calcium Trace & Double-Exponential Fit\",\n",
    "            f\"Residuals (Observed - Expected)\"\n",
    "        ),\n",
    "        row_heights=[0.6, 0.4]\n",
    "    )\n",
    "    \n",
    "    # Top panel: Calcium trace and exponential fit\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=times_clean,\n",
    "            y=values_clean,\n",
    "            mode='lines',\n",
    "            name='Calcium trace',\n",
    "            line=dict(color='black', width=2),\n",
    "            showlegend=True\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    if np.any(np.isfinite(exponential_fit)):\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=times_clean,\n",
    "                y=exponential_fit,\n",
    "                mode='lines',\n",
    "                name='Double-exponential fit',\n",
    "                line=dict(color='red', width=2, dash='dash'),\n",
    "                showlegend=True\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "    \n",
    "    # Mark peak\n",
    "    if np.isfinite(peak_value):\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=[peak_time],\n",
    "                y=[peak_value],\n",
    "                mode='markers',\n",
    "                name='Peak',\n",
    "                marker=dict(color='crimson', size=12, symbol='star'),\n",
    "                showlegend=True\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "    \n",
    "    # Mark onset\n",
    "    if np.isfinite(onset_time):\n",
    "        fig.add_vline(\n",
    "            x=onset_time,\n",
    "            line=dict(color='green', width=2, dash='dot'),\n",
    "            annotation_text=f\"Onset: {onset_time:.2f}s\",\n",
    "            annotation_position=\"top\",\n",
    "            row=1, col=1\n",
    "        )\n",
    "    \n",
    "    # Mark fit start time (if offset from peak)\n",
    "    if DECAY_FIT_START_OFFSET != 0.0:\n",
    "        fit_start_time = peak_time + DECAY_FIT_START_OFFSET\n",
    "        fig.add_vline(\n",
    "            x=fit_start_time,\n",
    "            line=dict(color='purple', width=2, dash='dashdot'),\n",
    "            annotation_text=f\"Fit start: {fit_start_time:.2f}s\",\n",
    "            annotation_position=\"bottom\",\n",
    "            row=1, col=1\n",
    "        )\n",
    "    \n",
    "    # Highlight main peak window (used for fitting & main_peak_residual_auc)\n",
    "    fig.add_vrect(\n",
    "        x0=POST_ALIGNMENT_WINDOW[0],\n",
    "        x1=POST_ALIGNMENT_WINDOW[1],\n",
    "        fillcolor=\"lightgreen\",\n",
    "        opacity=0.15,\n",
    "        layer=\"below\",\n",
    "        line_width=0,\n",
    "        annotation_text=\"Main peak fit window\",\n",
    "        annotation_position=\"top right\",\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Highlight offset analysis window\n",
    "    fig.add_vrect(\n",
    "        x0=OFFSET_ANALYSIS_START,\n",
    "        x1=OFFSET_ANALYSIS_END,\n",
    "        fillcolor=\"lightblue\",\n",
    "        opacity=0.2,\n",
    "        layer=\"below\",\n",
    "        line_width=0,\n",
    "        annotation_text=\"Offset window\",\n",
    "        annotation_position=\"top left\",\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Bottom panel: Residuals\n",
    "    if np.any(np.isfinite(residuals)):\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=times_clean,\n",
    "                y=residuals,\n",
    "                mode='lines',\n",
    "                name='Residuals',\n",
    "                line=dict(color='blue', width=2),\n",
    "                showlegend=True\n",
    "            ),\n",
    "            row=2, col=1\n",
    "        )\n",
    "        \n",
    "        # Highlight positive residuals in main peak window (fit window)\n",
    "        main_peak_mask = (times_clean >= peak_time) & (times_clean <= POST_ALIGNMENT_WINDOW[1])\n",
    "        if main_peak_mask.any():\n",
    "            main_peak_times_plot = times_clean[main_peak_mask]\n",
    "            main_peak_residuals_plot = residuals[main_peak_mask]\n",
    "            main_positive_mask = main_peak_residuals_plot > 0\n",
    "            \n",
    "            if main_positive_mask.any():\n",
    "                fig.add_trace(\n",
    "                    go.Scatter(\n",
    "                        x=main_peak_times_plot[main_positive_mask],\n",
    "                        y=main_peak_residuals_plot[main_positive_mask],\n",
    "                        mode='lines',\n",
    "                        name='Main peak residual AUC',\n",
    "                        fill='tozeroy',\n",
    "                        fillcolor='rgba(0, 255, 0, 0.2)',\n",
    "                        line=dict(color='green', width=3),\n",
    "                        showlegend=True\n",
    "                    ),\n",
    "                    row=2, col=1\n",
    "                )\n",
    "        \n",
    "        # Highlight positive residuals in offset AUC window (only where AUC is calculated)\n",
    "        offset_auc_mask = (times_clean >= OFFSET_AUC_WINDOW[0]) & (times_clean <= OFFSET_AUC_WINDOW[1])\n",
    "        if offset_auc_mask.any():\n",
    "            offset_auc_times = times_clean[offset_auc_mask]\n",
    "            offset_auc_residuals = residuals[offset_auc_mask]\n",
    "            positive_mask = offset_auc_residuals > 0\n",
    "            \n",
    "            if positive_mask.any():\n",
    "                fig.add_trace(\n",
    "                    go.Scatter(\n",
    "                        x=offset_auc_times[positive_mask],\n",
    "                        y=offset_auc_residuals[positive_mask],\n",
    "                        mode='lines',\n",
    "                        name='Offset residual AUC',\n",
    "                        fill='tozeroy',\n",
    "                        fillcolor='rgba(255, 165, 0, 0.3)',\n",
    "                        line=dict(color='orange', width=3),\n",
    "                        showlegend=True\n",
    "                    ),\n",
    "                    row=2, col=1\n",
    "                )\n",
    "                \n",
    "                # Add marker showing the AUC value at midpoint of window\n",
    "                if np.isfinite(offset_residual_auc):\n",
    "                    midpoint_time = (OFFSET_AUC_WINDOW[0] + OFFSET_AUC_WINDOW[1]) / 2\n",
    "                    # Find the max positive residual in the window for marker placement\n",
    "                    max_positive_residual = float(np.max(offset_auc_residuals[positive_mask]))\n",
    "                    fig.add_trace(\n",
    "                        go.Scatter(\n",
    "                            x=[midpoint_time],\n",
    "                            y=[max_positive_residual],\n",
    "                            mode='markers+text',\n",
    "                            name=f'AUC={offset_residual_auc:.3f}',\n",
    "                            marker=dict(color='darkorange', size=14, symbol='star'),\n",
    "                            text=[f'AUC={offset_residual_auc:.3f}'],\n",
    "                            textposition='top center',\n",
    "                            textfont=dict(size=12, color='darkorange'),\n",
    "                            showlegend=True\n",
    "                        ),\n",
    "                        row=2, col=1\n",
    "                    )\n",
    "        \n",
    "        # Zero line\n",
    "        fig.add_hline(\n",
    "            y=0,\n",
    "            line=dict(color='gray', width=1, dash='dash'),\n",
    "            row=2, col=1\n",
    "        )\n",
    "        \n",
    "        # Highlight offset window\n",
    "        fig.add_vrect(\n",
    "            x0=OFFSET_ANALYSIS_START,\n",
    "            x1=OFFSET_ANALYSIS_END,\n",
    "            fillcolor=\"lightblue\",\n",
    "            opacity=0.2,\n",
    "            layer=\"below\",\n",
    "            line_width=0,\n",
    "            row=2, col=1\n",
    "        )\n",
    "        \n",
    "        # Highlight offset AUC integration window\n",
    "        fig.add_vrect(\n",
    "            x0=OFFSET_AUC_WINDOW[0],\n",
    "            x1=OFFSET_AUC_WINDOW[1],\n",
    "            fillcolor=\"yellow\",\n",
    "            opacity=0.15,\n",
    "            layer=\"below\",\n",
    "            line_width=0,\n",
    "            annotation_text=\"Offset AUC window\",\n",
    "            annotation_position=\"bottom left\",\n",
    "            row=2, col=1\n",
    "        )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_xaxes(title_text=\"Time (s)\", row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"z-score\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Residual z-score\", row=2, col=1)\n",
    "    \n",
    "    # Add title with metrics\n",
    "    fit_offset_text = f\" | Fit offset: +{DECAY_FIT_START_OFFSET:.2f}s\" if DECAY_FIT_START_OFFSET != 0.0 else \"\"\n",
    "    metrics_text = (\n",
    "        f\"Mouse: {mouse_id} | {cohort_id} | {day_label}<br>\"\n",
    "        f\"<sub>Peak: {peak_value:.3f} | Baseline: {baseline:.3f}{fit_offset_text}<br>\"\n",
    "        f\"Fast: œÑ1={tau1:.3f}s, A1={A1:.3f} | Slow: œÑ2={tau2:.3f}s, A2={A2:.3f}<br>\"\n",
    "        f\"Main peak residual AUC: {main_peak_residual_auc:.3f} | \"\n",
    "        f\"Offset AUC: {offset_residual_auc:.3f}</sub>\"\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title_text=metrics_text,\n",
    "        height=800,\n",
    "        showlegend=True,\n",
    "        legend=dict(x=1.02, y=1, xanchor='left', yanchor='top'),\n",
    "        hovermode='x unified'\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "def generate_diagnostic_plots_for_all_mice(\n",
    "    mouse_signal_frames: Dict[str, pd.DataFrame],\n",
    "    signal_metrics: pd.DataFrame,\n",
    "    signal_column: str,\n",
    "    cohort_id: str,\n",
    "    day_label: str,\n",
    "    show_plots: bool = True,\n",
    "    save_plots: bool = False,\n",
    "    output_dir: Optional[Path] = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate diagnostic plots for all mice showing exponential fits and residuals.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    mouse_signal_frames : Dict[str, pd.DataFrame]\n",
    "        Dictionary mapping mouse IDs to their signal dataframes\n",
    "    signal_metrics : pd.DataFrame\n",
    "        Computed metrics for each mouse/signal\n",
    "    signal_column : str\n",
    "        Signal column to analyze\n",
    "    cohort_id : str\n",
    "        Cohort identifier\n",
    "    day_label : str\n",
    "        Experiment day label\n",
    "    show_plots : bool\n",
    "        Whether to display plots (default: True)\n",
    "    save_plots : bool\n",
    "        Whether to save plots as HTML (default: False)\n",
    "    output_dir : Optional[Path]\n",
    "        Directory to save plots (required if save_plots=True)\n",
    "    \"\"\"\n",
    "    if not mouse_signal_frames:\n",
    "        print(\"‚ö†Ô∏è No mouse signal frames available for diagnostic plotting.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"DIAGNOSTIC PLOTS: Exponential Fit & Residuals\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Signal: {signal_column}\")\n",
    "    print(f\"Generating plots for {len(mouse_signal_frames)} mice...\")\n",
    "    print()\n",
    "    \n",
    "    if save_plots and output_dir is None:\n",
    "        print(\"‚ö†Ô∏è save_plots=True but no output_dir provided. Plots will not be saved.\")\n",
    "        save_plots = False\n",
    "    \n",
    "    if save_plots:\n",
    "        output_dir = Path(output_dir)\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    for mouse_id in sorted(mouse_signal_frames.keys()):\n",
    "        mouse_df = mouse_signal_frames[mouse_id]\n",
    "        \n",
    "        # Get metrics for this mouse\n",
    "        mouse_metrics_rows = signal_metrics[\n",
    "            (signal_metrics[\"mouse\"] == mouse_id) & \n",
    "            (signal_metrics[\"signal\"] == signal_column)\n",
    "        ]\n",
    "        \n",
    "        if mouse_metrics_rows.empty:\n",
    "            print(f\"‚ö†Ô∏è No metrics found for mouse {mouse_id}, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        # Convert first row to dict\n",
    "        metrics_dict = mouse_metrics_rows.iloc[0].to_dict()\n",
    "        \n",
    "        # Generate plot\n",
    "        fig = plot_exponential_fit_and_residuals_per_mouse(\n",
    "            mouse_id,\n",
    "            mouse_df,\n",
    "            signal_column,\n",
    "            metrics_dict,\n",
    "            cohort_id,\n",
    "            day_label,\n",
    "        )\n",
    "        \n",
    "        if fig is None:\n",
    "            continue\n",
    "        \n",
    "        # Display plot\n",
    "        if show_plots:\n",
    "            fig.show()\n",
    "        \n",
    "        # Save plot\n",
    "        if save_plots:\n",
    "            filename = f\"{sanitize_for_filename(cohort_id)}_{sanitize_for_filename(day_label)}_{mouse_id}_{signal_column}_fit_diagnostic.html\"\n",
    "            output_path = output_dir / filename\n",
    "            fig.write_html(str(output_path))\n",
    "            print(f\"‚úÖ Saved: {output_path.name}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Diagnostic plotting complete!\")\n",
    "\n",
    "\n",
    "# Run diagnostic plots if data available\n",
    "if 'mouse_signal_frames_by_day' in locals() and mouse_signal_frames_by_day:\n",
    "    if 'signal_metrics_by_day' in locals() and signal_metrics_by_day:\n",
    "        # Plot for ALL available days\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"GENERATING DIAGNOSTIC PLOTS FOR ALL DAYS\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        for day_label in mouse_signal_frames_by_day.keys():\n",
    "            print(f\"\\nProcessing day: {day_label}\")\n",
    "            mouse_frames = mouse_signal_frames_by_day[day_label]\n",
    "            metrics_df = signal_metrics_by_day.get(day_label)\n",
    "            \n",
    "            if metrics_df is not None and not metrics_df.empty:\n",
    "                # Determine output directory for this day\n",
    "                day_output_dir = per_day_output_dirs.get(day_label, main_data_dir) / \"diagnostic_plots\"\n",
    "                \n",
    "                generate_diagnostic_plots_for_all_mice(\n",
    "                    mouse_frames,\n",
    "                    metrics_df,\n",
    "                    'z_560_Baseline',\n",
    "                    cohort_identifier,\n",
    "                    day_label,\n",
    "                    show_plots=True,\n",
    "                    save_plots=True,\n",
    "                    output_dir=day_output_dir,\n",
    "                )\n",
    "                print(f\"‚úÖ Completed diagnostic plots for {day_label}\")\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è No signal metrics available for {day_label}\")\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"ALL DIAGNOSTIC PLOTS COMPLETE\")\n",
    "        print(f\"{'='*60}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è signal_metrics_by_day not available for diagnostic plotting.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No mouse signal frames available for diagnostic plotting.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8a3b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINAL CONDITION METRIC PLOT (WITH CONNECTING LINES)\n",
    "#---------------------------------------------------------------------------------------------------#\n",
    "def plot_condition_metric(\n",
    "    metric_name: str,\n",
    "    metric_df: pd.DataFrame,\n",
    "    condition_order: List[str],\n",
    "    signal_order: List[str],\n",
    "    mouse_colors: Dict[str, tuple],\n",
    "    connect_lines: bool = True,\n",
    "    no_line_conditions: Optional[List[str]] = None,\n",
    ") -> plt.Figure:\n",
    "    \"\"\"Generate scatter plots for a given metric across conditions.\n",
    "\n",
    "    When connect_lines is True, mice present in all conditions are shown with\n",
    "    connecting lines; otherwise individual points are plotted without lines.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    no_line_conditions : Optional[List[str]]\n",
    "        List of conditions that should always be plotted as individual points\n",
    "        without connecting lines (e.g., combined conditions like \"day3+day4\")\n",
    "    \"\"\"\n",
    "    styled_mouse_colors = OrderedDict((str(mouse), color) for mouse, color in mouse_colors.items())\n",
    "    if no_line_conditions is None:\n",
    "        no_line_conditions = []\n",
    "    \n",
    "    style_context = {\n",
    "        \"font.size\": 15,\n",
    "        \"font.family\": \"sans-serif\",\n",
    "        \"font.sans-serif\": [\"Arial\"],\n",
    "        \"axes.titlesize\": 15,\n",
    "        \"axes.labelsize\": 15,\n",
    "        \"legend.fontsize\": 10,\n",
    "        \"xtick.labelsize\": 15,\n",
    "        \"ytick.labelsize\": 15,\n",
    "    }\n",
    "\n",
    "    with plt.rc_context(style_context):\n",
    "        signals_present = [sig for sig in signal_order if sig in metric_df[\"signal\"].unique()]\n",
    "        width_cm = 5.0 * max(1, len(signals_present))\n",
    "        height_cm = 7.0\n",
    "        fig_width_in, fig_height_in = cm_to_inches(width_cm, height_cm)\n",
    "\n",
    "        if not signals_present:\n",
    "            fig, ax = plt.subplots(figsize=(fig_width_in, fig_height_in))\n",
    "            ax.text(\n",
    "                0.5,\n",
    "                0.5,\n",
    "                f\"No data available for metric {metric_name}\",\n",
    "                ha=\"center\",\n",
    "                va=\"center\",\n",
    "            )\n",
    "            ax.axis(\"off\")\n",
    "            return fig\n",
    "\n",
    "        fig, axes = plt.subplots(\n",
    "            1,\n",
    "            len(signals_present),\n",
    "            figsize=(fig_width_in, fig_height_in),\n",
    "            sharey=True,\n",
    "        )\n",
    "        if len(signals_present) == 1:\n",
    "            axes = [axes]\n",
    "\n",
    "        positions = np.arange(len(condition_order), dtype=float)\n",
    "        mean_legend_handle = None\n",
    "        mouse_handles: OrderedDict[str, object] = OrderedDict()\n",
    "        \n",
    "        # Separate conditions into paired (with lines) and unpaired (no lines)\n",
    "        paired_conditions = [c for c in condition_order if c not in no_line_conditions]\n",
    "        unpaired_conditions = [c for c in condition_order if c in no_line_conditions]\n",
    "\n",
    "        for ax, signal in zip(axes, signals_present):\n",
    "            subset = metric_df[metric_df[\"signal\"] == signal].copy()\n",
    "            if subset.empty:\n",
    "                ax.axis(\"off\")\n",
    "                continue\n",
    "\n",
    "            subset[\"condition\"] = pd.Categorical(\n",
    "                subset[\"condition\"], categories=condition_order, ordered=True\n",
    "            )\n",
    "            subset = subset.sort_values([\"mouse\", \"condition\"])\n",
    "\n",
    "            # Plot paired conditions (with connecting lines if connect_lines=True)\n",
    "            if connect_lines and paired_conditions:\n",
    "                for mouse, color in styled_mouse_colors.items():\n",
    "                    mouse_subset = subset[subset[\"mouse\"] == mouse]\n",
    "                    if mouse_subset.empty:\n",
    "                        continue\n",
    "                    # Filter to only paired conditions to avoid duplicate index issues\n",
    "                    mouse_subset_paired = mouse_subset[mouse_subset[\"condition\"].isin(paired_conditions)]\n",
    "                    if mouse_subset_paired.empty:\n",
    "                        continue\n",
    "                    mouse_series = mouse_subset_paired.set_index(\"condition\")[\"value\"]\n",
    "                    # Only plot paired conditions with lines\n",
    "                    paired_series = mouse_series.reindex(paired_conditions)\n",
    "                    if paired_series.isna().any():\n",
    "                        continue\n",
    "                    paired_values = paired_series.to_numpy(dtype=float)\n",
    "                    paired_positions = np.array([condition_order.index(c) for c in paired_conditions])\n",
    "                    line, = ax.plot(\n",
    "                        paired_positions,\n",
    "                        paired_values,\n",
    "                        marker=\"o\",\n",
    "                        linestyle=\"-\",\n",
    "                        color=color,\n",
    "                        linewidth=1.4,\n",
    "                        markersize=5,\n",
    "                        alpha=0.7,\n",
    "                        zorder=2,\n",
    "                    )\n",
    "                    if mouse not in mouse_handles:\n",
    "                        mouse_handles[mouse] = line\n",
    "            elif not connect_lines:\n",
    "                # Original behavior when connect_lines=False (unpaired design)\n",
    "                for mouse, color in styled_mouse_colors.items():\n",
    "                    mouse_subset = subset[subset[\"mouse\"] == mouse]\n",
    "                    if mouse_subset.empty:\n",
    "                        continue\n",
    "                    \n",
    "                    jitter = (hash((mouse, signal)) % 5 - 2) * 0.04\n",
    "                    \n",
    "                    # Iterate through rows directly to handle any duplicate condition entries\n",
    "                    for _, row in mouse_subset.iterrows():\n",
    "                        cond = row[\"condition\"]\n",
    "                        value = row[\"value\"]\n",
    "                        \n",
    "                        if cond not in condition_order:\n",
    "                            continue\n",
    "                        if pd.isna(value):\n",
    "                            continue\n",
    "                        \n",
    "                        cond_idx = condition_order.index(cond)\n",
    "                        point = ax.plot(\n",
    "                            positions[cond_idx] + jitter,\n",
    "                            float(value),\n",
    "                            marker=\"o\",\n",
    "                            linestyle=\"None\",\n",
    "                            color=color,\n",
    "                            markersize=6,\n",
    "                            alpha=0.8,\n",
    "                            zorder=2,\n",
    "                        )[0]\n",
    "                        if mouse not in mouse_handles:\n",
    "                            mouse_handles[mouse] = point\n",
    "            \n",
    "            # Plot unpaired conditions (no lines, just individual points)\n",
    "            # These conditions may have duplicate mouse entries (e.g., combined day3+day4)\n",
    "            if unpaired_conditions:\n",
    "                for mouse, color in styled_mouse_colors.items():\n",
    "                    mouse_subset = subset[subset[\"mouse\"] == mouse]\n",
    "                    if mouse_subset.empty:\n",
    "                        continue\n",
    "                    # Filter to only unpaired conditions\n",
    "                    mouse_subset_unpaired = mouse_subset[mouse_subset[\"condition\"].isin(unpaired_conditions)]\n",
    "                    if mouse_subset_unpaired.empty:\n",
    "                        continue\n",
    "                    \n",
    "                    jitter = (hash((mouse, signal)) % 5 - 2) * 0.04\n",
    "                    \n",
    "                    # Iterate through rows directly to handle duplicate condition entries\n",
    "                    for _, row in mouse_subset_unpaired.iterrows():\n",
    "                        cond = row[\"condition\"]\n",
    "                        value = row[\"value\"]\n",
    "                        \n",
    "                        if cond not in condition_order:\n",
    "                            continue\n",
    "                        if pd.isna(value):\n",
    "                            continue\n",
    "                        \n",
    "                        cond_idx = condition_order.index(cond)\n",
    "                        ax.plot(\n",
    "                            positions[cond_idx] + jitter,\n",
    "                            float(value),\n",
    "                            marker=\"o\",\n",
    "                            linestyle=\"None\",\n",
    "                            color=color,\n",
    "                            markersize=6,\n",
    "                            alpha=0.8,\n",
    "                            zorder=2,\n",
    "                        )\n",
    "\n",
    "            means = subset.groupby(\"condition\")[\"value\"].mean().reindex(condition_order)\n",
    "            sems = subset.groupby(\"condition\")[\"value\"].apply(_paired_sem).reindex(condition_order)\n",
    "            mean_values = means.to_numpy(dtype=float)\n",
    "            sem_values = sems.to_numpy(dtype=float)\n",
    "            sem_values = np.where(np.isfinite(sem_values), sem_values, 0.0)\n",
    "            valid_mask = np.isfinite(mean_values)\n",
    "            positions_valid = positions[valid_mask]\n",
    "            mean_values_valid = mean_values[valid_mask]\n",
    "            sem_values_valid = sem_values[valid_mask]\n",
    "            ax.fill_between(\n",
    "                positions_valid,\n",
    "                mean_values_valid - sem_values_valid,\n",
    "                mean_values_valid + sem_values_valid,\n",
    "                color=\"0.7\",\n",
    "                alpha=0.25,\n",
    "                zorder=3,\n",
    "            )\n",
    "            mean_line, = ax.plot(\n",
    "                positions_valid,\n",
    "                mean_values_valid,\n",
    "                color=\"black\",\n",
    "                linewidth=2.0,\n",
    "                marker=\"o\",\n",
    "                markersize=6,\n",
    "                zorder=4,\n",
    "            )\n",
    "            if mean_legend_handle is None:\n",
    "                mean_legend_handle = mean_line\n",
    "\n",
    "            condition_labels = [\n",
    "                subset[subset[\"condition\"] == cond][\"condition_label\"].iloc[0]\n",
    "                if not subset[subset[\"condition\"] == cond].empty\n",
    "                else cond\n",
    "                for cond in condition_order\n",
    "            ]\n",
    "\n",
    "            ax.set_xticks(positions)\n",
    "            ax.set_xticklabels(condition_labels, rotation=15, ha=\"right\")\n",
    "            ax.set_xlabel(\"Condition\", fontfamily=\"Arial\")\n",
    "            clean_title = signal.replace(\"_Baseline\", \"\").replace(\"_\", \" \")\n",
    "            ax.set_title(clean_title, fontfamily=\"Arial\")\n",
    "            ax.tick_params(axis=\"both\", labelsize=15)\n",
    "            for tick in ax.get_xticklabels() + ax.get_yticklabels():\n",
    "                tick.set_fontfamily(\"Arial\")\n",
    "            ax.grid(False)\n",
    "\n",
    "        axes[0].set_ylabel(metric_name.replace(\"_\", \" \").title(), fontfamily=\"Arial\")\n",
    "\n",
    "        legend_handles: List[object] = []\n",
    "        legend_labels: List[str] = []\n",
    "        if mouse_handles:\n",
    "            legend_handles.extend(mouse_handles.values())\n",
    "            legend_labels.extend(mouse_handles.keys())\n",
    "        if mean_legend_handle is not None:\n",
    "            legend_handles.append(mean_legend_handle)\n",
    "            legend_labels.append(\"Mean ¬± SEM\")\n",
    "\n",
    "        if legend_handles:\n",
    "            legend_columns = 1 if len(legend_labels) <= 10 else 2\n",
    "            legend = axes[0].legend(\n",
    "                legend_handles,\n",
    "                legend_labels,\n",
    "                loc=\"upper left\",\n",
    "                bbox_to_anchor=(1.02, 1.0),\n",
    "                frameon=False,\n",
    "                borderaxespad=0.0,\n",
    "                prop={\"family\": \"Arial\", \"size\": 10},\n",
    "                ncol=legend_columns,\n",
    "            )\n",
    "            legend._legend_box.sep = 4\n",
    "\n",
    "        fig.tight_layout(rect=(0, 0, 0.78, 1))\n",
    "        return fig\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756455a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONDITION COMPARISON (DAY 3 / DAY 4 / NO HALT)\n",
    "#---------------------------------------------------------------------------------------------------#\n",
    "if GENERATE_CONDITION_COMPARISON:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"CONDITION COMPARISON\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    TARGET_SIGNAL = \"z_560_Baseline\"\n",
    "    TARGET_METRICS = [\n",
    "        \"peak\",\n",
    "        \"onset_time\",\n",
    "        \"decay_tau1\",\n",
    "        \"mean_fluorescence_2_to_8s\",\n",
    "        \"main_peak_residual_auc\",\n",
    "        \"offset_residual_auc\",\n",
    "    ]\n",
    "    CONDITION_ORDER = [\"Apply_halt_day3\", \"Apply_halt_day4\", \"No_halt\"]\n",
    "    \n",
    "    # Metrics that should EXCLUDE \"No halt\" condition from plots and statistics\n",
    "    METRICS_EXCLUDE_NO_HALT = [\n",
    "        \"offset_residual_auc\",\n",
    "        \"decay_tau1\",\n",
    "        \"onset_time\",\n",
    "    ]\n",
    "    \n",
    "    print(f\"üéØ TARGET_METRICS to process: {TARGET_METRICS}\")\n",
    "    print(f\"üö´ METRICS_EXCLUDE_NO_HALT: {METRICS_EXCLUDE_NO_HALT}\")\n",
    "\n",
    "    condition_metrics_frames: Dict[str, pd.DataFrame] = {}\n",
    "    condition_sources_map: Dict[str, Dict[str, Path]] = {}\n",
    "\n",
    "    for condition_key, cfg in COMPARE_CALCIUM_METRICS_ACROSS_CONDITIONS.items():\n",
    "        metrics_df_condition, sources_condition = compute_condition_signal_metrics(\n",
    "            condition_key,\n",
    "            cfg,\n",
    "            selected_mice,\n",
    "            available_mice,\n",
    "        )\n",
    "        if metrics_df_condition.empty:\n",
    "            print(f\"‚ö†Ô∏è Skipping condition '{condition_key}' (no metrics computed).\")\n",
    "            continue\n",
    "        if TARGET_SIGNAL not in metrics_df_condition[\"signal\"].unique():\n",
    "            print(\n",
    "                f\"‚ö†Ô∏è Skipping condition '{condition_key}' (no '{TARGET_SIGNAL}' signal present).\"\n",
    "            )\n",
    "            continue\n",
    "        condition_metrics_frames[condition_key] = metrics_df_condition\n",
    "        condition_sources_map[condition_key] = sources_condition\n",
    "        print(\n",
    "            f\"‚úÖ Condition '{condition_key}' loaded: \"\n",
    "            f\"{metrics_df_condition['mouse'].nunique()} mice, \"\n",
    "            f\"{metrics_df_condition['signal'].nunique()} signals.\"\n",
    "        )\n",
    "\n",
    "    if len(condition_metrics_frames) < 2:\n",
    "        print(\"‚ö†Ô∏è At least two conditions with valid data are required for comparison. Skipping analysis.\")\n",
    "    else:\n",
    "        combined_metrics_wide = pd.concat(\n",
    "            condition_metrics_frames.values(), ignore_index=True\n",
    "        )\n",
    "\n",
    "        long_df = combined_metrics_wide.melt(\n",
    "            id_vars=[\n",
    "                \"mouse\",\n",
    "                \"signal\",\n",
    "                \"condition\",\n",
    "                \"condition_label\",\n",
    "                \"event_name\",\n",
    "            ],\n",
    "            value_vars=TARGET_METRICS,\n",
    "            var_name=\"metric\",\n",
    "            value_name=\"value\",\n",
    "        )\n",
    "        long_df = (\n",
    "            long_df.groupby(\n",
    "                [\"mouse\", \"signal\", \"condition\", \"condition_label\", \"event_name\", \"metric\"],\n",
    "                as_index=False,\n",
    "            )[\"value\"]\n",
    "            .mean()\n",
    "        )\n",
    "        long_df = long_df.dropna(subset=[\"value\"])\n",
    "        long_df = long_df[long_df[\"signal\"] == TARGET_SIGNAL].copy()\n",
    "        \n",
    "        print(f\"\\nüìä After melting and filtering for {TARGET_SIGNAL}:\")\n",
    "        print(f\"   Total rows: {len(long_df)}\")\n",
    "        print(f\"   Available metrics: {sorted(long_df['metric'].unique())}\")\n",
    "        print(f\"   Available conditions: {sorted(long_df['condition'].unique())}\")\n",
    "        print(f\"\\n   Metric value counts:\")\n",
    "        for metric in sorted(long_df['metric'].unique()):\n",
    "            metric_rows = len(long_df[long_df['metric'] == metric])\n",
    "            print(f\"      {metric}: {metric_rows} rows\")\n",
    "\n",
    "        if long_df.empty:\n",
    "            print(\"‚ö†Ô∏è No z_560_Baseline data available for comparison; skipping analysis.\")\n",
    "        else:\n",
    "            preferred_order = CONDITION_ORDER\n",
    "            available_conditions = [\n",
    "                cond for cond in preferred_order if cond in long_df[\"condition\"].unique()\n",
    "            ]\n",
    "\n",
    "            if len(available_conditions) < 2:\n",
    "                print(\"‚ö†Ô∏è Not enough conditions with data for comparison; skipping analysis.\")\n",
    "            else:\n",
    "                signal_order = [TARGET_SIGNAL]\n",
    "\n",
    "                all_mice_for_colors = sorted(\n",
    "                    {\n",
    "                        str(mouse)\n",
    "                        for df in condition_metrics_frames.values()\n",
    "                        for mouse in df[\"mouse\"].astype(str).unique()\n",
    "                    }\n",
    "                )\n",
    "                global_mouse_colors = assign_mouse_colors_consistent(all_mice_for_colors)\n",
    "\n",
    "                resolved_main_dir = Path(main_data_dir).resolve()\n",
    "                cohort_dir = resolved_main_dir.parent if resolved_main_dir.parent != resolved_main_dir else resolved_main_dir\n",
    "\n",
    "                comparison_output_dir = cohort_dir / \"calcium_analysis\"\n",
    "                comparison_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "                outputs_written = False\n",
    "\n",
    "                summary_rows: List[Dict[str, object]] = []\n",
    "                stats_records: List[pd.DataFrame] = []\n",
    "\n",
    "                for metric_name in TARGET_METRICS:\n",
    "                    print(f\"\\n{'='*50}\")\n",
    "                    print(f\"üîç PROCESSING METRIC: {metric_name}\")\n",
    "                    print(f\"{'='*50}\")\n",
    "                    \n",
    "                    # Determine which conditions to include for this metric\n",
    "                    if metric_name in METRICS_EXCLUDE_NO_HALT:\n",
    "                        # Exclude \"No_halt\" condition for specific metrics\n",
    "                        condition_subset = [\n",
    "                            cond for cond in preferred_order \n",
    "                            if cond in long_df[\"condition\"].unique() and cond != \"No_halt\"\n",
    "                        ]\n",
    "                        print(f\"üìä Metric '{metric_name}': excluding 'No_halt' condition from analysis\")\n",
    "                    else:\n",
    "                        condition_subset = [\n",
    "                            cond for cond in preferred_order if cond in long_df[\"condition\"].unique()\n",
    "                        ]\n",
    "                    \n",
    "                    print(f\"   Available conditions in long_df: {sorted(long_df['condition'].unique())}\")\n",
    "                    print(f\"   Condition subset for analysis: {condition_subset}\")\n",
    "                    \n",
    "                    if len(condition_subset) < 2:\n",
    "                        print(\n",
    "                            f\"‚ö†Ô∏è Metric '{metric_name}' does not have enough conditions for comparison; skipping.\"\n",
    "                        )\n",
    "                        continue\n",
    "\n",
    "                    metric_long_full = long_df[\n",
    "                        (long_df[\"metric\"] == metric_name)\n",
    "                        & (long_df[\"condition\"].isin(condition_subset))\n",
    "                    ].copy()\n",
    "                    \n",
    "                    print(f\"   Rows in metric_long_full before processing: {len(metric_long_full)}\")\n",
    "                    print(f\"   Unique mice: {sorted(metric_long_full['mouse'].unique()) if not metric_long_full.empty else 'None'}\")\n",
    "                    \n",
    "                    # For offset_residual_auc only: create combined day3+day4 condition\n",
    "                    add_combined_column = False\n",
    "                    if metric_name == \"offset_residual_auc\":\n",
    "                        print(f\"   üéØ This is offset_residual_auc - checking for combined column...\")\n",
    "                        print(f\"   'Apply_halt_day3' in subset: {'Apply_halt_day3' in condition_subset}\")\n",
    "                        print(f\"   'Apply_halt_day4' in subset: {'Apply_halt_day4' in condition_subset}\")\n",
    "                        \n",
    "                        if \"Apply_halt_day3\" in condition_subset and \"Apply_halt_day4\" in condition_subset:\n",
    "                            # Create combined condition with all day3 and day4 data\n",
    "                            combined_data = metric_long_full[\n",
    "                                metric_long_full[\"condition\"].isin([\"Apply_halt_day3\", \"Apply_halt_day4\"])\n",
    "                            ].copy()\n",
    "                            print(f\"   ‚úÖ Creating combined column with {len(combined_data)} data points\")\n",
    "                            combined_data[\"condition\"] = \"Apply_halt_day3+day4\"\n",
    "                            combined_data[\"condition_label\"] = \"Apply halt day 3+4\"\n",
    "                            \n",
    "                            # Append combined data to metric_long_full\n",
    "                            metric_long_full = pd.concat([metric_long_full, combined_data], ignore_index=True)\n",
    "                            \n",
    "                            # Add combined condition to subset (at the end)\n",
    "                            condition_subset = condition_subset + [\"Apply_halt_day3+day4\"]\n",
    "                            add_combined_column = True\n",
    "                            print(f\"   üìä Added combined 'day3+day4' column\")\n",
    "                            print(f\"   Updated condition_subset: {condition_subset}\")\n",
    "                        else:\n",
    "                            print(f\"   ‚ö†Ô∏è Cannot create combined column - missing day3 or day4\")\n",
    "                    \n",
    "                    if metric_long_full.empty:\n",
    "                        print(f\"‚ö†Ô∏è No data for metric '{metric_name}', skipping.\")\n",
    "                        continue\n",
    "                    metric_long_full[\"mouse\"] = metric_long_full[\"mouse\"].astype(str)\n",
    "                    \n",
    "                    print(f\"   Final rows in metric_long_full: {len(metric_long_full)}\")\n",
    "\n",
    "                    mice_by_condition = {\n",
    "                        cond: set(metric_long_full.loc[metric_long_full[\"condition\"] == cond, \"mouse\"])\n",
    "                        for cond in condition_subset\n",
    "                    }\n",
    "                    common_mice = set.intersection(*(mice_by_condition[cond] for cond in condition_subset)) if mice_by_condition else set()\n",
    "                    common_mice_list = sorted(common_mice)\n",
    "                    counts_equal = len({len(mice_by_condition[cond]) for cond in condition_subset if mice_by_condition[cond]}) == 1\n",
    "                    has_complete_overlap = (\n",
    "                        bool(common_mice_list)\n",
    "                        and counts_equal\n",
    "                        and all(len(common_mice_list) == len(mice_by_condition[cond]) for cond in condition_subset)\n",
    "                    )\n",
    "\n",
    "                    if has_complete_overlap:\n",
    "                        analysis_mode = \"paired\"\n",
    "                        metric_long = metric_long_full[metric_long_full[\"mouse\"].isin(common_mice_list)].copy()\n",
    "                    else:\n",
    "                        analysis_mode = \"unpaired\"\n",
    "                        metric_long = metric_long_full.copy()\n",
    "                    paired_mice_count = len(common_mice_list)\n",
    "\n",
    "                    if metric_long.empty:\n",
    "                        print(f\"‚ö†Ô∏è After alignment, no usable data for metric '{metric_name}'. Skipping.\")\n",
    "                        continue\n",
    "\n",
    "                    condition_label_map = (\n",
    "                        metric_long.drop_duplicates(subset=[\"condition\", \"condition_label\"])\n",
    "                        .set_index(\"condition\")[\"condition_label\"]\n",
    "                        .to_dict()\n",
    "                    )\n",
    "                    metric_long[\"condition_label\"] = metric_long[\"condition\"].map(condition_label_map)\n",
    "                    metric_long[\"condition\"] = pd.Categorical(\n",
    "                        metric_long[\"condition\"], categories=condition_subset, ordered=True\n",
    "                    )\n",
    "\n",
    "                    mouse_colors = OrderedDict(\n",
    "                        (mouse, global_mouse_colors[mouse])\n",
    "                        for mouse in global_mouse_colors\n",
    "                        if mouse in metric_long[\"mouse\"].unique()\n",
    "                    )\n",
    "\n",
    "                    if not mouse_colors:\n",
    "                        print(\n",
    "                            f\"‚ö†Ô∏è No color assignments available for metric '{metric_name}'. Skipping.\"\n",
    "                        )\n",
    "                        continue\n",
    "\n",
    "                    print(\n",
    "                        f\"Analyzing metric '{metric_name}' using {analysis_mode} design across conditions: {condition_subset}\"\n",
    "                    )\n",
    "\n",
    "                    mode_notes = []\n",
    "                    if analysis_mode == \"paired\":\n",
    "                        mode_notes.append(\n",
    "                            \"Paired design: restricted to mice present in every condition.\"\n",
    "                        )\n",
    "                    else:\n",
    "                        mode_notes.append(\n",
    "                            \"Unpaired design: mouse rosters differ across conditions.\"\n",
    "                        )\n",
    "\n",
    "                    # For statistics, exclude combined conditions (which have duplicate mice)\n",
    "                    stats_condition_subset = [c for c in condition_subset if \"+\" not in c]\n",
    "                    stats_metric_long = metric_long[metric_long[\"condition\"].isin(stats_condition_subset)].copy()\n",
    "                    \n",
    "                    if add_combined_column:\n",
    "                        print(f\"   üìä Statistics will use conditions: {stats_condition_subset}\")\n",
    "                        print(f\"   üìä (excluding combined column from stats to avoid duplicate mice)\")\n",
    "                        mode_notes.append(\n",
    "                            f\"Combined condition '{[c for c in condition_subset if '+' in c]}' \"\n",
    "                            \"included in plot and descriptive statistics but excluded from \"\n",
    "                            \"inferential tests (ANOVA/pairwise) to avoid duplicate mouse entries.\"\n",
    "                        )\n",
    "                    \n",
    "                    summary_df, pairwise_df, analysis_notes = compute_repeated_measures_stats(\n",
    "                        stats_metric_long,\n",
    "                        stats_condition_subset,\n",
    "                        signal_order,\n",
    "                        metric_name,\n",
    "                    )\n",
    "                    analysis_notes = mode_notes + analysis_notes\n",
    "\n",
    "                    # Determine which conditions should not have connecting lines\n",
    "                    no_line_conditions_for_plot = []\n",
    "                    if add_combined_column:\n",
    "                        no_line_conditions_for_plot = [\"Apply_halt_day3+day4\"]\n",
    "                        print(f\"   üìä Will plot with no_line_conditions: {no_line_conditions_for_plot}\")\n",
    "                    \n",
    "                    print(f\"   üé® Creating plot for {metric_name}...\")\n",
    "                    fig = plot_condition_metric(\n",
    "                        metric_name,\n",
    "                        metric_long,\n",
    "                        condition_subset,\n",
    "                        signal_order,\n",
    "                        mouse_colors,\n",
    "                        connect_lines=(analysis_mode == \"paired\"),\n",
    "                        no_line_conditions=no_line_conditions_for_plot,\n",
    "                    )\n",
    "\n",
    "                    cohort_part = sanitize_for_filename(cohort_identifier, \"cohort\")\n",
    "                    day_part = sanitize_for_filename(experiment_day, \"day\")\n",
    "                    metric_part = sanitize_for_filename(metric_name, metric_name)\n",
    "\n",
    "                    plot_filename = (\n",
    "                        comparison_output_dir\n",
    "                        / f\"{cohort_part}_{day_part}_{metric_part}_{analysis_mode}_condition_comparison.pdf\"\n",
    "                    )\n",
    "                    print(f\"   üíæ Saving plot to: {plot_filename}\")\n",
    "                    fig.savefig(plot_filename, dpi=300, bbox_inches=\"tight\")\n",
    "                    plt.close(fig)\n",
    "                    print(f\"   ‚úÖ Plot saved successfully\")\n",
    "\n",
    "                    plot_data_path = (\n",
    "                        comparison_output_dir\n",
    "                        / f\"{cohort_part}_{day_part}_{metric_part}_{analysis_mode}_condition_comparison_plot_data.csv\"\n",
    "                    )\n",
    "                    metric_long.sort_values(\n",
    "                        [\"signal\", \"mouse\", \"condition\"]\n",
    "                    ).to_csv(plot_data_path, index=False)\n",
    "\n",
    "                    stats_frames = []\n",
    "                    if not summary_df.empty:\n",
    "                        stats_frames.append(summary_df)\n",
    "                    if not pairwise_df.empty:\n",
    "                        stats_frames.append(pairwise_df)\n",
    "                    if analysis_notes:\n",
    "                        stats_frames.append(\n",
    "                            pd.DataFrame(\n",
    "                                {\n",
    "                                    \"analysis\": [\"note\"] * len(analysis_notes),\n",
    "                                    \"metric\": [metric_name] * len(analysis_notes),\n",
    "                                    \"note\": analysis_notes,\n",
    "                                }\n",
    "                            )\n",
    "                        )\n",
    "\n",
    "                    if stats_frames:\n",
    "                        stats_output = pd.concat(stats_frames, ignore_index=True)\n",
    "                    else:\n",
    "                        stats_output = pd.DataFrame(\n",
    "                            {\n",
    "                                \"analysis\": [\"note\"],\n",
    "                                \"metric\": [metric_name],\n",
    "                                \"note\": [\"No statistics computed.\"],\n",
    "                            }\n",
    "                        )\n",
    "\n",
    "                    stats_output[\"cohort\"] = cohort_identifier\n",
    "                    stats_output[\"experiment_day\"] = experiment_day\n",
    "                    stats_output[\"analysis_mode\"] = analysis_mode\n",
    "                    stats_output[\"paired_mice_n\"] = paired_mice_count\n",
    "                    stats_output[\"unique_mice_total\"] = metric_long[\"mouse\"].nunique()\n",
    "                    stats_output_path = (\n",
    "                        comparison_output_dir\n",
    "                        / f\"{cohort_part}_{day_part}_{metric_part}_{analysis_mode}_condition_comparison_stats.csv\"\n",
    "                    )\n",
    "                    stats_output.to_csv(stats_output_path, index=False)\n",
    "\n",
    "                    stats_records.append(\n",
    "                        stats_output.assign(\n",
    "                            plot_filename=plot_filename.name,\n",
    "                            plot_data_filename=plot_data_path.name,\n",
    "                            stats_filename=stats_output_path.name,\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                    print(\n",
    "                        f\"üìÑ Metric '{metric_name}': saved plot ({plot_filename.name}), \"\n",
    "                        f\"data ({plot_data_path.name}), stats ({stats_output_path.name})\"\n",
    "                    )\n",
    "\n",
    "                    for cond in condition_subset:\n",
    "                        cond_values = metric_long.loc[\n",
    "                            metric_long[\"condition\"] == cond, \"value\"\n",
    "                        ]\n",
    "                        n_obs = int(cond_values.count())\n",
    "                        mean_val = float(cond_values.mean()) if n_obs else np.nan\n",
    "                        if n_obs > 1:\n",
    "                            sem_val = float(cond_values.std(ddof=1) / np.sqrt(n_obs))\n",
    "                        else:\n",
    "                            sem_val = 0.0 if n_obs == 1 else np.nan\n",
    "                        summary_rows.append(\n",
    "                            {\n",
    "                                \"metric\": metric_name,\n",
    "                                \"condition\": cond,\n",
    "                                \"condition_label\": condition_label_map.get(cond, cond),\n",
    "                                \"mean\": mean_val,\n",
    "                                \"sem\": sem_val,\n",
    "                                \"n\": n_obs,\n",
    "                            }\n",
    "                        )\n",
    "\n",
    "                    outputs_written = True\n",
    "\n",
    "                if outputs_written:\n",
    "                    summary_table = pd.DataFrame(summary_rows)\n",
    "                    if not summary_table.empty:\n",
    "                        resolved_main_dir = Path(main_data_dir).resolve()\n",
    "                        cohort_dir = (\n",
    "                            resolved_main_dir.parent\n",
    "                            if resolved_main_dir.parent != resolved_main_dir\n",
    "                            else resolved_main_dir\n",
    "                        )\n",
    "                        comparison_output_dir = cohort_dir / \"calcium_analysis\"\n",
    "                        comparison_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "                        comparison_summary_path = (\n",
    "                            comparison_output_dir\n",
    "                            / f\"{sanitize_for_filename(cohort_identifier)}_{sanitize_for_filename(experiment_day)}_z560_metric_summary.csv\"\n",
    "                        )\n",
    "                        summary_table.sort_values([\"metric\", \"condition\"]).to_csv(\n",
    "                            comparison_summary_path, index=False\n",
    "                        )\n",
    "                        print(f\"‚úÖ Summary table saved to {comparison_summary_path}\")\n",
    "                    if stats_records:\n",
    "                        combined_stats = pd.concat(stats_records, ignore_index=True)\n",
    "                        combined_stats_path = (\n",
    "                            comparison_output_dir\n",
    "                            / f\"{sanitize_for_filename(cohort_identifier)}_{sanitize_for_filename(experiment_day)}_z560_condition_stats_all.csv\"\n",
    "                        )\n",
    "                        combined_stats.to_csv(combined_stats_path, index=False)\n",
    "                        print(f\"‚úÖ Combined stats saved to {combined_stats_path}\")\n",
    "                    print(\n",
    "                        f\"‚úÖ Calcium analysis outputs saved to: {comparison_output_dir}\"\n",
    "                    )\n",
    "                else:\n",
    "                    print(\"‚ö†Ô∏è No condition comparison outputs were generated.\")\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è  Condition comparison disabled (GENERATE_CONDITION_COMPARISON=False)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1618acab",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# GRAND AVERAGE VS PER-MOUSE PEAK OVERLAY\n",
    "#---------------------------------------------------------------------------------------------------#\n",
    "def plot_grand_average_vs_peaks(\n",
    "    grand_mean: pd.Series,\n",
    "    grand_sem: pd.Series,\n",
    "    per_mouse_metrics: pd.DataFrame,\n",
    "    signal: str,\n",
    "    condition_label: str,\n",
    "    cohort_identifier: str,\n",
    "    experiment_day: str,\n",
    "    output_dir: Path,\n",
    "    reference_time: Optional[float] = None,\n",
    ") -> Optional[Path]:\n",
    "    \"\"\"Plot the grand-average trace with per-mouse peak overlay and save to disk.\"\"\"\n",
    "    if grand_mean is None or grand_mean.empty:\n",
    "        print(f\"‚ö†Ô∏è Grand-average data unavailable for signal '{signal}'.\")\n",
    "        return None\n",
    "\n",
    "    if grand_sem is None or grand_sem.empty:\n",
    "        grand_sem = pd.Series(0.0, index=grand_mean.index)\n",
    "\n",
    "    peaks = (\n",
    "        per_mouse_metrics[per_mouse_metrics[\"signal\"] == signal][\"peak\"].apply(pd.to_numeric, errors=\"coerce\")\n",
    "        if \"signal\" in per_mouse_metrics.columns\n",
    "        else pd.Series(dtype=float)\n",
    "    )\n",
    "    peaks = peaks.dropna()\n",
    "    if peaks.empty:\n",
    "        print(f\"‚ö†Ô∏è No per-mouse peak values available for signal '{signal}'.\")\n",
    "        return None\n",
    "\n",
    "    peak_mean = float(peaks.mean())\n",
    "    peak_sem = float(peaks.std(ddof=1) / np.sqrt(len(peaks))) if len(peaks) > 1 else 0.0\n",
    "\n",
    "    time_points = pd.to_numeric(grand_mean.index, errors=\"coerce\")\n",
    "    values = pd.to_numeric(grand_mean.values, errors=\"coerce\")\n",
    "    sem_values = pd.to_numeric(grand_sem.reindex(grand_mean.index).fillna(0).values, errors=\"coerce\")\n",
    "\n",
    "    valid_mask = np.isfinite(time_points) & np.isfinite(values) & np.isfinite(sem_values)\n",
    "    time_points = time_points[valid_mask]\n",
    "    values = values[valid_mask]\n",
    "    sem_values = sem_values[valid_mask]\n",
    "\n",
    "    if time_points.size == 0:\n",
    "        print(f\"‚ö†Ô∏è Grand-average data for signal '{signal}' contains no finite values.\")\n",
    "        return None\n",
    "\n",
    "    if reference_time is None:\n",
    "        peak_idx = int(np.argmax(values))\n",
    "        reference_time = float(time_points[peak_idx])\n",
    "\n",
    "    jitter = np.linspace(-0.15, 0.15, len(peaks)) if len(peaks) > 1 else np.array([0.0])\n",
    "    scatter_x = reference_time + jitter\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    ax.plot(time_points, values, color=\"black\", linewidth=2, label=\"Grand average\")\n",
    "    ax.fill_between(time_points, values - sem_values, values + sem_values, color=\"grey\", alpha=0.25, label=\"Grand avg ¬± SEM\")\n",
    "\n",
    "    band_bottom = peak_mean - peak_sem\n",
    "    band_top = peak_mean + peak_sem\n",
    "    ax.axhspan(band_bottom, band_top, color=\"#d62728\", alpha=0.15, label=\"Per-mouse peak mean ¬± SEM\")\n",
    "    ax.axhline(peak_mean, color=\"#d62728\", linestyle=\"--\", linewidth=2)\n",
    "    ax.scatter(scatter_x, peaks.values, color=\"#d62728\", edgecolors=\"black\", linewidths=0.8, s=55, zorder=5, label=\"Per-mouse peaks\")\n",
    "\n",
    "    annotation = (\n",
    "        f\"Per-mouse peak mean = {peak_mean:.3f}\\n\"\n",
    "        f\"SEM = {peak_sem:.3f}\\n\"\n",
    "        f\"n = {len(peaks)}\"\n",
    "    )\n",
    "    ax.text(\n",
    "        0.02,\n",
    "        0.98,\n",
    "        annotation,\n",
    "        transform=ax.transAxes,\n",
    "        ha=\"left\",\n",
    "        va=\"top\",\n",
    "        fontsize=10,\n",
    "        bbox=dict(boxstyle=\"round\", facecolor=\"white\", alpha=0.85),\n",
    "    )\n",
    "\n",
    "    ax.set_title(f\"{signal.replace('_', ' ')} | {condition_label}\")\n",
    "    ax.set_xlabel(\"Time (s)\")\n",
    "    ax.set_ylabel(\"z-score\")\n",
    "    ax.grid(True, alpha=0.2)\n",
    "\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    unique = dict(zip(labels, handles))\n",
    "    ax.legend(unique.values(), unique.keys(), loc=\"lower right\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    cohort_part = sanitize_for_filename(cohort_identifier, \"cohort\")\n",
    "    day_part = sanitize_for_filename(experiment_day, \"day\")\n",
    "    signal_part = sanitize_for_filename(signal, signal)\n",
    "    condition_part = sanitize_for_filename(condition_label, \"condition\")\n",
    "    output_path = output_dir / f\"{cohort_part}_{day_part}_{signal_part}_{condition_part}_grand_vs_peaks.pdf\"\n",
    "    fig.savefig(output_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "    print(f\"‚úÖ Grand-average vs peaks plot saved to: {output_path}\")\n",
    "    return output_path\n",
    "\n",
    "\n",
    "def _compute_grand_average_for_condition(\n",
    "    condition_key: str,\n",
    "    target_signal: str,\n",
    "    target_mice: Optional[List[str]] = None,\n",
    ") -> Tuple[Optional[pd.Series], Optional[pd.Series]]:\n",
    "    cfg = COMPARE_CALCIUM_METRICS_ACROSS_CONDITIONS.get(condition_key)\n",
    "    if cfg is None:\n",
    "        print(f\"‚ö†Ô∏è Unknown condition '{condition_key}' ‚Äî skipping grand-average overlay.\")\n",
    "        return None, None\n",
    "\n",
    "    data_dirs = cfg.get(\"data_dirs\", [])\n",
    "    event_name_cond = cfg.get(\"event_name\")\n",
    "    if not data_dirs:\n",
    "        print(f\"‚ö†Ô∏è No data directories configured for condition '{condition_key}'.\")\n",
    "        return None, None\n",
    "\n",
    "    mice_subset = []\n",
    "    if target_mice:\n",
    "        mice_subset = sorted({str(m) for m in target_mice})\n",
    "    elif 'condition_metrics_frames' in locals():\n",
    "        metrics_frame = condition_metrics_frames.get(condition_key)\n",
    "        if metrics_frame is not None and not metrics_frame.empty:\n",
    "            mice_subset = sorted(metrics_frame['mouse'].astype(str).unique())\n",
    "\n",
    "    loaded = load_aligned_data(\n",
    "        data_dirs,\n",
    "        event_name_cond,\n",
    "        mice_subset,\n",
    "        available_mice,\n",
    "    )\n",
    "    if not loaded:\n",
    "        print(f\"‚ö†Ô∏è Unable to load aligned data for condition '{condition_key}'.\")\n",
    "        return None, None\n",
    "\n",
    "    frames, _ = extract_mouse_dataframes_from_loaded(\n",
    "        loaded,\n",
    "        signal_columns=[target_signal],\n",
    "    )\n",
    "    if not frames:\n",
    "        print(f\"‚ö†Ô∏è No aligned traces available for condition '{condition_key}'.\")\n",
    "        return None, None\n",
    "\n",
    "    series_list = []\n",
    "    for mouse_id, df in frames.items():\n",
    "        if target_signal not in df.columns:\n",
    "            continue\n",
    "        series = df[target_signal].astype(float).dropna()\n",
    "        if series.empty:\n",
    "            continue\n",
    "        if series.index.duplicated().any():\n",
    "            series = series.groupby(series.index).mean()\n",
    "        series.name = str(mouse_id)\n",
    "        series_list.append(series)\n",
    "\n",
    "    if not series_list:\n",
    "        print(f\"‚ö†Ô∏è Condition '{condition_key}' lacks usable '{target_signal}' traces.\")\n",
    "        return None, None\n",
    "\n",
    "    combined = pd.concat(series_list, axis=1)\n",
    "    combined = combined.sort_index()\n",
    "\n",
    "    grand_mean = combined.mean(axis=1)\n",
    "    grand_sem = combined.sem(axis=1)\n",
    "    if grand_sem.isna().all():\n",
    "        grand_sem = pd.Series(0.0, index=grand_mean.index)\n",
    "    else:\n",
    "        grand_sem = grand_sem.fillna(0.0)\n",
    "\n",
    "    return grand_mean, grand_sem\n",
    "\n",
    "\n",
    "if 'condition_metrics_frames' not in locals() or not condition_metrics_frames:\n",
    "    print(\"‚ö†Ô∏è Condition metrics unavailable; run the comparison cell before plotting overlays.\")\n",
    "else:\n",
    "    target_signal = 'z_560_Baseline'\n",
    "    resolved_main_dir = Path(main_data_dir).resolve()\n",
    "    cohort_dir = resolved_main_dir.parent if resolved_main_dir.parent != resolved_main_dir else resolved_main_dir\n",
    "    overlay_output_dir = cohort_dir / 'calcium_analysis'\n",
    "    overlay_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    apply_candidates = [\n",
    "        key for key in (\"Apply_halt_day4\", \"Apply_halt_day3\")\n",
    "        if key in condition_metrics_frames\n",
    "    ]\n",
    "    overlay_plan: List[str] = []\n",
    "    if apply_candidates:\n",
    "        overlay_plan.append(apply_candidates[0])\n",
    "    if 'No_halt' in condition_metrics_frames:\n",
    "        overlay_plan.append('No_halt')\n",
    "\n",
    "    if not overlay_plan:\n",
    "        print(\"‚ö†Ô∏è No suitable conditions found for grand-average overlays.\")\n",
    "    else:\n",
    "        for condition_key in overlay_plan:\n",
    "            per_mouse_metrics = condition_metrics_frames[condition_key]\n",
    "            if per_mouse_metrics is None or per_mouse_metrics.empty:\n",
    "                print(f\"‚ö†Ô∏è No per-mouse metrics available for '{condition_key}'. Skipping overlay.\")\n",
    "                continue\n",
    "\n",
    "            mice_for_condition = per_mouse_metrics['mouse'].astype(str).unique().tolist()\n",
    "            grand_mean, grand_sem = _compute_grand_average_for_condition(\n",
    "                condition_key,\n",
    "                target_signal,\n",
    "                mice_for_condition,\n",
    "            )\n",
    "            if grand_mean is None or grand_mean.empty:\n",
    "                print(f\"‚ö†Ô∏è Could not compute grand average for condition '{condition_key}'.\")\n",
    "                continue\n",
    "\n",
    "            condition_label_for_plot = COMPARE_CALCIUM_METRICS_ACROSS_CONDITIONS[condition_key][\"label\"]\n",
    "            plot_grand_average_vs_peaks(\n",
    "                grand_mean,\n",
    "                grand_sem,\n",
    "                per_mouse_metrics,\n",
    "                target_signal,\n",
    "                condition_label_for_plot,\n",
    "                cohort_identifier,\n",
    "                experiment_day,\n",
    "                overlay_output_dir,\n",
    "                reference_time=POST_ALIGNMENT_WINDOW_START + POST_ALIGNMENT_WINDOW_DURATION / 2,\n",
    "            )"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "aeon",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
