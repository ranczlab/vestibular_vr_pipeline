{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a329aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "# this notebook SAVES halt aligned data and baselined data as CSV together with PLOTS, different compared to the previous SANDBOX_2_noSLEAP#\n",
    "#---------------------------------------------------------------------------------------------------#\n",
    "from pathlib import Path\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "import plotly.express as px\n",
    "import plotly.subplots as sp\n",
    "import math\n",
    "from pprint import pprint\n",
    "\n",
    "from matplotlib.collections import LineCollection\n",
    "import seaborn as sns\n",
    "import traceback\n",
    "import gc\n",
    "from typing import Optional, Tuple, List, Dict, Any\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy.stats import mode\n",
    "from scipy.integrate import cumulative_trapezoid\n",
    "from scipy.signal import correlate\n",
    "import json\n",
    "%config Completer.use_jedi = False  # Fixes autocomplete issues\n",
    "%config InlineBackend.figure_format = 'retina'  # Improves plot resolution\n",
    "\n",
    "import gc # garbage collector for removing large variables from memory instantly \n",
    "import importlib #for force updating changed packages \n",
    "\n",
    "#import harp\n",
    "import harp_resources.process\n",
    "import harp_resources.utils\n",
    "from harp_resources import process, utils # Reassign to maintain direct references for force updating \n",
    "#from sleap import load_and_process as lp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9c5d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------\n",
    "# data paths setup\n",
    "#-------------------------------\n",
    "data_dirs = [  # Add your data directories here\n",
    "    # Path('~/RANCZLAB-NAS/data/ONIX/20250409_Cohort3_rotation/Vestibular_mismatch_day1').expanduser(),\n",
    "    Path('/Volumes/RanczLab2/20241125_Cohort1_rotation/Visual_mismatch_day4').expanduser()\n",
    "    # Path('/Volumes/RanczLab2/20250409_Cohort3_rotation/Visual_mismatch_day4').expanduser()\n",
    "]\n",
    "# Collect raw data paths (excluding '_processedData' dirs)\n",
    "rawdata_paths = []\n",
    "for data_dir in data_dirs:\n",
    "    subdirs = [p for p in data_dir.iterdir() if p.is_dir() and not p.name.endswith('_processedData')]\n",
    "    rawdata_paths.extend(subdirs)  # Collect all subdirectories\n",
    "\n",
    "# Build processed data paths\n",
    "data_paths = [raw.parent / f\"{raw.name}_processedData/downsampled_data\" for raw in rawdata_paths]\n",
    "# Print data paths in a more readable format\n",
    "print(\"Processed Data Paths:\")\n",
    "pprint(data_paths)\n",
    "\n",
    "#-------------------------------\n",
    "# initial variables setup\n",
    "#-------------------------------\n",
    "time_window_start = -5  # s, FOR PLOTTING PURPOSES\n",
    "time_window_end = 10  # s, FOR PLOTTING PURPOSES\n",
    "baseline_window = (-1, 0)  # s, FOR baselining averages\n",
    "plot_width = 14\n",
    "\n",
    "event_name = \"No halt\"  # Apply halt: 2s, No halt, DrumWithReverseflow block started, DrumBase block started\n",
    "vestibular_mismatch = False\n",
    "common_resampled_rate = 1000  # in Hz\n",
    "plot_fig1 = False\n",
    "\n",
    "# for saccades\n",
    "framerate = 59.77  # Hz (in the future, should come from saved data)\n",
    "threshold = 65  # px/s FIXME make this adaptive\n",
    "refractory_period = pd.Timedelta(milliseconds=100)  # msec, using pd.Timedelta for datetime index\n",
    "plot_saccade_detection_QC = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caec3450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load downsampled data for each data path\n",
    "#-------------------------------\n",
    "loaded_data = {}  # Dictionary to store loaded data for each path\n",
    "\n",
    "for idx, data_path in enumerate(data_paths, start=1):\n",
    "    print(f\"\\nProcessing data path {idx}/{len(data_paths)}: {data_path}\")\n",
    "    try:\n",
    "        # Load all parquet files for this data path\n",
    "        photometry_tracking_encoder_data = pd.read_parquet(data_path / \"photometry_tracking_encoder_data.parquet\", engine=\"pyarrow\")\n",
    "        camera_photodiode_data = pd.read_parquet(data_path / \"camera_photodiode_data.parquet\", engine=\"pyarrow\")\n",
    "        experiment_events = pd.read_parquet(data_path / \"experiment_events.parquet\", engine=\"pyarrow\")\n",
    "        photometry_info = pd.read_parquet(data_path / \"photometry_info.parquet\", engine=\"pyarrow\")\n",
    "        session_settings = pd.read_parquet(data_path / \"session_settings.parquet\", engine=\"pyarrow\")\n",
    "        session_settings[\"metadata\"] = session_settings[\"metadata\"].apply(process.safe_from_json)\n",
    "        \n",
    "        print(f\"✅ Successfully loaded all parquet files for {data_path.name}\")\n",
    "        \n",
    "        # Calculate time differences between event_name events\n",
    "        event_times = experiment_events[experiment_events[\"Event\"] == event_name].index\n",
    "        if len(event_times) > 1:\n",
    "            time_diffs = event_times.to_series().diff().dropna().dt.total_seconds()\n",
    "            # Print the 5 shortest time differences\n",
    "            # print(\"5 shortest time differences between events:\")\n",
    "            # print(time_diffs.nsmallest(5))\n",
    "            if (time_diffs < 10).any():\n",
    "                print(f\"⚠️ Warning: Some '{event_name}' events are less than 10 seconds apart. Consider applying a filter to events.\")\n",
    "        else:\n",
    "            print(f\"ℹ️ INFO: Found {len(event_times)} events with name '{event_name}' - not enough to calculate differences\")\n",
    "        \n",
    "        # Check experiment events and get mouse name\n",
    "        mouse_name = process.check_exp_events(experiment_events, photometry_info, verbose=True)\n",
    "        \n",
    "        # Store all loaded data in the dictionary\n",
    "        loaded_data[data_path] = {\n",
    "            \"photometry_tracking_encoder_data\": photometry_tracking_encoder_data,\n",
    "            \"camera_photodiode_data\": camera_photodiode_data,\n",
    "            \"experiment_events\": experiment_events,\n",
    "            \"photometry_info\": photometry_info,\n",
    "            \"session_settings\": session_settings,\n",
    "            \"mouse_name\": mouse_name\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ ERROR processing data path {data_path}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\n✅ Finished loading data for all {len(loaded_data)} successfully processed data paths\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638c9ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create DFs and plot figure for each data path\n",
    "#---------------------------------------------------\n",
    "# Dictionary to store analysis results for each data path\n",
    "data_path_variables = {}\n",
    "\n",
    "for idx, data_path in enumerate(data_paths, start=1):\n",
    "    print(f\"\\n--------- Processing analysis for data path {idx}/{len(data_paths)}: {data_path} ---------\")\n",
    "    \n",
    "    # Skip if data wasn't successfully loaded for this path\n",
    "    if data_path not in loaded_data:\n",
    "        print(f\"⚠️ Skipping analysis for {data_path} - data not loaded successfully\")\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        # Extract data from loaded_data dictionary\n",
    "        photometry_tracking_encoder_data = loaded_data[data_path][\"photometry_tracking_encoder_data\"]\n",
    "        camera_photodiode_data = loaded_data[data_path][\"camera_photodiode_data\"]\n",
    "        experiment_events = loaded_data[data_path][\"experiment_events\"]\n",
    "        mouse_name = loaded_data[data_path][\"mouse_name\"]\n",
    "        session_name = f\"{mouse_name}_{data_path.name}\"  # Assuming session_name is constructed this way\n",
    "        \n",
    "        # Create dataframe to analyze\n",
    "        df_to_analyze = photometry_tracking_encoder_data[\"Photodiode_int\"]  # Using downsampled values in common time grid\n",
    "        # df_to_analyze = camera_photodiode_data[\"Photodiode\"]  # Use async raw values if needed for troubleshooting\n",
    "        \n",
    "        # Determine halt times based on different conditions\n",
    "        if vestibular_mismatch or event_name == \"No halt\":  # Determine halt times based on experiment events\n",
    "            events_matching_name = experiment_events[experiment_events[\"Event\"] == event_name]\n",
    "            if events_matching_name.empty:\n",
    "                print(f\"⚠️ WARNING: No events found with name '{event_name}', skipping this data path\")\n",
    "                continue\n",
    "                \n",
    "            photodiode_halts = events_matching_name.index.tolist()\n",
    "            nearest_indices = photometry_tracking_encoder_data.index.get_indexer(photodiode_halts, method='nearest')\n",
    "            photodiode_halts = photometry_tracking_encoder_data.index[nearest_indices]  # Align to downsampled data time grid\n",
    "            print(f\"ℹ️ INFO: vestibular MM or 'No halt', no signal in the photodiode, using experiment events for MM times\")\n",
    "            photodiode_delay_min = photodiode_delay_avg = photodiode_delay_max = None\n",
    "        else:  # Determine exact halt times based on photodiode signal\n",
    "            try:\n",
    "                photodiode_halts, photodiode_delay_min, photodiode_delay_avg, photodiode_delay_max = process.analyze_photodiode(\n",
    "                    df_to_analyze, experiment_events, event_name, plot=True\n",
    "                )\n",
    "                print(f\"✅ Successfully analyzed photodiode signal for {data_path.name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ ERROR analyzing photodiode signal: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        # Store analysis results\n",
    "        data_path_variables[data_path] = {\n",
    "            \"photodiode_halts\": photodiode_halts,\n",
    "            \"photodiode_delay_min\": photodiode_delay_min,\n",
    "            \"photodiode_delay_avg\": photodiode_delay_avg,\n",
    "            \"photodiode_delay_max\": photodiode_delay_max,\n",
    "            \"session_name\": session_name\n",
    "        }\n",
    "        \n",
    "        # Plot figure if requested\n",
    "        if plot_fig1:\n",
    "            try:\n",
    "                process.plot_figure_1(\n",
    "                    photometry_tracking_encoder_data, \n",
    "                    session_name, \n",
    "                    save_path, \n",
    "                    common_resampled_rate, \n",
    "                    photodiode_halts, \n",
    "                    save_figure=True, \n",
    "                    show_figure=True, \n",
    "                    downsample_factor=50\n",
    "                )\n",
    "                print(f\"✅ Successfully created figure 1 for {data_path.name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ ERROR creating figure 1: {str(e)}\")\n",
    "        else:\n",
    "            print(f\"ℹ️ INFO: skipping figure 1 for {data_path.name}\")\n",
    "        \n",
    "        # Clean up to free memory\n",
    "        del df_to_analyze\n",
    "        gc.collect()\n",
    "        \n",
    "        print(f\"✅ Completed analysis for data path: {data_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ ERROR during analysis of {data_path}: {str(e)}\")\n",
    "\n",
    "print(f\"\\n✅ Finished analyzing all {len(data_path_variables)} successfully processed data paths\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb2573e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #----oldcode-----------------------------------------------\n",
    "# # Create aligned data and plot fluorescence traces for each data path (with improved error handling)\n",
    "# #---------------------------------------------------\n",
    "\n",
    "# # Check if requiorange variables exist\n",
    "# required_vars = ['time_window_start', 'time_window_end']\n",
    "# for var in required_vars:\n",
    "#     if var not in globals():\n",
    "#         print(f\"⚠️ ERROR: Required variable '{var}' is not defined\")\n",
    "#         # Set default values\n",
    "#         if var == 'time_window_start':\n",
    "#             time_window_start = -5  # Default: 5 seconds before halt\n",
    "#             print(f\"  Setting default value: {var} = {time_window_start}\")\n",
    "#         elif var == 'time_window_end':\n",
    "#             time_window_end = 10  # Default: 10 seconds after halt\n",
    "#             print(f\"  Setting default value: {var} = {time_window_end}\")\n",
    "\n",
    "# # Define save_plots if not already defined\n",
    "# if 'save_plots' not in globals():\n",
    "#     save_plots = False\n",
    "#     print(f\"Setting default value: save_plots = {save_plots}\")\n",
    "\n",
    "# for idx, data_path in enumerate(data_paths, start=1):\n",
    "#     print(f\"\\n--------- Creating fluorescence plots for data path {idx}/{len(data_paths)}: {data_path} ---------\")\n",
    "    \n",
    "#     # Skip if data wasn't successfully analyzed for this path\n",
    "#     if data_path not in data_path_variables:\n",
    "#         print(f\"⚠️ Skipping fluorescence plots for {data_path} - analysis not completed successfully\")\n",
    "#         continue\n",
    "    \n",
    "#     try:\n",
    "#         # Verify data exists before attempting to extract it\n",
    "#         if data_path not in loaded_data:\n",
    "#             print(f\"⚠️ ERROR: data_path {data_path} not found in loaded_data\")\n",
    "#             continue\n",
    "            \n",
    "#         if \"photometry_tracking_encoder_data\" not in loaded_data[data_path]:\n",
    "#             print(f\"⚠️ ERROR: 'photometry_tracking_encoder_data' not found in loaded_data[{data_path}]\")\n",
    "#             continue\n",
    "            \n",
    "#         if \"photodiode_halts\" not in data_path_variables[data_path]:\n",
    "#             print(f\"⚠️ ERROR: 'photodiode_halts' not found in data_path_variables[{data_path}]\")\n",
    "#             continue\n",
    "            \n",
    "#         # Extract data from loaded_data and data_path_variables dictionaries\n",
    "#         photometry_tracking_encoder_data = loaded_data[data_path][\"photometry_tracking_encoder_data\"]\n",
    "#         photodiode_halts = data_path_variables[data_path][\"photodiode_halts\"]\n",
    "        \n",
    "#         # Check if session_name exists in data_path_variables\n",
    "#         if \"session_name\" in data_path_variables[data_path]:\n",
    "#             session_name = data_path_variables[data_path][\"session_name\"]\n",
    "#         else:\n",
    "#             # Try to generate session_name\n",
    "#             if \"mouse_name\" in loaded_data[data_path]:\n",
    "#                 mouse_name = loaded_data[data_path][\"mouse_name\"]\n",
    "#                 session_name = f\"{mouse_name}_{data_path.name}\"\n",
    "#             else:\n",
    "#                 session_name = f\"session_{data_path.name}\"\n",
    "#                 print(f\"⚠️ WARNING: Using generic session name: {session_name}\")\n",
    "        \n",
    "#         print(f\"Creating aligned data for {len(photodiode_halts)} events in {data_path.name}\")\n",
    "        \n",
    "#         # Check if required columns exist in photometry data\n",
    "#         required_columns = [\"z_470\", \"z_560\"]\n",
    "#         missing_columns = [col for col in required_columns if col not in photometry_tracking_encoder_data.columns]\n",
    "#         if missing_columns:\n",
    "#             print(f\"⚠️ ERROR: Missing columns in photometry_tracking_encoder_data: {missing_columns}\")\n",
    "#             print(f\"Available columns: {photometry_tracking_encoder_data.columns.tolist()}\")\n",
    "#             continue\n",
    "        \n",
    "#         # --- Data Alignment ---\n",
    "#         aligned_data = []\n",
    "#         for halt_time in photodiode_halts:\n",
    "#             try:\n",
    "#                 window_data = photometry_tracking_encoder_data.loc[\n",
    "#                     (photometry_tracking_encoder_data.index >= halt_time + pd.Timedelta(seconds=time_window_start)) &\n",
    "#                     (photometry_tracking_encoder_data.index <= halt_time + pd.Timedelta(seconds=time_window_end))\n",
    "#                 ].copy()\n",
    "                \n",
    "#                 if window_data.empty:\n",
    "#                     print(f\"⚠️ WARNING: No data found for window around halt time {halt_time}\")\n",
    "#                     continue\n",
    "                    \n",
    "#                 window_data[\"Time (s)\"] = (window_data.index - halt_time).total_seconds()\n",
    "#                 window_data[\"Halt Time\"] = halt_time\n",
    "#                 aligned_data.append(window_data)\n",
    "#             except Exception as e:\n",
    "#                 print(f\"⚠️ ERROR processing halt time {halt_time}: {str(e)}\")\n",
    "#                 continue\n",
    "        \n",
    "#         if not aligned_data:\n",
    "#             print(f\"⚠️ WARNING: No aligned data created for {data_path.name}, skipping plotting\")\n",
    "#             continue\n",
    "            \n",
    "#         aligned_df = pd.concat(aligned_data, ignore_index=True)\n",
    "        \n",
    "#         # --- Subplot Grid Setup ---\n",
    "#         n_events = len(photodiode_halts)\n",
    "#         n_cols = 4\n",
    "#         n_rows = math.ceil(n_events / n_cols)\n",
    "        \n",
    "#         print(f\"Creating subplot grid with {n_rows} rows and {n_cols} columns for {n_events} events\")\n",
    "        \n",
    "#         # Create subplots with a single (default) y-axis in each cell\n",
    "#         specs = [[{} for _ in range(n_cols)] for _ in range(n_rows)]\n",
    "#         subplot_titles = [f'Event: {halt_time}' for halt_time in photodiode_halts]\n",
    "#         fig = sp.make_subplots(rows=n_rows, cols=n_cols, subplot_titles=subplot_titles, specs=specs)\n",
    "        \n",
    "#         # Base extra is used to create unique axis IDs starting after the auto-assigned primary axes\n",
    "#         base_extra = n_events + 1\n",
    "        \n",
    "#         for i, halt_time in enumerate(photodiode_halts):\n",
    "#             try:\n",
    "#                 row = (i // n_cols) + 1\n",
    "#                 col = (i % n_cols) + 1\n",
    "                \n",
    "#                 subset = aligned_df[aligned_df[\"Halt Time\"] == halt_time]\n",
    "#                 if subset.empty:\n",
    "#                     print(f\"⚠️ WARNING: No data found for halt time {halt_time}\")\n",
    "#                     continue\n",
    "                \n",
    "#                 # Verify data exists in subset\n",
    "#                 for col_name in [\"Time (s)\", \"z_470\", \"z_560\"]:\n",
    "#                     if col_name not in subset.columns:\n",
    "#                         print(f\"⚠️ ERROR: Column '{col_name}' not found in aligned data subset\")\n",
    "#                         print(f\"Available columns: {subset.columns.tolist()}\")\n",
    "#                         raise KeyError(f\"Missing column: {col_name}\")\n",
    "                    \n",
    "#                 # -- Fluorescence Traces on Primary y-axis --\n",
    "#                 fig.add_trace(\n",
    "#                     go.Scatter(\n",
    "#                         x=subset[\"Time (s)\"],\n",
    "#                         y=subset[\"z_470\"],\n",
    "#                         mode='lines',\n",
    "#                         name='z_470',\n",
    "#                         line=dict(color='green')\n",
    "#                     ),\n",
    "#                     row=row, col=col\n",
    "#                 )\n",
    "                \n",
    "#                 fig.add_trace(\n",
    "#                     go.Scatter(\n",
    "#                         x=subset[\"Time (s)\"],\n",
    "#                         y=subset[\"z_560\"],\n",
    "#                         mode='lines',\n",
    "#                         name='z_560',\n",
    "#                         line=dict(color='red')\n",
    "#                     ),\n",
    "#                     row=row, col=col\n",
    "#                 )\n",
    "                \n",
    "#                 # Determine the subplot's x-axis anchor\n",
    "#                 xaxis_number = (row - 1) * n_cols + col\n",
    "#                 x_anchor = \"x\" if xaxis_number == 1 else f\"x{xaxis_number}\"\n",
    "                \n",
    "#                 # The primary y-axis for this subplot\n",
    "#                 primary_y = \"y\" if xaxis_number == 1 else f\"y{xaxis_number}\"\n",
    "                \n",
    "#             except Exception as e:\n",
    "#                 print(f\"⚠️ ERROR processing subplot for halt time {halt_time}: {str(e)}\")\n",
    "#                 continue\n",
    "        \n",
    "#         # --- Update Common Axis Labels ---\n",
    "#         fig.update_xaxes(title_text=\"Time (s)\")\n",
    "#         fig.update_yaxes(title_text=\"Fluorescence (z-score)\")\n",
    "        \n",
    "#         # Create a descriptive title that includes session name\n",
    "#         title = f\"Fluorescence for each event - {session_name}\"\n",
    "        \n",
    "#         fig.update_layout(\n",
    "#             height=400 * n_rows,\n",
    "#             width=350 * n_cols,\n",
    "#             title_text=title,\n",
    "#             template='plotly_white'\n",
    "#         )\n",
    "        \n",
    "#         # Save the figure if needed\n",
    "#         if save_plots:\n",
    "#             try:\n",
    "#                 # Ensure save_path exists\n",
    "#                 if 'save_path' not in globals() or save_path is None:\n",
    "#                     from pathlib import Path\n",
    "#                     save_path = Path('./output')\n",
    "#                     save_path.mkdir(exist_ok=True)\n",
    "#                     print(f\"Creating default save_path: {save_path}\")\n",
    "                    \n",
    "#                 output_file = save_path / f\"{session_name}_fluorescence_events.html\"\n",
    "#                 fig.write_html(str(output_file))\n",
    "#                 print(f\"✅ Saved fluorescence plot to {output_file}\")\n",
    "#             except Exception as e:\n",
    "#                 print(f\"⚠️ ERROR saving fluorescence plot: {str(e)}\")\n",
    "        \n",
    "#         # Display the figure\n",
    "#         fig.show()\n",
    "        \n",
    "#         # Clean up to free memory\n",
    "#         del aligned_data\n",
    "#         gc.collect()\n",
    "        \n",
    "#         print(f\"✅ Completed fluorescence plots for data path: {data_path}\")\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         import traceback\n",
    "#         print(f\"⚠️ ERROR creating fluorescence plots for {data_path}: {str(e)}\")\n",
    "#         print(\"Detailed error traceback:\")\n",
    "#         traceback.print_exc()  # This will print the full stack trace\n",
    "\n",
    "# print(f\"\\n✅ Finished creating fluorescence plots for all successfully processed data paths\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77ee9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #----oldcode-----------------------------------------------\n",
    "# # Create aligned data and plot comprehensive figures for each data path\n",
    "# #---------------------------------------------------\n",
    "\n",
    "# print(f\"Using time window: {time_window_start}s to {time_window_end}s relative to halt\")\n",
    "\n",
    "# # Iterate through each data path\n",
    "# for idx, data_path in enumerate(data_paths, start=1):\n",
    "#     print(f\"\\n--------- Processing {idx}/{len(data_paths)}: {data_path} ---------\")\n",
    "    \n",
    "#     if data_path not in data_path_variables:\n",
    "#         print(f\"⚠️ Skipping {data_path} - no analysis data found\")\n",
    "#         continue\n",
    "    \n",
    "#     try:\n",
    "#         # Extract data\n",
    "#         data = loaded_data[data_path]\n",
    "#         vars_ = data_path_variables[data_path]\n",
    "        \n",
    "#         df = data[\"photometry_tracking_encoder_data\"]\n",
    "#         halts = vars_[\"photodiode_halts\"]\n",
    "        \n",
    "#         session_name = vars_.get(\"session_name\")\n",
    "#         if not session_name:\n",
    "#             mouse_name = data.get(\"mouse_name\", \"unknown_mouse\")\n",
    "#             session_name = f\"{mouse_name}_{data_path.stem}\"\n",
    "#             print(f\"⚠️ No session_name found, using: {session_name}\")\n",
    "\n",
    "#         event_name = event_name\n",
    "#         print(f\"Aligning {len(halts)} events for session '{session_name}'\")\n",
    "\n",
    "#         # Align data to each halt event\n",
    "#         aligned_data = []\n",
    "#         for i, halt_time in enumerate(halts):\n",
    "#             window = df.loc[\n",
    "#                 (df.index >= halt_time + pd.Timedelta(seconds=time_window_start)) &\n",
    "#                 (df.index <= halt_time + pd.Timedelta(seconds=time_window_end))\n",
    "#             ].copy()\n",
    "\n",
    "#             if window.empty:\n",
    "#                 print(f\"⚠️ No data in window around halt {halt_time}\")\n",
    "#                 continue\n",
    "\n",
    "#             window[\"Time (s)\"] = (window.index - halt_time).total_seconds()\n",
    "#             window[\"Halt Time\"] = halt_time\n",
    "#             aligned_data.append(window)\n",
    "\n",
    "#         if not aligned_data:\n",
    "#             print(f\"⚠️ No aligned data generated for {session_name}, skipping\")\n",
    "#             continue\n",
    "\n",
    "#         aligned_df = pd.concat(aligned_data, ignore_index=True)\n",
    "\n",
    "#         # Save CSV in same folder as data\n",
    "#         aligned_dir = data_path.parent / \"aligned_data\"\n",
    "#         aligned_dir.mkdir(exist_ok=True)\n",
    "\n",
    "#         aligned_file = aligned_dir / f\"{session_name}_{event_name}_aligned.csv\"\n",
    "#         aligned_df.to_csv(aligned_file, index=False)\n",
    "#         print(f\"✅ Saved aligned data to {aligned_file}\")\n",
    "\n",
    "#         # Fill in missing columns with dummy data (except required)\n",
    "#         required_columns = [\"Time (s)\", \"Photodiode_int\", \"z_470\", \"z_560\", \"Motor_Velocity\", \"Velocity_0X\", \"Velocity_0Y\"]\n",
    "#         for col in required_columns:\n",
    "#             if col not in aligned_df.columns:\n",
    "#                 print(f\"⚠️ Missing column: {col}, adding zeros\")\n",
    "#                 aligned_df[col] = 0\n",
    "\n",
    "#         # Compute group mean and SEM\n",
    "#         mean_df = aligned_df.groupby(\"Time (s)\").mean()\n",
    "#         sem_df = aligned_df.groupby(\"Time (s)\").sem()\n",
    "\n",
    "#         # Create figure\n",
    "#         print(f\"📈 Creating plot for {session_name}\")\n",
    "#         fig, axes = plt.subplots(1, 2, figsize=(14, 6), sharex=True)\n",
    "\n",
    "#         ## Plot 1: Individual Traces\n",
    "#         ax1 = axes[0]\n",
    "#         for halt in aligned_df[\"Halt Time\"].unique():\n",
    "#             subset = aligned_df[aligned_df[\"Halt Time\"] == halt]\n",
    "#             ax1.plot(subset[\"Time (s)\"], subset[\"Photodiode_int\"], color='grey', alpha=0.5)\n",
    "\n",
    "#         ax1.set_title('Photodiode, z_470, and z_560')\n",
    "#         ax1.set_xlabel(\"Time (s)\")\n",
    "#         ax1.set_ylabel(\"Photodiode\")\n",
    "\n",
    "#         ax1_2 = ax1.twinx()\n",
    "#         for halt in aligned_df[\"Halt Time\"].unique():\n",
    "#             subset = aligned_df[aligned_df[\"Halt Time\"] == halt]\n",
    "#             ax1_2.plot(subset[\"Time (s)\"], subset[\"z_470\"], color='green', alpha=0.5)\n",
    "#             ax1_2.plot(subset[\"Time (s)\"], subset[\"z_560\"], color='red', alpha=0.5)\n",
    "\n",
    "#         ax1_2.set_ylabel(\"Fluorescence (z-score)\", color='green')\n",
    "\n",
    "#         ## Plot 2: Mean + SEM\n",
    "#         ax2 = axes[1]\n",
    "#         ax2.plot(mean_df.index, mean_df[\"Photodiode_int\"], color='grey')\n",
    "#         ax2.fill_between(mean_df.index, mean_df[\"Photodiode_int\"] - sem_df[\"Photodiode_int\"],\n",
    "#                          mean_df[\"Photodiode_int\"] + sem_df[\"Photodiode_int\"], color='grey', alpha=0.2)\n",
    "#         ax2.set_xlabel(\"Time (s)\")\n",
    "#         ax2.set_ylabel(\"Photodiode\")\n",
    "#         ax2.set_title(\"Mean & SEM\")\n",
    "\n",
    "#         ax2_2 = ax2.twinx()\n",
    "#         ax2_2.plot(mean_df.index, mean_df[\"z_470\"], color='green')\n",
    "#         ax2_2.fill_between(mean_df.index, mean_df[\"z_470\"] - sem_df[\"z_470\"], \n",
    "#                            mean_df[\"z_470\"] + sem_df[\"z_470\"], color='green', alpha=0.2)\n",
    "\n",
    "#         ax2_2.plot(mean_df.index, mean_df[\"z_560\"], color='red')\n",
    "#         ax2_2.fill_between(mean_df.index, mean_df[\"z_560\"] - sem_df[\"z_560\"], \n",
    "#                            mean_df[\"z_560\"] + sem_df[\"z_560\"], color='red', alpha=0.2)\n",
    "\n",
    "#         ax2_2.set_ylabel(\"Fluorescence (z-score)\", color='green')\n",
    "\n",
    "#         # Save figure in same folder as data\n",
    "#         fig.suptitle(f\"{session_name} - {event_name}\")\n",
    "#         fig.tight_layout()\n",
    "#         figure_file = data_path.parent / f\"{session_name}_{event_name}.pdf\"\n",
    "#         fig.savefig(figure_file, dpi=300)\n",
    "#         plt.close(fig)\n",
    "#         print(f\"✅ Saved figure to {figure_file}\")\n",
    "\n",
    "#     except Exception as e:\n",
    "#         import traceback\n",
    "#         print(f\"❌ ERROR processing {data_path}: {str(e)}\")\n",
    "#         traceback.print_exc()\n",
    "\n",
    "# print(\"\\n✅ Finished all data paths.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8ad4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----oldcode--------------------------------------------------\n",
    "# #separates RIGHT vs LEFT TURNS, creates heatmaps, and comprehensive (Mean +- sem) aligned data for each data path\n",
    "# #---------------------------------------------------\n",
    "# from matplotlib.collections import LineCollection\n",
    "# import seaborn as sns\n",
    "\n",
    "# import traceback\n",
    "# def process_aligned_data(df, halt_time, time_window_start, time_window_end):\n",
    "#     \"\"\"Process a single halt event efficiently.\"\"\"\n",
    "#     window_start = halt_time + pd.Timedelta(seconds=time_window_start)\n",
    "#     window_end = halt_time + pd.Timedelta(seconds=time_window_end)\n",
    "#     mask = (df.index >= window_start) & (df.index <= window_end)\n",
    "    \n",
    "#     if not mask.any():\n",
    "#         return None\n",
    "    \n",
    "#     window = df.loc[mask].copy()\n",
    "#     window[\"Time (s)\"] = (window.index - halt_time).total_seconds()\n",
    "#     window[\"Halt Time\"] = halt_time\n",
    "#     return window\n",
    "\n",
    "# def create_heatmap(pivot_data, session_name, event_name, channel, save_path, figsize=(10, 6)):\n",
    "#     \"\"\"Create and save heatmap efficiently.\"\"\"\n",
    "#     # Base normalize\n",
    "#     baseline_cols = (pivot_data.columns >= -1) & (pivot_data.columns < 0)\n",
    "#     if baseline_cols.any():\n",
    "#         baseline_means = pivot_data.loc[:, baseline_cols].mean(axis=1)\n",
    "#         normalized_data = pivot_data.subtract(baseline_means, axis=0)\n",
    "#     else:\n",
    "#         normalized_data = pivot_data\n",
    "    \n",
    "#     # Create figure with optimized settings\n",
    "#     fig, ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "#     # Use fewer colors for better performance\n",
    "#     sns.heatmap(normalized_data, cmap=\"RdBu_r\", center=0, ax=ax, \n",
    "#                 cbar_kws={'label': f'Normalized {channel}'},\n",
    "#                 rasterized=True)  # Rasterize for smaller file size\n",
    "    \n",
    "#     ax.set_title(f\"Heatmap ({channel}) - {session_name}\")\n",
    "#     ax.set_xlabel(\"Time (s)\")\n",
    "#     ax.set_ylabel(\"Event\")\n",
    "    \n",
    "#     # Set y-axis ticks to simple numbers instead of datetime\n",
    "#     n_events = len(normalized_data.index)\n",
    "#     y_tick_positions = range(0, n_events, max(1, n_events // 10))  # Show ~10 ticks max\n",
    "#     y_tick_labels = [str(i+1) for i in y_tick_positions]  # Event numbers 1, 2, 3, etc.\n",
    "#     ax.set_yticks(y_tick_positions)\n",
    "#     ax.set_yticklabels(y_tick_labels)\n",
    "    \n",
    "#     # Add event line if 0 exists in columns\n",
    "#     if 0 in normalized_data.columns:\n",
    "#         zero_idx = list(normalized_data.columns).index(0)\n",
    "#         ax.axvline(zero_idx, linestyle='--', color='black', alpha=0.7)\n",
    "    \n",
    "#     # Optimize tick labels - show every 2nd second\n",
    "#     time_cols = normalized_data.columns\n",
    "#     tick_indices = [i for i, val in enumerate(time_cols) \n",
    "#                    if isinstance(val, (int, float)) and val % 2 == 0]\n",
    "#     tick_labels = [f\"{int(time_cols[i])}\" for i in tick_indices]\n",
    "    \n",
    "#     ax.set_xticks(tick_indices)\n",
    "#     ax.set_xticklabels(tick_labels, rotation=45)\n",
    "    \n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig(save_path, dpi=200, bbox_inches='tight')  # Lower DPI for efficiency\n",
    "#     plt.close(fig)\n",
    "    \n",
    "#     return normalized_data\n",
    "\n",
    "# def create_summary_plot(aligned_df, session_name, event_name, save_path):\n",
    "#     \"\"\"Create summary plots efficiently.\"\"\"\n",
    "#     # Pre-compute grouped statistics once\n",
    "#     grouped = aligned_df.groupby(\"Time (s)\")\n",
    "#     mean_df = grouped.mean()\n",
    "#     sem_df = grouped.sem()\n",
    "    \n",
    "#     fig, axes = plt.subplots(1, 4, figsize=(28, 6), sharex=True)\n",
    "#     # Separate left vs right turn halts based on Motor_Velocity before time0\n",
    "#     left_turn_halts = []\n",
    "#     right_turn_halts = []\n",
    "#     for halt in aligned_df[\"Halt Time\"].unique():\n",
    "#         subset = aligned_df[(aligned_df[\"Halt Time\"] == halt) & \n",
    "#                             (aligned_df[\"Time (s)\"] >= -1) & \n",
    "#                             (aligned_df[\"Time (s)\"] < 0)]\n",
    "#         mean_velocity = subset[\"Motor_Velocity\"].mean()\n",
    "        \n",
    "#         if mean_velocity < 0:  # Negative mean velocity indicates left turn\n",
    "#             left_turn_halts.append(halt)\n",
    "#         elif mean_velocity > 0:  # Positive mean velocity indicates right turn\n",
    "#             right_turn_halts.append(halt)\n",
    "\n",
    "#     # Create separate DataFrames for left and right turn halts\n",
    "#     left_turn_df = aligned_df[aligned_df[\"Halt Time\"].isin(left_turn_halts)]\n",
    "#     right_turn_df = aligned_df[aligned_df[\"Halt Time\"].isin(right_turn_halts)]\n",
    "\n",
    "#     # Save left and right turn DataFrames to CSV\n",
    "#     left_turn_file = aligned_dir / f\"{session_name}_{event_name}_left_turns.csv\"\n",
    "#     right_turn_file = aligned_dir / f\"{session_name}_{event_name}_right_turns.csv\"\n",
    "\n",
    "#     left_turn_df.to_csv(left_turn_file, index=False, float_format='%.4f')\n",
    "#     right_turn_df.to_csv(right_turn_file, index=False, float_format='%.4f')\n",
    "\n",
    "#     print(f\"Saved left turn data to {left_turn_file}\")\n",
    "#     print(f\"Saved right turn data to {right_turn_file}\")\n",
    "    \n",
    "#     # Create subplots for z-score data (row 1) and dfF data (row 2)\n",
    "#     fig, axes = plt.subplots(2, 3, figsize=(21, 12), sharex=True)\n",
    "    \n",
    "#     # Row 1: Z-score data\n",
    "#     # Left plot - Individual traces for left turn halts (z_470, z_560, and Motor_Velocity)\n",
    "#     ax1 = axes[0, 0]\n",
    "#     left_lines_470 = []\n",
    "#     left_lines_560 = []\n",
    "#     left_lines_motor = []\n",
    "#     for halt in left_turn_df[\"Halt Time\"].unique():\n",
    "#         subset = left_turn_df[left_turn_df[\"Halt Time\"] == halt]\n",
    "#         time_vals = subset[\"Time (s)\"].values\n",
    "#         left_lines_470.append(list(zip(time_vals, subset[\"z_470\"].values)))\n",
    "#         left_lines_560.append(list(zip(time_vals, subset[\"z_560\"].values)))\n",
    "#         left_lines_motor.append(list(zip(time_vals, subset[\"Motor_Velocity\"].values)))\n",
    "\n",
    "#     left_lc_470 = LineCollection(left_lines_470, colors='cornflowerblue', alpha=0.3, linewidths=1)\n",
    "#     left_lc_560 = LineCollection(left_lines_560, colors='red', alpha=0.3, linewidths=1)\n",
    "#     ax1.add_collection(left_lc_470)\n",
    "#     ax1.add_collection(left_lc_560)\n",
    "\n",
    "#     # Create a secondary y-axis for motor velocity\n",
    "#     ax1_motor = ax1.twinx()\n",
    "#     left_lc_motor = LineCollection(left_lines_motor, colors='slategray', alpha=0.3, linewidths=1)\n",
    "#     ax1_motor.add_collection(left_lc_motor)\n",
    "#     ax1_motor.set_ylabel(\"Motor Velocity\", color='slategray')\n",
    "#     ax1_motor.tick_params(axis='y', labelcolor='slategray')\n",
    "#     ax1_motor.autoscale()  # Automatically fit the axis to the plot\n",
    "\n",
    "#     ax1.set_title('Left Turn Traces (z_470, z_560, Motor_Velocity)')\n",
    "#     ax1.set_xlabel(\"Time (s)\")\n",
    "#     ax1.set_ylabel(\"Fluorescence (z-score)\")\n",
    "#     ax1.axvline(0, linestyle='--', color='black', alpha=0.7)\n",
    "#     ax1.autoscale()\n",
    "    \n",
    "#     # Right plot - Individual traces for right turn halts (z_470 & z_560)\n",
    "#     ax2 = axes[0, 1]\n",
    "#     right_lines_470 = []\n",
    "#     right_lines_560 = []\n",
    "#     right_lines_motor = []\n",
    "#     for halt in right_turn_df[\"Halt Time\"].unique():\n",
    "#         subset = right_turn_df[right_turn_df[\"Halt Time\"] == halt]\n",
    "#         time_vals = subset[\"Time (s)\"].values\n",
    "#         right_lines_470.append(list(zip(time_vals, subset[\"z_470\"].values)))\n",
    "#         right_lines_560.append(list(zip(time_vals, subset[\"z_560\"].values)))\n",
    "#         right_lines_motor.append(list(zip(time_vals, subset[\"Motor_Velocity\"].values)))\n",
    "#     right_lc_470 = LineCollection(right_lines_470, colors='cornflowerblue', alpha=0.3, linewidths=1)\n",
    "#     right_lc_560 = LineCollection(right_lines_560, colors='red', alpha=0.3, linewidths=1)\n",
    "#     right_lc_motor = LineCollection(right_lines_motor, colors='slategray', alpha=0.3, linewidths=1)\n",
    "#     ax2.add_collection(right_lc_470)\n",
    "#     ax2.add_collection(right_lc_560)\n",
    "    \n",
    "#     # Add secondary y-axis for motor velocity\n",
    "#     ax2_motor = ax2.twinx()\n",
    "#     ax2_motor.add_collection(right_lc_motor)\n",
    "#     ax2_motor.set_ylabel(\"Motor Velocity\", color='slategray')\n",
    "#     ax2_motor.tick_params(axis='y', labelcolor='slategray')\n",
    "#     ax2_motor.autoscale()  # Automatically fit the axis to the plot\n",
    "    \n",
    "#     ax2.set_title('Right Turn Traces (z_470, z_560, Motor_Velocity)')\n",
    "#     ax2.set_xlabel(\"Time (s)\")\n",
    "#     ax2.set_ylabel(\"Fluorescence (z-score)\")\n",
    "#     ax2.axvline(0, linestyle='--', color='black', alpha=0.7)\n",
    "#     ax2.autoscale()\n",
    "    \n",
    "#     # Middle plot - Mean ± SEM for left and right turns (z_470 & z_560) with motor data\n",
    "#     ax3 = axes[0, 2]\n",
    "#     time_index = mean_df.index.values\n",
    "#     for color, channel in [('cornflowerblue', 'z_470'), ('red', 'z_560')]:\n",
    "#         ax3.plot(time_index, left_turn_df.groupby(\"Time (s)\").mean()[channel], color=color, linestyle='--', linewidth=2, label=f\"Left {channel}\")\n",
    "#         ax3.fill_between(time_index,\n",
    "#                          left_turn_df.groupby(\"Time (s)\").mean()[channel] - left_turn_df.groupby(\"Time (s)\").sem()[channel],\n",
    "#                          left_turn_df.groupby(\"Time (s)\").mean()[channel] + left_turn_df.groupby(\"Time (s)\").sem()[channel], \n",
    "#                          color=color, alpha=0.2)\n",
    "#         ax3.plot(time_index, right_turn_df.groupby(\"Time (s)\").mean()[channel], color=color, linewidth=2, label=f\"Right {channel}\")\n",
    "#         ax3.fill_between(time_index,\n",
    "#                          right_turn_df.groupby(\"Time (s)\").mean()[channel] - right_turn_df.groupby(\"Time (s)\").sem()[channel],\n",
    "#                          right_turn_df.groupby(\"Time (s)\").mean()[channel] + right_turn_df.groupby(\"Time (s)\").sem()[channel], \n",
    "#                          color=color, alpha=0.2)\n",
    "#     ax3.axvline(0, linestyle='--', color='black', alpha=0.7)\n",
    "#     ax3.set_xlabel(\"Time (s)\")\n",
    "#     ax3.set_ylabel(\"Fluorescence (z-score)\")\n",
    "#     ax3.set_title(\"Mean ± SEM (z_470 & z_560)\")\n",
    "#     ax3.legend(loc='upper left')  # Add legend for z_470 and z_560\n",
    "\n",
    "#     # Add secondary y-axis for motor data with SEM\n",
    "#     ax3_motor = ax3.twinx()\n",
    "#     motor_color = 'slategray'\n",
    "#     ax3_motor.plot(time_index, left_turn_df.groupby(\"Time (s)\").mean()[\"Motor_Velocity\"], color=motor_color, linestyle='--', linewidth=1.5, label=\"Left Motor\")\n",
    "#     ax3_motor.fill_between(time_index,\n",
    "#                            left_turn_df.groupby(\"Time (s)\").mean()[\"Motor_Velocity\"] - left_turn_df.groupby(\"Time (s)\").sem()[\"Motor_Velocity\"],\n",
    "#                            left_turn_df.groupby(\"Time (s)\").mean()[\"Motor_Velocity\"] + left_turn_df.groupby(\"Time (s)\").sem()[\"Motor_Velocity\"],\n",
    "#                            color=motor_color, alpha=0.2)\n",
    "#     ax3_motor.plot(time_index, right_turn_df.groupby(\"Time (s)\").mean()[\"Motor_Velocity\"], color=motor_color, linewidth=1.5, label=\"Right Motor\")\n",
    "#     ax3_motor.fill_between(time_index,\n",
    "#                            right_turn_df.groupby(\"Time (s)\").mean()[\"Motor_Velocity\"] - right_turn_df.groupby(\"Time (s)\").sem()[\"Motor_Velocity\"],\n",
    "#                            right_turn_df.groupby(\"Time (s)\").mean()[\"Motor_Velocity\"] + right_turn_df.groupby(\"Time (s)\").sem()[\"Motor_Velocity\"],\n",
    "#                            color=motor_color, alpha=0.2)\n",
    "#     ax3_motor.set_ylabel(\"Motor Velocity\", color=motor_color)\n",
    "#     ax3_motor.tick_params(axis='y', labelcolor=motor_color)\n",
    "#     ax3_motor.axhline(0, linestyle='--', color='gray', alpha=0.5)\n",
    "#     ax3_motor.legend(loc='upper right')  # Add legend for motor data\n",
    "    \n",
    "#     # # Row 2: dfF data\n",
    "#     # # Left plot - Individual traces for left turn halts (dfF_470 & dfF_560)\n",
    "#     # ax4 = axes[1, 0]\n",
    "#     # left_lines_dfF_470 = []\n",
    "#     # left_lines_dfF_560 = []\n",
    "#     # for halt in left_turn_df[\"Halt Time\"].unique():\n",
    "#     #     subset = left_turn_df[left_turn_df[\"Halt Time\"] == halt]\n",
    "#     #     time_vals = subset[\"Time (s)\"].values\n",
    "#     #     left_lines_dfF_470.append(list(zip(time_vals, subset[\"dfF_470\"].values)))\n",
    "#     #     left_lines_dfF_560.append(list(zip(time_vals, subset[\"dfF_560\"].values)))\n",
    "#     # left_lc_dfF_470 = LineCollection(left_lines_dfF_470, colors='blue', alpha=0.3, linewidths=1)\n",
    "#     # left_lc_dfF_560 = LineCollection(left_lines_dfF_560, colors='orange', alpha=0.3, linewidths=1)\n",
    "#     # ax4.add_collection(left_lc_dfF_470)\n",
    "#     # ax4.add_collection(left_lc_dfF_560)\n",
    "#     # ax4.set_title('Left Turn Traces (dfF_470 & dfF_560)')\n",
    "#     # ax4.set_xlabel(\"Time (s)\")\n",
    "#     # ax4.set_ylabel(\"Fluorescence (dfF)\")\n",
    "#     # ax4.axvline(0, linestyle='--', color='black', alpha=0.7)\n",
    "#     # ax4.autoscale()\n",
    "    \n",
    "#     # # Right plot - Individual traces for right turn halts (dfF_470 & dfF_560)\n",
    "#     # ax5 = axes[1, 1]\n",
    "#     # right_lines_dfF_470 = []\n",
    "#     # right_lines_dfF_560 = []\n",
    "#     # for halt in right_turn_df[\"Halt Time\"].unique():\n",
    "#     #     subset = right_turn_df[right_turn_df[\"Halt Time\"] == halt]\n",
    "#     #     time_vals = subset[\"Time (s)\"].values\n",
    "#     #     right_lines_dfF_470.append(list(zip(time_vals, subset[\"dfF_470\"].values)))\n",
    "#     #     right_lines_dfF_560.append(list(zip(time_vals, subset[\"dfF_560\"].values)))\n",
    "#     # right_lc_dfF_470 = LineCollection(right_lines_dfF_470, colors='blue', alpha=0.3, linewidths=1)\n",
    "#     # right_lc_dfF_560 = LineCollection(right_lines_dfF_560, colors='orange', alpha=0.3, linewidths=1)\n",
    "#     # ax5.add_collection(right_lc_dfF_470)\n",
    "#     # ax5.add_collection(right_lc_dfF_560)\n",
    "#     # ax5.set_title('Right Turn Traces (dfF_470 & dfF_560)')\n",
    "#     # ax5.set_xlabel(\"Time (s)\")\n",
    "#     # ax5.set_ylabel(\"Fluorescence (dfF)\")\n",
    "#     # ax5.axvline(0, linestyle='--', color='black', alpha=0.7)\n",
    "#     # ax5.autoscale()\n",
    "    \n",
    "#     # # Middle plot - Mean ± SEM for left and right turns (dfF_470 & dfF_560)\n",
    "#     # ax6 = axes[1, 2]\n",
    "#     # for color, channel in [('blue', 'dfF_470'), ('orange', 'dfF_560')]:\n",
    "#     #     ax6.plot(time_index, left_turn_df.groupby(\"Time (s)\").mean()[channel], color=color, linestyle='--', linewidth=2, label=f\"Left {channel}\")\n",
    "#     #     ax6.fill_between(time_index,\n",
    "#     #                      left_turn_df.groupby(\"Time (s)\").mean()[channel] - left_turn_df.groupby(\"Time (s)\").sem()[channel],\n",
    "#     #                      left_turn_df.groupby(\"Time (s)\").mean()[channel] + left_turn_df.groupby(\"Time (s)\").sem()[channel], \n",
    "#     #                      color=color, alpha=0.2)\n",
    "#     #     ax6.plot(time_index, right_turn_df.groupby(\"Time (s)\").mean()[channel], color=color, linewidth=2, label=f\"Right {channel}\")\n",
    "#     #     ax6.fill_between(time_index,\n",
    "#     #                      right_turn_df.groupby(\"Time (s)\").mean()[channel] - right_turn_df.groupby(\"Time (s)\").sem()[channel],\n",
    "#     #                      right_turn_df.groupby(\"Time (s)\").mean()[channel] + right_turn_df.groupby(\"Time (s)\").sem()[channel], \n",
    "#     #                      color=color, alpha=0.2)\n",
    "#     # ax6.axvline(0, linestyle='--', color='black', alpha=0.7)\n",
    "#     # ax6.set_xlabel(\"Time (s)\")\n",
    "#     # ax6.set_ylabel(\"Fluorescence (dfF)\")\n",
    "#     # ax6.set_title(\"Mean ± SEM (dfF_470 & dfF_560)\")\n",
    "#     # ax6.legend()\n",
    "    \n",
    "#     fig.suptitle(f\"{session_name} - {event_name}\")\n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig(save_path, dpi=200, bbox_inches='tight')\n",
    "#     plt.close(fig)\n",
    "#     plt.ioff()  # Turn off interactive mode to suppress figure display\n",
    "\n",
    "# # Required columns defined once\n",
    "# REQUIRED_COLUMNS = [\"Time (s)\", \"Photodiode_int\", \"z_470\", \"z_560\",\"dfF_470\", \"dfF_560\",\n",
    "#                    \"Motor_Velocity\", \"Velocity_0X\", \"Velocity_0Y\"]\n",
    "\n",
    "# for idx, data_path in enumerate(data_paths, start=1):\n",
    "#     print(f\"\\n--------- Processing {idx}/{len(data_paths)}: {data_path} ---------\")\n",
    "\n",
    "#     if data_path not in data_path_variables:\n",
    "#         print(f\"Skipping {data_path} - no analysis data found\")\n",
    "#         continue\n",
    "\n",
    "#     try:\n",
    "#         # Load data references\n",
    "#         data = loaded_data[data_path]\n",
    "#         vars_ = data_path_variables[data_path]\n",
    "#         df = data[\"photometry_tracking_encoder_data\"]\n",
    "#         halts = vars_[\"photodiode_halts\"]\n",
    "\n",
    "#         # Get session name\n",
    "#         session_name = vars_.get(\"session_name\")\n",
    "#         if not session_name:\n",
    "#             mouse_name = data.get(\"mouse_name\", \"unknown_mouse\")\n",
    "#             session_name = f\"{mouse_name}_{data_path.stem}\"\n",
    "#             print(f\"No session_name found, using: {session_name}\")\n",
    "\n",
    "#         print(f\"Aligning {len(halts)} events for session '{session_name}'\")\n",
    "\n",
    "#         # Process all halt events efficiently\n",
    "#         aligned_data = []\n",
    "#         for halt_time in halts:\n",
    "#             window_data = process_aligned_data(df, halt_time, time_window_start, time_window_end)\n",
    "#             if window_data is not None:\n",
    "#                 aligned_data.append(window_data)\n",
    "\n",
    "#         if not aligned_data:\n",
    "#             print(f\"No aligned data generated for {session_name}, skipping\")\n",
    "#             continue\n",
    "\n",
    "#         # Concatenate once\n",
    "#         aligned_df = pd.concat(aligned_data, ignore_index=True)\n",
    "\n",
    "#         # Save aligned data\n",
    "#         aligned_dir = data_path.parent / \"aligned_data\"\n",
    "#         aligned_dir.mkdir(exist_ok=True)\n",
    "#         aligned_file = aligned_dir / f\"{session_name}_{event_name}_aligned.csv\"\n",
    "        \n",
    "#         # Use efficient CSV writing\n",
    "#         aligned_df.to_csv(aligned_file, index=False, float_format='%.4f')\n",
    "#         print(f\"Saved aligned data to {aligned_file}\")\n",
    "\n",
    "#         # Create plots\n",
    "#         print(f\"📈 Creating plots for {session_name}\")\n",
    "        \n",
    "#         # Summary plot\n",
    "#         summary_path = data_path.parent / f\"{session_name}_{event_name}.pdf\"\n",
    "#         create_summary_plot(aligned_df, session_name, event_name, summary_path)\n",
    "#         print(f\"Saved summary plot\")\n",
    "\n",
    "#         # Heatmaps - only create if we have enough data\n",
    "#         unique_halts = aligned_df[\"Halt Time\"].nunique()\n",
    "#         if unique_halts > 1:\n",
    "#             # z_470 heatmap\n",
    "#             pivot_470 = aligned_df.pivot_table(index=\"Halt Time\", columns=\"Time (s)\", \n",
    "#                                              values=\"z_470\", aggfunc='first')\n",
    "#             heatmap_470_path = data_path.parent / f\"{session_name}_{event_name}_heatmap_z_470.pdf\"\n",
    "#             create_heatmap(pivot_470, session_name, event_name, \"z_470\", heatmap_470_path)\n",
    "#             print(f\"Saved z_470 heatmap\")\n",
    "\n",
    "#             # z_560 heatmap  \n",
    "#             pivot_560 = aligned_df.pivot_table(index=\"Halt Time\", columns=\"Time (s)\", \n",
    "#                                              values=\"z_560\", aggfunc='first')\n",
    "#             heatmap_560_path = data_path.parent / f\"{session_name}_{event_name}_heatmap_z_560.pdf\"\n",
    "#             create_heatmap(pivot_560, session_name, event_name, \"z_560\", heatmap_560_path)\n",
    "#             print(f\"Saved z_560 heatmap\")\n",
    "\n",
    "#             # df470 heatmap\n",
    "#             pivot_470df = aligned_df.pivot_table(index=\"Halt Time\", columns=\"Time (s)\", \n",
    "#                                              values=\"dfF_470\", aggfunc='first')\n",
    "#             heatmap_470df_path = data_path.parent / f\"{session_name}_{event_name}_heatmap_dfF_470.pdf\"\n",
    "#             create_heatmap(pivot_470df, session_name, event_name, \"dfF_470\", heatmap_470df_path)\n",
    "#             print(f\"Saved df_470 heatmap\")\n",
    "\n",
    "#             #df560 heatmap\n",
    "#             pivot_560df = aligned_df.pivot_table(index=\"Halt Time\", columns=\"Time (s)\", \n",
    "#                                             values=\"dfF_560\", aggfunc='first')\n",
    "#             heatmap_560df_path = data_path.parent / f\"{session_name}_{event_name}_heatmap_df_560.pdf\"\n",
    "#             create_heatmap(pivot_560df, session_name, event_name, \"dfF_560\", heatmap_560df_path)\n",
    "#             print(f\"Saved df_560 heatmap\")\n",
    "#         else:\n",
    "#             print(\"Insufficient data for heatmaps (need >1 event)\")\n",
    "\n",
    "#         # Explicit memory cleanup\n",
    "#         del aligned_data, aligned_df\n",
    "#         if 'pivot_470' in locals():\n",
    "#             del pivot_470\n",
    "#         if 'pivot_560' in locals():\n",
    "#             del pivot_560\n",
    "#         if 'pivot_470df' in locals():\n",
    "#             del pivot_470df\n",
    "#         if 'pivot_560df' in locals():\n",
    "#             del pivot_560df\n",
    "#         gc.collect()\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"ERROR processing {data_path}: {str(e)}\")\n",
    "#         traceback.print_exc()\n",
    "#         # Clean up on error\n",
    "#         gc.collect()\n",
    "\n",
    "# print(\"✅ Finished all data paths.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36d8084",
   "metadata": {},
   "outputs": [],
   "source": [
    "#separates RIGHT vs LEFT TURNS, creates heatmaps, and comprehensive (Mean +- sem) aligned data for each data path\n",
    "#----------------------------------------------------\n",
    "\"\"\"\n",
    "Refactored photometry analysis code for processing aligned behavioral data.\n",
    "Separates left vs right turns, creates heatmaps, and generates comprehensive plots.\n",
    "\"\"\"\n",
    "class PhotometryAnalyzer:\n",
    "    \"\"\"Class for analyzing photometry data with behavioral events.\"\"\"\n",
    "    \n",
    "    # Class constants\n",
    "    REQUIRED_COLUMNS = [\n",
    "        \"Time (s)\", \"Photodiode_int\", \"z_470\", \"z_560\", \n",
    "        \"dfF_470\", \"dfF_560\", \"Motor_Velocity\", \"Velocity_0X\", \"Velocity_0Y\"\n",
    "    ]\n",
    "    \n",
    "    FLUORESCENCE_CHANNELS = {\n",
    "        'z_470': {'color': 'cornflowerblue', 'label': 'z_470'},\n",
    "        'z_560': {'color': 'red', 'label': 'z_560'},\n",
    "        'dfF_470': {'color': 'blue', 'label': 'dfF_470'},\n",
    "        'dfF_560': {'color': 'orange', 'label': 'dfF_560'}\n",
    "    }\n",
    "    \n",
    "    def __init__(self, time_window: Tuple[float, float] = (time_window_start, time_window_end)):\n",
    "        \"\"\"\n",
    "        Initialize analyzer with time window parameters.\n",
    "        \n",
    "        Args:\n",
    "            time_window: Tuple of (start, end) times relative to event (seconds)\n",
    "        \"\"\"\n",
    "        self.time_window_start, self.time_window_end = time_window\n",
    "        \n",
    "    def process_aligned_data(self, df: pd.DataFrame, halt_time: pd.Timestamp) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Process a single halt event efficiently.\n",
    "        \n",
    "        Args:\n",
    "            df: Main dataframe with photometry and behavioral data\n",
    "            halt_time: Timestamp of the halt event\n",
    "            \n",
    "        Returns:\n",
    "            Windowed dataframe or None if no data in window\n",
    "        \"\"\"\n",
    "        window_start = halt_time + pd.Timedelta(seconds=self.time_window_start)\n",
    "        window_end = halt_time + pd.Timedelta(seconds=self.time_window_end)\n",
    "        mask = (df.index >= window_start) & (df.index <= window_end)\n",
    "        \n",
    "        if not mask.any():\n",
    "            return None\n",
    "        \n",
    "        window = df.loc[mask].copy()\n",
    "        window[\"Time (s)\"] = (window.index - halt_time).total_seconds()\n",
    "        window[\"Halt Time\"] = halt_time\n",
    "        return window\n",
    "    \n",
    "    def separate_turns(self, aligned_df: pd.DataFrame) -> Tuple[List[pd.Timestamp], List[pd.Timestamp]]:\n",
    "        \"\"\"\n",
    "        Separate halt events into left and right turns based on motor velocity.\n",
    "        \n",
    "        Args:\n",
    "            aligned_df: Aligned dataframe with all halt events\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (left_turn_halts, right_turn_halts)\n",
    "        \"\"\"\n",
    "        left_turn_halts = []\n",
    "        right_turn_halts = []\n",
    "        \n",
    "        for halt in aligned_df[\"Halt Time\"].unique():\n",
    "            # Look at motor velocity in pre-event window\n",
    "            subset = aligned_df[\n",
    "                (aligned_df[\"Halt Time\"] == halt) & \n",
    "                (aligned_df[\"Time (s)\"] >= -1) & \n",
    "                (aligned_df[\"Time (s)\"] < 0)\n",
    "            ]\n",
    "            \n",
    "            if subset.empty:\n",
    "                continue\n",
    "                \n",
    "            mean_velocity = subset[\"Motor_Velocity\"].mean()\n",
    "            \n",
    "            if mean_velocity < 0:  # Negative = left turn\n",
    "                left_turn_halts.append(halt)\n",
    "            elif mean_velocity > 0:  # Positive = right turn\n",
    "                right_turn_halts.append(halt)\n",
    "        \n",
    "        return left_turn_halts, right_turn_halts\n",
    "    \n",
    "    def save_turn_data(self, aligned_df: pd.DataFrame, left_turns: List, right_turns: List, \n",
    "                      session_name: str, event_name: str, output_dir: Path) -> None:\n",
    "        \"\"\"Save separated turn data to CSV files only if turns are detected.\"\"\"\n",
    "        \n",
    "        # Only save left turns if there are any\n",
    "        if left_turns:\n",
    "            left_df = aligned_df[aligned_df[\"Halt Time\"].isin(left_turns)]\n",
    "            left_file = output_dir / f\"{session_name}_{event_name}_left_turns.csv\"\n",
    "            left_df.to_csv(left_file, index=False, float_format='%.4f')\n",
    "            print(f\"Saved {len(left_turns)} left turns to {left_file}\")\n",
    "        else:\n",
    "            print(f\"No left turns detected - no CSV file saved\")\n",
    "        \n",
    "        # Only save right turns if there are any\n",
    "        if right_turns:\n",
    "            right_df = aligned_df[aligned_df[\"Halt Time\"].isin(right_turns)]\n",
    "            right_file = output_dir / f\"{session_name}_{event_name}_right_turns.csv\"\n",
    "            right_df.to_csv(right_file, index=False, float_format='%.4f')\n",
    "            print(f\"Saved {len(right_turns)} right turns to {right_file}\")\n",
    "        else:\n",
    "            print(f\"No right turns detected - no CSV file saved\")\n",
    "    \n",
    "    def create_heatmap(self, pivot_data: pd.DataFrame, session_name: str, event_name: str, \n",
    "                      channel: str, save_path: Path, figsize: Tuple[int, int] = (10, 6)) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create and save normalized heatmap.\n",
    "        \n",
    "        Args:\n",
    "            pivot_data: Pivoted data (events x time)\n",
    "            session_name: Name of session\n",
    "            event_name: Name of event type\n",
    "            channel: Channel name (e.g., 'z_470')\n",
    "            save_path: Path to save figure\n",
    "            figsize: Figure size tuple\n",
    "            \n",
    "        Returns:\n",
    "            Normalized data used for heatmap\n",
    "        \"\"\"\n",
    "        # Baseline normalization\n",
    "        baseline_cols = (pivot_data.columns >= -1) & (pivot_data.columns < 0)\n",
    "        if baseline_cols.any():\n",
    "            baseline_means = pivot_data.loc[:, baseline_cols].mean(axis=1)\n",
    "            normalized_data = pivot_data.subtract(baseline_means, axis=0)\n",
    "        else:\n",
    "            normalized_data = pivot_data\n",
    "        \n",
    "        # Create figure\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        \n",
    "        sns.heatmap(\n",
    "            normalized_data, \n",
    "            cmap=\"RdBu_r\", \n",
    "            center=0, \n",
    "            ax=ax,\n",
    "            cbar_kws={'label': f'Normalized {channel}'},\n",
    "            rasterized=True\n",
    "        )\n",
    "        \n",
    "        ax.set_title(f\"Heatmap ({channel}) - {session_name}\")\n",
    "        ax.set_xlabel(\"Time (s)\")\n",
    "        ax.set_ylabel(\"Event\")\n",
    "        \n",
    "        # Optimize y-axis ticks\n",
    "        n_events = len(normalized_data.index)\n",
    "        y_positions = range(0, n_events, max(1, n_events // 10))\n",
    "        y_labels = [str(i+1) for i in y_positions]\n",
    "        ax.set_yticks(y_positions)\n",
    "        ax.set_yticklabels(y_labels)\n",
    "        \n",
    "        # Add event line at time 0\n",
    "        if 0 in normalized_data.columns:\n",
    "            zero_idx = list(normalized_data.columns).index(0)\n",
    "            ax.axvline(zero_idx, linestyle='--', color='black', alpha=0.7)\n",
    "        \n",
    "        # Optimize x-axis ticks\n",
    "        time_cols = normalized_data.columns\n",
    "        tick_indices = [i for i, val in enumerate(time_cols) \n",
    "                       if isinstance(val, (int, float)) and val % 2 == 0]\n",
    "        tick_labels = [f\"{int(time_cols[i])}\" for i in tick_indices]\n",
    "        \n",
    "        ax.set_xticks(tick_indices)\n",
    "        ax.set_xticklabels(tick_labels, rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path, dpi=200, bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "        \n",
    "        return normalized_data\n",
    "    \n",
    "    def create_line_collections(self, df: pd.DataFrame, channels: List[str]) -> Dict[str, LineCollection]:\n",
    "        \"\"\"Create line collections for individual traces.\"\"\"\n",
    "        line_collections = {}\n",
    "        \n",
    "        for channel in channels:\n",
    "            lines = []\n",
    "            for halt in df[\"Halt Time\"].unique():\n",
    "                subset = df[df[\"Halt Time\"] == halt]\n",
    "                time_vals = subset[\"Time (s)\"].values\n",
    "                channel_vals = subset[channel].values\n",
    "                lines.append(list(zip(time_vals, channel_vals)))\n",
    "            \n",
    "            color = self.FLUORESCENCE_CHANNELS.get(channel, {}).get('color', 'gray')\n",
    "            line_collections[channel] = LineCollection(lines, colors=color, alpha=0.3, linewidths=1)\n",
    "        \n",
    "        return line_collections\n",
    "    \n",
    "    def add_mean_sem_plot(self, ax: plt.Axes, df: pd.DataFrame, channels: List[str], \n",
    "                         turn_type: str, line_style: str = '-') -> None:\n",
    "        \"\"\"Add mean ± SEM traces to axis.\"\"\"\n",
    "        grouped = df.groupby(\"Time (s)\")\n",
    "        time_index = grouped.mean().index.values\n",
    "        \n",
    "        for channel in channels:\n",
    "            color = self.FLUORESCENCE_CHANNELS[channel]['color']\n",
    "            label = f\"{turn_type} {channel}\"\n",
    "            \n",
    "            mean_vals = grouped.mean()[channel]\n",
    "            sem_vals = grouped.sem()[channel]\n",
    "            \n",
    "            ax.plot(time_index, mean_vals, color=color, linestyle=line_style, \n",
    "                   linewidth=2, label=label)\n",
    "            ax.fill_between(time_index, mean_vals - sem_vals, mean_vals + sem_vals,\n",
    "                           color=color, alpha=0.2)\n",
    "    \n",
    "    def create_summary_plot(self, aligned_df: pd.DataFrame, session_name: str, \n",
    "                          event_name: str, save_path: Path) -> None:\n",
    "        \"\"\"\n",
    "        Create comprehensive summary plots comparing left vs right turns.\n",
    "        \n",
    "        Args:\n",
    "            aligned_df: Aligned dataframe with all events\n",
    "            session_name: Session identifier\n",
    "            event_name: Event type name\n",
    "            save_path: Path to save the plot\n",
    "        \"\"\"\n",
    "        # Separate turns\n",
    "        left_turns, right_turns = self.separate_turns(aligned_df)\n",
    "        \n",
    "        if not left_turns and not right_turns:\n",
    "            print(\"No valid turns found for summary plot\")\n",
    "            return\n",
    "        \n",
    "        # Create DataFrames for each turn type\n",
    "        left_df = aligned_df[aligned_df[\"Halt Time\"].isin(left_turns)]\n",
    "        right_df = aligned_df[aligned_df[\"Halt Time\"].isin(right_turns)]\n",
    "        \n",
    "        # Create figure\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(21, 6), sharex=True)\n",
    "        z_channels = ['z_470', 'z_560']\n",
    "        \n",
    "        # Left plot - Left turn traces\n",
    "        if not left_df.empty:\n",
    "            ax1 = axes[0]\n",
    "            left_collections = self.create_line_collections(left_df, z_channels + ['Motor_Velocity'])\n",
    "            \n",
    "            # Add fluorescence traces\n",
    "            for channel in z_channels:\n",
    "                ax1.add_collection(left_collections[channel])\n",
    "            \n",
    "            # Add motor velocity on secondary axis\n",
    "            ax1_motor = ax1.twinx()\n",
    "            ax1_motor.add_collection(left_collections['Motor_Velocity'])\n",
    "            ax1_motor.set_ylabel(\"Motor Velocity\", color='slategray')\n",
    "            ax1_motor.tick_params(axis='y', labelcolor='slategray')\n",
    "            ax1_motor.autoscale()\n",
    "            \n",
    "            ax1.set_title(f'Left Turn Traces (n={len(left_turns)})')\n",
    "            ax1.set_ylabel(\"Fluorescence (z-score)\")\n",
    "            ax1.axvline(0, linestyle='--', color='black', alpha=0.7)\n",
    "            ax1.autoscale()\n",
    "        \n",
    "        # Right plot - Right turn traces  \n",
    "        if not right_df.empty:\n",
    "            ax2 = axes[1]\n",
    "            right_collections = self.create_line_collections(right_df, z_channels + ['Motor_Velocity'])\n",
    "            \n",
    "            # Add fluorescence traces\n",
    "            for channel in z_channels:\n",
    "                ax2.add_collection(right_collections[channel])\n",
    "            \n",
    "            # Add motor velocity on secondary axis\n",
    "            ax2_motor = ax2.twinx()\n",
    "            ax2_motor.add_collection(right_collections['Motor_Velocity'])\n",
    "            ax2_motor.set_ylabel(\"Motor Velocity\", color='slategray')\n",
    "            ax2_motor.tick_params(axis='y', labelcolor='slategray')\n",
    "            ax2_motor.autoscale()\n",
    "            \n",
    "            ax2.set_title(f'Right Turn Traces (n={len(right_turns)})')\n",
    "            ax2.set_ylabel(\"Fluorescence (z-score)\")\n",
    "            ax2.axvline(0, linestyle='--', color='black', alpha=0.7)\n",
    "            ax2.autoscale()\n",
    "        \n",
    "        # Comparison plot - Mean ± SEM\n",
    "        ax3 = axes[2]\n",
    "        \n",
    "        if not left_df.empty:\n",
    "            self.add_mean_sem_plot(ax3, left_df, z_channels, \"Left\", '--')\n",
    "        if not right_df.empty:\n",
    "            self.add_mean_sem_plot(ax3, right_df, z_channels, \"Right\", '-')\n",
    "        \n",
    "        # Add motor velocity comparison\n",
    "        ax3_motor = ax3.twinx()\n",
    "        motor_color = 'slategray'\n",
    "        \n",
    "        if not left_df.empty:\n",
    "            left_motor_grouped = left_df.groupby(\"Time (s)\")\n",
    "            time_idx = left_motor_grouped.mean().index.values\n",
    "            mean_motor = left_motor_grouped.mean()[\"Motor_Velocity\"]\n",
    "            sem_motor = left_motor_grouped.sem()[\"Motor_Velocity\"]\n",
    "            \n",
    "            ax3_motor.plot(time_idx, mean_motor, color=motor_color, linestyle='--', \n",
    "                          linewidth=1.5, label=\"Left Motor\")\n",
    "            ax3_motor.fill_between(time_idx, mean_motor - sem_motor, mean_motor + sem_motor,\n",
    "                                  color=motor_color, alpha=0.2)\n",
    "        \n",
    "        if not right_df.empty:\n",
    "            right_motor_grouped = right_df.groupby(\"Time (s)\")\n",
    "            time_idx = right_motor_grouped.mean().index.values\n",
    "            mean_motor = right_motor_grouped.mean()[\"Motor_Velocity\"]\n",
    "            sem_motor = right_motor_grouped.sem()[\"Motor_Velocity\"]\n",
    "            \n",
    "            ax3_motor.plot(time_idx, mean_motor, color=motor_color, linestyle='-',\n",
    "                          linewidth=1.5, label=\"Right Motor\")\n",
    "            ax3_motor.fill_between(time_idx, mean_motor - sem_motor, mean_motor + sem_motor,\n",
    "                                  color=motor_color, alpha=0.2)\n",
    "        \n",
    "        ax3_motor.set_ylabel(\"Motor Velocity\", color=motor_color)\n",
    "        ax3_motor.tick_params(axis='y', labelcolor=motor_color)\n",
    "        ax3_motor.axhline(0, linestyle='--', color='gray', alpha=0.5)\n",
    "        ax3_motor.legend(loc='upper right')\n",
    "        \n",
    "        ax3.axvline(0, linestyle='--', color='black', alpha=0.7)\n",
    "        ax3.set_xlabel(\"Time (s)\")\n",
    "        ax3.set_ylabel(\"Fluorescence (z-score)\")\n",
    "        ax3.set_title(\"Mean ± SEM Comparison\")\n",
    "        ax3.legend(loc='upper left')\n",
    "        \n",
    "        # Format all x-axes\n",
    "        for ax in axes:\n",
    "            ax.set_xlabel(\"Time (s)\")\n",
    "        \n",
    "        fig.suptitle(f\"{session_name} - {event_name}\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path, dpi=200, bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "        plt.ioff()\n",
    "    \n",
    "    def process_session(self, data_path: Path, data: Dict[str, Any], \n",
    "                       variables: Dict[str, Any], event_name: str = \"halt\") -> None:\n",
    "        \"\"\"\n",
    "        Process a complete session of data.\n",
    "        \n",
    "        Args:\n",
    "            data_path: Path to the data directory\n",
    "            data: Dictionary containing loaded data\n",
    "            variables: Dictionary containing analysis variables\n",
    "            event_name: Name of the event type being analyzed\n",
    "        \"\"\"\n",
    "        print(f\"\\n--------- Processing: {data_path} ---------\")\n",
    "        \n",
    "        try:\n",
    "            # Extract data components\n",
    "            df = data[\"photometry_tracking_encoder_data\"]\n",
    "            halts = variables[\"photodiode_halts\"]\n",
    "            \n",
    "            # Get session name\n",
    "            session_name = variables.get(\"session_name\")\n",
    "            if not session_name:\n",
    "                mouse_name = data.get(\"mouse_name\", \"unknown_mouse\")\n",
    "                session_name = f\"{mouse_name}_{data_path.stem}\"\n",
    "                print(f\"No session_name found, using: {session_name}\")\n",
    "            \n",
    "            print(f\"Aligning {len(halts)} events for session '{session_name}'\")\n",
    "            \n",
    "            # Process all halt events\n",
    "            aligned_data = []\n",
    "            for halt_time in halts:\n",
    "                window_data = self.process_aligned_data(df, halt_time)\n",
    "                if window_data is not None:\n",
    "                    aligned_data.append(window_data)\n",
    "            \n",
    "            if not aligned_data:\n",
    "                print(f\"No aligned data generated for {session_name}, skipping\")\n",
    "                return\n",
    "            \n",
    "            # Combine all aligned data\n",
    "            aligned_df = pd.concat(aligned_data, ignore_index=True)\n",
    "            \n",
    "            # Create output directory\n",
    "            aligned_dir = data_path.parent / \"aligned_data\"\n",
    "            aligned_dir.mkdir(exist_ok=True)\n",
    "            \n",
    "            # Save main aligned data\n",
    "            aligned_file = aligned_dir / f\"{session_name}_{event_name}_aligned.csv\"\n",
    "            aligned_df.to_csv(aligned_file, index=False, float_format='%.4f')\n",
    "            print(f\"Saved aligned data to {aligned_file}\")\n",
    "            \n",
    "            # Separate and save turn data\n",
    "            left_turns, right_turns = self.separate_turns(aligned_df)\n",
    "            self.save_turn_data(aligned_df, left_turns, right_turns, \n",
    "                              session_name, event_name, aligned_dir)\n",
    "            \n",
    "            # Create summary plot\n",
    "            print(f\"📈 Creating plots for {session_name}\")\n",
    "            summary_path = data_path.parent / f\"{session_name}_{event_name}.pdf\"\n",
    "            self.create_summary_plot(aligned_df, session_name, event_name, summary_path)\n",
    "            print(f\"Saved summary plot to {summary_path}\")\n",
    "            \n",
    "            # Create heatmaps if sufficient data\n",
    "            unique_halts = aligned_df[\"Halt Time\"].nunique()\n",
    "            if unique_halts > 1:\n",
    "                self._create_all_heatmaps(aligned_df, session_name, event_name, data_path)\n",
    "            else:\n",
    "                print(\"Insufficient data for heatmaps (need >1 event)\")\n",
    "            \n",
    "            # Cleanup\n",
    "            del aligned_data, aligned_df\n",
    "            gc.collect()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"ERROR processing {data_path}: {str(e)}\")\n",
    "            traceback.print_exc()\n",
    "            gc.collect()\n",
    "    \n",
    "    def _create_all_heatmaps(self, aligned_df: pd.DataFrame, session_name: str, \n",
    "                           event_name: str, data_path: Path) -> None:\n",
    "        \"\"\"Create all heatmaps for different channels.\"\"\"\n",
    "        heatmap_channels = ['z_470', 'z_560', 'dfF_470', 'dfF_560']\n",
    "        \n",
    "        for channel in heatmap_channels:\n",
    "            try:\n",
    "                # Create pivot table\n",
    "                pivot_data = aligned_df.pivot_table(\n",
    "                    index=\"Halt Time\", \n",
    "                    columns=\"Time (s)\", \n",
    "                    values=channel, \n",
    "                    aggfunc='first'\n",
    "                )\n",
    "                \n",
    "                # Create heatmap\n",
    "                heatmap_path = data_path.parent / f\"{session_name}_{event_name}_heatmap_{channel}.pdf\"\n",
    "                self.create_heatmap(pivot_data, session_name, event_name, channel, heatmap_path)\n",
    "                print(f\"Saved {channel} heatmap\")\n",
    "                \n",
    "                # Cleanup\n",
    "                del pivot_data\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error creating {channel} heatmap: {e}\")\n",
    "        \n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "def main(data_paths: List[Path], loaded_data: Dict, data_path_variables: Dict, \n",
    "         event_name: str = \"halt\", time_window: Tuple[float, float] = (-5, 10)):\n",
    "    \"\"\"\n",
    "    Main processing function.\n",
    "    \n",
    "    Args:\n",
    "        data_paths: List of data directory paths\n",
    "        loaded_data: Dictionary of loaded data for each path\n",
    "        data_path_variables: Dictionary of analysis variables for each path\n",
    "        event_name: Name of event type (default: \"halt\")\n",
    "        time_window: Tuple of (start, end) times relative to event in seconds\n",
    "    \"\"\"\n",
    "    # Initialize analyzer\n",
    "    analyzer = PhotometryAnalyzer(time_window)\n",
    "    \n",
    "    # Process each data path\n",
    "    for idx, data_path in enumerate(data_paths, start=1):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Processing {idx}/{len(data_paths)}: {data_path.name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        if data_path not in data_path_variables:\n",
    "            print(f\"Skipping {data_path} - no analysis data found\")\n",
    "            continue\n",
    "        \n",
    "        if data_path not in loaded_data:\n",
    "            print(f\"Skipping {data_path} - no loaded data found\")\n",
    "            continue\n",
    "        \n",
    "        # Process this session\n",
    "        analyzer.process_session(\n",
    "            data_path, \n",
    "            loaded_data[data_path], \n",
    "            data_path_variables[data_path], \n",
    "            event_name\n",
    "        )\n",
    "    \n",
    "    print(\"\\n✅ Finished processing all data paths.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de826ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_window = (time_window_start, time_window_end) \n",
    "main(data_paths, loaded_data, data_path_variables, \n",
    "     event_name=event_name, time_window=time_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1364a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----oldcode----------------------------\n",
    "#  def baseline_aligned_data(aligned_df, baseline_window, mouse_name):\n",
    "#     # ---------------- Baseline Correction ----------------\n",
    "#     baseline_df = aligned_df[\n",
    "#         (aligned_df[\"Time (s)\"] >= baseline_window[0]) & \n",
    "#         (aligned_df[\"Time (s)\"] <= baseline_window[1])\n",
    "#     ].groupby(\"Halt Time\").mean()\n",
    "\n",
    "#     for signal_name in [\"z_470\", \"z_560\", \"Motor_Velocity\", \"Velocity_0X\", \"Velocity_0Y\"]:\n",
    "#         aligned_df[f\"{signal_name}_Baseline\"] = aligned_df[signal_name] - aligned_df[\"Halt Time\"].map(baseline_df[signal_name])\n",
    "\n",
    "#     # ---------------- Mean and SEM ----------------\n",
    "#     mean_baseline_df = aligned_df.groupby(\"Time (s)\").mean()\n",
    "#     sem_baseline_df = aligned_df.groupby(\"Time (s)\").sem()\n",
    "\n",
    "#     def get_symmetric_ylim(mean_data, sem_data):\n",
    "#         max_abs_value = max(\n",
    "#             abs(mean_data).max() + sem_data.max(),\n",
    "#             abs(mean_data).min() - sem_data.min()\n",
    "#         )\n",
    "#         return (-max_abs_value, max_abs_value)\n",
    "\n",
    "#     # ---------------- Plotting ----------------\n",
    "#     fig, ax = plt.subplots(figsize=(plot_width, 6))\n",
    "\n",
    "#     ax.plot(mean_baseline_df.index, mean_baseline_df[\"Photodiode_int\"], color='grey', alpha=0.8)\n",
    "#     ax.fill_between(mean_baseline_df.index,\n",
    "#                     mean_baseline_df[\"Photodiode_int\"] - sem_baseline_df[\"Photodiode_int\"],\n",
    "#                     mean_baseline_df[\"Photodiode_int\"] + sem_baseline_df[\"Photodiode_int\"],\n",
    "#                     color='grey', alpha=0.2)\n",
    "\n",
    "#     ax.set_xlabel('Time (s) relative to halt')\n",
    "#     ax.set_ylabel('Photodiode', color='grey')\n",
    "#     ax.set_title(f'Baselined Mean & SEM of All Signals - {mouse_name}')\n",
    "\n",
    "#     # z_470 and z_560\n",
    "#     ax2 = ax.twinx()\n",
    "#     ax2.plot(mean_baseline_df.index, mean_baseline_df[\"z_470_Baseline\"], color='green', alpha=0.8)\n",
    "#     ax2.fill_between(mean_baseline_df.index,\n",
    "#                      mean_baseline_df[\"z_470_Baseline\"] - sem_baseline_df[\"z_470_Baseline\"],\n",
    "#                      mean_baseline_df[\"z_470_Baseline\"] + sem_baseline_df[\"z_470_Baseline\"],\n",
    "#                      color='green', alpha=0.2)\n",
    "#     ax2.plot(mean_baseline_df.index, mean_baseline_df[\"z_560_Baseline\"], color='red', alpha=0.8)\n",
    "#     ax2.fill_between(mean_baseline_df.index,\n",
    "#                      mean_baseline_df[\"z_560_Baseline\"] - sem_baseline_df[\"z_560_Baseline\"],\n",
    "#                      mean_baseline_df[\"z_560_Baseline\"] + sem_baseline_df[\"z_560_Baseline\"],\n",
    "#                      color='red', alpha=0.2)\n",
    "#     ax2.set_ylabel('Fluorescence (z-score, red 560nm)', color='green')\n",
    "#     ax2.set_ylim(get_symmetric_ylim(\n",
    "#         pd.concat([mean_baseline_df[\"z_470_Baseline\"], mean_baseline_df[\"z_560_Baseline\"]]),\n",
    "#         pd.concat([sem_baseline_df[\"z_470_Baseline\"], sem_baseline_df[\"z_560_Baseline\"]])\n",
    "#     ))\n",
    "#     ax2.yaxis.label.set_color('green')\n",
    "\n",
    "#     # Motor velocity\n",
    "#     ax3 = ax.twinx()\n",
    "#     ax3.spines['right'].set_position(('outward', 50))\n",
    "#     ax3.plot(mean_baseline_df.index, mean_baseline_df[\"Motor_Velocity_Baseline\"], color='#00008B', alpha=0.8)\n",
    "#     ax3.fill_between(mean_baseline_df.index,\n",
    "#                      mean_baseline_df[\"Motor_Velocity_Baseline\"] - sem_baseline_df[\"Motor_Velocity_Baseline\"],\n",
    "#                      mean_baseline_df[\"Motor_Velocity_Baseline\"] + sem_baseline_df[\"Motor_Velocity_Baseline\"],\n",
    "#                      color='#00008B', alpha=0.2)\n",
    "#     ax3.set_ylabel('Motor Velocity (deg/s²)', color='#00008B')\n",
    "#     ax3.set_ylim(get_symmetric_ylim(mean_baseline_df[\"Motor_Velocity_Baseline\"], sem_baseline_df[\"Motor_Velocity_Baseline\"]))\n",
    "#     ax3.yaxis.label.set_color('#00008B')\n",
    "\n",
    "#     # Running velocity (Velocity_0X)\n",
    "#     ax4 = ax.twinx()\n",
    "#     ax4.spines['right'].set_position(('outward', 100))\n",
    "#     ax4.plot(mean_baseline_df.index, mean_baseline_df[\"Velocity_0X_Baseline\"] * 1000, color='orange', alpha=0.8)\n",
    "#     ax4.fill_between(mean_baseline_df.index,\n",
    "#                      (mean_baseline_df[\"Velocity_0X_Baseline\"] - sem_baseline_df[\"Velocity_0X_Baseline\"]) * 1000,\n",
    "#                      (mean_baseline_df[\"Velocity_0X_Baseline\"] + sem_baseline_df[\"Velocity_0X_Baseline\"]) * 1000,\n",
    "#                      color='orange', alpha=0.2)\n",
    "#     ax4.set_ylabel('Running velocity (mm/s²) WRONG SCALE?', color='orange')\n",
    "#     ax4.set_ylim(get_symmetric_ylim(mean_baseline_df[\"Velocity_0X_Baseline\"] * 1000, sem_baseline_df[\"Velocity_0X_Baseline\"] * 1000))\n",
    "#     ax4.yaxis.label.set_color('orange')\n",
    "\n",
    "#     # Turning velocity (Velocity_0Y)\n",
    "#     ax5 = ax.twinx()\n",
    "#     ax5.spines['right'].set_position(('outward', 150))\n",
    "#     ax5.plot(mean_baseline_df.index, mean_baseline_df[\"Velocity_0Y_Baseline\"], color='#4682B4', alpha=0.8)\n",
    "#     ax5.fill_between(mean_baseline_df.index,\n",
    "#                      mean_baseline_df[\"Velocity_0Y_Baseline\"] - sem_baseline_df[\"Velocity_0Y_Baseline\"],\n",
    "#                      mean_baseline_df[\"Velocity_0Y_Baseline\"] + sem_baseline_df[\"Velocity_0Y_Baseline\"],\n",
    "#                      color='#4682B4', alpha=0.2)\n",
    "#     ax5.set_ylabel('Turning velocity (deg/s²) WRONG SCALE?', color='#4682B4')\n",
    "#     ax5.set_ylim(get_symmetric_ylim(mean_baseline_df[\"Velocity_0Y_Baseline\"], sem_baseline_df[\"Velocity_0Y_Baseline\"]))\n",
    "#     ax5.yaxis.label.set_color('#4682B4')\n",
    "\n",
    "#     fig.tight_layout()\n",
    "\n",
    "#     # Save the figure\n",
    "#     try:          \n",
    "#         figure_file = data_path.parent / f\"{session_name}_{event_name}_baselined.pdf\"\n",
    "#         fig.savefig(figure_file, dpi=1200, bbox_inches='tight')\n",
    "#         print(f\"✅ Saved figure to {figure_file}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"⚠️ ERROR saving figure: {str(e)}\")    \n",
    "\n",
    "#     plt.close(fig)\n",
    "#     return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83515f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----oldcode----------------------------\n",
    "# #--------baselining definitions--------------------------------------------\n",
    "# def process_aligned_data_folders(data_dirs, baseline_window, event_name=event_name, plot_width=12, create_plots=True):\n",
    "#     \"\"\"\n",
    "#     Process all aligned_data folders and generate baseline plots.\n",
    "    \n",
    "#     Parameters:\n",
    "#     -----------\n",
    "#     data_dirs : list\n",
    "#         List of Path objects pointing to your main data directories\n",
    "#     baseline_window : tuple\n",
    "#         Tuple of (start_time, end_time) for baseline window\n",
    "#     event_name : str\n",
    "#         Event name for file naming (default: \"halt\")\n",
    "#     plot_width : int\n",
    "#         Width of the plot in inches\n",
    "#     create_plots : bool\n",
    "#         Whether to create and save plots (default: True)\n",
    "#     \"\"\"\n",
    "    \n",
    "#     results = {\n",
    "#         'processed': [],\n",
    "#         'errors': [],\n",
    "#         'total_folders': 0\n",
    "#     }\n",
    "    \n",
    "#     # Find all aligned_data folders\n",
    "#     aligned_folders = []\n",
    "#     for data_dir in data_dirs:\n",
    "#         print(f\"Searching in: {data_dir}\")\n",
    "#         # Find all aligned_data folders recursively\n",
    "#         found_folders = list(data_dir.rglob(\"aligned_data\"))\n",
    "#         aligned_folders.extend(found_folders)\n",
    "#         print(f\"  Found {len(found_folders)} aligned_data folders\")\n",
    "    \n",
    "#     results['total_folders'] = len(aligned_folders)\n",
    "#     print(f\"\\nTotal aligned_data folders found: {len(aligned_folders)}\")\n",
    "    \n",
    "#     for aligned_folder in aligned_folders:\n",
    "#         try:\n",
    "#             print(f\"\\n📁 Processing folder: {aligned_folder}\")\n",
    "            \n",
    "#             # Find only the original aligned CSV files (exclude already processed baselined files)\n",
    "#             all_csv_files = list(aligned_folder.glob(\"*.csv\"))\n",
    "#             csv_files = [f for f in all_csv_files if not f.name.endswith('_baselined_data.csv')]\n",
    "            \n",
    "#             if not csv_files:\n",
    "#                 print(f\"  ⚠️  No original aligned CSV files found in {aligned_folder}\")\n",
    "#                 print(f\"  Available files: {[f.name for f in all_csv_files]}\")\n",
    "#                 results['errors'].append({\n",
    "#                     'folder': str(aligned_folder),\n",
    "#                     'error': 'No original aligned CSV files found',\n",
    "#                     'status': 'skipped'\n",
    "#                 })\n",
    "#                 continue\n",
    "#             print(f\"  Found {len(csv_files)} aligned CSV files to process\")\n",
    "            \n",
    "#             for csv_file in csv_files:\n",
    "#                 try:\n",
    "#                     # Check if the CSV file name matches the event name\n",
    "#                     if event_name not in csv_file.name:\n",
    "#                         print(f\"    ⚠️ Skipping {csv_file.name} as it does not match the event name '{event_name}'\")\n",
    "#                         continue\n",
    "                    \n",
    "#                     print(f\"    📊 Processing: {csv_file.name}\")\n",
    "                    \n",
    "#                     # Load the data\n",
    "#                     aligned_df = pd.read_csv(csv_file)\n",
    "                    \n",
    "#                     # Create aligned DataFrames for left and right turns\n",
    "#                     left_turns_csv = csv_file.with_name(csv_file.stem.replace(\"aligned\", \"left_turns\") + \".csv\")\n",
    "#                     right_turns_csv = csv_file.with_name(csv_file.stem.replace(\"aligned\", \"right_turns\") + \".csv\")\n",
    "                    \n",
    "#                     if left_turns_csv.exists():\n",
    "#                         print(f\"    📂 Found left turns CSV: {left_turns_csv.name}\")\n",
    "#                         left_turns_df = pd.read_csv(left_turns_csv)\n",
    "#                     else:\n",
    "#                         print(f\"    ⚠️ Left turns CSV not found: {left_turns_csv.name}\")\n",
    "#                         left_turns_df = None\n",
    "                    \n",
    "#                     if right_turns_csv.exists():\n",
    "#                         print(f\"    📂 Found right turns CSV: {right_turns_csv.name}\")\n",
    "#                         right_turns_df = pd.read_csv(right_turns_csv)\n",
    "#                     else:\n",
    "#                         print(f\"    ⚠️ Right turns CSV not found: {right_turns_csv.name}\")\n",
    "#                         right_turns_df = None\n",
    "                    \n",
    "#                     # Clean up the mouse name (remove extra suffixes)\n",
    "#                     mouse_name = csv_file.stem.replace('_aligned', '').replace('_downsampled_data_Apply halt: 2s', '').split('_')[0]\n",
    "#                     # Get session name from the folder structure\n",
    "#                     session_name = aligned_folder.parent.name\n",
    "#                     # Check if required columns exist\n",
    "#                     required_columns = [\"Time (s)\", \"Halt Time\", \"z_470\", \"z_560\", \"Motor_Velocity\", \n",
    "#                                       \"Velocity_0X\", \"Velocity_0Y\", \"Photodiode_int\"]\n",
    "#                     missing_columns = [col for col in required_columns if col not in aligned_df.columns]\n",
    "                    \n",
    "#                     if missing_columns:\n",
    "#                         print(f\"    ⚠️  Missing columns: {missing_columns}\")\n",
    "#                         print(f\"    Available columns: {list(aligned_df.columns)}\")\n",
    "#                         raise ValueError(f\"Missing required columns: {missing_columns}\")\n",
    "                    \n",
    "#                     # Process the data and create plot\n",
    "#                     fig = baseline_aligned_data_simple(\n",
    "#                         aligned_df=aligned_df,\n",
    "#                         baseline_window=baseline_window,\n",
    "#                         mouse_name=mouse_name,\n",
    "#                         session_name=session_name,\n",
    "#                         event_name=event_name,\n",
    "#                         output_folder=aligned_folder,\n",
    "#                         csv_file=csv_file,\n",
    "#                         plot_width=plot_width,\n",
    "#                         create_plots=create_plots\n",
    "#                     )\n",
    "                    \n",
    "#                     results['processed'].append({\n",
    "#                         'file': str(csv_file),\n",
    "#                         'mouse_name': mouse_name,\n",
    "#                         'session_name': session_name,\n",
    "#                         'folder': str(aligned_folder),\n",
    "#                         'status': 'success'\n",
    "#                     })\n",
    "                    \n",
    "#                 except Exception as e:\n",
    "#                     error_info = {\n",
    "#                         'file': str(csv_file),\n",
    "#                         'error': str(e),\n",
    "#                         'status': 'failed'\n",
    "#                     }\n",
    "#                     results['errors'].append(error_info)\n",
    "#                     print(f\"    ❌ Error processing {csv_file.name}: {str(e)}\")\n",
    "                    \n",
    "#         except Exception as e:\n",
    "#             error_info = {\n",
    "#                 'folder': str(aligned_folder),\n",
    "#                 'error': str(e),\n",
    "#                 'status': 'failed'\n",
    "#             }\n",
    "#             results['errors'].append(error_info)\n",
    "#             print(f\"❌ Error accessing {aligned_folder}: {str(e)}\")\n",
    "    \n",
    "#     # Print summary\n",
    "#     print(f\"\\n{'='*60}\")\n",
    "#     print(f\"PROCESSING SUMMARY\")\n",
    "#     print(f\"{'='*60}\")\n",
    "#     print(f\"Total aligned_data folders: {results['total_folders']}\")\n",
    "#     print(f\"Successfully processed files: {len(results['processed'])}\")\n",
    "#     print(f\"Errors encountered: {len(results['errors'])}\")\n",
    "    \n",
    "#     if results['errors']:\n",
    "#         print(f\"\\nErrors:\")\n",
    "#         for error in results['errors']:\n",
    "#             if 'file' in error:\n",
    "#                 print(f\"  - File {Path(error['file']).name}: {error['error']}\")\n",
    "#             else:\n",
    "#                 print(f\"  - Folder {Path(error['folder']).name}: {error['error']}\")\n",
    "    \n",
    "#     if results['processed']:\n",
    "#         print(f\"\\nSuccessfully processed:\")\n",
    "#         for proc in results['processed']:\n",
    "#             print(f\"  - {proc['mouse_name']} in {Path(proc['folder']).parent.name}\")\n",
    "    \n",
    "#     return results\n",
    "\n",
    "# def baseline_aligned_data_simple(aligned_df, baseline_window, mouse_name, session_name, event_name, output_folder, csv_file, plot_width=12, create_plots=True):\n",
    "# # def baseline_aligned_data_simple(aligned_df, baseline_window, mouse_name, session_name, event_name, output_folder, plot_width=12, create_plots=True):\n",
    "#     \"\"\"\n",
    "#     Simple baseline correction and plotting function.\n",
    "    \n",
    "#     Parameters:\n",
    "#     -----------\n",
    "#     create_plots : bool\n",
    "#         Whether to create and save plots (default: True)\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # # Check if baseline file already exists and skip if it does\n",
    "#     # baseline_data_file = output_folder / f\"{mouse_name}_{event_name}_baselined_data.csv\"\n",
    "#     # if baseline_data_file.exists():\n",
    "#     #     print(f\"      ⚠️  Baseline file already exists, skipping: {baseline_data_file.name}\")\n",
    "#     #     return None\n",
    "\n",
    "#     # # Check if the file corresponds to left or right turn apply halt events\n",
    "#     # if \"left_turns\" in csv_file.name or \"right_turns\" in csv_file.name:\n",
    "#     #     print(f\"      📂 Processing left/right turn apply halt file: {csv_file.name}\")\n",
    "#     # else:\n",
    "#     #     print(f\"      ⚠️  Skipping eft/right turn apply halt file: {csv_file.name}\")\n",
    "#     #     return None\n",
    "#     print(f\"      🔄 Performing baseline correction...\")\n",
    "\n",
    "#     # ---------------- Baseline Correction ----------------\n",
    "#     # Make a copy to avoid modifying the original data\n",
    "#     aligned_df_copy = aligned_df.copy()\n",
    "#     left_turns_file = align\n",
    "    \n",
    "#     baseline_df = aligned_df_copy[\n",
    "#         (aligned_df_copy[\"Time (s)\"] >= baseline_window[0]) & \n",
    "#         (aligned_df_copy[\"Time (s)\"] <= baseline_window[1])\n",
    "#     ].groupby(\"Halt Time\").mean(numeric_only=True)\n",
    "\n",
    "#     baseline_df_left_turns = aligned_df_copy[\n",
    "#         (aligned_df_copy[\"Time (s)\"] >= baseline_window[0]) & \n",
    "#         (aligned_df_copy[\"Time (s)\"] <= baseline_window[1])\n",
    "#     ].groupby(\"Halt Time\").mean(numeric_only=True)\n",
    "\n",
    "#     # Create baseline-corrected columns\n",
    "#     for signal_name in [\"z_470\", \"z_560\", \"Motor_Velocity\", \"Velocity_0X\", \"Velocity_0Y\"]:\n",
    "#         if signal_name in aligned_df_copy.columns:\n",
    "#             aligned_df_copy[f\"{signal_name}_Baseline\"] = aligned_df_copy[signal_name] - aligned_df_copy[\"Halt Time\"].map(baseline_df[signal_name])\n",
    "#         else:\n",
    "#             print(f\"      ⚠️  Column {signal_name} not found, skipping...\")\n",
    "\n",
    "#     # Define the baseline data file path\n",
    "#     # Define the baseline data file path\n",
    "#     baseline_data_file = output_folder / f\"{mouse_name}_{event_name}_baselined_data.csv\"\n",
    "#     left_turns_file = output_folder / f\"{mouse_name}_{event_name}_left_turns_baselined_data.csv\"\n",
    "#     right_turns_file = output_folder / f\"{mouse_name}_{event_name}_right_turns_baselined_data.csv\"\n",
    "\n",
    "#     # Save the baseline-corrected data\n",
    "#     aligned_df_copy.to_csv(baseline_data_file, index=False)\n",
    "#     print(f\"      💾 Saved baseline data to: {baseline_data_file.name}\")\n",
    "\n",
    "#     # ---------------- Mean and SEM ----------------\n",
    "#     # Select only numeric columns for aggregation\n",
    "#     numeric_columns = aligned_df_copy.select_dtypes(include=['number']).columns\n",
    "#     mean_baseline_df = aligned_df_copy.groupby(\"Time (s)\")[numeric_columns].mean()\n",
    "#     sem_baseline_df = aligned_df_copy.groupby(\"Time (s)\")[numeric_columns].sem()\n",
    "\n",
    "#     def get_symmetric_ylim(mean_data, sem_data):\n",
    "#         max_abs_value = max(\n",
    "#             abs(mean_data).max() + sem_data.max(),\n",
    "#             abs(mean_data).min() - sem_data.min()\n",
    "#         )\n",
    "#         return (-max_abs_value, max_abs_value)\n",
    "\n",
    "#     print(f\"      📊 Creating plot...\")\n",
    "\n",
    "#     # ---------------- Plotting ----------------\n",
    "#     fig, ax = plt.subplots(figsize=(plot_width, 6))\n",
    "\n",
    "#     # Photodiode\n",
    "#     ax.plot(mean_baseline_df.index, mean_baseline_df[\"Photodiode_int\"], color='grey', alpha=0.8, linewidth=2)\n",
    "#     ax.fill_between(mean_baseline_df.index,\n",
    "#                     mean_baseline_df[\"Photodiode_int\"] - sem_baseline_df[\"Photodiode_int\"],\n",
    "#                     mean_baseline_df[\"Photodiode_int\"] + sem_baseline_df[\"Photodiode_int\"],\n",
    "#                     color='grey', alpha=0.2)\n",
    "\n",
    "#     ax.set_xlabel('Time (s) relative to halt')\n",
    "#     ax.set_ylabel('Photodiode', color='grey')\n",
    "#     ax.set_title(f'Baselined Signals - {mouse_name} ({session_name})')\n",
    "\n",
    "#     # z_470 and z_560 (Fluorescence)\n",
    "#     ax2 = ax.twinx()\n",
    "#     ax2.plot(mean_baseline_df.index, mean_baseline_df[\"z_470_Baseline\"], color='green', alpha=0.8, linewidth=2, label='470nm')\n",
    "#     ax2.fill_between(mean_baseline_df.index,\n",
    "#                      mean_baseline_df[\"z_470_Baseline\"] - sem_baseline_df[\"z_470_Baseline\"],\n",
    "#                      mean_baseline_df[\"z_470_Baseline\"] + sem_baseline_df[\"z_470_Baseline\"],\n",
    "#                      color='green', alpha=0.2)\n",
    "#     ax2.plot(mean_baseline_df.index, mean_baseline_df[\"z_560_Baseline\"], color='red', alpha=0.8, linewidth=2, label='560nm')\n",
    "#     ax2.fill_between(mean_baseline_df.index,\n",
    "#                      mean_baseline_df[\"z_560_Baseline\"] - sem_baseline_df[\"z_560_Baseline\"],\n",
    "#                      mean_baseline_df[\"z_560_Baseline\"] + sem_baseline_df[\"z_560_Baseline\"],\n",
    "#                      color='red', alpha=0.2)\n",
    "#     ax2.set_ylabel('Fluorescence (z-score)', color='green')\n",
    "#     ax2.set_ylim(get_symmetric_ylim(\n",
    "#         pd.concat([mean_baseline_df[\"z_470_Baseline\"], mean_baseline_df[\"z_560_Baseline\"]]),\n",
    "#         pd.concat([sem_baseline_df[\"z_470_Baseline\"], sem_baseline_df[\"z_560_Baseline\"]])\n",
    "#     ))\n",
    "#     ax2.yaxis.label.set_color('green')\n",
    "\n",
    "#     # Motor velocity\n",
    "#     ax3 = ax.twinx()\n",
    "#     ax3.spines['right'].set_position(('outward', 50))\n",
    "#     ax3.plot(mean_baseline_df.index, mean_baseline_df[\"Motor_Velocity_Baseline\"], color='#00008B', alpha=0.8, linewidth=2)\n",
    "#     ax3.fill_between(mean_baseline_df.index,\n",
    "#                      mean_baseline_df[\"Motor_Velocity_Baseline\"] - sem_baseline_df[\"Motor_Velocity_Baseline\"],\n",
    "#                      mean_baseline_df[\"Motor_Velocity_Baseline\"] + sem_baseline_df[\"Motor_Velocity_Baseline\"],\n",
    "#                      color='#00008B', alpha=0.2)\n",
    "#     ax3.set_ylabel('Motor Velocity (deg/s²)', color='#00008B')\n",
    "#     ax3.set_ylim(get_symmetric_ylim(mean_baseline_df[\"Motor_Velocity_Baseline\"], sem_baseline_df[\"Motor_Velocity_Baseline\"]))\n",
    "#     ax3.yaxis.label.set_color('#00008B')\n",
    "\n",
    "#     # Running velocity (Velocity_0X)\n",
    "#     ax4 = ax.twinx()\n",
    "#     ax4.spines['right'].set_position(('outward', 100))\n",
    "#     ax4.plot(mean_baseline_df.index, mean_baseline_df[\"Velocity_0X_Baseline\"] * 1000, color='orange', alpha=0.8, linewidth=2)\n",
    "#     ax4.fill_between(mean_baseline_df.index,\n",
    "#                      (mean_baseline_df[\"Velocity_0X_Baseline\"] - sem_baseline_df[\"Velocity_0X_Baseline\"]) * 1000,\n",
    "#                      (mean_baseline_df[\"Velocity_0X_Baseline\"] + sem_baseline_df[\"Velocity_0X_Baseline\"]) * 1000,\n",
    "#                      color='orange', alpha=0.2)\n",
    "#     ax4.set_ylabel('Running velocity (mm/s²)', color='orange')\n",
    "#     ax4.set_ylim(get_symmetric_ylim(mean_baseline_df[\"Velocity_0X_Baseline\"] * 1000, sem_baseline_df[\"Velocity_0X_Baseline\"] * 1000))\n",
    "#     ax4.yaxis.label.set_color('orange')\n",
    "\n",
    "#     # Turning velocity (Velocity_0Y)\n",
    "#     ax5 = ax.twinx()\n",
    "#     ax5.spines['right'].set_position(('outward', 150))\n",
    "#     ax5.plot(mean_baseline_df.index, mean_baseline_df[\"Velocity_0Y_Baseline\"], color='#4682B4', alpha=0.8, linewidth=2)\n",
    "#     ax5.fill_between(mean_baseline_df.index,\n",
    "#                      mean_baseline_df[\"Velocity_0Y_Baseline\"] - sem_baseline_df[\"Velocity_0Y_Baseline\"],\n",
    "#                      mean_baseline_df[\"Velocity_0Y_Baseline\"] + sem_baseline_df[\"Velocity_0Y_Baseline\"],\n",
    "#                      color='#4682B4', alpha=0.2)\n",
    "#     ax5.set_ylabel('Turning velocity (deg/s²)', color='#4682B4')\n",
    "#     ax5.set_ylim(get_symmetric_ylim(mean_baseline_df[\"Velocity_0Y_Baseline\"], sem_baseline_df[\"Velocity_0Y_Baseline\"]))\n",
    "#     ax5.yaxis.label.set_color('#4682B4')\n",
    "\n",
    "#     # Add vertical line at event time (t=0)\n",
    "#     ax.axvline(x=0, color='black', linestyle='--', alpha=0.5, linewidth=1)\n",
    "\n",
    "#     fig.tight_layout()\n",
    "\n",
    "#     figure_file = output_folder / f\"{session_name}_{event_name}_baselined.pdf\"\n",
    "\n",
    "#     # Save the figure\n",
    "#     fig.savefig(figure_file, format='pdf', bbox_inches='tight')\n",
    "#     print(f\"      💾 Saved plot to: {figure_file.name}\")\n",
    "#     plt.close(fig)\n",
    "#     return fig\n",
    "\n",
    "\n",
    "# # Example usage:\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Process all aligned_data folders\n",
    "#     results = process_aligned_data_folders(\n",
    "#         data_dirs=data_dirs,\n",
    "#         baseline_window=baseline_window,\n",
    "#         event_name=event_name,\n",
    "#         plot_width=plot_width,\n",
    "#         create_plots=True  # Ensure this is set to True to save plots\n",
    "#     )\n",
    "    \n",
    "#     print(f\"\\n🎉 Processing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda60d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASELINING\n",
    "#----------------------------------------------------\n",
    "def process_aligned_data_folders(data_dirs, baseline_window, event_name=event_name, plot_width=12, create_plots=True):\n",
    "    \"\"\"\n",
    "    Process all aligned_data folders and generate baseline plots.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data_dirs : list\n",
    "        List of Path objects pointing to your main data directories\n",
    "    baseline_window : tuple\n",
    "        Tuple of (start_time, end_time) for baseline window\n",
    "    event_name : str\n",
    "        Event name for file naming (default: \"halt\")\n",
    "    plot_width : int\n",
    "        Width of the plot in inches\n",
    "    create_plots : bool\n",
    "        Whether to create and save plots (default: True)\n",
    "    \"\"\"\n",
    "    \n",
    "    results = {\n",
    "        'processed': [],\n",
    "        'errors': [],\n",
    "        'total_folders': 0\n",
    "    }\n",
    "    \n",
    "    # Find all aligned_data folders\n",
    "    aligned_folders = []\n",
    "    for data_dir in data_dirs:\n",
    "        print(f\"Searching in: {data_dir}\")\n",
    "        # Find all aligned_data folders recursively\n",
    "        found_folders = list(data_dir.rglob(\"aligned_data\"))\n",
    "        aligned_folders.extend(found_folders)\n",
    "        print(f\"  Found {len(found_folders)} aligned_data folders\")\n",
    "    \n",
    "    results['total_folders'] = len(aligned_folders)\n",
    "    print(f\"\\nTotal aligned_data folders found: {len(aligned_folders)}\")\n",
    "    \n",
    "    for aligned_folder in aligned_folders:\n",
    "        try:\n",
    "            print(f\"\\n📁 Processing folder: {aligned_folder}\")\n",
    "            \n",
    "            # Find only the original aligned CSV files (exclude already processed baselined files and turn files)\n",
    "            all_csv_files = list(aligned_folder.glob(\"*.csv\"))\n",
    "            csv_files = [f for f in all_csv_files if not f.name.endswith('_baselined_data.csv') \n",
    "                        and not f.name.endswith('_left_turns.csv') \n",
    "                        and not f.name.endswith('_right_turns.csv')]\n",
    "            \n",
    "            if not csv_files:\n",
    "                print(f\"  ⚠️  No original aligned CSV files found in {aligned_folder}\")\n",
    "                print(f\"  Available files: {[f.name for f in all_csv_files]}\")\n",
    "                results['errors'].append({\n",
    "                    'folder': str(aligned_folder),\n",
    "                    'error': 'No original aligned CSV files found',\n",
    "                    'status': 'skipped'\n",
    "                })\n",
    "                continue\n",
    "            print(f\"  Found {len(csv_files)} aligned CSV files to process\")\n",
    "            \n",
    "            for csv_file in csv_files:\n",
    "                try:\n",
    "                    # Check if the CSV file name matches the event name\n",
    "                    if event_name not in csv_file.name:\n",
    "                        print(f\"    ⚠️ Skipping {csv_file.name} as it does not match the event name '{event_name}'\")\n",
    "                        continue\n",
    "                    \n",
    "                    print(f\"    📊 Processing: {csv_file.name}\")\n",
    "                    \n",
    "                    # Load the data\n",
    "                    aligned_df = pd.read_csv(csv_file)\n",
    "                    \n",
    "                    # Create aligned DataFrames for left and right turns\n",
    "                    # Replace '_aligned' with '_left_turns' and '_right_turns'\n",
    "                    left_turns_csv = csv_file.with_name(csv_file.stem.replace('_aligned', '_left_turns') + \".csv\")\n",
    "                    right_turns_csv = csv_file.with_name(csv_file.stem.replace('_aligned', '_right_turns') + \".csv\")\n",
    "                    \n",
    "                    left_turns_df = None\n",
    "                    right_turns_df = None\n",
    "                    \n",
    "                    if left_turns_csv.exists():\n",
    "                        print(f\"    📂 Found left turns CSV: {left_turns_csv.name}\")\n",
    "                        left_turns_df = pd.read_csv(left_turns_csv)\n",
    "                    else:\n",
    "                        print(f\"    ⚠️ Left turns CSV not found: {left_turns_csv.name}\")\n",
    "                    \n",
    "                    if right_turns_csv.exists():\n",
    "                        print(f\"    📂 Found right turns CSV: {right_turns_csv.name}\")\n",
    "                        right_turns_df = pd.read_csv(right_turns_csv)\n",
    "                    else:\n",
    "                        print(f\"    ⚠️ Right turns CSV not found: {right_turns_csv.name}\")\n",
    "                    \n",
    "                    # Clean up the mouse name (remove extra suffixes)\n",
    "                    mouse_name = csv_file.stem.replace('_aligned', '').replace('_downsampled_data_Apply halt: 2s', '').split('_')[0]\n",
    "                    # Get session name from the folder structure\n",
    "                    session_name = aligned_folder.parent.name\n",
    "                    \n",
    "                    # Check if required columns exist\n",
    "                    required_columns = [\"Time (s)\", \"Halt Time\", \"z_470\", \"z_560\", \"Motor_Velocity\", \n",
    "                                      \"Velocity_0X\", \"Velocity_0Y\", \"Photodiode_int\"]\n",
    "                    missing_columns = [col for col in required_columns if col not in aligned_df.columns]\n",
    "                    \n",
    "                    if missing_columns:\n",
    "                        print(f\"    ⚠️  Missing columns: {missing_columns}\")\n",
    "                        print(f\"    Available columns: {list(aligned_df.columns)}\")\n",
    "                        raise ValueError(f\"Missing required columns: {missing_columns}\")\n",
    "                    \n",
    "                    # Process the data and create plot\n",
    "                    fig = baseline_aligned_data_simple(\n",
    "                        aligned_df=aligned_df,\n",
    "                        left_turns_df=left_turns_df,\n",
    "                        right_turns_df=right_turns_df,\n",
    "                        baseline_window=baseline_window,\n",
    "                        mouse_name=mouse_name,\n",
    "                        session_name=session_name,\n",
    "                        event_name=event_name,\n",
    "                        output_folder=aligned_folder,\n",
    "                        csv_file=csv_file,\n",
    "                        plot_width=plot_width,\n",
    "                        create_plots=create_plots\n",
    "                    )\n",
    "                    \n",
    "                    results['processed'].append({\n",
    "                        'file': str(csv_file),\n",
    "                        'mouse_name': mouse_name,\n",
    "                        'session_name': session_name,\n",
    "                        'folder': str(aligned_folder),\n",
    "                        'status': 'success'\n",
    "                    })\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    error_info = {\n",
    "                        'file': str(csv_file),\n",
    "                        'error': str(e),\n",
    "                        'status': 'failed'\n",
    "                    }\n",
    "                    results['errors'].append(error_info)\n",
    "                    print(f\"    ❌ Error processing {csv_file.name}: {str(e)}\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            error_info = {\n",
    "                'folder': str(aligned_folder),\n",
    "                'error': str(e),\n",
    "                'status': 'failed'\n",
    "            }\n",
    "            results['errors'].append(error_info)\n",
    "            print(f\"❌ Error accessing {aligned_folder}: {str(e)}\")\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"PROCESSING SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Total aligned_data folders: {results['total_folders']}\")\n",
    "    print(f\"Successfully processed files: {len(results['processed'])}\")\n",
    "    print(f\"Errors encountered: {len(results['errors'])}\")\n",
    "    \n",
    "    if results['errors']:\n",
    "        print(f\"\\nErrors:\")\n",
    "        for error in results['errors']:\n",
    "            if 'file' in error:\n",
    "                print(f\"  - File {Path(error['file']).name}: {error['error']}\")\n",
    "            else:\n",
    "                print(f\"  - Folder {Path(error['folder']).name}: {error['error']}\")\n",
    "    \n",
    "    if results['processed']:\n",
    "        print(f\"\\nSuccessfully processed:\")\n",
    "        for proc in results['processed']:\n",
    "            print(f\"  - {proc['mouse_name']} in {Path(proc['folder']).parent.name}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def baseline_aligned_data_simple(aligned_df, left_turns_df, right_turns_df, baseline_window, mouse_name, session_name, event_name, output_folder, csv_file, plot_width=12, create_plots=True):\n",
    "    \"\"\"\n",
    "    Simple baseline correction and plotting function.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    aligned_df : pd.DataFrame\n",
    "        Main aligned data\n",
    "    left_turns_df : pd.DataFrame or None\n",
    "        Left turns data\n",
    "    right_turns_df : pd.DataFrame or None\n",
    "        Right turns data\n",
    "    baseline_window : tuple\n",
    "        Tuple of (start_time, end_time) for baseline window\n",
    "    mouse_name : str\n",
    "        Mouse name for file naming\n",
    "    session_name : str\n",
    "        Session name for file naming\n",
    "    event_name : str\n",
    "        Event name for file naming\n",
    "    output_folder : Path\n",
    "        Output folder path\n",
    "    csv_file : Path\n",
    "        Original CSV file path\n",
    "    plot_width : int\n",
    "        Width of the plot in inches\n",
    "    create_plots : bool\n",
    "        Whether to create and save plots (default: True)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"      🔄 Performing baseline correction...\")\n",
    "\n",
    "    def baseline_dataframe(df, baseline_window, mouse_name, event_name, output_folder, suffix=\"\"):\n",
    "        \"\"\"Helper function to baseline a single dataframe\"\"\"\n",
    "        # Make a copy to avoid modifying the original data\n",
    "        df_copy = df.copy()\n",
    "        \n",
    "        # Calculate baseline values\n",
    "        baseline_df = df_copy[\n",
    "            (df_copy[\"Time (s)\"] >= baseline_window[0]) & \n",
    "            (df_copy[\"Time (s)\"] <= baseline_window[1])\n",
    "        ].groupby(\"Halt Time\").mean(numeric_only=True)\n",
    "        \n",
    "        # Create baseline-corrected columns\n",
    "        for signal_name in [\"z_470\", \"z_560\", \"Motor_Velocity\", \"Velocity_0X\", \"Velocity_0Y\"]:\n",
    "            if signal_name in df_copy.columns:\n",
    "                df_copy[f\"{signal_name}_Baseline\"] = df_copy[signal_name] - df_copy[\"Halt Time\"].map(baseline_df[signal_name])\n",
    "            else:\n",
    "                print(f\"      ⚠️  Column {signal_name} not found in {suffix} data, skipping...\")\n",
    "        \n",
    "        # Define the baseline data file path\n",
    "        if suffix:\n",
    "            baseline_data_file = output_folder / f\"{mouse_name}_{event_name}_{suffix}_baselined_data.csv\"\n",
    "        else:\n",
    "            baseline_data_file = output_folder / f\"{mouse_name}_{event_name}_baselined_data.csv\"\n",
    "        \n",
    "        # Save the baseline-corrected data\n",
    "        df_copy.to_csv(baseline_data_file, index=False)\n",
    "        print(f\"      💾 Saved {suffix} baseline data to: {baseline_data_file.name}\")\n",
    "        \n",
    "        return df_copy\n",
    "\n",
    "    # ---------------- Baseline Correction ----------------\n",
    "    # Process main aligned data\n",
    "    aligned_df_baselined = baseline_dataframe(aligned_df, baseline_window, mouse_name, event_name, output_folder)\n",
    "    \n",
    "    # Process left turns data if available\n",
    "    left_turns_df_baselined = None\n",
    "    if left_turns_df is not None:\n",
    "        print(f\"      🔄 Processing left turns data...\")\n",
    "        left_turns_df_baselined = baseline_dataframe(left_turns_df, baseline_window, mouse_name, event_name, output_folder, \"left_turns\")\n",
    "    \n",
    "    # Process right turns data if available\n",
    "    right_turns_df_baselined = None\n",
    "    if right_turns_df is not None:\n",
    "        print(f\"      🔄 Processing right turns data...\")\n",
    "        right_turns_df_baselined = baseline_dataframe(right_turns_df, baseline_window, mouse_name, event_name, output_folder, \"right_turns\")\n",
    "\n",
    "    # ---------------- Mean and SEM for plotting (using main aligned data) ----------------\n",
    "    # Select only numeric columns for aggregation\n",
    "    numeric_columns = aligned_df_baselined.select_dtypes(include=['number']).columns\n",
    "    mean_baseline_df = aligned_df_baselined.groupby(\"Time (s)\")[numeric_columns].mean()\n",
    "    sem_baseline_df = aligned_df_baselined.groupby(\"Time (s)\")[numeric_columns].sem()\n",
    "\n",
    "    def get_symmetric_ylim(mean_data, sem_data):\n",
    "        max_abs_value = max(\n",
    "            abs(mean_data).max() + sem_data.max(),\n",
    "            abs(mean_data).min() - sem_data.min()\n",
    "        )\n",
    "        return (-max_abs_value, max_abs_value)\n",
    "\n",
    "    if create_plots:\n",
    "        print(f\"      📊 Creating plot...\")\n",
    "\n",
    "        # ---------------- Plotting ----------------\n",
    "        fig, ax = plt.subplots(figsize=(plot_width, 6))\n",
    "\n",
    "        # Photodiode\n",
    "        ax.plot(mean_baseline_df.index, mean_baseline_df[\"Photodiode_int\"], color='grey', alpha=0.8, linewidth=2)\n",
    "        ax.fill_between(mean_baseline_df.index,\n",
    "                        mean_baseline_df[\"Photodiode_int\"] - sem_baseline_df[\"Photodiode_int\"],\n",
    "                        mean_baseline_df[\"Photodiode_int\"] + sem_baseline_df[\"Photodiode_int\"],\n",
    "                        color='grey', alpha=0.2)\n",
    "\n",
    "        ax.set_xlabel('Time (s) relative to halt')\n",
    "        ax.set_ylabel('Photodiode', color='grey')\n",
    "        ax.set_title(f'Baselined Signals - {mouse_name} ({session_name})')\n",
    "\n",
    "        # z_470 and z_560 (Fluorescence)\n",
    "        ax2 = ax.twinx()\n",
    "        ax2.plot(mean_baseline_df.index, mean_baseline_df[\"z_470_Baseline\"], color='green', alpha=0.8, linewidth=2, label='470nm')\n",
    "        ax2.fill_between(mean_baseline_df.index,\n",
    "                         mean_baseline_df[\"z_470_Baseline\"] - sem_baseline_df[\"z_470_Baseline\"],\n",
    "                         mean_baseline_df[\"z_470_Baseline\"] + sem_baseline_df[\"z_470_Baseline\"],\n",
    "                         color='green', alpha=0.2)\n",
    "        ax2.plot(mean_baseline_df.index, mean_baseline_df[\"z_560_Baseline\"], color='red', alpha=0.8, linewidth=2, label='560nm')\n",
    "        ax2.fill_between(mean_baseline_df.index,\n",
    "                         mean_baseline_df[\"z_560_Baseline\"] - sem_baseline_df[\"z_560_Baseline\"],\n",
    "                         mean_baseline_df[\"z_560_Baseline\"] + sem_baseline_df[\"z_560_Baseline\"],\n",
    "                         color='red', alpha=0.2)\n",
    "        ax2.set_ylabel('Fluorescence (z-score)', color='green')\n",
    "        ax2.set_ylim(get_symmetric_ylim(\n",
    "            pd.concat([mean_baseline_df[\"z_470_Baseline\"], mean_baseline_df[\"z_560_Baseline\"]]),\n",
    "            pd.concat([sem_baseline_df[\"z_470_Baseline\"], sem_baseline_df[\"z_560_Baseline\"]])\n",
    "        ))\n",
    "        ax2.yaxis.label.set_color('green')\n",
    "\n",
    "        # Motor velocity\n",
    "        ax3 = ax.twinx()\n",
    "        ax3.spines['right'].set_position(('outward', 50))\n",
    "        ax3.plot(mean_baseline_df.index, mean_baseline_df[\"Motor_Velocity_Baseline\"], color='#00008B', alpha=0.8, linewidth=2)\n",
    "        ax3.fill_between(mean_baseline_df.index,\n",
    "                         mean_baseline_df[\"Motor_Velocity_Baseline\"] - sem_baseline_df[\"Motor_Velocity_Baseline\"],\n",
    "                         mean_baseline_df[\"Motor_Velocity_Baseline\"] + sem_baseline_df[\"Motor_Velocity_Baseline\"],\n",
    "                         color='#00008B', alpha=0.2)\n",
    "        ax3.set_ylabel('Motor Velocity (deg/s²)', color='#00008B')\n",
    "        ax3.set_ylim(get_symmetric_ylim(mean_baseline_df[\"Motor_Velocity_Baseline\"], sem_baseline_df[\"Motor_Velocity_Baseline\"]))\n",
    "        ax3.yaxis.label.set_color('#00008B')\n",
    "\n",
    "        # Running velocity (Velocity_0X)\n",
    "        ax4 = ax.twinx()\n",
    "        ax4.spines['right'].set_position(('outward', 100))\n",
    "        ax4.plot(mean_baseline_df.index, mean_baseline_df[\"Velocity_0X_Baseline\"] * 1000, color='orange', alpha=0.8, linewidth=2)\n",
    "        ax4.fill_between(mean_baseline_df.index,\n",
    "                         (mean_baseline_df[\"Velocity_0X_Baseline\"] - sem_baseline_df[\"Velocity_0X_Baseline\"]) * 1000,\n",
    "                         (mean_baseline_df[\"Velocity_0X_Baseline\"] + sem_baseline_df[\"Velocity_0X_Baseline\"]) * 1000,\n",
    "                         color='orange', alpha=0.2)\n",
    "        ax4.set_ylabel('Running velocity (mm/s²)', color='orange')\n",
    "        ax4.set_ylim(get_symmetric_ylim(mean_baseline_df[\"Velocity_0X_Baseline\"] * 1000, sem_baseline_df[\"Velocity_0X_Baseline\"] * 1000))\n",
    "        ax4.yaxis.label.set_color('orange')\n",
    "\n",
    "        # Turning velocity (Velocity_0Y)\n",
    "        ax5 = ax.twinx()\n",
    "        ax5.spines['right'].set_position(('outward', 150))\n",
    "        ax5.plot(mean_baseline_df.index, mean_baseline_df[\"Velocity_0Y_Baseline\"], color='#4682B4', alpha=0.8, linewidth=2)\n",
    "        ax5.fill_between(mean_baseline_df.index,\n",
    "                         mean_baseline_df[\"Velocity_0Y_Baseline\"] - sem_baseline_df[\"Velocity_0Y_Baseline\"],\n",
    "                         mean_baseline_df[\"Velocity_0Y_Baseline\"] + sem_baseline_df[\"Velocity_0Y_Baseline\"],\n",
    "                         color='#4682B4', alpha=0.2)\n",
    "        ax5.set_ylabel('Turning velocity (deg/s²)', color='#4682B4')\n",
    "        ax5.set_ylim(get_symmetric_ylim(mean_baseline_df[\"Velocity_0Y_Baseline\"], sem_baseline_df[\"Velocity_0Y_Baseline\"]))\n",
    "        ax5.yaxis.label.set_color('#4682B4')\n",
    "\n",
    "        # Add vertical line at event time (t=0)\n",
    "        ax.axvline(x=0, color='black', linestyle='--', alpha=0.5, linewidth=1)\n",
    "\n",
    "        fig.tight_layout()\n",
    "\n",
    "        figure_file = output_folder / f\"{session_name}_{event_name}_baselined.pdf\"\n",
    "\n",
    "        # Save the figure\n",
    "        fig.savefig(figure_file, format='pdf', bbox_inches='tight')\n",
    "        print(f\"      💾 Saved plot to: {figure_file.name}\")\n",
    "        plt.close(fig)\n",
    "        return fig\n",
    "    else:\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c205cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Process all aligned_data folders\n",
    "    results = process_aligned_data_folders(\n",
    "        data_dirs=data_dirs,\n",
    "        baseline_window=baseline_window,\n",
    "        event_name=event_name,\n",
    "        plot_width=plot_width,\n",
    "        create_plots=True  # Ensure this is set to True to save plots\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n🎉 Processing complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aeon2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
