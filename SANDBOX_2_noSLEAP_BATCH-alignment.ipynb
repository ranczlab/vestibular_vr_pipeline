{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a329aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "# this notebook SAVES halt aligned data and baselined data as CSV together with PLOTS, different compared to the previous SANDBOX_2_noSLEAP#\n",
    "#---------------------------------------------------------------------------------------------------#\n",
    "from pathlib import Path\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.collections import LineCollection\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "import plotly.express as px\n",
    "import plotly.subplots as sp\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "import math\n",
    "from pprint import pprint\n",
    "\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.signal import correlate\n",
    "from scipy.stats import mode\n",
    "from scipy.integrate import cumulative_trapezoid\n",
    "from scipy.signal import correlate\n",
    "import seaborn as sns\n",
    "\n",
    "from typing import Tuple, Dict, Any, List, Optional\n",
    "import warnings\n",
    "\n",
    "import traceback\n",
    "import gc\n",
    "import json\n",
    "\n",
    "%config Completer.use_jedi = False  # Fixes autocomplete issues\n",
    "%config InlineBackend.figure_format = 'retina'  # Improves plot resolution\n",
    "\n",
    "import gc # garbage collector for removing large variables from memory instantly \n",
    "import importlib #for force updating changed packages \n",
    "\n",
    "#import harp\n",
    "import harp_resources.process\n",
    "import harp_resources.utils\n",
    "from harp_resources import process, utils # Reassign to maintain direct references for force updating \n",
    "#from sleap import load_and_process as lp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fcccb9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fcf7b138",
   "metadata": {},
   "source": [
    "## Notebook Overview\n",
    "This sandbox notebook loads downsampled ONIX behavioral datasets, slices them by experiment intervals, and runs the full `BehavioralAnalyzer` workflow. The process:\n",
    "\n",
    "- Configure input directories, event labels, baseline windows, and plotting flags.\n",
    "- Load parquet files for photometry, tracking, encoder, and event metadata for each session.\n",
    "- Instantiate `BehavioralAnalyzer` objects to compute running, turning, and platform metrics, saving per-animal figures/CSVs plus a cohort summary.\n",
    "\n",
    "### Key Parameters\n",
    "- `data_dirs`, `cohort_data_dir`: select which sessions are processed.\n",
    "- `event_name`, `vestibular_mismatch`: govern halt detection and slicing logic.\n",
    "- `time_window_start/end`, `baseline_window`, `common_resampled_rate`, `plot_fig1`: plotting and preprocessing controls.\n",
    "- `run_behavioral_analysis` arguments expose `encoder_column`, `min_bout_duration_s`, `running_percentile`, `turning_percentile`, and `turning_velocity_column` before launching the batch run.\n",
    "\n",
    "### Behavioral Analyzer Options\n",
    "When constructing `BehavioralAnalyzer`, you can adjust:\n",
    "- `threshold_plot_bins`: histogram resolution for threshold estimation.\n",
    "- `min_bout_duration_s`: minimum bout length for running/turning detection.\n",
    "- `running_percentile`: percentile-based threshold for running bouts.\n",
    "- `turning_percentile`: optional percentile for turning (falls back to median when `None`).\n",
    "- `turning_velocity_column`: choose the turning signal (`Motor_Velocity`, `Velocity_0Y`, etc.).\n",
    "\n",
    "Each call to `analyze_turning` also lets you override the column via the `column` argument, enabling quick comparisons between motor encoder- and flow-based turning metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed9c5d4c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PosixPath('/Users/rancze/Documents/Data/vestVR/20250409_Cohort3_rotation/Visual_mismatch_day4/B6J2780-2025-04-28T13-10-18'),\n",
      " PosixPath('/Users/rancze/Documents/Data/vestVR/20250409_Cohort3_rotation/Visual_mismatch_day4/B6J2783-2025-04-28T14-57-30'),\n",
      " PosixPath('/Users/rancze/Documents/Data/vestVR/20250409_Cohort3_rotation/Visual_mismatch_day4/B6J2781-2025-04-28T13-45-40'),\n",
      " PosixPath('/Users/rancze/Documents/Data/vestVR/20250409_Cohort3_rotation/Visual_mismatch_day4/B6J2782-2025-04-28T14-22-03')]\n",
      "Processed Data Paths (existing only):\n",
      "[PosixPath('/Users/rancze/Documents/Data/vestVR/20250409_Cohort3_rotation/Visual_mismatch_day4/B6J2780-2025-04-28T13-10-18_processedData/downsampled_data'),\n",
      " PosixPath('/Users/rancze/Documents/Data/vestVR/20250409_Cohort3_rotation/Visual_mismatch_day4/B6J2783-2025-04-28T14-57-30_processedData/downsampled_data'),\n",
      " PosixPath('/Users/rancze/Documents/Data/vestVR/20250409_Cohort3_rotation/Visual_mismatch_day4/B6J2781-2025-04-28T13-45-40_processedData/downsampled_data'),\n",
      " PosixPath('/Users/rancze/Documents/Data/vestVR/20250409_Cohort3_rotation/Visual_mismatch_day4/B6J2782-2025-04-28T14-22-03_processedData/downsampled_data')]\n"
     ]
    }
   ],
   "source": [
    "# data paths setup\n",
    "#-------------------------------\n",
    "data_dirs = [  # Add your data directories here\n",
    "    # Path('~/RANCZLAB-NAS/data/ONIX/20250409_Cohort3_rotation/Vestibular_mismatch_day1').expanduser(),\n",
    "    # Path('/home/ikharitonov/RANCZLAB-NAS/data/ONIX/20250923_Cohort6_rotation/EXP_1_fluoxetine_1').expanduser()\n",
    "    # Path('/Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4').expanduser() # for Nora\n",
    "    Path('/Users/rancze/Documents/Data/vestVR/20250409_Cohort3_rotation/Visual_mismatch_day4').expanduser() # for Ede\n",
    "]\n",
    "# FIXME can we do this straight from the data_dirs path (i.e. walk one directory back?)\n",
    "# cohort_data_dir = Path('/Volumes/RanczLab2/Cohort1_rotation/').expanduser() # for Nora\n",
    "cohort_data_dir = Path('/Users/rancze/Documents/Data/vestVR/20250409_Cohort3_rotation/').expanduser() # for Ede\n",
    "\n",
    "# Collect raw data paths (excluding '_processedData' dirs)\n",
    "rawdata_paths = []\n",
    "for data_dir in data_dirs:\n",
    "    subdirs = [p for p in data_dir.iterdir() if p.is_dir() and not p.name.endswith('_processedData')]\n",
    "    rawdata_paths.extend(subdirs)  # Collect all subdirectories\n",
    "pprint(rawdata_paths)\n",
    "\n",
    "# Build processed data paths - only include paths that actually exist\n",
    "data_paths = []\n",
    "for raw in rawdata_paths:\n",
    "    processed_path = raw.parent / f\"{raw.name}_processedData/downsampled_data\"\n",
    "    if processed_path.exists():\n",
    "        data_paths.append(processed_path)\n",
    "    else:\n",
    "        print(f\"⚠️ Skipping {raw.name}: processed data path does not exist: {processed_path}\")\n",
    "\n",
    "# Print data paths in a more readable format\n",
    "print(\"Processed Data Paths (existing only):\")\n",
    "pprint(data_paths)\n",
    "\n",
    "#-------------------------------\n",
    "# initial variables setup\n",
    "#-------------------------------\n",
    "time_window_start = -5  # s, FOR PLOTTING PURPOSES\n",
    "time_window_end = 10  # s, FOR PLOTTING PURPOSES\n",
    "baseline_window = (-1, 0)  # s, FOR baselining averages\n",
    "plot_width = 14\n",
    "\n",
    "event_name = \"Apply halt: 2s\"  # Apply halt: 2s, No halt, DrumWithReverseflow block started, DrumBase block started\n",
    "vestibular_mismatch = False\n",
    "common_resampled_rate = 1000  # in Hz\n",
    "plot_fig1 = False # save_path not defined, commented out FIg 1 creation\n",
    "\n",
    "# # for saccades\n",
    "# framerate = 59.77  # Hz (in the future, should come from saved data)\n",
    "# threshold = 65  # px/s FIXME make this adaptive\n",
    "# refractory_period = pd.Timedelta(milliseconds=100)  # msec, using pd.Timedelta for datetime index\n",
    "# plot_saccade_detection_QC = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "caec3450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing data path 1/4: /Users/rancze/Documents/Data/vestVR/20250409_Cohort3_rotation/Visual_mismatch_day4/B6J2780-2025-04-28T13-10-18_processedData/downsampled_data\n",
      "✅ Successfully loaded all parquet files for downsampled_data\n",
      "ℹ️ Mousename: B6J2780\n",
      "ℹ️ Unique events and their counts:\n",
      "Event\n",
      "Wait for run threshold...            15\n",
      "Check halt probability               13\n",
      "No halt                               8\n",
      "Halt delay: 0s                        5\n",
      "Apply halt: 2s                        5\n",
      "Block timer elapsed                   2\n",
      "Sync signal started                   1\n",
      "Homing platform                       1\n",
      "DrumBase block started                1\n",
      "DrumWithReverseHalt block started     1\n",
      "Name: count, dtype: int64\n",
      "ℹ️ block events\n",
      "                                                        Event  Time Difference\n",
      "Time                                                                          \n",
      "1904-01-04 22:46:19.890240             DrumBase block started              NaN\n",
      "1904-01-04 22:51:20.180256                Block timer elapsed       300.290016\n",
      "1904-01-04 22:51:20.187648  DrumWithReverseHalt block started         0.007392\n",
      "1904-01-04 23:16:20.240256                Block timer elapsed      1500.052608\n",
      "\n",
      "Processing data path 2/4: /Users/rancze/Documents/Data/vestVR/20250409_Cohort3_rotation/Visual_mismatch_day4/B6J2783-2025-04-28T14-57-30_processedData/downsampled_data\n",
      "✅ Successfully loaded all parquet files for downsampled_data\n",
      "⚠️ Warning: Some 'Apply halt: 2s' events are less than 10 seconds apart. Consider applying a filter to events.\n",
      "ℹ️ Mousename: B6J2783\n",
      "ℹ️ Unique events and their counts:\n",
      "Event\n",
      "Wait for run threshold...            92\n",
      "Check halt probability               90\n",
      "No halt                              60\n",
      "Halt delay: 0s                       30\n",
      "Apply halt: 2s                       30\n",
      "Block timer elapsed                   2\n",
      "Sync signal started                   1\n",
      "Homing platform                       1\n",
      "DrumBase block started                1\n",
      "DrumWithReverseHalt block started     1\n",
      "Name: count, dtype: int64\n",
      "ℹ️ block events\n",
      "                                                        Event  Time Difference\n",
      "Time                                                                          \n",
      "1904-01-05 00:33:08.360256             DrumBase block started              NaN\n",
      "1904-01-05 00:38:08.505824                Block timer elapsed       300.145568\n",
      "1904-01-05 00:38:08.505824  DrumWithReverseHalt block started         0.000000\n",
      "1904-01-05 01:03:08.610240                Block timer elapsed      1500.104416\n",
      "\n",
      "Processing data path 3/4: /Users/rancze/Documents/Data/vestVR/20250409_Cohort3_rotation/Visual_mismatch_day4/B6J2781-2025-04-28T13-45-40_processedData/downsampled_data\n",
      "✅ Successfully loaded all parquet files for downsampled_data\n",
      "ℹ️ Mousename: B6J2781\n",
      "ℹ️ Unique events and their counts:\n",
      "Event\n",
      "Wait for run threshold...            31\n",
      "Check halt probability               29\n",
      "No halt                              17\n",
      "Halt delay: 0s                       12\n",
      "Apply halt: 2s                       12\n",
      "Block timer elapsed                   2\n",
      "Logging started                       1\n",
      "Sync signal started                   1\n",
      "Homing platform                       1\n",
      "DrumBase block started                1\n",
      "DrumWithReverseHalt block started     1\n",
      "Name: count, dtype: int64\n",
      "ℹ️ block events\n",
      "                                                        Event  Time Difference\n",
      "Time                                                                          \n",
      "1904-01-04 23:21:20.465952             DrumBase block started              NaN\n",
      "1904-01-04 23:26:20.620256                Block timer elapsed       300.154304\n",
      "1904-01-04 23:26:20.620256  DrumWithReverseHalt block started         0.000000\n",
      "1904-01-04 23:51:20.760256                Block timer elapsed      1500.140000\n",
      "\n",
      "Processing data path 4/4: /Users/rancze/Documents/Data/vestVR/20250409_Cohort3_rotation/Visual_mismatch_day4/B6J2782-2025-04-28T14-22-03_processedData/downsampled_data\n",
      "✅ Successfully loaded all parquet files for downsampled_data\n",
      "⚠️ Warning: Some 'Apply halt: 2s' events are less than 10 seconds apart. Consider applying a filter to events.\n",
      "ℹ️ Mousename: B6J2782\n",
      "ℹ️ Unique events and their counts:\n",
      "Event\n",
      "Wait for run threshold...            44\n",
      "Check halt probability               42\n",
      "Halt delay: 0s                       22\n",
      "Apply halt: 2s                       22\n",
      "No halt                              20\n",
      "Block timer elapsed                   2\n",
      "Sync signal started                   1\n",
      "Homing platform                       1\n",
      "DrumBase block started                1\n",
      "DrumWithReverseHalt block started     1\n",
      "Name: count, dtype: int64\n",
      "ℹ️ block events\n",
      "                                                        Event  Time Difference\n",
      "Time                                                                          \n",
      "1904-01-04 23:58:26.090240             DrumBase block started              NaN\n",
      "1904-01-05 00:03:26.230240                Block timer elapsed       300.140000\n",
      "1904-01-05 00:03:26.240256  DrumWithReverseHalt block started         0.010016\n",
      "1904-01-05 00:28:26.260256                Block timer elapsed      1500.020000\n",
      "\n",
      "✅ Finished loading data for all 4 successfully processed data paths\n"
     ]
    }
   ],
   "source": [
    "# load downsampled data for each data path\n",
    "#-------------------------------\n",
    "loaded_data = {}  # Dictionary to store loaded data for each path\n",
    "\n",
    "for idx, data_path in enumerate(data_paths, start=1):\n",
    "    print(f\"\\nProcessing data path {idx}/{len(data_paths)}: {data_path}\")\n",
    "    try:\n",
    "        # Load all parquet files for this data path\n",
    "        photometry_tracking_encoder_data = pd.read_parquet(data_path / \"photometry_tracking_encoder_data.parquet\", engine=\"pyarrow\")\n",
    "        camera_photodiode_data = pd.read_parquet(data_path / \"camera_photodiode_data.parquet\", engine=\"pyarrow\")\n",
    "        experiment_events = pd.read_parquet(data_path / \"experiment_events.parquet\", engine=\"pyarrow\")\n",
    "        photometry_info = pd.read_parquet(data_path / \"photometry_info.parquet\", engine=\"pyarrow\")\n",
    "        session_settings = pd.read_parquet(data_path / \"session_settings.parquet\", engine=\"pyarrow\")\n",
    "        session_settings[\"metadata\"] = session_settings[\"metadata\"].apply(process.safe_from_json)\n",
    "        video_dataframes: Dict[str, pd.DataFrame] = {}\n",
    "        video_join_frames: List[pd.DataFrame] = []\n",
    "\n",
    "        for video_key, eye_suffix in ((\"VideoData1\", \"eye1\"), (\"VideoData2\", \"eye2\")):\n",
    "            video_path = data_path / f\"{video_key}_resampled.parquet\"\n",
    "            if not video_path.exists():\n",
    "                print(f\"ℹ️ No {video_key} resampled parquet found at {video_path}\")\n",
    "                continue\n",
    "\n",
    "            video_df = pd.read_parquet(video_path, engine=\"pyarrow\")\n",
    "            if \"Seconds\" in video_df.columns:\n",
    "                video_df = video_df.drop(columns=[\"Seconds\"])\n",
    "\n",
    "            rename_map = {\n",
    "                col: f\"{col}_{eye_suffix}\"\n",
    "                for col in video_df.columns\n",
    "                if not col.endswith(f\"_{eye_suffix}\")\n",
    "            }\n",
    "            if rename_map:\n",
    "                video_df = video_df.rename(columns=rename_map)\n",
    "\n",
    "            video_dataframes[video_key] = video_df\n",
    "            video_join_frames.append(video_df)\n",
    "            print(f\"✅ Loaded {video_key} resampled data from {video_path.name}\")\n",
    "\n",
    "        for video_df in video_join_frames:\n",
    "            photometry_tracking_encoder_data = photometry_tracking_encoder_data.join(video_df, how=\"left\")\n",
    "        \n",
    "        print(f\"✅ Successfully loaded all parquet files for {data_path.name}\")\n",
    "        \n",
    "        # Calculate time differences between event_name events\n",
    "        event_times = experiment_events[experiment_events[\"Event\"] == event_name].index\n",
    "        if len(event_times) > 1:\n",
    "            time_diffs = event_times.to_series().diff().dropna().dt.total_seconds()\n",
    "            # Print the 5 shortest time differences\n",
    "            # print(\"5 shortest time differences between events:\")\n",
    "            # print(time_diffs.nsmallest(5))\n",
    "            if (time_diffs < 10).any():\n",
    "                print(f\"⚠️ Warning: Some '{event_name}' events are less than 10 seconds apart. Consider applying a filter to events.\")\n",
    "        else:\n",
    "            print(f\"ℹ️ INFO: Found {len(event_times)} events with name '{event_name}' - not enough to calculate differences\")\n",
    "        \n",
    "        # Check experiment events and get mouse name\n",
    "        mouse_name = process.check_exp_events(experiment_events, photometry_info, verbose=True)\n",
    "        \n",
    "        # Store all loaded data in the dictionary\n",
    "        loaded_data[data_path] = {\n",
    "            \"photometry_tracking_encoder_data\": photometry_tracking_encoder_data,\n",
    "            \"camera_photodiode_data\": camera_photodiode_data,\n",
    "            \"experiment_events\": experiment_events,\n",
    "            \"photometry_info\": photometry_info,\n",
    "            \"session_settings\": session_settings,\n",
    "            \"video_data\": video_dataframes,\n",
    "            \"mouse_name\": mouse_name\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ ERROR processing data path {data_path}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\n✅ Finished loading data for all {len(loaded_data)} successfully processed data paths\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fb0c6e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BEHAVIORAL ANALYSIS CLASS\n",
    "#--------------------------------\n",
    "class BehavioralAnalyzer:\n",
    "    \"\"\"\n",
    "    Analyzer for running, turning, and platform velocity behaviors.\n",
    "    Implements adaptive thresholding using dual Gaussian fitting.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_path: Path, loaded_data: Dict[str, Any],\n",
    "                 threshold_plot_bins: int = 50,\n",
    "                 min_bout_duration_s: float = 0.2,\n",
    "                 running_percentile: float = 10,\n",
    "                 turning_percentile: Optional[float] = None,\n",
    "                 turning_velocity_column: str = \"Motor_Velocity\"):\n",
    "        \"\"\"\n",
    "        Initialize analyzer with loaded data.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        data_path : Path\n",
    "            Path to the data directory\n",
    "        loaded_data : Dict\n",
    "            Dictionary containing loaded dataframes\n",
    "        threshold_plot_bins : int\n",
    "            Number of bins for threshold plots\n",
    "        min_bout_duration_s : float\n",
    "            Minimum duration for a bout to be considered valid (seconds).\n",
    "            This is converted to samples internally based on the data sampling rate.\n",
    "        running_percentile : float\n",
    "            Percentile to use for running threshold calculation (default: 10)\n",
    "        turning_percentile : Optional[float]\n",
    "            Percentile to use on values below median for turning threshold (default: 25).\n",
    "            If None, uses 25th percentile.\n",
    "        turning_velocity_column : str\n",
    "            Column name to use for turning analysis (e.g., \"Motor_Velocity\" or \"Velocity_0Y\").\n",
    "        \"\"\"\n",
    "        self.data_path = data_path\n",
    "        self.mouse_name = loaded_data.get(\"mouse_name\", \"Unknown\")\n",
    "        self.photometry_tracking_encoder_data = loaded_data[\"photometry_tracking_encoder_data\"]\n",
    "        self.experiment_events = loaded_data[\"experiment_events\"]\n",
    "        \n",
    "        # Results storage\n",
    "        self.results = {}\n",
    "        self.figures = {}\n",
    "        \n",
    "        # Thresholding configuration\n",
    "        self.threshold_plot_bins = threshold_plot_bins\n",
    "        self.min_bout_duration_s = min_bout_duration_s\n",
    "        self.running_percentile = running_percentile\n",
    "        self.turning_percentile = turning_percentile\n",
    "        \n",
    "        # Turning configuration\n",
    "        self.turning_velocity_column = turning_velocity_column\n",
    "        self.turning_source_label = self._format_turning_source_label(turning_velocity_column)\n",
    "        if turning_velocity_column not in self.photometry_tracking_encoder_data.columns:\n",
    "            print(f\"⚠️ Warning: Column '{turning_velocity_column}' not found in data. Available columns: {self.photometry_tracking_encoder_data.columns.tolist()}\")\n",
    "        \n",
    "    def _format_turning_source_label(self, column_name: str) -> str:\n",
    "        if column_name == \"Motor_Velocity\":\n",
    "            return \"motor\"\n",
    "        return column_name.lower().replace(\" \", \"_\")\n",
    "        \n",
    "    def _is_visual_mismatch_experiment(self) -> bool:\n",
    "        \"\"\"\n",
    "        Check if this is a visual mismatch experiment based on the data path.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        bool\n",
    "            True if visual mismatch experiment, False otherwise\n",
    "        \"\"\"\n",
    "        path_str = str(self.data_path).lower()\n",
    "        return 'visual_mismatch' in path_str or 'visual mismatch' in path_str\n",
    "    \n",
    "    def slice_data_until_block_timer_elapsed(self, interval: str = \"first_10min\"):\n",
    "        \"\"\"\n",
    "        Slice data based on experiment type and interval specification.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        interval : str\n",
    "            Interval specification:\n",
    "            - \"first_10min\": First 10 minutes (600 seconds) - default for non-visual mismatch\n",
    "            - \"first_block\": Until first \"Block timer elapsed\" - for visual mismatch first interval\n",
    "            - \"last_block\": Until last \"Block timer elapsed\" - for visual mismatch second interval\n",
    "        \"\"\"\n",
    "        start_time = self.photometry_tracking_encoder_data.index[0]\n",
    "        \n",
    "        if interval == \"first_10min\":\n",
    "            # Default: first 10 minutes\n",
    "            end_time = start_time + pd.Timedelta(seconds=600)\n",
    "            self.sliced_data = self.photometry_tracking_encoder_data[\n",
    "                (self.photometry_tracking_encoder_data.index >= start_time) & \n",
    "                (self.photometry_tracking_encoder_data.index <= end_time)\n",
    "            ].copy()\n",
    "            \n",
    "            if len(self.sliced_data) == 0:\n",
    "                print(\"⚠️ Warning: No data found in first 10 minutes. Using all data.\")\n",
    "                self.sliced_data = self.photometry_tracking_encoder_data.copy()\n",
    "            else:\n",
    "                duration_seconds = (self.sliced_data.index[-1] - self.sliced_data.index[0]).total_seconds()\n",
    "                print(f\"✅ Data sliced to first 10 minutes: {duration_seconds:.1f} seconds\")\n",
    "        \n",
    "        elif interval in [\"first_block\", \"last_block\"]:\n",
    "            # For visual mismatch: use block timer elapsed events\n",
    "            block_elapsed_events = self.experiment_events[\n",
    "                self.experiment_events[\"Event\"] == \"Block timer elapsed\"\n",
    "            ]\n",
    "            \n",
    "            if len(block_elapsed_events) == 0:\n",
    "                print(f\"⚠️ Warning: No 'Block timer elapsed' event found for {interval}. Using all data.\")\n",
    "                self.sliced_data = self.photometry_tracking_encoder_data.copy()\n",
    "                return\n",
    "            \n",
    "            if interval == \"first_block\":\n",
    "                end_time = block_elapsed_events.index[0]  # First occurrence\n",
    "                print(f\"✅ Data sliced until first 'Block timer elapsed' at {end_time}\")\n",
    "            else:  # last_block\n",
    "                end_time = block_elapsed_events.index[-1]  # Last occurrence\n",
    "                print(f\"✅ Data sliced until last 'Block timer elapsed' at {end_time}\")\n",
    "            \n",
    "            # Slice the data\n",
    "            self.sliced_data = self.photometry_tracking_encoder_data[\n",
    "                (self.photometry_tracking_encoder_data.index >= start_time) & \n",
    "                (self.photometry_tracking_encoder_data.index <= end_time)\n",
    "            ].copy()\n",
    "            \n",
    "            duration_seconds = (self.sliced_data.index[-1] - self.sliced_data.index[0]).total_seconds()\n",
    "            print(f\"   Duration: {duration_seconds:.1f} seconds\")\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown interval specification: {interval}\")\n",
    "\n",
    "    \n",
    "    def _save_figure(self, fig: plt.Figure, save_path: Path, description: str = \"plot\") -> None:\n",
    "        \"\"\"\n",
    "        Save a matplotlib figure consistently.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        fig : plt.Figure\n",
    "            Figure to save\n",
    "        save_path : Path\n",
    "            Path to save the figure\n",
    "        description : str\n",
    "            Description for logging\n",
    "        \"\"\"\n",
    "        plt.tight_layout()\n",
    "        fig.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "        print(f\"✅ Saved {description}: {save_path}\")\n",
    "    \n",
    "    def _plot_thresholded_timeseries(self, \n",
    "                                     time_seconds: np.ndarray,\n",
    "                                     values: np.ndarray,\n",
    "                                     threshold: float,\n",
    "                                     title: str,\n",
    "                                     ylabel: str,\n",
    "                                     save_path: Path,\n",
    "                                     mask_above: Optional[np.ndarray] = None) -> None:\n",
    "        fig, ax = plt.subplots(figsize=(12, 4))\n",
    "        ax.plot(time_seconds, values, color='gray', linewidth=0.8, alpha=0.7, label='Velocity')\n",
    "        if mask_above is None:\n",
    "            mask_above = values > threshold\n",
    "        # Overlay segments above threshold\n",
    "        ax.plot(time_seconds[mask_above], values[mask_above], color='orange', linewidth=1.2, label='Above threshold')\n",
    "        ax.axhline(threshold, color='green', linestyle='--', linewidth=1.2, label=f'Threshold = {threshold:.2f}')\n",
    "        ax.set_title(title, fontsize=12, fontweight='bold')\n",
    "        ax.set_xlabel('Time (s)', fontsize=11)\n",
    "        ax.set_ylabel(ylabel, fontsize=11)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.legend(fontsize=10)\n",
    "        self._save_figure(fig, save_path, \"thresholded timeseries plot\")\n",
    "\n",
    "    def _save_histogram(self,\n",
    "                        values: np.ndarray,\n",
    "                        bins: int,\n",
    "                        threshold: Optional[float],\n",
    "                        title: str,\n",
    "                        xlabel: str,\n",
    "                        save_path: Path) -> None:\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        ax.hist(values, bins=bins, color='gray', alpha=0.7, edgecolor='white')\n",
    "        if threshold is not None:\n",
    "            ax.axvline(threshold, color='green', linestyle='--', linewidth=1.5, label=f'Threshold = {threshold:.2f}')\n",
    "            ax.legend(fontsize=10)\n",
    "        ax.set_title(title, fontsize=12, fontweight='bold')\n",
    "        ax.set_xlabel(xlabel, fontsize=11)\n",
    "        ax.set_ylabel('Count', fontsize=11)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        self._save_figure(fig, save_path, \"histogram\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def _mad(data: np.ndarray) -> float:\n",
    "        median = np.median(data)\n",
    "        return np.median(np.abs(data - median))\n",
    "    \n",
    "    def find_threshold_robust(self,\n",
    "                              velocities: np.ndarray,\n",
    "                              positive_only: bool = True,\n",
    "                              plot: bool = True,\n",
    "                              title: str = \"Velocity Distribution (robust)\",\n",
    "                              plot_bins: int = 50,\n",
    "                              percentile: float = 10) -> Tuple[float, Optional[plt.Figure]]:\n",
    "        \"\"\"\n",
    "        Simple threshold for separating stationary vs running.\n",
    "        Excludes values below 0.01 and uses the specified percentile as the threshold.\n",
    "        \"\"\"\n",
    "        if positive_only:\n",
    "            valid_velocities = velocities[(velocities > 0) & np.isfinite(velocities)]\n",
    "        else:\n",
    "            valid_velocities = velocities[np.isfinite(velocities)]\n",
    "            valid_velocities = np.abs(valid_velocities)\n",
    "        \n",
    "        # Exclude values below 0.01 as requested\n",
    "        valid_velocities = valid_velocities[valid_velocities >= 0.01]\n",
    "        \n",
    "        if len(valid_velocities) < 100:\n",
    "            print(f\"⚠️ Warning: Not enough valid data points ({len(valid_velocities)}) after filtering\")\n",
    "            return 0.0, None\n",
    "        \n",
    "        # Calculate threshold as specified percentile of the distribution\n",
    "        threshold = np.percentile(valid_velocities, percentile)\n",
    "\n",
    "        # Create plot if requested\n",
    "        fig = None\n",
    "        if plot:\n",
    "            fig, ax = plt.subplots(figsize=(10, 6))\n",
    "            \n",
    "            # Plot histogram\n",
    "            ax.hist(valid_velocities, bins=plot_bins, density=True, alpha=0.6, \n",
    "                   color='gray', edgecolor='white', label='Data')\n",
    "            \n",
    "            # Plot threshold\n",
    "            ax.axvline(threshold, color='green', linestyle='-', linewidth=2, \n",
    "                      label=f'Threshold ({percentile:.0f}th percentile) = {threshold:.2f}')\n",
    "            ax.axvline(0.01, color='red', linestyle='--', linewidth=1, alpha=0.5, label='Min value (0.01)')\n",
    "            \n",
    "            ax.set_xlabel('Velocity (m/s)', fontsize=12)\n",
    "            ax.set_ylabel('Density', fontsize=12)\n",
    "            ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "            ax.legend(fontsize=10)\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "        \n",
    "        print(f\"✅ Threshold found ({percentile:.0f}th percentile): {threshold:.3f}\")\n",
    "        \n",
    "        return threshold, fig\n",
    "    \n",
    "    def find_threshold_turning_correlation_based(self,\n",
    "                                                 running_velocities: np.ndarray,\n",
    "                                                 turning_velocities: np.ndarray,\n",
    "                                                 window_size_s: float = 1.0,\n",
    "                                                 correlation_threshold: float = 0.5,\n",
    "                                                 plot: bool = True,\n",
    "                                                 title: str = \"Turning Velocity Distribution (Correlation-based)\",\n",
    "                                                 plot_bins: int = 50,\n",
    "                                                 percentile: float = 10) -> Tuple[float, Optional[plt.Figure]]:\n",
    "        \"\"\"\n",
    "        Find turning threshold using correlation-based approach.\n",
    "        Uses periods where running and turning are highly correlated to define the threshold.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        running_velocities : np.ndarray\n",
    "            Running velocity values (Velocity_0X)\n",
    "        turning_velocities : np.ndarray\n",
    "            Turning velocity values (Motor_Velocity, absolute values)\n",
    "        window_size_s : float\n",
    "            Window size in seconds for rolling correlation calculation\n",
    "        correlation_threshold : float\n",
    "            Minimum correlation coefficient to consider as \"high correlation\" period\n",
    "        plot : bool\n",
    "            Whether to create a plot\n",
    "        title : str\n",
    "            Plot title\n",
    "        plot_bins : int\n",
    "            Number of bins for histogram\n",
    "        percentile : float\n",
    "            Percentile to use for threshold calculation on high-correlation periods\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        threshold : float\n",
    "            Calculated threshold\n",
    "        fig : Optional[plt.Figure]\n",
    "            Figure if plot=True, None otherwise\n",
    "        \"\"\"\n",
    "        # Remove NaN values (use views where possible for memory efficiency)\n",
    "        valid_mask = ~(np.isnan(running_velocities) | np.isnan(turning_velocities))\n",
    "        running_valid = running_velocities[valid_mask].astype(np.float32)  # Use float32 to save memory\n",
    "        turning_valid = turning_velocities[valid_mask].astype(np.float32)\n",
    "        \n",
    "        if len(running_valid) < 100:\n",
    "            print(f\"⚠️ Warning: Not enough valid data points ({len(running_valid)})\")\n",
    "            return 0.0, None\n",
    "        \n",
    "        # Note: Keep running_valid and turning_valid for plotting, clean up later\n",
    "        \n",
    "        # Estimate sampling rate from data length\n",
    "        # Assume reasonable sampling rate (typically 1000 Hz for this data)\n",
    "        # Calculate window size in samples - use at least 100 samples\n",
    "        # For typical 1000 Hz data, 1 second = 1000 samples\n",
    "        window_samples = max(100, int(window_size_s * 1000))  # Assume ~1000 Hz\n",
    "        \n",
    "        # Try to get actual sampling rate if sliced_data is available\n",
    "        if hasattr(self, 'sliced_data') and len(self.sliced_data) > 1:\n",
    "            try:\n",
    "                sampling_interval = pd.Timedelta(self.sliced_data.index.to_series().diff().median()).total_seconds()\n",
    "                if sampling_interval > 0:\n",
    "                    sampling_rate = 1.0 / sampling_interval\n",
    "                    window_samples = max(100, int(window_size_s * sampling_rate))\n",
    "                    print(f\"   Using rolling correlation window: {window_samples} samples ({window_size_s}s at {sampling_rate:.0f} Hz)\")\n",
    "                else:\n",
    "                    print(f\"   Using rolling correlation window: {window_samples} samples (estimated)\")\n",
    "            except:\n",
    "                print(f\"   Using rolling correlation window: {window_samples} samples (estimated)\")\n",
    "        else:\n",
    "            print(f\"   Using rolling correlation window: {window_samples} samples (estimated)\")\n",
    "        \n",
    "        # For plotting, estimate sampling interval\n",
    "        sampling_interval = 1.0 / 1000  # Default assumption\n",
    "        if hasattr(self, 'sliced_data') and len(self.sliced_data) > 1:\n",
    "            try:\n",
    "                sampling_interval = pd.Timedelta(self.sliced_data.index.to_series().diff().median()).total_seconds()\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Calculate rolling correlation efficiently using pandas (optimized in C)\n",
    "        print(\"   Calculating rolling correlation (optimized)...\")\n",
    "        n = len(running_valid)\n",
    "        \n",
    "        # Use pandas DataFrame rolling correlation - much faster than Python loop\n",
    "        # Only create DataFrame temporarily for correlation calculation\n",
    "        df_corr = pd.DataFrame({\n",
    "            'running': running_valid,\n",
    "            'turning': turning_valid\n",
    "        }, dtype=np.float32)  # Use float32 to save memory\n",
    "        \n",
    "        # Calculate rolling correlation using pandas (optimized implementation)\n",
    "        rolling_corr_series = df_corr['running'].rolling(\n",
    "            window=window_samples,\n",
    "            min_periods=max(10, window_samples // 4)\n",
    "        ).corr(df_corr['turning'])\n",
    "        \n",
    "        # Identify high-correlation periods directly from series (memory efficient)\n",
    "        high_corr_mask = (rolling_corr_series.fillna(0.0).abs() >= correlation_threshold).values\n",
    "        \n",
    "        # Extract turning velocities from high-correlation periods\n",
    "        turning_high_corr = turning_valid[high_corr_mask]\n",
    "        \n",
    "        # Clean up DataFrame to free memory\n",
    "        del df_corr, rolling_corr_series\n",
    "        gc.collect()\n",
    "        \n",
    "        print(f\"   ✓ Rolling correlation calculated ({n} samples)\")\n",
    "        \n",
    "        if len(turning_high_corr) < 100:\n",
    "            print(f\"⚠️ Warning: Not enough high-correlation data points ({len(turning_high_corr)}). \"\n",
    "                  f\"Try lowering correlation_threshold or increasing window_size_s.\")\n",
    "            # Fallback: use all data\n",
    "            turning_high_corr = turning_valid\n",
    "            print(f\"   Using all data as fallback\")\n",
    "        \n",
    "        # Filter values below 0.01 (same as running)\n",
    "        valid_turning = turning_high_corr[turning_high_corr >= 0.01]\n",
    "        \n",
    "        if len(valid_turning) < 100:\n",
    "            print(f\"⚠️ Warning: Not enough valid data points after filtering\")\n",
    "            return 0.0, None\n",
    "        \n",
    "        # Calculate threshold using percentile on high-correlation periods\n",
    "        threshold = np.percentile(valid_turning, percentile)\n",
    "        method = f\"{percentile:.0f}th percentile (high-correlation periods)\"\n",
    "        \n",
    "        # Create plot if requested\n",
    "        fig = None\n",
    "        if plot:\n",
    "            fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 10))\n",
    "            \n",
    "            # Plot 1: Time series of running and turning velocities (no correlation line)\n",
    "            time_axis = np.arange(len(running_valid)) * sampling_interval\n",
    "            ax1.plot(time_axis, np.abs(running_valid), color='blue', alpha=0.5, linewidth=0.5, label='|Running|')\n",
    "            ax1.plot(time_axis, turning_valid, color='red', alpha=0.5, linewidth=0.5, label='|Turning|')\n",
    "            ax1.set_xlabel('Time (s)', fontsize=11)\n",
    "            ax1.set_ylabel('Velocity (m/s or deg/s)', fontsize=11)\n",
    "            ax1.set_title(f'Velocity Time Series (Window: {window_size_s}s) - {self.mouse_name}', \n",
    "                         fontsize=12, fontweight='bold')\n",
    "            ax1.legend(loc='upper left', fontsize=9)\n",
    "            ax1.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Plot 2: Histogram of turning velocities (highlighting high-correlation periods)\n",
    "            ax2.hist(turning_valid, bins=plot_bins, density=True, alpha=0.3, \n",
    "                    color='gray', edgecolor='white', label='All Turning Velocities')\n",
    "            ax2.hist(turning_high_corr, bins=plot_bins, density=True, alpha=0.6, \n",
    "                    color='orange', edgecolor='white', label='High-Correlation Periods')\n",
    "            ax2.axvline(threshold, color='green', linestyle='-', linewidth=2, \n",
    "                       label=f'Threshold ({method}) = {threshold:.2f}')\n",
    "            ax2.axvline(0.01, color='red', linestyle='--', linewidth=1, alpha=0.5, label='Min value (0.01)')\n",
    "            ax2.set_xlabel('Velocity (deg/s)', fontsize=12)\n",
    "            ax2.set_ylabel('Density', fontsize=12)\n",
    "            ax2.set_title(title, fontsize=14, fontweight='bold')\n",
    "            ax2.legend(fontsize=10)\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # Clean up large arrays after plotting\n",
    "            del turning_valid, running_valid\n",
    "        \n",
    "        # Calculate statistics (before cleanup)\n",
    "        high_corr_pct = (np.sum(high_corr_mask) / len(high_corr_mask)) * 100 if len(high_corr_mask) > 0 else 0.0\n",
    "        print(f\"✅ Turning threshold found ({method}): {threshold:.3f}\")\n",
    "        print(f\"   High-correlation periods: {high_corr_pct:.1f}% of data\")\n",
    "        print(f\"   Correlation threshold: {correlation_threshold}, Window size: {window_size_s}s\")\n",
    "        \n",
    "        return threshold, fig\n",
    "    \n",
    "    def find_threshold_turning(self,\n",
    "                               velocities: np.ndarray,\n",
    "                               plot: bool = True,\n",
    "                               title: str = \"Turning Velocity Distribution\",\n",
    "                               plot_bins: int = 50) -> Tuple[float, Optional[plt.Figure]]:\n",
    "        \"\"\"\n",
    "        Find threshold for turning behavior using the median.\n",
    "        Only uses values above minimum threshold (0.01) to calculate the median.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        velocities : np.ndarray\n",
    "            Absolute velocity values\n",
    "        plot : bool\n",
    "            Whether to create a plot\n",
    "        title : str\n",
    "            Plot title\n",
    "        plot_bins : int\n",
    "            Number of bins for histogram\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        threshold : float\n",
    "            Calculated threshold (median of values above 0.01)\n",
    "        fig : Optional[plt.Figure]\n",
    "            Figure if plot=True, None otherwise\n",
    "        \"\"\"\n",
    "        # Filter valid velocities\n",
    "        valid_velocities = velocities[np.isfinite(velocities) & (velocities >= 0)]\n",
    "        \n",
    "        if len(valid_velocities) < 100:\n",
    "            print(f\"⚠️ Warning: Not enough valid data points ({len(valid_velocities)}) after filtering\")\n",
    "            return 0.0, None\n",
    "        \n",
    "        # Only use values above 0.01 (minimum threshold)\n",
    "        min_threshold = 0.01\n",
    "        valid_velocities_filtered = valid_velocities[valid_velocities >= min_threshold]\n",
    "        \n",
    "        if len(valid_velocities_filtered) < 100:\n",
    "            print(f\"⚠️ Warning: Not enough valid data points after filtering\")\n",
    "            return 0.0, None\n",
    "        \n",
    "        # Use median of values above minimum threshold as the turning threshold\n",
    "        threshold = np.median(valid_velocities_filtered)\n",
    "        method = f\"median (of values above {min_threshold})\"\n",
    "\n",
    "        # Create plot if requested\n",
    "        fig = None\n",
    "        if plot:\n",
    "            fig, ax = plt.subplots(figsize=(10, 6))\n",
    "            \n",
    "            # Plot histogram of all filtered velocities\n",
    "            ax.hist(valid_velocities_filtered, bins=plot_bins, density=True, alpha=0.6, \n",
    "                   color='gray', edgecolor='white', label='All velocities')\n",
    "            \n",
    "            # Plot threshold (median)\n",
    "            ax.axvline(threshold, color='green', linestyle='-', linewidth=2, \n",
    "                      label=f'Threshold (median) = {threshold:.2f}')\n",
    "            ax.axvline(min_threshold, color='red', linestyle='--', linewidth=1, alpha=0.5, label=f'Min threshold ({min_threshold})')\n",
    "            \n",
    "            ax.set_xlabel('Velocity (deg/s)', fontsize=12)\n",
    "            ax.set_ylabel('Density', fontsize=12)\n",
    "            ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "            ax.legend(fontsize=10)\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "        \n",
    "        print(f\"✅ Turning threshold found ({method}): {threshold:.3f}\")\n",
    "        \n",
    "        return threshold, fig\n",
    "    \n",
    "    def _calculate_min_bout_samples(self, sampling_interval: float) -> int:\n",
    "        \"\"\"\n",
    "        Calculate minimum bout samples from duration in seconds.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        sampling_interval : float\n",
    "            Sampling interval in seconds\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        int\n",
    "            Minimum number of samples required for a valid bout\n",
    "        \"\"\"\n",
    "        if sampling_interval > 0:\n",
    "            return int(round(self.min_bout_duration_s / sampling_interval))\n",
    "        return 0\n",
    "    \n",
    "    @staticmethod\n",
    "    def _detect_bouts_above_threshold(signal_values: np.ndarray,\n",
    "                                      threshold: float,\n",
    "                                      min_bout_samples: int = 0) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Detect behavioral bouts by identifying values above threshold and filtering by minimum duration.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        signal_values : np.ndarray\n",
    "            Signal values to analyze\n",
    "        threshold : float\n",
    "            Threshold value above which bouts are detected\n",
    "        min_bout_samples : int\n",
    "            Minimum number of consecutive samples required for a valid bout\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        np.ndarray\n",
    "            Boolean array indicating which samples are part of valid bouts\n",
    "        \"\"\"\n",
    "        # Simple threshold: values above threshold\n",
    "        above_threshold = signal_values > threshold\n",
    "        \n",
    "        # Apply minimum bout duration filter\n",
    "        if min_bout_samples > 0:\n",
    "            # Find contiguous regions above threshold\n",
    "            out = np.zeros_like(above_threshold, dtype=bool)\n",
    "            i = 0\n",
    "            while i < len(above_threshold):\n",
    "                if above_threshold[i]:\n",
    "                    # Find the end of this contiguous region\n",
    "                    start = i\n",
    "                    while i < len(above_threshold) and above_threshold[i]:\n",
    "                        i += 1\n",
    "                    end = i\n",
    "                    bout_length = end - start\n",
    "                    # Only keep if it meets minimum duration\n",
    "                    if bout_length >= min_bout_samples:\n",
    "                        out[start:end] = True\n",
    "                else:\n",
    "                    i += 1\n",
    "            return out\n",
    "        else:\n",
    "            return above_threshold\n",
    "\n",
    "    def analyze_running(self, save_dir: Path, suffix: str = \"\"):   \n",
    "        \"\"\"\n",
    "        Analyze running behavior from velocity_0x column.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        save_dir : Path\n",
    "            Directory to save outputs\n",
    "        suffix : str\n",
    "            Suffix to append to filenames (e.g., \"_first_block\", \"_last_block\")\n",
    "        \"\"\"\n",
    "        print(\"ANALYZING RUNNING BEHAVIOR\")\n",
    "        \n",
    "        velocities = self.sliced_data[\"Velocity_0X\"].values\n",
    "        \n",
    "        # Find robust threshold (positive velocities only)\n",
    "        threshold, fig = self.find_threshold_robust(\n",
    "            velocities,\n",
    "            positive_only=True,\n",
    "            plot=True,\n",
    "            title=f\"Running Velocity Distribution (robust) - {self.mouse_name}{suffix}\",\n",
    "            plot_bins=self.threshold_plot_bins,\n",
    "            percentile=self.running_percentile\n",
    "        )\n",
    "        # Save threshold plot\n",
    "        if fig is not None:\n",
    "            fig_path = save_dir / f\"{self.mouse_name}_running_distribution{suffix}.png\"\n",
    "            self._save_figure(fig, fig_path, \"running distribution plot\")\n",
    "            self.figures[f'running_distribution{suffix}'] = fig_path\n",
    "        \n",
    "        # Identify running bouts with threshold and min duration\n",
    "        sampling_interval = pd.Timedelta(self.sliced_data.index.to_series().diff().median()).total_seconds()\n",
    "        min_bout_samples = self._calculate_min_bout_samples(sampling_interval)\n",
    "        pos_velocities = velocities.copy()\n",
    "        pos_velocities[pos_velocities < 0] = 0\n",
    "        is_running = self._detect_bouts_above_threshold(\n",
    "            pos_velocities,\n",
    "            threshold=threshold,\n",
    "            min_bout_samples=min_bout_samples\n",
    "        )\n",
    "        \n",
    "        # Calculate statistics\n",
    "        running_velocities = velocities[is_running]\n",
    "        \n",
    "        if len(running_velocities) > 0:\n",
    "            avg_velocity = np.mean(running_velocities)\n",
    "            std_velocity = np.std(running_velocities)\n",
    "        else:\n",
    "            avg_velocity = 0.0\n",
    "            std_velocity = 0.0\n",
    "        \n",
    "        # Time calculations\n",
    "        total_time = len(self.sliced_data) * sampling_interval\n",
    "        running_time = np.sum(is_running) * sampling_interval\n",
    "        running_time_pct = (running_time / total_time) * 100\n",
    "        \n",
    "        # Distance calculation (only positive velocities, exclude backward)\n",
    "        positive_velocities = velocities.copy()\n",
    "        positive_velocities[positive_velocities < 0] = 0\n",
    "        travelled_distance = np.sum(positive_velocities) * sampling_interval\n",
    "        \n",
    "        # Store results with suffix in key\n",
    "        result_key = f'running{suffix}' if suffix else 'running'\n",
    "        self.results[result_key] = {\n",
    "            'threshold': threshold,\n",
    "            'avg_velocity': avg_velocity,\n",
    "            'std_velocity': std_velocity,\n",
    "            'running_time_seconds': running_time,\n",
    "            'running_time_percentage': running_time_pct,\n",
    "            'travelled_distance_m': travelled_distance,\n",
    "            'total_time_seconds': total_time\n",
    "        }\n",
    "        \n",
    "        # Save CSV\n",
    "        df = pd.DataFrame([{\n",
    "            'Mouse': self.mouse_name,\n",
    "            'Average_Velocity_m_s': avg_velocity,\n",
    "            'SD_Velocity_m_s': std_velocity,\n",
    "            'Running_Time_Percentage': running_time_pct,\n",
    "            'Travelled_Distance_m': travelled_distance,\n",
    "            'Threshold_m_s': threshold\n",
    "        }])\n",
    "        csv_path = save_dir / f\"{self.mouse_name}_running_stats{suffix}.csv\"\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        print(f\"✅ Saved running statistics: {csv_path}\")\n",
    "\n",
    "        # Create and save thresholded time series plot (only positive values considered for thresholding)\n",
    "        time_seconds = (self.sliced_data.index - self.sliced_data.index[0]).total_seconds()\n",
    "        pos_vel = velocities.copy()\n",
    "        pos_vel[pos_vel < 0] = 0  # only positive shown for running\n",
    "        ts_path = save_dir / f\"{self.mouse_name}_running_thresholded_timeseries{suffix}.png\"\n",
    "        self._plot_thresholded_timeseries(\n",
    "            time_seconds=time_seconds,\n",
    "            values=pos_vel,\n",
    "            threshold=threshold,\n",
    "            title=f\"Running Velocity (positive only) with Threshold - {self.mouse_name}{suffix}\",\n",
    "            ylabel='Velocity_0X (m/s)',\n",
    "            save_path=ts_path,\n",
    "            mask_above=(pos_vel > threshold)\n",
    "        )\n",
    "        self.figures[f'running_thresholded_timeseries{suffix}'] = ts_path\n",
    "\n",
    "        # # Save histogram of raw positive running velocities\n",
    "        # hist_path = save_dir / f\"{self.mouse_name}_running_velocity_hist.png\"\n",
    "        # self._save_histogram(\n",
    "        #     values=pos_vel[pos_vel > 0],\n",
    "        #     bins=100,\n",
    "        #     threshold=threshold,\n",
    "        #     title=f\"Histogram of Running Velocity (positive) - {self.mouse_name}\",\n",
    "        #     xlabel='Velocity_0X (m/s)',\n",
    "        #     save_path=hist_path\n",
    "        # )\n",
    "        # self.figures['running_velocity_hist'] = hist_path\n",
    "        \n",
    "    def analyze_turning(self, save_dir: Path, plot: bool = True, suffix: str = \"\", column: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        Analyze turning behavior from the configured turning velocity column.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        save_dir : Path\n",
    "            Directory to save outputs\n",
    "        plot : bool\n",
    "            Whether to create plots\n",
    "        suffix : str\n",
    "            Suffix to append to filenames (e.g., \"_first_block\", \"_last_block\")\n",
    "        column : Optional[str]\n",
    "            Override the configured turning column for this analysis call.\n",
    "        \"\"\"\n",
    "        selected_column = column or self.turning_velocity_column\n",
    "        column_label = self._format_turning_source_label(selected_column)\n",
    "        print(f\"ANALYZING TURNING BEHAVIOR (column: {selected_column})\")\n",
    "        \n",
    "        # Check if selected column exists\n",
    "        if selected_column not in self.sliced_data.columns:\n",
    "            print(f\"⚠️ Warning: Column '{selected_column}' not found in data.\")\n",
    "            print(f\"   Available columns: {self.sliced_data.columns.tolist()}\")\n",
    "            return\n",
    "        \n",
    "        turning_values = self.sliced_data[selected_column].values\n",
    "        \n",
    "        # Use median as threshold\n",
    "        abs_velocities = np.abs(turning_values)\n",
    "        threshold, fig = self.find_threshold_turning(\n",
    "            abs_velocities,\n",
    "            plot=plot,\n",
    "            title=f\"Turning Velocity Distribution ({selected_column}) - {self.mouse_name}{suffix}\",\n",
    "            plot_bins=self.threshold_plot_bins\n",
    "        )\n",
    "        \n",
    "        # Save threshold plot\n",
    "        if fig is not None:\n",
    "            label_suffix = f\"_{column_label}\" if column_label != \"motor\" else \"\"\n",
    "            fig_path = save_dir / f\"{self.mouse_name}_turning_distribution{label_suffix}{suffix}.png\"\n",
    "            self._save_figure(fig, fig_path, \"turning distribution plot\")\n",
    "            self.figures[f'turning_distribution{suffix}'] = fig_path\n",
    "        \n",
    "        # Identify turning bouts with threshold and min duration\n",
    "        sampling_interval = pd.Timedelta(self.sliced_data.index.to_series().diff().median()).total_seconds()\n",
    "        min_bout_samples = self._calculate_min_bout_samples(sampling_interval)\n",
    "        abs_turn = np.abs(turning_values)\n",
    "        is_turning = self._detect_bouts_above_threshold(\n",
    "            abs_turn,\n",
    "            threshold=threshold,\n",
    "            min_bout_samples=min_bout_samples\n",
    "        )\n",
    "        \n",
    "        # Calculate statistics for turning bouts\n",
    "        turning_velocities = turning_values[is_turning]\n",
    "        \n",
    "        if len(turning_velocities) > 0:\n",
    "            avg_velocity = np.mean(np.abs(turning_velocities))\n",
    "            std_velocity = np.std(np.abs(turning_velocities))\n",
    "        else:\n",
    "            avg_velocity = 0.0\n",
    "            std_velocity = 0.0\n",
    "        \n",
    "        # Time calculations\n",
    "        total_time = len(self.sliced_data) * sampling_interval\n",
    "        turning_time = np.sum(is_turning) * sampling_interval\n",
    "        turning_time_pct = (turning_time / total_time) * 100\n",
    "        \n",
    "        # Turned distance (sum of absolute left and right movement)\n",
    "        turned_distance = np.sum(np.abs(turning_values)) * sampling_interval\n",
    "        \n",
    "        # Turn direction percentages\n",
    "        left_turns = np.sum(turning_values[is_turning] < 0)\n",
    "        right_turns = np.sum(turning_values[is_turning] > 0)\n",
    "        total_turns = left_turns + right_turns\n",
    "        \n",
    "        if total_turns > 0:\n",
    "            left_pct = (left_turns / total_turns) * 100\n",
    "            right_pct = (right_turns / total_turns) * 100\n",
    "        else:\n",
    "            left_pct = 0.0\n",
    "            right_pct = 0.0\n",
    "        \n",
    "        # Store results with suffix in key\n",
    "        result_key = f'turning{suffix}' if suffix else 'turning'\n",
    "        self.results[result_key] = {\n",
    "            'threshold': threshold,\n",
    "            'avg_velocity': avg_velocity,\n",
    "            'std_velocity': std_velocity,\n",
    "            'turning_time_seconds': turning_time,\n",
    "            'turning_time_percentage': turning_time_pct,\n",
    "            'turned_distance_m': turned_distance,\n",
    "            'left_percentage': left_pct,\n",
    "            'right_percentage': right_pct,\n",
    "            'total_time_seconds': total_time,\n",
    "            'turning_column': selected_column\n",
    "        }\n",
    "        \n",
    "        # Save CSV\n",
    "        df = pd.DataFrame([{\n",
    "            'Mouse': self.mouse_name,\n",
    "            'Average_Velocity_deg_s': avg_velocity,\n",
    "            'SD_Velocity_deg_s': std_velocity,\n",
    "            'Turning_Time_Percentage': turning_time_pct,\n",
    "            'Turned_Distance_deg': turned_distance,\n",
    "            'Left_Turn_Percentage': left_pct,\n",
    "            'Right_Turn_Percentage': right_pct,\n",
    "            'Threshold_deg_s': threshold,\n",
    "            'Turning_Column': selected_column\n",
    "        }])\n",
    "        csv_label_suffix = f\"_{column_label}\" if column_label != \"motor\" else \"\"\n",
    "        csv_path = save_dir / f\"{self.mouse_name}_turning_stats{csv_label_suffix}{suffix}.csv\"\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        print(f\"✅ Saved turning statistics: {csv_path}\")\n",
    "        \n",
    "        # Create and save thresholded time series plot for turning (use absolute velocity for thresholding/plotting)\n",
    "        time_seconds = (self.sliced_data.index - self.sliced_data.index[0]).total_seconds()\n",
    "        ts_label_suffix = f\"_{column_label}\" if column_label != \"motor\" else \"\"\n",
    "        ts_path = save_dir / f\"{self.mouse_name}_turning_thresholded_timeseries{ts_label_suffix}{suffix}.png\"\n",
    "        self._plot_thresholded_timeseries(\n",
    "            time_seconds=time_seconds,\n",
    "            values=abs_turn,\n",
    "            threshold=threshold,\n",
    "            title=f\"Absolute Turning Velocity ({selected_column}) with Threshold - {self.mouse_name}{suffix}\",\n",
    "            ylabel=f'|{selected_column}|',\n",
    "            save_path=ts_path,\n",
    "            mask_above=(abs_turn > threshold)\n",
    "        )\n",
    "        self.figures[f'turning_thresholded_timeseries{suffix}'] = ts_path\n",
    "    \n",
    "    def analyze_platform_velocity(self, save_dir: Path, \n",
    "                                  encoder_column: str = \"Motor_Velocity\",\n",
    "                                  suffix: str = \"\"):\n",
    "        \"\"\"\n",
    "        Analyze platform velocity and cross-correlation with turning.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        save_dir : Path\n",
    "            Directory to save outputs\n",
    "        encoder_column : str\n",
    "            Column name for motor speed: Motor_velocity\n",
    "        suffix : str\n",
    "            Suffix to append to filenames (e.g., \"_first_block\", \"_last_block\")\n",
    "        \"\"\"\n",
    "        print(\"ANALYZING PLATFORM VELOCITY\")\n",
    "        \n",
    "        if encoder_column not in self.sliced_data.columns:\n",
    "            print(f\"⚠️ Warning: Column '{encoder_column}' not found in data.\")\n",
    "            print(f\"   Available columns: {self.sliced_data.columns.tolist()}\")\n",
    "            return\n",
    "        \n",
    "        motor_velocity = self.sliced_data[\"Motor_Velocity\"].values\n",
    "        turning_velocity = self.sliced_data[\"Velocity_0Y\"].values\n",
    "        \n",
    "        # Remove NaN values for correlation\n",
    "        valid_mask = ~(np.isnan(motor_velocity) | np.isnan(turning_velocity))\n",
    "        motor_valid = motor_velocity[valid_mask]\n",
    "        turning_valid = turning_velocity[valid_mask]\n",
    "        \n",
    "        if len(motor_valid) < 100:\n",
    "            print(f\"⚠️ Warning: Not enough valid data points for correlation\")\n",
    "            return\n",
    "        \n",
    "        # Cross-correlation\n",
    "        correlation = correlate(motor_valid, turning_valid, mode='full')\n",
    "        lags = np.arange(-len(motor_valid) + 1, len(motor_valid))\n",
    "        \n",
    "        # Find lag at maximum correlation\n",
    "        max_corr_idx = np.argmax(np.abs(correlation))\n",
    "        optimal_lag = lags[max_corr_idx]\n",
    "        \n",
    "        # Convert lag to time (samples to seconds)\n",
    "        sampling_interval = pd.Timedelta(self.sliced_data.index.to_series().diff().median()).total_seconds()\n",
    "        optimal_lag_time = optimal_lag * sampling_interval\n",
    "        \n",
    "        # Pearson correlation at optimal lag\n",
    "        if optimal_lag > 0:\n",
    "            enc_shifted = motor_valid[optimal_lag:]\n",
    "            turn_shifted = turning_valid[:-optimal_lag]\n",
    "        elif optimal_lag < 0:\n",
    "            enc_shifted = motor_valid[:optimal_lag]\n",
    "            turn_shifted = turning_valid[-optimal_lag:]\n",
    "        else:\n",
    "            enc_shifted = motor_valid\n",
    "            turn_shifted = turning_valid\n",
    "        \n",
    "        pearson_r, p_value = pearsonr(enc_shifted, turn_shifted)\n",
    "        \n",
    "        # Gain (divide one by the other) - use mean of absolute values\n",
    "        mean_encoder = np.mean(np.abs(enc_shifted))\n",
    "        mean_turning = np.mean(np.abs(turn_shifted))\n",
    "        \n",
    "        if mean_turning != 0:\n",
    "            gain = mean_encoder / mean_turning\n",
    "        else:\n",
    "            gain = np.nan\n",
    "        \n",
    "        # Store results with suffix in key\n",
    "        result_key = f'platform_velocity{suffix}' if suffix else 'platform_velocity'\n",
    "        self.results[result_key] = {\n",
    "            'optimal_lag_samples': optimal_lag,\n",
    "            'optimal_lag_seconds': optimal_lag_time,\n",
    "            'pearson_r': pearson_r,\n",
    "            'p_value': p_value,\n",
    "            'gain': gain,\n",
    "            'mean_motor_velocity': mean_encoder,\n",
    "            'mean_turning_velocity': mean_turning\n",
    "        }\n",
    "        \n",
    "        # Save CSV\n",
    "        df = pd.DataFrame([{\n",
    "            'Mouse': self.mouse_name,\n",
    "            'Optimal_Lag_Samples': optimal_lag,\n",
    "            'Optimal_Lag_Seconds': optimal_lag_time,\n",
    "            'Pearson_R': pearson_r,\n",
    "            'P_Value': p_value,\n",
    "            'Gain_Encoder_to_Turning': gain\n",
    "        }])\n",
    "        csv_path = save_dir / f\"{self.mouse_name}_platform_velocity_stats{suffix}.csv\"\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        print(f\"✅ Saved platform velocity statistics: {csv_path}\")\n",
    "        \n",
    "        # Create cross-correlation plot\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
    "        \n",
    "        # Plot cross-correlation\n",
    "        lag_times = lags * sampling_interval\n",
    "        ax1.plot(lag_times, correlation)\n",
    "        ax1.axvline(optimal_lag_time, color='r', linestyle='--', \n",
    "                   label=f'Optimal lag = {optimal_lag_time:.3f}s')\n",
    "        ax1.set_xlabel('Lag (seconds)', fontsize=12)\n",
    "        ax1.set_ylabel('Cross-correlation', fontsize=12)\n",
    "        ax1.set_title(f'Cross-correlation: flowY vs Motor Velocity - {self.mouse_name}{suffix}', \n",
    "                     fontsize=14, fontweight='bold')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot scatter at optimal lag\n",
    "        ax2.scatter(enc_shifted, turn_shifted, alpha=0.1, s=1)\n",
    "        ax2.set_xlabel('motor Velocity (degrees/s)', fontsize=12)\n",
    "        ax2.set_ylabel('flowY Velocity (degrees/s)', fontsize=12)\n",
    "        ax2.set_title(f'motor vs flowY (Lag={optimal_lag_time:.3f}s, r={pearson_r:.3f})', \n",
    "                     fontsize=12)\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        fig_path = save_dir / f\"{self.mouse_name}_cross_correlation{suffix}.png\"\n",
    "        self._save_figure(fig, fig_path, \"cross-correlation plot\")\n",
    "        self.figures[f'cross_correlation{suffix}'] = fig_path\n",
    "\n",
    "    \n",
    "    def run_full_analysis(self, output_dir: Path, encoder_column: str = \"Motor_Velocity\",\n",
    "                         experiment_day: str = None):\n",
    "        \"\"\"\n",
    "        Run complete analysis pipeline.\n",
    "        \n",
    "        For visual mismatch experiments, runs analysis twice:\n",
    "        1. First interval: until first \"Block timer elapsed\"\n",
    "        2. Second interval: until last \"Block timer elapsed\"\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        output_dir : Path\n",
    "            Directory to save all outputs\n",
    "        encoder_column : str\n",
    "            Column name for encoder velocity\n",
    "        experiment_day : str\n",
    "            Experiment day identifier (e.g., \"Day1\", \"Baseline\")\n",
    "        \"\"\"\n",
    "        # Create output directory\n",
    "        save_dir = data_dir / self.mouse_name\n",
    "        save_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Store experiment day for later use\n",
    "        self.experiment_day = experiment_day if experiment_day else \"Unknown\"\n",
    "        \n",
    "        # Check if this is a visual mismatch experiment\n",
    "        is_visual_mismatch = self._is_visual_mismatch_experiment()\n",
    "        \n",
    "        if is_visual_mismatch:\n",
    "            # Visual mismatch: analyze two intervals\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"VISUAL MISMATCH EXPERIMENT DETECTED\")\n",
    "            print(f\"STARTING FULL ANALYSIS FOR: {self.mouse_name}\")\n",
    "            print(f\"Experiment Day: {self.experiment_day}\")\n",
    "            print(f\"Output directory: {save_dir}\")\n",
    "            print(f\"{'='*60}\")\n",
    "            \n",
    "            # First interval: until first block timer elapsed\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"ANALYZING FIRST INTERVAL (until first Block timer elapsed)\")\n",
    "            print(f\"{'='*60}\")\n",
    "            self.slice_data_until_block_timer_elapsed(interval=\"first_block\")\n",
    "            self.analyze_running(save_dir, suffix=\"_first_block\")\n",
    "            self.analyze_turning(save_dir, suffix=\"_first_block\")\n",
    "            self.analyze_platform_velocity(save_dir, encoder_column, suffix=\"_first_block\")\n",
    "            \n",
    "            # Second interval: until last block timer elapsed\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"ANALYZING SECOND INTERVAL (until last Block timer elapsed)\")\n",
    "            print(f\"{'='*60}\")\n",
    "            self.slice_data_until_block_timer_elapsed(interval=\"last_block\")\n",
    "            self.analyze_running(save_dir, suffix=\"_last_block\")\n",
    "            self.analyze_turning(save_dir, suffix=\"_last_block\")\n",
    "            self.analyze_platform_velocity(save_dir, encoder_column, suffix=\"_last_block\")\n",
    "            \n",
    "        else:\n",
    "            # Standard analysis: first 10 minutes\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"STARTING FULL ANALYSIS FOR: {self.mouse_name}\")\n",
    "            print(f\"Experiment Day: {self.experiment_day}\")\n",
    "            print(f\"Output directory: {save_dir}\")\n",
    "            print(f\"{'='*60}\")\n",
    "            \n",
    "            # Step 1: Slice data\n",
    "            self.slice_data_until_block_timer_elapsed(interval=\"first_10min\")\n",
    "            \n",
    "            # Step 2: Analyze running\n",
    "            self.analyze_running(save_dir)\n",
    "            \n",
    "            # Step 3: Analyze turning\n",
    "            self.analyze_turning(save_dir)\n",
    "            \n",
    "            # Step 4: Analyze platform velocity\n",
    "            self.analyze_platform_velocity(save_dir, encoder_column)\n",
    "        \n",
    "        # Save summary\n",
    "        summary_path = save_dir / f\"{self.mouse_name}_analysis_summary.json\"\n",
    "        with open(summary_path, 'w') as f:\n",
    "            import json\n",
    "            json.dump(self.results, f, indent=2, default=str)\n",
    "        print(f\"\\n✅ Saved analysis summary: {summary_path}\")\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"✅ ANALYSIS COMPLETE FOR: {self.mouse_name}\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "        \n",
    "        return self.results, self.figures\n",
    "    \n",
    "    def get_cohort_row_data(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Get data formatted for cohort-level CSV.\n",
    "        \n",
    "        For visual mismatch experiments, includes data for both intervals.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        Dict with Animal_ID, Experiment_Day, and all behavioral variables\n",
    "        \"\"\"\n",
    "        row_data = {\n",
    "            'Animal_ID': self.mouse_name,\n",
    "            'Experiment_Day': self.experiment_day\n",
    "        }\n",
    "        \n",
    "        # Helper function to add data with prefix\n",
    "        def add_behavioral_data(result_key, prefix):\n",
    "            if result_key in self.results:\n",
    "                data = self.results[result_key]\n",
    "                # Handle prefix: if empty, no prefix; otherwise add underscore\n",
    "                prefix_str = f\"{prefix}_\" if prefix else \"\"\n",
    "                \n",
    "                if 'running' in result_key:\n",
    "                    row_data.update({\n",
    "                        f'{prefix_str}running_threshold_m_per_s': data['threshold'],\n",
    "                        f'{prefix_str}running_velocity_avg_m_per_s': data['avg_velocity'],\n",
    "                        f'{prefix_str}running_velocity_sd_m_per_s': data['std_velocity'],\n",
    "                        f'{prefix_str}running_time_percentage': data['running_time_percentage'],\n",
    "                        f'{prefix_str}running_distance_travelled_m': data['travelled_distance_m'],\n",
    "                        f'{prefix_str}running_time_seconds': data['running_time_seconds'],\n",
    "                        f'{prefix_str}running_total_time_seconds': data['total_time_seconds']\n",
    "                    })\n",
    "                elif 'turning' in result_key:\n",
    "                    row_data.update({\n",
    "                        f'{prefix_str}turning_threshold_m_per_s': data['threshold'],\n",
    "                        f'{prefix_str}turning_velocity_avg_m_per_s': data['avg_velocity'],\n",
    "                        f'{prefix_str}turning_velocity_sd_m_per_s': data['std_velocity'],\n",
    "                        f'{prefix_str}turning_time_percentage': data['turning_time_percentage'],\n",
    "                        f'{prefix_str}turning_distance_turned_m': data['turned_distance_m'],\n",
    "                        f'{prefix_str}turning_left_percentage': data['left_percentage'],\n",
    "                        f'{prefix_str}turning_right_percentage': data['right_percentage'],\n",
    "                        f'{prefix_str}turning_time_seconds': data['turning_time_seconds'],\n",
    "                        f'{prefix_str}turning_total_time_seconds': data['total_time_seconds']\n",
    "                    })\n",
    "                elif 'platform_velocity' in result_key:\n",
    "                    row_data.update({\n",
    "                        f'{prefix_str}platform_cross_corr_lag_samples': data['optimal_lag_samples'],\n",
    "                        f'{prefix_str}platform_cross_corr_lag_seconds': data['optimal_lag_seconds'],\n",
    "                        f'{prefix_str}platform_cross_corr_pearson_r': data['pearson_r'],\n",
    "                        f'{prefix_str}platform_cross_corr_p_value': data['p_value'],\n",
    "                        f'{prefix_str}platform_gain_encoder_to_turning': data['gain'],\n",
    "                        f'{prefix_str}platform_mean_motor_velocity_m_per_s': data['mean_motor_velocity'],\n",
    "                        f'{prefix_str}platform_mean_turning_velocity_m_per_s': data['mean_turning_velocity']\n",
    "                    })\n",
    "        \n",
    "        # Check if this is visual mismatch (has both intervals)\n",
    "        has_first_block = 'running_first_block' in self.results\n",
    "        has_last_block = 'running_last_block' in self.results\n",
    "        \n",
    "        if has_first_block and has_last_block:\n",
    "            # Visual mismatch: add both intervals\n",
    "            add_behavioral_data('running_first_block', 'first_block')\n",
    "            add_behavioral_data('turning_first_block', 'first_block')\n",
    "            add_behavioral_data('platform_velocity_first_block', 'first_block')\n",
    "            \n",
    "            add_behavioral_data('running_last_block', 'last_block')\n",
    "            add_behavioral_data('turning_last_block', 'last_block')\n",
    "            add_behavioral_data('platform_velocity_last_block', 'last_block')\n",
    "        else:\n",
    "            # Standard: add single interval data\n",
    "            add_behavioral_data('running', '')\n",
    "            add_behavioral_data('turning', '')\n",
    "            add_behavioral_data('platform_velocity', '')\n",
    "        \n",
    "        return row_data\n",
    "\n",
    "# Example usage with your existing code:\n",
    "def run_behavioral_analysis(loaded_data: Dict[Path, Dict], \n",
    "                           output_base_dir: Path,\n",
    "                           encoder_column: str = \"Motor_Velocity\",\n",
    "                           experiment_day: str = None,\n",
    "                           plot_bins: int = 50,\n",
    "                           min_bout_duration_s: float = 0.2,\n",
    "                           running_percentile: float = 10,\n",
    "                           turning_percentile: Optional[float] = None,\n",
    "                           turning_velocity_column: str = \"Motor_Velocity\"):\n",
    "    \"\"\"\n",
    "    Run behavioral analysis for all loaded data paths.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    loaded_data : Dict\n",
    "        Dictionary from your data loading code\n",
    "    output_base_dir : Path\n",
    "        Base directory for saving all outputs\n",
    "    encoder_column : str\n",
    "        Column name for encoder velocity data\n",
    "    experiment_day : str\n",
    "        Experiment day identifier (e.g., \"Day1\", \"Baseline\")\n",
    "    plot_bins : int\n",
    "        Number of bins for threshold plots\n",
    "    min_bout_duration_s : float\n",
    "        Minimum duration for a bout to be considered valid (seconds).\n",
    "        This is converted to samples internally based on the data sampling rate.\n",
    "    running_percentile : float\n",
    "        Percentile to use for running threshold calculation (default: 10)\n",
    "    turning_percentile : Optional[float]\n",
    "        Percentile to use on values below median for turning threshold (default: 25).\n",
    "        If None, uses 25th percentile.\n",
    "    turning_velocity_column : str\n",
    "        Column to use for turning analysis (e.g., \"Motor_Velocity\" or \"Velocity_0Y\").\n",
    "    \"\"\"\n",
    "    output_base_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    all_results = {}\n",
    "    cohort_data = []\n",
    "    \n",
    "    for data_path, data_dict in loaded_data.items():\n",
    "        try:\n",
    "            print(f\"\\n{'#'*60}\")\n",
    "            print(f\"Processing: {data_path.name}\")\n",
    "            print(f\"{'#'*60}\")\n",
    "            \n",
    "            # Determine experiment day from path if not provided\n",
    "            if experiment_day is None:\n",
    "                # Try to extract from path (customize this logic as needed)\n",
    "                exp_day = extract_experiment_day_from_path(data_path)\n",
    "            else:\n",
    "                exp_day = experiment_day\n",
    "            \n",
    "            analyzer = BehavioralAnalyzer(\n",
    "                data_path,\n",
    "                data_dict,\n",
    "                threshold_plot_bins=plot_bins,\n",
    "                min_bout_duration_s=min_bout_duration_s,\n",
    "                running_percentile=running_percentile,\n",
    "                turning_percentile=turning_percentile,\n",
    "                turning_velocity_column=turning_velocity_column\n",
    "            )\n",
    "            results, figures = analyzer.run_full_analysis(output_base_dir, encoder_column, exp_day)\n",
    "            \n",
    "            all_results[data_path] = {\n",
    "                'results': results,\n",
    "                'figures': figures,\n",
    "                'analyzer': analyzer\n",
    "            }\n",
    "            \n",
    "            # Collect data for cohort CSV\n",
    "            cohort_data.append(analyzer.get_cohort_row_data())\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ ERROR analyzing {data_path}: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "    \n",
    "    # Save cohort-level CSV\n",
    "    if cohort_data:\n",
    "        save_cohort_csv(cohort_data, output_base_dir)\n",
    "    \n",
    "    print(f\"\\n{'#'*60}\")\n",
    "    print(f\"✅ COMPLETED ANALYSIS FOR {len(all_results)}/{len(loaded_data)} DATASETS\")\n",
    "    print(f\"{'#'*60}\\n\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "def extract_experiment_day_from_path(data_path: Path) -> str:\n",
    "    \"\"\"\n",
    "    Extract experiment day from the data path.\n",
    "    Customize this function based on your naming conventions.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data_path : Path\n",
    "        Path to the processed data\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    str : Experiment day identifier\n",
    "    \"\"\"\n",
    "    # Example: Extract from parent directory name\n",
    "    # Adjust this logic based on your folder structure\n",
    "    path_parts = data_path.parts\n",
    "    \n",
    "    # Look for common day identifiers in path\n",
    "    for part in reversed(path_parts):\n",
    "        part_lower = part.lower()\n",
    "        if 'day' in part_lower or 'baseline' in part_lower or 'test' in part_lower:\n",
    "            return part\n",
    "    \n",
    "    # If no day found, try to extract from the immediate parent\n",
    "    parent_name = data_path.parent.name\n",
    "    if not parent_name.endswith('_processedData'):\n",
    "        return parent_name\n",
    "    \n",
    "    # Fallback: use grandparent name\n",
    "    return data_path.parent.parent.name\n",
    "\n",
    "# def save_cohort_csv(cohort_data: List[Dict[str, Any]], output_base_dir: Path):\n",
    "def save_cohort_csv(cohort_data: List[Dict[str, Any]], data_dir: Path):\n",
    "\n",
    "    \"\"\"\n",
    "    Save or append cohort-level behavioral data to CSV.\n",
    "    Creates file 2 levels above _processedData folder.\n",
    "    Appends new data but overwrites if Animal_ID + Experiment_Day already exists.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    cohort_data : List[Dict]\n",
    "        List of dictionaries containing behavioral data for each animal\n",
    "\n",
    "    \"\"\"\n",
    "    # Navigate to cohort level (2 levels above _processedData)\n",
    "    # Find the first data path to determine cohort directory\n",
    "    cohort_dir = data_dir.parent.parent if '_processedData' in str(data_dir) else cohort_data_dir\n",
    "    \n",
    "    # Create cohort CSV path\n",
    "    cohort_csv_path = cohort_dir / \"cohort_behavioral_analysis.csv\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"SAVING COHORT-LEVEL CSV\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Cohort CSV path: {cohort_csv_path}\")\n",
    "    \n",
    "    # Convert new data to DataFrame\n",
    "    new_df = pd.DataFrame(cohort_data)\n",
    "    \n",
    "    # Ensure Animal_ID and Experiment_Day are first columns\n",
    "    cols = ['Animal_ID', 'Experiment_Day'] + [col for col in new_df.columns \n",
    "                                               if col not in ['Animal_ID', 'Experiment_Day']]\n",
    "    new_df = new_df[cols]\n",
    "    \n",
    "    # Check if file exists\n",
    "    if cohort_csv_path.exists():\n",
    "        print(f\"📄 Existing cohort CSV found, loading...\")\n",
    "        existing_df = pd.read_csv(cohort_csv_path)\n",
    "        \n",
    "        # Check if columns match\n",
    "        existing_cols = set(existing_df.columns)\n",
    "        new_cols = set(new_df.columns)\n",
    "        \n",
    "        if existing_cols != new_cols:\n",
    "            print(f\"⚠️  Column mismatch detected:\")\n",
    "            print(f\"   Columns in existing file: {sorted(existing_cols)}\")\n",
    "            print(f\"   Columns in new data: {sorted(new_cols)}\")\n",
    "            \n",
    "            # Add missing columns to existing data\n",
    "            for col in new_cols - existing_cols:\n",
    "                existing_df[col] = np.nan\n",
    "                print(f\"   ➕ Added missing column: {col}\")\n",
    "            \n",
    "            # Add missing columns to new data\n",
    "            for col in existing_cols - new_cols:\n",
    "                new_df[col] = np.nan\n",
    "                print(f\"   ➕ Added missing column to new data: {col}\")\n",
    "        \n",
    "        # Remove rows with matching Animal_ID and Experiment_Day\n",
    "        for _, row in new_df.iterrows():\n",
    "            animal_id = row['Animal_ID']\n",
    "            exp_day = row['Experiment_Day']\n",
    "            \n",
    "            mask = (existing_df['Animal_ID'] == animal_id) & (existing_df['Experiment_Day'] == exp_day)\n",
    "            if mask.any():\n",
    "                print(f\"   🔄 Overwriting existing data for {animal_id} - {exp_day}\")\n",
    "                existing_df = existing_df[~mask]\n",
    "        \n",
    "        # Append new data\n",
    "        combined_df = pd.concat([existing_df, new_df], ignore_index=True)\n",
    "        \n",
    "        # Sort by Animal_ID and Experiment_Day\n",
    "        combined_df = combined_df.sort_values(['Animal_ID', 'Experiment_Day']).reset_index(drop=True)\n",
    "        \n",
    "        print(f\"   ✅ Appended {len(new_df)} rows to existing data\")\n",
    "        print(f\"   📊 Total rows in cohort CSV: {len(combined_df)}\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"📄 Creating new cohort CSV file...\")\n",
    "        combined_df = new_df\n",
    "        print(f\"   ✅ Created new file with {len(combined_df)} rows\")\n",
    "    \n",
    "    # Save to CSV\n",
    "    combined_df.to_csv(cohort_csv_path, index=False)\n",
    "    print(f\"✅ Saved cohort CSV: {cohort_csv_path}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c5937f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "############################################################\n",
      "Processing: downsampled_data\n",
      "############################################################\n",
      "\n",
      "============================================================\n",
      "VISUAL MISMATCH EXPERIMENT DETECTED\n",
      "STARTING FULL ANALYSIS FOR: B6J2718\n",
      "Experiment Day: Visual_mismatch_day4\n",
      "Output directory: /Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2718\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ANALYZING FIRST INTERVAL (until first Block timer elapsed)\n",
      "============================================================\n",
      "✅ Data sliced until first 'Block timer elapsed' at 1904-01-01 01:09:31.250240\n",
      "   Duration: 390.2 seconds\n",
      "ANALYZING RUNNING BEHAVIOR\n",
      "✅ Threshold found (10th percentile): 0.019\n",
      "✅ Saved running distribution plot: /Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2718/B6J2718_running_distribution_first_block.png\n",
      "✅ Saved running statistics: /Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2718/B6J2718_running_stats_first_block.csv\n",
      "✅ Saved thresholded timeseries plot: /Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2718/B6J2718_running_thresholded_timeseries_first_block.png\n",
      "ANALYZING TURNING BEHAVIOR (column: Velocity_0Y)\n",
      "✅ Turning threshold found (median (of values above 0.01)): 6.927\n",
      "✅ Saved turning distribution plot: /Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2718/B6J2718_turning_distribution_velocity_0y_first_block.png\n",
      "✅ Saved turning statistics: /Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2718/B6J2718_turning_stats_velocity_0y_first_block.csv\n",
      "✅ Saved thresholded timeseries plot: /Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2718/B6J2718_turning_thresholded_timeseries_velocity_0y_first_block.png\n",
      "ANALYZING PLATFORM VELOCITY\n",
      "✅ Saved platform velocity statistics: /Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2718/B6J2718_platform_velocity_stats_first_block.csv\n",
      "✅ Saved cross-correlation plot: /Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2718/B6J2718_cross_correlation_first_block.png\n",
      "\n",
      "============================================================\n",
      "ANALYZING SECOND INTERVAL (until last Block timer elapsed)\n",
      "============================================================\n",
      "✅ Data sliced until last 'Block timer elapsed' at 1904-01-01 01:34:31.470240\n",
      "   Duration: 1890.4 seconds\n",
      "ANALYZING RUNNING BEHAVIOR\n",
      "✅ Threshold found (10th percentile): 0.027\n",
      "✅ Saved running distribution plot: /Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2718/B6J2718_running_distribution_last_block.png\n",
      "✅ Saved running statistics: /Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2718/B6J2718_running_stats_last_block.csv\n",
      "✅ Saved thresholded timeseries plot: /Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2718/B6J2718_running_thresholded_timeseries_last_block.png\n",
      "ANALYZING TURNING BEHAVIOR (column: Velocity_0Y)\n",
      "✅ Turning threshold found (median (of values above 0.01)): 12.749\n",
      "✅ Saved turning distribution plot: /Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2718/B6J2718_turning_distribution_velocity_0y_last_block.png\n",
      "✅ Saved turning statistics: /Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2718/B6J2718_turning_stats_velocity_0y_last_block.csv\n",
      "✅ Saved thresholded timeseries plot: /Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2718/B6J2718_turning_thresholded_timeseries_velocity_0y_last_block.png\n",
      "ANALYZING PLATFORM VELOCITY\n",
      "✅ Saved platform velocity statistics: /Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2718/B6J2718_platform_velocity_stats_last_block.csv\n",
      "✅ Saved cross-correlation plot: /Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2718/B6J2718_cross_correlation_last_block.png\n",
      "\n",
      "✅ Saved analysis summary: /Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2718/B6J2718_analysis_summary.json\n",
      "\n",
      "============================================================\n",
      "✅ ANALYSIS COMPLETE FOR: B6J2718\n",
      "============================================================\n",
      "\n",
      "\n",
      "############################################################\n",
      "Processing: downsampled_data\n",
      "############################################################\n",
      "\n",
      "============================================================\n",
      "VISUAL MISMATCH EXPERIMENT DETECTED\n",
      "STARTING FULL ANALYSIS FOR: B6J2719\n",
      "Experiment Day: Visual_mismatch_day4\n",
      "Output directory: /Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2719\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ANALYZING FIRST INTERVAL (until first Block timer elapsed)\n",
      "============================================================\n",
      "✅ Data sliced until first 'Block timer elapsed' at 1904-01-01 01:47:05.800256\n",
      "   Duration: 358.4 seconds\n",
      "ANALYZING RUNNING BEHAVIOR\n",
      "✅ Threshold found (10th percentile): 0.028\n",
      "✅ Saved running distribution plot: /Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2719/B6J2719_running_distribution_first_block.png\n",
      "✅ Saved running statistics: /Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2719/B6J2719_running_stats_first_block.csv\n",
      "✅ Saved thresholded timeseries plot: /Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2719/B6J2719_running_thresholded_timeseries_first_block.png\n",
      "ANALYZING TURNING BEHAVIOR (column: Velocity_0Y)\n",
      "✅ Turning threshold found (median (of values above 0.01)): 6.484\n",
      "✅ Saved turning distribution plot: /Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2719/B6J2719_turning_distribution_velocity_0y_first_block.png\n",
      "✅ Saved turning statistics: /Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2719/B6J2719_turning_stats_velocity_0y_first_block.csv\n",
      "✅ Saved thresholded timeseries plot: /Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2719/B6J2719_turning_thresholded_timeseries_velocity_0y_first_block.png\n",
      "ANALYZING PLATFORM VELOCITY\n",
      "✅ Saved platform velocity statistics: /Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2719/B6J2719_platform_velocity_stats_first_block.csv\n",
      "✅ Saved cross-correlation plot: /Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2719/B6J2719_cross_correlation_first_block.png\n",
      "\n",
      "============================================================\n",
      "ANALYZING SECOND INTERVAL (until last Block timer elapsed)\n",
      "============================================================\n",
      "✅ Data sliced until last 'Block timer elapsed' at 1904-01-01 02:12:05.830240\n",
      "   Duration: 1858.5 seconds\n",
      "ANALYZING RUNNING BEHAVIOR\n",
      "✅ Threshold found (10th percentile): 0.029\n",
      "✅ Saved running distribution plot: /Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2719/B6J2719_running_distribution_last_block.png\n",
      "✅ Saved running statistics: /Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2719/B6J2719_running_stats_last_block.csv\n",
      "✅ Saved thresholded timeseries plot: /Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2719/B6J2719_running_thresholded_timeseries_last_block.png\n",
      "ANALYZING TURNING BEHAVIOR (column: Velocity_0Y)\n",
      "✅ Turning threshold found (median (of values above 0.01)): 3.193\n",
      "✅ Saved turning distribution plot: /Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2719/B6J2719_turning_distribution_velocity_0y_last_block.png\n",
      "✅ Saved turning statistics: /Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2719/B6J2719_turning_stats_velocity_0y_last_block.csv\n",
      "✅ Saved thresholded timeseries plot: /Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2719/B6J2719_turning_thresholded_timeseries_velocity_0y_last_block.png\n",
      "ANALYZING PLATFORM VELOCITY\n",
      "✅ Saved platform velocity statistics: /Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2719/B6J2719_platform_velocity_stats_last_block.csv\n",
      "✅ Saved cross-correlation plot: /Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2719/B6J2719_cross_correlation_last_block.png\n",
      "\n",
      "✅ Saved analysis summary: /Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2719/B6J2719_analysis_summary.json\n",
      "\n",
      "============================================================\n",
      "✅ ANALYSIS COMPLETE FOR: B6J2719\n",
      "============================================================\n",
      "\n",
      "\n",
      "############################################################\n",
      "Processing: downsampled_data\n",
      "############################################################\n",
      "\n",
      "============================================================\n",
      "VISUAL MISMATCH EXPERIMENT DETECTED\n",
      "STARTING FULL ANALYSIS FOR: B6J2721\n",
      "Experiment Day: Visual_mismatch_day4\n",
      "Output directory: /Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2721\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ANALYZING FIRST INTERVAL (until first Block timer elapsed)\n",
      "============================================================\n",
      "✅ Data sliced until first 'Block timer elapsed' at 1904-01-01 02:26:01.410240\n",
      "   Duration: 384.3 seconds\n",
      "ANALYZING RUNNING BEHAVIOR\n",
      "✅ Threshold found (10th percentile): 0.016\n",
      "✅ Saved running distribution plot: /Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2721/B6J2721_running_distribution_first_block.png\n",
      "✅ Saved running statistics: /Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2721/B6J2721_running_stats_first_block.csv\n",
      "✅ Saved thresholded timeseries plot: /Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2721/B6J2721_running_thresholded_timeseries_first_block.png\n",
      "ANALYZING TURNING BEHAVIOR (column: Velocity_0Y)\n",
      "✅ Turning threshold found (median (of values above 0.01)): 4.049\n",
      "✅ Saved turning distribution plot: /Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2721/B6J2721_turning_distribution_velocity_0y_first_block.png\n",
      "✅ Saved turning statistics: /Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2721/B6J2721_turning_stats_velocity_0y_first_block.csv\n",
      "✅ Saved thresholded timeseries plot: /Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2721/B6J2721_turning_thresholded_timeseries_velocity_0y_first_block.png\n",
      "ANALYZING PLATFORM VELOCITY\n",
      "✅ Saved platform velocity statistics: /Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2721/B6J2721_platform_velocity_stats_first_block.csv\n",
      "✅ Saved cross-correlation plot: /Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2721/B6J2721_cross_correlation_first_block.png\n",
      "\n",
      "============================================================\n",
      "ANALYZING SECOND INTERVAL (until last Block timer elapsed)\n",
      "============================================================\n",
      "✅ Data sliced until last 'Block timer elapsed' at 1904-01-01 02:51:01.437664\n",
      "   Duration: 1884.3 seconds\n",
      "ANALYZING RUNNING BEHAVIOR\n",
      "✅ Threshold found (10th percentile): 0.035\n",
      "✅ Saved running distribution plot: /Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2721/B6J2721_running_distribution_last_block.png\n",
      "✅ Saved running statistics: /Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2721/B6J2721_running_stats_last_block.csv\n",
      "✅ Saved thresholded timeseries plot: /Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2721/B6J2721_running_thresholded_timeseries_last_block.png\n",
      "ANALYZING TURNING BEHAVIOR (column: Velocity_0Y)\n",
      "✅ Turning threshold found (median (of values above 0.01)): 3.726\n",
      "✅ Saved turning distribution plot: /Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2721/B6J2721_turning_distribution_velocity_0y_last_block.png\n",
      "✅ Saved turning statistics: /Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2721/B6J2721_turning_stats_velocity_0y_last_block.csv\n",
      "✅ Saved thresholded timeseries plot: /Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2721/B6J2721_turning_thresholded_timeseries_velocity_0y_last_block.png\n",
      "ANALYZING PLATFORM VELOCITY\n",
      "✅ Saved platform velocity statistics: /Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2721/B6J2721_platform_velocity_stats_last_block.csv\n",
      "✅ Saved cross-correlation plot: /Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2721/B6J2721_cross_correlation_last_block.png\n",
      "\n",
      "✅ Saved analysis summary: /Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2721/B6J2721_analysis_summary.json\n",
      "\n",
      "============================================================\n",
      "✅ ANALYSIS COMPLETE FOR: B6J2721\n",
      "============================================================\n",
      "\n",
      "\n",
      "############################################################\n",
      "Processing: downsampled_data\n",
      "############################################################\n",
      "\n",
      "============================================================\n",
      "VISUAL MISMATCH EXPERIMENT DETECTED\n",
      "STARTING FULL ANALYSIS FOR: B6J2722\n",
      "Experiment Day: Visual_mismatch_day4\n",
      "Output directory: /Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2722\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ANALYZING FIRST INTERVAL (until first Block timer elapsed)\n",
      "============================================================\n",
      "✅ Data sliced until first 'Block timer elapsed' at 1904-01-01 03:03:28.710240\n",
      "   Duration: 388.2 seconds\n",
      "ANALYZING RUNNING BEHAVIOR\n",
      "✅ Threshold found (10th percentile): 0.022\n",
      "✅ Saved running distribution plot: /Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2722/B6J2722_running_distribution_first_block.png\n",
      "✅ Saved running statistics: /Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2722/B6J2722_running_stats_first_block.csv\n",
      "✅ Saved thresholded timeseries plot: /Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2722/B6J2722_running_thresholded_timeseries_first_block.png\n",
      "ANALYZING TURNING BEHAVIOR (column: Velocity_0Y)\n",
      "✅ Turning threshold found (median (of values above 0.01)): 2.636\n",
      "✅ Saved turning distribution plot: /Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2722/B6J2722_turning_distribution_velocity_0y_first_block.png\n",
      "✅ Saved turning statistics: /Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2722/B6J2722_turning_stats_velocity_0y_first_block.csv\n",
      "✅ Saved thresholded timeseries plot: /Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2722/B6J2722_turning_thresholded_timeseries_velocity_0y_first_block.png\n",
      "ANALYZING PLATFORM VELOCITY\n",
      "✅ Saved platform velocity statistics: /Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2722/B6J2722_platform_velocity_stats_first_block.csv\n",
      "✅ Saved cross-correlation plot: /Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2722/B6J2722_cross_correlation_first_block.png\n",
      "\n",
      "============================================================\n",
      "ANALYZING SECOND INTERVAL (until last Block timer elapsed)\n",
      "============================================================\n",
      "✅ Data sliced until last 'Block timer elapsed' at 1904-01-01 03:28:29.270240\n",
      "   Duration: 1888.8 seconds\n",
      "ANALYZING RUNNING BEHAVIOR\n",
      "✅ Threshold found (10th percentile): 0.055\n",
      "✅ Saved running distribution plot: /Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2722/B6J2722_running_distribution_last_block.png\n",
      "✅ Saved running statistics: /Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2722/B6J2722_running_stats_last_block.csv\n",
      "✅ Saved thresholded timeseries plot: /Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2722/B6J2722_running_thresholded_timeseries_last_block.png\n",
      "ANALYZING TURNING BEHAVIOR (column: Velocity_0Y)\n",
      "✅ Turning threshold found (median (of values above 0.01)): 7.692\n",
      "✅ Saved turning distribution plot: /Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2722/B6J2722_turning_distribution_velocity_0y_last_block.png\n",
      "✅ Saved turning statistics: /Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2722/B6J2722_turning_stats_velocity_0y_last_block.csv\n",
      "✅ Saved thresholded timeseries plot: /Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2722/B6J2722_turning_thresholded_timeseries_velocity_0y_last_block.png\n",
      "ANALYZING PLATFORM VELOCITY\n",
      "✅ Saved platform velocity statistics: /Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2722/B6J2722_platform_velocity_stats_last_block.csv\n",
      "✅ Saved cross-correlation plot: /Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2722/B6J2722_cross_correlation_last_block.png\n",
      "\n",
      "✅ Saved analysis summary: /Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2722/B6J2722_analysis_summary.json\n",
      "\n",
      "============================================================\n",
      "✅ ANALYSIS COMPLETE FOR: B6J2722\n",
      "============================================================\n",
      "\n",
      "\n",
      "############################################################\n",
      "Processing: downsampled_data\n",
      "############################################################\n",
      "\n",
      "============================================================\n",
      "VISUAL MISMATCH EXPERIMENT DETECTED\n",
      "STARTING FULL ANALYSIS FOR: B6J2723\n",
      "Experiment Day: Visual_mismatch_day4\n",
      "Output directory: /Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2723\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ANALYZING FIRST INTERVAL (until first Block timer elapsed)\n",
      "============================================================\n",
      "✅ Data sliced until first 'Block timer elapsed' at 1904-01-01 03:39:14.340256\n",
      "   Duration: 342.6 seconds\n",
      "ANALYZING RUNNING BEHAVIOR\n",
      "✅ Threshold found (10th percentile): 0.020\n",
      "✅ Saved running distribution plot: /Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2723/B6J2723_running_distribution_first_block.png\n",
      "✅ Saved running statistics: /Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2723/B6J2723_running_stats_first_block.csv\n",
      "✅ Saved thresholded timeseries plot: /Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2723/B6J2723_running_thresholded_timeseries_first_block.png\n",
      "ANALYZING TURNING BEHAVIOR (column: Velocity_0Y)\n",
      "✅ Turning threshold found (median (of values above 0.01)): 2.156\n",
      "✅ Saved turning distribution plot: /Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2723/B6J2723_turning_distribution_velocity_0y_first_block.png\n",
      "✅ Saved turning statistics: /Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2723/B6J2723_turning_stats_velocity_0y_first_block.csv\n",
      "✅ Saved thresholded timeseries plot: /Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2723/B6J2723_turning_thresholded_timeseries_velocity_0y_first_block.png\n",
      "ANALYZING PLATFORM VELOCITY\n",
      "✅ Saved platform velocity statistics: /Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2723/B6J2723_platform_velocity_stats_first_block.csv\n",
      "✅ Saved cross-correlation plot: /Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2723/B6J2723_cross_correlation_first_block.png\n",
      "\n",
      "============================================================\n",
      "ANALYZING SECOND INTERVAL (until last Block timer elapsed)\n",
      "============================================================\n",
      "✅ Data sliced until last 'Block timer elapsed' at 1904-01-01 04:04:14.720256\n",
      "   Duration: 1843.0 seconds\n",
      "ANALYZING RUNNING BEHAVIOR\n",
      "✅ Threshold found (10th percentile): 0.040\n",
      "✅ Saved running distribution plot: /Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2723/B6J2723_running_distribution_last_block.png\n",
      "✅ Saved running statistics: /Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2723/B6J2723_running_stats_last_block.csv\n",
      "✅ Saved thresholded timeseries plot: /Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2723/B6J2723_running_thresholded_timeseries_last_block.png\n",
      "ANALYZING TURNING BEHAVIOR (column: Velocity_0Y)\n",
      "✅ Turning threshold found (median (of values above 0.01)): 7.399\n",
      "✅ Saved turning distribution plot: /Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2723/B6J2723_turning_distribution_velocity_0y_last_block.png\n",
      "✅ Saved turning statistics: /Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2723/B6J2723_turning_stats_velocity_0y_last_block.csv\n",
      "✅ Saved thresholded timeseries plot: /Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2723/B6J2723_turning_thresholded_timeseries_velocity_0y_last_block.png\n",
      "ANALYZING PLATFORM VELOCITY\n",
      "✅ Saved platform velocity statistics: /Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2723/B6J2723_platform_velocity_stats_last_block.csv\n",
      "✅ Saved cross-correlation plot: /Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2723/B6J2723_cross_correlation_last_block.png\n",
      "\n",
      "✅ Saved analysis summary: /Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2723/B6J2723_analysis_summary.json\n",
      "\n",
      "============================================================\n",
      "✅ ANALYSIS COMPLETE FOR: B6J2723\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "SAVING COHORT-LEVEL CSV\n",
      "============================================================\n",
      "Cohort CSV path: /Volumes/RanczLab2/Cohort1_rotation/cohort_behavioral_analysis.csv\n",
      "📄 Existing cohort CSV found, loading...\n",
      "   ✅ Appended 5 rows to existing data\n",
      "   📊 Total rows in cohort CSV: 11\n",
      "✅ Saved cohort CSV: /Volumes/RanczLab2/Cohort1_rotation/cohort_behavioral_analysis.csv\n",
      "============================================================\n",
      "\n",
      "\n",
      "############################################################\n",
      "✅ COMPLETED ANALYSIS FOR 5/5 DATASETS\n",
      "############################################################\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{PosixPath('/Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2718-2024-12-11T13-49-13_processedData/downsampled_data'): {'results': {'running_first_block': {'threshold': 0.018556706008410622,\n",
       "    'avg_velocity': 0.06299029365746579,\n",
       "    'std_velocity': 0.024054396538518964,\n",
       "    'running_time_seconds': 20.197,\n",
       "    'running_time_percentage': 5.176514611729367,\n",
       "    'travelled_distance_m': 1.6587900397009232,\n",
       "    'total_time_seconds': 390.166},\n",
       "   'turning_first_block': {'threshold': 6.927044190933026,\n",
       "    'avg_velocity': 72.46162740869039,\n",
       "    'std_velocity': 72.15066335667841,\n",
       "    'turning_time_seconds': 56.154,\n",
       "    'turning_time_percentage': 14.39233556998816,\n",
       "    'turned_distance_m': 6443.408906094021,\n",
       "    'left_percentage': 63.00352601773693,\n",
       "    'right_percentage': 36.99647398226306,\n",
       "    'total_time_seconds': 390.166,\n",
       "    'turning_column': 'Velocity_0Y'},\n",
       "   'platform_velocity_first_block': {'optimal_lag_samples': 112,\n",
       "    'optimal_lag_seconds': 0.112,\n",
       "    'pearson_r': -0.9526270420532954,\n",
       "    'p_value': 0.0,\n",
       "    'gain': 0.07309932207343987,\n",
       "    'mean_motor_velocity': 1.20747920382016,\n",
       "    'mean_turning_velocity': 16.518336553204357},\n",
       "   'running_last_block': {'threshold': 0.02695503581752278,\n",
       "    'avg_velocity': 0.14079754369305636,\n",
       "    'std_velocity': 0.07452918552842369,\n",
       "    'running_time_seconds': 582.7760000000001,\n",
       "    'running_time_percentage': 30.828412821508415,\n",
       "    'travelled_distance_m': 85.76937470859934,\n",
       "    'total_time_seconds': 1890.386},\n",
       "   'turning_last_block': {'threshold': 12.74915738722339,\n",
       "    'avg_velocity': 111.9252443388973,\n",
       "    'std_velocity': 74.0765378366893,\n",
       "    'turning_time_seconds': 602.813,\n",
       "    'turning_time_percentage': 31.88835507668804,\n",
       "    'turned_distance_m': 80673.22631217983,\n",
       "    'left_percentage': 67.29350561451064,\n",
       "    'right_percentage': 32.706494385489364,\n",
       "    'total_time_seconds': 1890.386,\n",
       "    'turning_column': 'Velocity_0Y'},\n",
       "   'platform_velocity_last_block': {'optimal_lag_samples': 111,\n",
       "    'optimal_lag_seconds': 0.111,\n",
       "    'pearson_r': -0.9972196179751809,\n",
       "    'p_value': 0.0,\n",
       "    'gain': 0.08204528710251995,\n",
       "    'mean_motor_velocity': 3.501475452515717,\n",
       "    'mean_turning_velocity': 42.67735023146957}},\n",
       "  'figures': {'running_distribution_first_block': PosixPath('/Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2718/B6J2718_running_distribution_first_block.png'),\n",
       "   'running_thresholded_timeseries_first_block': PosixPath('/Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2718/B6J2718_running_thresholded_timeseries_first_block.png'),\n",
       "   'turning_distribution_first_block': PosixPath('/Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2718/B6J2718_turning_distribution_velocity_0y_first_block.png'),\n",
       "   'turning_thresholded_timeseries_first_block': PosixPath('/Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2718/B6J2718_turning_thresholded_timeseries_velocity_0y_first_block.png'),\n",
       "   'cross_correlation_first_block': PosixPath('/Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2718/B6J2718_cross_correlation_first_block.png'),\n",
       "   'running_distribution_last_block': PosixPath('/Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2718/B6J2718_running_distribution_last_block.png'),\n",
       "   'running_thresholded_timeseries_last_block': PosixPath('/Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2718/B6J2718_running_thresholded_timeseries_last_block.png'),\n",
       "   'turning_distribution_last_block': PosixPath('/Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2718/B6J2718_turning_distribution_velocity_0y_last_block.png'),\n",
       "   'turning_thresholded_timeseries_last_block': PosixPath('/Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2718/B6J2718_turning_thresholded_timeseries_velocity_0y_last_block.png'),\n",
       "   'cross_correlation_last_block': PosixPath('/Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2718/B6J2718_cross_correlation_last_block.png')},\n",
       "  'analyzer': <__main__.BehavioralAnalyzer at 0x3c5ab1350>},\n",
       " PosixPath('/Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2719-2024-12-11T14-26-30_processedData/downsampled_data'): {'results': {'running_first_block': {'threshold': 0.028321145205635938,\n",
       "    'avg_velocity': 0.10671753863772515,\n",
       "    'std_velocity': 0.03755434442242579,\n",
       "    'running_time_seconds': 60.795,\n",
       "    'running_time_percentage': 16.96076106515644,\n",
       "    'travelled_distance_m': 6.922097521380713,\n",
       "    'total_time_seconds': 358.445},\n",
       "   'turning_first_block': {'threshold': 6.483924047824065,\n",
       "    'avg_velocity': 107.11860432704283,\n",
       "    'std_velocity': 90.97029294472622,\n",
       "    'turning_time_seconds': 67.67,\n",
       "    'turning_time_percentage': 18.87876801182887,\n",
       "    'turned_distance_m': 9268.95654583256,\n",
       "    'left_percentage': 67.41392049652727,\n",
       "    'right_percentage': 32.58607950347274,\n",
       "    'total_time_seconds': 358.445,\n",
       "    'turning_column': 'Velocity_0Y'},\n",
       "   'platform_velocity_first_block': {'optimal_lag_samples': 133,\n",
       "    'optimal_lag_seconds': 0.133,\n",
       "    'pearson_r': -0.9564175632920602,\n",
       "    'p_value': 0.0,\n",
       "    'gain': 0.0784528772122002,\n",
       "    'mean_motor_velocity': 2.0287524310353233,\n",
       "    'mean_turning_velocity': 25.85950322188862},\n",
       "   'running_last_block': {'threshold': 0.02890275241976159,\n",
       "    'avg_velocity': 0.10721026327855987,\n",
       "    'std_velocity': 0.04259901619811033,\n",
       "    'running_time_seconds': 344.16700000000003,\n",
       "    'running_time_percentage': 18.518785563432385,\n",
       "    'travelled_distance_m': 38.65667458121485,\n",
       "    'total_time_seconds': 1858.4750000000001},\n",
       "   'turning_last_block': {'threshold': 3.1929677482432437,\n",
       "    'avg_velocity': 95.14920130445381,\n",
       "    'std_velocity': 80.75958384194716,\n",
       "    'turning_time_seconds': 441.581,\n",
       "    'turning_time_percentage': 23.76039494747037,\n",
       "    'turned_distance_m': 49174.32836271419,\n",
       "    'left_percentage': 61.022326594667796,\n",
       "    'right_percentage': 38.977673405332204,\n",
       "    'total_time_seconds': 1858.4750000000001,\n",
       "    'turning_column': 'Velocity_0Y'},\n",
       "   'platform_velocity_last_block': {'optimal_lag_samples': 115,\n",
       "    'optimal_lag_seconds': 0.115,\n",
       "    'pearson_r': -0.9912970190391663,\n",
       "    'p_value': 0.0,\n",
       "    'gain': 0.08331167130664414,\n",
       "    'mean_motor_velocity': 2.204521228916047,\n",
       "    'mean_turning_velocity': 26.461133168267573}},\n",
       "  'figures': {'running_distribution_first_block': PosixPath('/Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2719/B6J2719_running_distribution_first_block.png'),\n",
       "   'running_thresholded_timeseries_first_block': PosixPath('/Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2719/B6J2719_running_thresholded_timeseries_first_block.png'),\n",
       "   'turning_distribution_first_block': PosixPath('/Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2719/B6J2719_turning_distribution_velocity_0y_first_block.png'),\n",
       "   'turning_thresholded_timeseries_first_block': PosixPath('/Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2719/B6J2719_turning_thresholded_timeseries_velocity_0y_first_block.png'),\n",
       "   'cross_correlation_first_block': PosixPath('/Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2719/B6J2719_cross_correlation_first_block.png'),\n",
       "   'running_distribution_last_block': PosixPath('/Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2719/B6J2719_running_distribution_last_block.png'),\n",
       "   'running_thresholded_timeseries_last_block': PosixPath('/Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2719/B6J2719_running_thresholded_timeseries_last_block.png'),\n",
       "   'turning_distribution_last_block': PosixPath('/Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2719/B6J2719_turning_distribution_velocity_0y_last_block.png'),\n",
       "   'turning_thresholded_timeseries_last_block': PosixPath('/Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2719/B6J2719_turning_thresholded_timeseries_velocity_0y_last_block.png'),\n",
       "   'cross_correlation_last_block': PosixPath('/Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2719/B6J2719_cross_correlation_last_block.png')},\n",
       "  'analyzer': <__main__.BehavioralAnalyzer at 0x3044d4550>},\n",
       " PosixPath('/Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2721-2024-12-11T15-05-01_processedData/downsampled_data'): {'results': {'running_first_block': {'threshold': 0.015540294303059025,\n",
       "    'avg_velocity': 0.09164682261592012,\n",
       "    'std_velocity': 0.05351171542958733,\n",
       "    'running_time_seconds': 29.354,\n",
       "    'running_time_percentage': 7.638541926888166,\n",
       "    'travelled_distance_m': 3.0244291155656016,\n",
       "    'total_time_seconds': 384.288},\n",
       "   'turning_first_block': {'threshold': 4.049050824075392,\n",
       "    'avg_velocity': 84.16402923033552,\n",
       "    'std_velocity': 78.08426019939093,\n",
       "    'turning_time_seconds': 67.617,\n",
       "    'turning_time_percentage': 17.595397202098425,\n",
       "    'turned_distance_m': 7487.219226239855,\n",
       "    'left_percentage': 64.8816126122129,\n",
       "    'right_percentage': 35.118387387787095,\n",
       "    'total_time_seconds': 384.288,\n",
       "    'turning_column': 'Velocity_0Y'},\n",
       "   'platform_velocity_first_block': {'optimal_lag_samples': 109,\n",
       "    'optimal_lag_seconds': 0.109,\n",
       "    'pearson_r': -0.9991625127627897,\n",
       "    'p_value': 0.0,\n",
       "    'gain': 0.0842881481833897,\n",
       "    'mean_motor_velocity': 1.6392575682762696,\n",
       "    'mean_turning_velocity': 19.448257004171683},\n",
       "   'running_last_block': {'threshold': 0.03544355493071726,\n",
       "    'avg_velocity': 0.12906354730837774,\n",
       "    'std_velocity': 0.05566390480940156,\n",
       "    'running_time_seconds': 439.702,\n",
       "    'running_time_percentage': 23.33483343558087,\n",
       "    'travelled_distance_m': 59.267054431271305,\n",
       "    'total_time_seconds': 1884.316},\n",
       "   'turning_last_block': {'threshold': 3.7261104144017234,\n",
       "    'avg_velocity': 93.86108326503769,\n",
       "    'std_velocity': 75.89870796159035,\n",
       "    'turning_time_seconds': 525.2090000000001,\n",
       "    'turning_time_percentage': 27.872660424260054,\n",
       "    'turned_distance_m': 56306.59681147749,\n",
       "    'left_percentage': 62.93094748947562,\n",
       "    'right_percentage': 37.06905251052438,\n",
       "    'total_time_seconds': 1884.316,\n",
       "    'turning_column': 'Velocity_0Y'},\n",
       "   'platform_velocity_last_block': {'optimal_lag_samples': 110,\n",
       "    'optimal_lag_seconds': 0.11,\n",
       "    'pearson_r': -0.999240988859744,\n",
       "    'p_value': 0.0,\n",
       "    'gain': 0.08356656416954299,\n",
       "    'mean_motor_velocity': 2.4972579500926884,\n",
       "    'mean_turning_velocity': 29.8834584730103}},\n",
       "  'figures': {'running_distribution_first_block': PosixPath('/Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2721/B6J2721_running_distribution_first_block.png'),\n",
       "   'running_thresholded_timeseries_first_block': PosixPath('/Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2721/B6J2721_running_thresholded_timeseries_first_block.png'),\n",
       "   'turning_distribution_first_block': PosixPath('/Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2721/B6J2721_turning_distribution_velocity_0y_first_block.png'),\n",
       "   'turning_thresholded_timeseries_first_block': PosixPath('/Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2721/B6J2721_turning_thresholded_timeseries_velocity_0y_first_block.png'),\n",
       "   'cross_correlation_first_block': PosixPath('/Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2721/B6J2721_cross_correlation_first_block.png'),\n",
       "   'running_distribution_last_block': PosixPath('/Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2721/B6J2721_running_distribution_last_block.png'),\n",
       "   'running_thresholded_timeseries_last_block': PosixPath('/Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2721/B6J2721_running_thresholded_timeseries_last_block.png'),\n",
       "   'turning_distribution_last_block': PosixPath('/Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2721/B6J2721_turning_distribution_velocity_0y_last_block.png'),\n",
       "   'turning_thresholded_timeseries_last_block': PosixPath('/Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2721/B6J2721_turning_thresholded_timeseries_velocity_0y_last_block.png'),\n",
       "   'cross_correlation_last_block': PosixPath('/Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2721/B6J2721_cross_correlation_last_block.png')},\n",
       "  'analyzer': <__main__.BehavioralAnalyzer at 0x2a97243d0>},\n",
       " PosixPath('/Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2722-2024-12-11T15-42-27_processedData/downsampled_data'): {'results': {'running_first_block': {'threshold': 0.022026875968554765,\n",
       "    'avg_velocity': 0.07281872101314531,\n",
       "    'std_velocity': 0.026341149445620993,\n",
       "    'running_time_seconds': 13.172,\n",
       "    'running_time_percentage': 3.3931750092093838,\n",
       "    'travelled_distance_m': 1.3275066865686767,\n",
       "    'total_time_seconds': 388.19100000000003},\n",
       "   'turning_first_block': {'threshold': 2.6364251416849616,\n",
       "    'avg_velocity': 139.45755036565086,\n",
       "    'std_velocity': 68.10248152685027,\n",
       "    'turning_time_seconds': 40.131,\n",
       "    'turning_time_percentage': 10.337952193636637,\n",
       "    'turned_distance_m': 6538.985937410491,\n",
       "    'left_percentage': 0.0,\n",
       "    'right_percentage': 100.0,\n",
       "    'total_time_seconds': 388.19100000000003,\n",
       "    'turning_column': 'Velocity_0Y'},\n",
       "   'platform_velocity_first_block': {'optimal_lag_samples': 111,\n",
       "    'optimal_lag_seconds': 0.111,\n",
       "    'pearson_r': -0.9995364583363188,\n",
       "    'p_value': 0.0,\n",
       "    'gain': 0.08458076230409203,\n",
       "    'mean_motor_velocity': 1.425146286668729,\n",
       "    'mean_turning_velocity': 16.849532303160387},\n",
       "   'running_last_block': {'threshold': 0.05517177684665645,\n",
       "    'avg_velocity': 0.1432960640890335,\n",
       "    'std_velocity': 0.05186001104796057,\n",
       "    'running_time_seconds': 561.674,\n",
       "    'running_time_percentage': 29.73785321622596,\n",
       "    'travelled_distance_m': 84.35252863131454,\n",
       "    'total_time_seconds': 1888.751},\n",
       "   'turning_last_block': {'threshold': 7.6923711297049255,\n",
       "    'avg_velocity': 115.28233691869507,\n",
       "    'std_velocity': 74.86875669362725,\n",
       "    'turning_time_seconds': 562.835,\n",
       "    'turning_time_percentage': 29.799322409359412,\n",
       "    'turned_distance_m': 74608.33630342224,\n",
       "    'left_percentage': 28.06364209759521,\n",
       "    'right_percentage': 71.9363579024048,\n",
       "    'total_time_seconds': 1888.751,\n",
       "    'turning_column': 'Velocity_0Y'},\n",
       "   'platform_velocity_last_block': {'optimal_lag_samples': 111,\n",
       "    'optimal_lag_seconds': 0.111,\n",
       "    'pearson_r': -0.9988743682363829,\n",
       "    'p_value': 0.0,\n",
       "    'gain': 0.08332138270025262,\n",
       "    'mean_motor_velocity': 3.2915028631589367,\n",
       "    'mean_turning_velocity': 39.50369948852226}},\n",
       "  'figures': {'running_distribution_first_block': PosixPath('/Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2722/B6J2722_running_distribution_first_block.png'),\n",
       "   'running_thresholded_timeseries_first_block': PosixPath('/Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2722/B6J2722_running_thresholded_timeseries_first_block.png'),\n",
       "   'turning_distribution_first_block': PosixPath('/Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2722/B6J2722_turning_distribution_velocity_0y_first_block.png'),\n",
       "   'turning_thresholded_timeseries_first_block': PosixPath('/Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2722/B6J2722_turning_thresholded_timeseries_velocity_0y_first_block.png'),\n",
       "   'cross_correlation_first_block': PosixPath('/Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2722/B6J2722_cross_correlation_first_block.png'),\n",
       "   'running_distribution_last_block': PosixPath('/Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2722/B6J2722_running_distribution_last_block.png'),\n",
       "   'running_thresholded_timeseries_last_block': PosixPath('/Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2722/B6J2722_running_thresholded_timeseries_last_block.png'),\n",
       "   'turning_distribution_last_block': PosixPath('/Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2722/B6J2722_turning_distribution_velocity_0y_last_block.png'),\n",
       "   'turning_thresholded_timeseries_last_block': PosixPath('/Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2722/B6J2722_turning_thresholded_timeseries_velocity_0y_last_block.png'),\n",
       "   'cross_correlation_last_block': PosixPath('/Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2722/B6J2722_cross_correlation_last_block.png')},\n",
       "  'analyzer': <__main__.BehavioralAnalyzer at 0x282c32490>},\n",
       " PosixPath('/Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2723-2024-12-11T16-18-56_processedData/downsampled_data'): {'results': {'running_first_block': {'threshold': 0.020189340861426944,\n",
       "    'avg_velocity': 0.062012613215616644,\n",
       "    'std_velocity': 0.018953233666879985,\n",
       "    'running_time_seconds': 43.376,\n",
       "    'running_time_percentage': 12.658981467970229,\n",
       "    'travelled_distance_m': 3.102718477110778,\n",
       "    'total_time_seconds': 342.65000000000003},\n",
       "   'turning_first_block': {'threshold': 2.1560548336371865,\n",
       "    'avg_velocity': 64.37682966360913,\n",
       "    'std_velocity': 50.97359613651387,\n",
       "    'turning_time_seconds': 74.296,\n",
       "    'turning_time_percentage': 21.682766671530715,\n",
       "    'turned_distance_m': 5574.080156570774,\n",
       "    'left_percentage': 21.369925702595026,\n",
       "    'right_percentage': 78.63007429740497,\n",
       "    'total_time_seconds': 342.65000000000003,\n",
       "    'turning_column': 'Velocity_0Y'},\n",
       "   'platform_velocity_first_block': {'optimal_lag_samples': 112,\n",
       "    'optimal_lag_seconds': 0.112,\n",
       "    'pearson_r': -0.9973679591200344,\n",
       "    'p_value': 0.0,\n",
       "    'gain': 0.08566478839586926,\n",
       "    'mean_motor_velocity': 1.393992971358691,\n",
       "    'mean_turning_velocity': 16.272648277806393},\n",
       "   'running_last_block': {'threshold': 0.04007844333980635,\n",
       "    'avg_velocity': 0.11836063533382221,\n",
       "    'std_velocity': 0.042632192666122753,\n",
       "    'running_time_seconds': 689.783,\n",
       "    'running_time_percentage': 37.42657471663511,\n",
       "    'travelled_distance_m': 85.74088199771403,\n",
       "    'total_time_seconds': 1843.03},\n",
       "   'turning_last_block': {'threshold': 7.399448615492603,\n",
       "    'avg_velocity': 64.30191373742973,\n",
       "    'std_velocity': 47.7679457879231,\n",
       "    'turning_time_seconds': 631.913,\n",
       "    'turning_time_percentage': 34.28663667981531,\n",
       "    'turned_distance_m': 47680.53590809067,\n",
       "    'left_percentage': 18.506661518278626,\n",
       "    'right_percentage': 81.49333848172138,\n",
       "    'total_time_seconds': 1843.03,\n",
       "    'turning_column': 'Velocity_0Y'},\n",
       "   'platform_velocity_last_block': {'optimal_lag_samples': 113,\n",
       "    'optimal_lag_seconds': 0.113,\n",
       "    'pearson_r': -0.9983998572814682,\n",
       "    'p_value': 0.0,\n",
       "    'gain': 0.08385954794316038,\n",
       "    'mean_motor_velocity': 2.1696179453876394,\n",
       "    'mean_turning_velocity': 25.872044371837024}},\n",
       "  'figures': {'running_distribution_first_block': PosixPath('/Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2723/B6J2723_running_distribution_first_block.png'),\n",
       "   'running_thresholded_timeseries_first_block': PosixPath('/Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2723/B6J2723_running_thresholded_timeseries_first_block.png'),\n",
       "   'turning_distribution_first_block': PosixPath('/Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2723/B6J2723_turning_distribution_velocity_0y_first_block.png'),\n",
       "   'turning_thresholded_timeseries_first_block': PosixPath('/Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2723/B6J2723_turning_thresholded_timeseries_velocity_0y_first_block.png'),\n",
       "   'cross_correlation_first_block': PosixPath('/Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2723/B6J2723_cross_correlation_first_block.png'),\n",
       "   'running_distribution_last_block': PosixPath('/Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2723/B6J2723_running_distribution_last_block.png'),\n",
       "   'running_thresholded_timeseries_last_block': PosixPath('/Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2723/B6J2723_running_thresholded_timeseries_last_block.png'),\n",
       "   'turning_distribution_last_block': PosixPath('/Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2723/B6J2723_turning_distribution_velocity_0y_last_block.png'),\n",
       "   'turning_thresholded_timeseries_last_block': PosixPath('/Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2723/B6J2723_turning_thresholded_timeseries_velocity_0y_last_block.png'),\n",
       "   'cross_correlation_last_block': PosixPath('/Volumes/RanczLab2/Cohort1_rotation/Visual_mismatch_day4/B6J2723/B6J2723_cross_correlation_last_block.png')},\n",
       "  'analyzer': <__main__.BehavioralAnalyzer at 0x2f7d52490>}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_behavioral_analysis(loaded_data, data_dir, encoder_column=\"Motor_Velocity\", min_bout_duration_s=1, running_percentile=10, turning_velocity_column= \"Velocity_0Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638c9ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create DFs and plot figure for each data path\n",
    "#---------------------------------------------------\n",
    "# Dictionary to store analysis results for each data path\n",
    "data_path_variables = {}\n",
    "\n",
    "for idx, data_path in enumerate(data_paths, start=1):\n",
    "    print(f\"\\n--------- Processing analysis for data path {idx}/{len(data_paths)}: {data_path} ---------\")\n",
    "    \n",
    "    # Skip if data wasn't successfully loaded for this path\n",
    "    if data_path not in loaded_data:\n",
    "        print(f\"⚠️ Skipping analysis for {data_path} - data not loaded successfully\")\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        # Extract data from loaded_data dictionary\n",
    "        photometry_tracking_encoder_data = loaded_data[data_path][\"photometry_tracking_encoder_data\"]\n",
    "        camera_photodiode_data = loaded_data[data_path][\"camera_photodiode_data\"]\n",
    "        experiment_events = loaded_data[data_path][\"experiment_events\"]\n",
    "        mouse_name = loaded_data[data_path][\"mouse_name\"]\n",
    "        session_name = f\"{mouse_name}_{data_path.name}\"  # Assuming session_name is constructed this way\n",
    "        \n",
    "        # Create dataframe to analyze\n",
    "        df_to_analyze = photometry_tracking_encoder_data[\"Photodiode_int\"]  # Using downsampled values in common time grid\n",
    "        # df_to_analyze = camera_photodiode_data[\"Photodiode\"]  # Use async raw values if needed for troubleshooting\n",
    "        \n",
    "        # Determine halt times based on different conditions\n",
    "        if vestibular_mismatch or event_name == \"No halt\":  # Determine halt times based on experiment events\n",
    "            events_matching_name = experiment_events[experiment_events[\"Event\"] == event_name]\n",
    "            if events_matching_name.empty:\n",
    "                print(f\"⚠️ WARNING: No events found with name '{event_name}', skipping this data path\")\n",
    "                continue\n",
    "                \n",
    "            photodiode_halts = events_matching_name.index.tolist()\n",
    "            nearest_indices = photometry_tracking_encoder_data.index.get_indexer(photodiode_halts, method='nearest')\n",
    "            photodiode_halts = photometry_tracking_encoder_data.index[nearest_indices]  # Align to downsampled data time grid\n",
    "            print(f\"ℹ️ INFO: vestibular MM or 'No halt', no signal in the photodiode, using experiment events for MM times\")\n",
    "            photodiode_delay_min = photodiode_delay_avg = photodiode_delay_max = None\n",
    "        else:  # Determine exact halt times based on photodiode signal\n",
    "            try:\n",
    "                photodiode_halts, photodiode_delay_min, photodiode_delay_avg, photodiode_delay_max = process.analyze_photodiode(\n",
    "                    df_to_analyze, experiment_events, event_name, plot=True\n",
    "                )\n",
    "                print(f\"✅ Successfully analyzed photodiode signal for {data_path.name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ ERROR analyzing photodiode signal: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        # Store analysis results\n",
    "        data_path_variables[data_path] = {\n",
    "            \"photodiode_halts\": photodiode_halts,\n",
    "            \"photodiode_delay_min\": photodiode_delay_min,\n",
    "            \"photodiode_delay_avg\": photodiode_delay_avg,\n",
    "            \"photodiode_delay_max\": photodiode_delay_max,\n",
    "            \"session_name\": session_name\n",
    "        }\n",
    "        \n",
    "        # Plot figure if requested\n",
    "        if plot_fig1:\n",
    "            try:\n",
    "                # process.plot_figure_1(\n",
    "                #     photometry_tracking_encoder_data, \n",
    "                #     session_name, \n",
    "                #     save_path, \n",
    "                #     common_resampled_rate, \n",
    "                #     photodiode_halts, \n",
    "                #     save_figure=True, \n",
    "                #     show_figure=True, \n",
    "                #     downsample_factor=50\n",
    "                # )\n",
    "                # print(f\"✅ Successfully created figure 1 for {data_path.name}\")\n",
    "                pass\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ ERROR creating figure 1: {str(e)}\")\n",
    "        else:\n",
    "            print(f\"ℹ️ INFO: skipping figure 1 for {data_path.name}\")\n",
    "        \n",
    "        # Clean up to free memory\n",
    "        del df_to_analyze\n",
    "        gc.collect()\n",
    "        \n",
    "        print(f\"✅ Completed analysis for data path: {data_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ ERROR during analysis of {data_path}: {str(e)}\")\n",
    "\n",
    "print(f\"\\n✅ Finished analyzing all {len(data_path_variables)} successfully processed data paths\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36d8084",
   "metadata": {},
   "outputs": [],
   "source": [
    "#separates RIGHT vs LEFT TURNS, creates heatmaps, and comprehensive (Mean +- sem) aligned data for each data path\n",
    "#----------------------------------------------------\n",
    "\"\"\"\n",
    "Refactored photometry analysis code for processing aligned behavioral data.\n",
    "Separates left vs right turns, creates heatmaps, and generates comprehensive plots.\n",
    "\"\"\"\n",
    "class PhotometryAnalyzer:\n",
    "    \"\"\"Class for analyzing photometry data with behavioral events.\"\"\"\n",
    "    \n",
    "    # Class constants\n",
    "    REQUIRED_COLUMNS = [\n",
    "        \"Time (s)\", \"Photodiode_int\", \"z_470\", \"z_560\", \n",
    "        \"dfF_470\", \"dfF_560\", \"Motor_Velocity\", \"Velocity_0X\", \"Velocity_0Y\"\n",
    "    ]\n",
    "    \n",
    "    FLUORESCENCE_CHANNELS = {\n",
    "        'z_470': {'color': 'cornflowerblue', 'label': 'z_470'},\n",
    "        'z_560': {'color': 'red', 'label': 'z_560'},\n",
    "        'dfF_470': {'color': 'blue', 'label': 'dfF_470'},\n",
    "        'dfF_560': {'color': 'orange', 'label': 'dfF_560'}\n",
    "    }\n",
    "    \n",
    "    def __init__(self, time_window: Tuple[float, float] = (time_window_start, time_window_end)):\n",
    "        \"\"\"\n",
    "        Initialize analyzer with time window parameters.\n",
    "        \n",
    "        Args:\n",
    "            time_window: Tuple of (start, end) times relative to event (seconds)\n",
    "        \"\"\"\n",
    "        self.time_window_start, self.time_window_end = time_window\n",
    "        \n",
    "    def process_aligned_data(self, df: pd.DataFrame, halt_time: pd.Timestamp) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Process a single halt event efficiently.\n",
    "        \n",
    "        Args:\n",
    "            df: Main dataframe with photometry and behavioral data\n",
    "            halt_time: Timestamp of the halt event\n",
    "            \n",
    "        Returns:\n",
    "            Windowed dataframe or None if no data in window\n",
    "        \"\"\"\n",
    "        window_start = halt_time + pd.Timedelta(seconds=self.time_window_start)\n",
    "        window_end = halt_time + pd.Timedelta(seconds=self.time_window_end)\n",
    "        mask = (df.index >= window_start) & (df.index <= window_end)\n",
    "        \n",
    "        if not mask.any():\n",
    "            return None\n",
    "        \n",
    "        window = df.loc[mask].copy()\n",
    "        window[\"Time (s)\"] = (window.index - halt_time).total_seconds()\n",
    "        window[\"Halt Time\"] = halt_time\n",
    "        return window\n",
    "    \n",
    "    def separate_turns(self, aligned_df: pd.DataFrame) -> Tuple[List[pd.Timestamp], List[pd.Timestamp]]:\n",
    "        \"\"\"\n",
    "        Separate halt events into left and right turns based on motor velocity.\n",
    "        \n",
    "        Args:\n",
    "            aligned_df: Aligned dataframe with all halt events\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (left_turn_halts, right_turn_halts)\n",
    "        \"\"\"\n",
    "        left_turn_halts = []\n",
    "        right_turn_halts = []\n",
    "        \n",
    "        for halt in aligned_df[\"Halt Time\"].unique():\n",
    "            # Look at motor velocity in pre-event window\n",
    "            subset = aligned_df[\n",
    "                (aligned_df[\"Halt Time\"] == halt) & \n",
    "                (aligned_df[\"Time (s)\"] >= -1) & \n",
    "                (aligned_df[\"Time (s)\"] < 0)\n",
    "            ]\n",
    "            \n",
    "            if subset.empty:\n",
    "                continue\n",
    "                \n",
    "            mean_velocity = subset[\"Motor_Velocity\"].mean()\n",
    "            \n",
    "            if mean_velocity < 0:  # Negative = left turn\n",
    "                left_turn_halts.append(halt)\n",
    "            elif mean_velocity > 0:  # Positive = right turn\n",
    "                right_turn_halts.append(halt)\n",
    "        \n",
    "        return left_turn_halts, right_turn_halts\n",
    "    \n",
    "    def save_turn_data(self, aligned_df: pd.DataFrame, left_turns: List, right_turns: List, \n",
    "                      session_name: str, event_name: str, output_dir: Path) -> None:\n",
    "        \"\"\"Save separated turn data to CSV files only if turns are detected.\"\"\"\n",
    "        \n",
    "        # Only save left turns if there are any\n",
    "        if left_turns:\n",
    "            left_df = aligned_df[aligned_df[\"Halt Time\"].isin(left_turns)]\n",
    "            left_file = output_dir / f\"{session_name}_{event_name}_left_turns.csv\"\n",
    "            left_df.to_csv(left_file, index=False, float_format='%.4f')\n",
    "            print(f\"Saved {len(left_turns)} left turns to {left_file}\")\n",
    "        else:\n",
    "            print(f\"No left turns detected - no CSV file saved\")\n",
    "        \n",
    "        # Only save right turns if there are any\n",
    "        if right_turns:\n",
    "            right_df = aligned_df[aligned_df[\"Halt Time\"].isin(right_turns)]\n",
    "            right_file = output_dir / f\"{session_name}_{event_name}_right_turns.csv\"\n",
    "            right_df.to_csv(right_file, index=False, float_format='%.4f')\n",
    "            print(f\"Saved {len(right_turns)} right turns to {right_file}\")\n",
    "        else:\n",
    "            print(f\"No right turns detected - no CSV file saved\")\n",
    "    \n",
    "    def create_heatmap(self, pivot_data: pd.DataFrame, session_name: str, event_name: str, \n",
    "                      channel: str, save_path: Path, figsize: Tuple[int, int] = (10, 6)) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create and save normalized heatmap.\n",
    "        \n",
    "        Args:\n",
    "            pivot_data: Pivoted data (events x time)\n",
    "            session_name: Name of session\n",
    "            event_name: Name of event type\n",
    "            channel: Channel name (e.g., 'z_470')\n",
    "            save_path: Path to save figure\n",
    "            figsize: Figure size tuple\n",
    "            \n",
    "        Returns:\n",
    "            Normalized data used for heatmap\n",
    "        \"\"\"\n",
    "        # Baseline normalization\n",
    "        baseline_cols = (pivot_data.columns >= -1) & (pivot_data.columns < 0)\n",
    "        if baseline_cols.any():\n",
    "            baseline_means = pivot_data.loc[:, baseline_cols].mean(axis=1)\n",
    "            normalized_data = pivot_data.subtract(baseline_means, axis=0)\n",
    "        else:\n",
    "            normalized_data = pivot_data\n",
    "        \n",
    "        # Create figure\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        \n",
    "        sns.heatmap(\n",
    "            normalized_data, \n",
    "            cmap=\"RdBu_r\", \n",
    "            center=0, \n",
    "            ax=ax,\n",
    "            cbar_kws={'label': f'Normalized {channel}'},\n",
    "            rasterized=True\n",
    "        )\n",
    "        \n",
    "        ax.set_title(f\"Heatmap ({channel}) - {session_name}\")\n",
    "        ax.set_xlabel(\"Time (s)\")\n",
    "        ax.set_ylabel(\"Event\")\n",
    "        \n",
    "        # Optimize y-axis ticks\n",
    "        n_events = len(normalized_data.index)\n",
    "        y_positions = range(0, n_events, max(1, n_events // 10))\n",
    "        y_labels = [str(i+1) for i in y_positions]\n",
    "        ax.set_yticks(y_positions)\n",
    "        ax.set_yticklabels(y_labels)\n",
    "        \n",
    "        # Add event line at time 0\n",
    "        if 0 in normalized_data.columns:\n",
    "            zero_idx = list(normalized_data.columns).index(0)\n",
    "            ax.axvline(zero_idx, linestyle='--', color='black', alpha=0.7)\n",
    "        \n",
    "        # Optimize x-axis ticks\n",
    "        time_cols = normalized_data.columns\n",
    "        tick_indices = [i for i, val in enumerate(time_cols) \n",
    "                       if isinstance(val, (int, float)) and val % 2 == 0]\n",
    "        tick_labels = [f\"{int(time_cols[i])}\" for i in tick_indices]\n",
    "        \n",
    "        ax.set_xticks(tick_indices)\n",
    "        ax.set_xticklabels(tick_labels, rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path, dpi=200, bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "        \n",
    "        return normalized_data\n",
    "    \n",
    "    def create_line_collections(self, df: pd.DataFrame, channels: List[str]) -> Dict[str, LineCollection]:\n",
    "        \"\"\"Create line collections for individual traces.\"\"\"\n",
    "        line_collections = {}\n",
    "        \n",
    "        for channel in channels:\n",
    "            lines = []\n",
    "            for halt in df[\"Halt Time\"].unique():\n",
    "                subset = df[df[\"Halt Time\"] == halt]\n",
    "                time_vals = subset[\"Time (s)\"].values\n",
    "                channel_vals = subset[channel].values\n",
    "                lines.append(list(zip(time_vals, channel_vals)))\n",
    "            \n",
    "            color = self.FLUORESCENCE_CHANNELS.get(channel, {}).get('color', 'gray')\n",
    "            line_collections[channel] = LineCollection(lines, colors=color, alpha=0.3, linewidths=1)\n",
    "        \n",
    "        return line_collections\n",
    "    \n",
    "    def add_mean_sem_plot(self, ax: plt.Axes, df: pd.DataFrame, channels: List[str], \n",
    "                         turn_type: str, line_style: str = '-') -> None:\n",
    "        \"\"\"Add mean ± SEM traces to axis.\"\"\"\n",
    "        grouped = df.groupby(\"Time (s)\")\n",
    "        time_index = grouped.mean().index.values\n",
    "        \n",
    "        for channel in channels:\n",
    "            color = self.FLUORESCENCE_CHANNELS[channel]['color']\n",
    "            label = f\"{turn_type} {channel}\"\n",
    "            \n",
    "            mean_vals = grouped.mean()[channel]\n",
    "            sem_vals = grouped.sem()[channel]\n",
    "            \n",
    "            ax.plot(time_index, mean_vals, color=color, linestyle=line_style, \n",
    "                   linewidth=2, label=label)\n",
    "            ax.fill_between(time_index, mean_vals - sem_vals, mean_vals + sem_vals,\n",
    "                           color=color, alpha=0.2)\n",
    "    \n",
    "    def create_summary_plot(self, aligned_df: pd.DataFrame, session_name: str, \n",
    "                          event_name: str, save_path: Path) -> None:\n",
    "        \"\"\"\n",
    "        Create comprehensive summary plots comparing left vs right turns.\n",
    "        \n",
    "        Args:\n",
    "            aligned_df: Aligned dataframe with all events\n",
    "            session_name: Session identifier\n",
    "            event_name: Event type name\n",
    "            save_path: Path to save the plot\n",
    "        \"\"\"\n",
    "        # Separate turns\n",
    "        left_turns, right_turns = self.separate_turns(aligned_df)\n",
    "        \n",
    "        if not left_turns and not right_turns:\n",
    "            print(\"No valid turns found for summary plot\")\n",
    "            return\n",
    "        \n",
    "        # Create DataFrames for each turn type\n",
    "        left_df = aligned_df[aligned_df[\"Halt Time\"].isin(left_turns)]\n",
    "        right_df = aligned_df[aligned_df[\"Halt Time\"].isin(right_turns)]\n",
    "        \n",
    "        # Create figure\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(21, 6), sharex=True)\n",
    "        z_channels = ['z_470', 'z_560']\n",
    "        \n",
    "        # Left plot - Left turn traces\n",
    "        if not left_df.empty:\n",
    "            ax1 = axes[0]\n",
    "            left_collections = self.create_line_collections(left_df, z_channels + ['Motor_Velocity'])\n",
    "            \n",
    "            # Add fluorescence traces\n",
    "            for channel in z_channels:\n",
    "                ax1.add_collection(left_collections[channel])\n",
    "            \n",
    "            # Add motor velocity on secondary axis\n",
    "            ax1_motor = ax1.twinx()\n",
    "            ax1_motor.add_collection(left_collections['Motor_Velocity'])\n",
    "            ax1_motor.set_ylabel(\"Motor Velocity\", color='slategray')\n",
    "            ax1_motor.tick_params(axis='y', labelcolor='slategray')\n",
    "            ax1_motor.autoscale()\n",
    "            \n",
    "            ax1.set_title(f'Left Turn Traces (n={len(left_turns)})')\n",
    "            ax1.set_ylabel(\"Fluorescence (z-score)\")\n",
    "            ax1.axvline(0, linestyle='--', color='black', alpha=0.7)\n",
    "            ax1.autoscale()\n",
    "        \n",
    "        # Right plot - Right turn traces  \n",
    "        if not right_df.empty:\n",
    "            ax2 = axes[1]\n",
    "            right_collections = self.create_line_collections(right_df, z_channels + ['Motor_Velocity'])\n",
    "            \n",
    "            # Add fluorescence traces\n",
    "            for channel in z_channels:\n",
    "                ax2.add_collection(right_collections[channel])\n",
    "            \n",
    "            # Add motor velocity on secondary axis\n",
    "            ax2_motor = ax2.twinx()\n",
    "            ax2_motor.add_collection(right_collections['Motor_Velocity'])\n",
    "            ax2_motor.set_ylabel(\"Motor Velocity\", color='slategray')\n",
    "            ax2_motor.tick_params(axis='y', labelcolor='slategray')\n",
    "            ax2_motor.autoscale()\n",
    "            \n",
    "            ax2.set_title(f'Right Turn Traces (n={len(right_turns)})')\n",
    "            ax2.set_ylabel(\"Fluorescence (z-score)\")\n",
    "            ax2.axvline(0, linestyle='--', color='black', alpha=0.7)\n",
    "            ax2.autoscale()\n",
    "        \n",
    "        # Comparison plot - Mean ± SEM\n",
    "        ax3 = axes[2]\n",
    "        \n",
    "        if not left_df.empty:\n",
    "            self.add_mean_sem_plot(ax3, left_df, z_channels, \"Left\", '--')\n",
    "        if not right_df.empty:\n",
    "            self.add_mean_sem_plot(ax3, right_df, z_channels, \"Right\", '-')\n",
    "        \n",
    "        # Add motor velocity comparison\n",
    "        ax3_motor = ax3.twinx()\n",
    "        motor_color = 'slategray'\n",
    "        \n",
    "        if not left_df.empty:\n",
    "            left_motor_grouped = left_df.groupby(\"Time (s)\")\n",
    "            time_idx = left_motor_grouped.mean().index.values\n",
    "            mean_motor = left_motor_grouped.mean()[\"Motor_Velocity\"]\n",
    "            sem_motor = left_motor_grouped.sem()[\"Motor_Velocity\"]\n",
    "            \n",
    "            ax3_motor.plot(time_idx, mean_motor, color=motor_color, linestyle='--', \n",
    "                          linewidth=1.5, label=\"Left Motor\")\n",
    "            ax3_motor.fill_between(time_idx, mean_motor - sem_motor, mean_motor + sem_motor,\n",
    "                                  color=motor_color, alpha=0.2)\n",
    "        \n",
    "        if not right_df.empty:\n",
    "            right_motor_grouped = right_df.groupby(\"Time (s)\")\n",
    "            time_idx = right_motor_grouped.mean().index.values\n",
    "            mean_motor = right_motor_grouped.mean()[\"Motor_Velocity\"]\n",
    "            sem_motor = right_motor_grouped.sem()[\"Motor_Velocity\"]\n",
    "            \n",
    "            ax3_motor.plot(time_idx, mean_motor, color=motor_color, linestyle='-',\n",
    "                          linewidth=1.5, label=\"Right Motor\")\n",
    "            ax3_motor.fill_between(time_idx, mean_motor - sem_motor, mean_motor + sem_motor,\n",
    "                                  color=motor_color, alpha=0.2)\n",
    "        \n",
    "        ax3_motor.set_ylabel(\"Motor Velocity\", color=motor_color)\n",
    "        ax3_motor.tick_params(axis='y', labelcolor=motor_color)\n",
    "        ax3_motor.axhline(0, linestyle='--', color='gray', alpha=0.5)\n",
    "        ax3_motor.legend(loc='upper right')\n",
    "        \n",
    "        ax3.axvline(0, linestyle='--', color='black', alpha=0.7)\n",
    "        ax3.set_xlabel(\"Time (s)\")\n",
    "        ax3.set_ylabel(\"Fluorescence (z-score)\")\n",
    "        ax3.set_title(\"Mean ± SEM Comparison\")\n",
    "        ax3.legend(loc='upper left')\n",
    "        \n",
    "        # Format all x-axes\n",
    "        for ax in axes:\n",
    "            ax.set_xlabel(\"Time (s)\")\n",
    "        \n",
    "        fig.suptitle(f\"{session_name} - {event_name}\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path, dpi=200, bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "        plt.ioff()\n",
    "    \n",
    "    def process_session(self, data_path: Path, data: Dict[str, Any], \n",
    "                       variables: Dict[str, Any], event_name: str = \"halt\") -> None:\n",
    "        \"\"\"\n",
    "        Process a complete session of data.\n",
    "        \n",
    "        Args:\n",
    "            data_path: Path to the data directory\n",
    "            data: Dictionary containing loaded data\n",
    "            variables: Dictionary containing analysis variables\n",
    "            event_name: Name of the event type being analyzed\n",
    "        \"\"\"\n",
    "        print(f\"\\n--------- Processing: {data_path} ---------\")\n",
    "        \n",
    "        try:\n",
    "            # Extract data components\n",
    "            df = data[\"photometry_tracking_encoder_data\"]\n",
    "            halts = variables[\"photodiode_halts\"]\n",
    "            \n",
    "            # Get session name\n",
    "            session_name = variables.get(\"session_name\")\n",
    "            if not session_name:\n",
    "                mouse_name = data.get(\"mouse_name\", \"unknown_mouse\")\n",
    "                session_name = f\"{mouse_name}_{data_path.stem}\"\n",
    "                print(f\"No session_name found, using: {session_name}\")\n",
    "            \n",
    "            print(f\"Aligning {len(halts)} events for session '{session_name}'\")\n",
    "            \n",
    "            # Process all halt events\n",
    "            aligned_data = []\n",
    "            for halt_time in halts:\n",
    "                window_data = self.process_aligned_data(df, halt_time)\n",
    "                if window_data is not None:\n",
    "                    aligned_data.append(window_data)\n",
    "            \n",
    "            if not aligned_data:\n",
    "                print(f\"No aligned data generated for {session_name}, skipping\")\n",
    "                return\n",
    "            \n",
    "            # Combine all aligned data\n",
    "            aligned_df = pd.concat(aligned_data, ignore_index=True)\n",
    "            \n",
    "            # Create output directory\n",
    "            aligned_dir = data_path.parent / \"aligned_data\"\n",
    "            aligned_dir.mkdir(exist_ok=True)\n",
    "            \n",
    "            # Save main aligned data\n",
    "            aligned_file = aligned_dir / f\"{session_name}_{event_name}_aligned.csv\"\n",
    "            aligned_df.to_csv(aligned_file, index=False, float_format='%.4f')\n",
    "            print(f\"Saved aligned data to {aligned_file}\")\n",
    "            \n",
    "            # Separate and save turn data\n",
    "            left_turns, right_turns = self.separate_turns(aligned_df)\n",
    "            self.save_turn_data(aligned_df, left_turns, right_turns, \n",
    "                              session_name, event_name, aligned_dir)\n",
    "            \n",
    "            # Create summary plot\n",
    "            print(f\"📈 Creating plots for {session_name}\")\n",
    "            summary_path = data_path.parent / f\"{session_name}_{event_name}.pdf\"\n",
    "            self.create_summary_plot(aligned_df, session_name, event_name, summary_path)\n",
    "            print(f\"Saved summary plot to {summary_path}\")\n",
    "            \n",
    "            # Create heatmaps if sufficient data\n",
    "            unique_halts = aligned_df[\"Halt Time\"].nunique()\n",
    "            if unique_halts > 1:\n",
    "                self._create_all_heatmaps(aligned_df, session_name, event_name, data_path)\n",
    "            else:\n",
    "                print(\"Insufficient data for heatmaps (need >1 event)\")\n",
    "            \n",
    "            # Cleanup\n",
    "            del aligned_data, aligned_df\n",
    "            gc.collect()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"ERROR processing {data_path}: {str(e)}\")\n",
    "            traceback.print_exc()\n",
    "            gc.collect()\n",
    "    \n",
    "    def _create_all_heatmaps(self, aligned_df: pd.DataFrame, session_name: str, \n",
    "                           event_name: str, data_path: Path) -> None:\n",
    "        \"\"\"Create all heatmaps for different channels.\"\"\"\n",
    "        heatmap_channels = ['z_470', 'z_560', 'dfF_470', 'dfF_560']\n",
    "        \n",
    "        for channel in heatmap_channels:\n",
    "            try:\n",
    "                # Create pivot table\n",
    "                pivot_data = aligned_df.pivot_table(\n",
    "                    index=\"Halt Time\", \n",
    "                    columns=\"Time (s)\", \n",
    "                    values=channel, \n",
    "                    aggfunc='first'\n",
    "                )\n",
    "                \n",
    "                # Create heatmap\n",
    "                heatmap_path = data_path.parent / f\"{session_name}_{event_name}_heatmap_{channel}.pdf\"\n",
    "                self.create_heatmap(pivot_data, session_name, event_name, channel, heatmap_path)\n",
    "                print(f\"Saved {channel} heatmap\")\n",
    "                \n",
    "                # Cleanup\n",
    "                del pivot_data\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error creating {channel} heatmap: {e}\")\n",
    "        \n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "def main(data_paths: List[Path], loaded_data: Dict, data_path_variables: Dict, \n",
    "         event_name: str = \"halt\", time_window: Tuple[float, float] = (-5, 10)):\n",
    "    \"\"\"\n",
    "    Main processing function.\n",
    "    \n",
    "    Args:\n",
    "        data_paths: List of data directory paths\n",
    "        loaded_data: Dictionary of loaded data for each path\n",
    "        data_path_variables: Dictionary of analysis variables for each path\n",
    "        event_name: Name of event type (default: \"halt\")\n",
    "        time_window: Tuple of (start, end) times relative to event in seconds\n",
    "    \"\"\"\n",
    "    # Initialize analyzer\n",
    "    analyzer = PhotometryAnalyzer(time_window)\n",
    "    \n",
    "    # Process each data path\n",
    "    for idx, data_path in enumerate(data_paths, start=1):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Processing {idx}/{len(data_paths)}: {data_path.name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        if data_path not in data_path_variables:\n",
    "            print(f\"Skipping {data_path} - no analysis data found\")\n",
    "            continue\n",
    "        \n",
    "        if data_path not in loaded_data:\n",
    "            print(f\"Skipping {data_path} - no loaded data found\")\n",
    "            continue\n",
    "        \n",
    "        # Process this session\n",
    "        analyzer.process_session(\n",
    "            data_path, \n",
    "            loaded_data[data_path], \n",
    "            data_path_variables[data_path], \n",
    "            event_name\n",
    "        )\n",
    "    \n",
    "    print(\"\\n✅ Finished processing all data paths.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de826ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_window = (time_window_start, time_window_end) \n",
    "main(data_paths, loaded_data, data_path_variables, \n",
    "     event_name=event_name, time_window=time_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda60d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASELINING\n",
    "#----------------------------------------------------\n",
    "def process_aligned_data_folders(data_dirs, baseline_window, event_name=event_name, plot_width=12, create_plots=True):\n",
    "    \"\"\"\n",
    "    Process all aligned_data folders and generate baseline plots.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data_dirs : list\n",
    "        List of Path objects pointing to your main data directories\n",
    "    baseline_window : tuple\n",
    "        Tuple of (start_time, end_time) for baseline window\n",
    "    event_name : str\n",
    "        Event name for file naming (default: \"halt\")\n",
    "    plot_width : int\n",
    "        Width of the plot in inches\n",
    "    create_plots : bool\n",
    "        Whether to create and save plots (default: True)\n",
    "    \"\"\"\n",
    "    \n",
    "    results = {\n",
    "        'processed': [],\n",
    "        'errors': [],\n",
    "        'total_folders': 0\n",
    "    }\n",
    "    \n",
    "    # Find all aligned_data folders\n",
    "    aligned_folders = []\n",
    "    for data_dir in data_dirs:\n",
    "        print(f\"Searching in: {data_dir}\")\n",
    "        # Find all aligned_data folders recursively\n",
    "        found_folders = list(data_dir.rglob(\"aligned_data\"))\n",
    "        aligned_folders.extend(found_folders)\n",
    "        print(f\"  Found {len(found_folders)} aligned_data folders\")\n",
    "    \n",
    "    results['total_folders'] = len(aligned_folders)\n",
    "    print(f\"\\nTotal aligned_data folders found: {len(aligned_folders)}\")\n",
    "    \n",
    "    for aligned_folder in aligned_folders:\n",
    "        try:\n",
    "            print(f\"\\n📁 Processing folder: {aligned_folder}\")\n",
    "            \n",
    "            # Find only the original aligned CSV files (exclude already processed baselined files and turn files)\n",
    "            all_csv_files = list(aligned_folder.glob(\"*.csv\"))\n",
    "            csv_files = [f for f in all_csv_files if not f.name.endswith('_baselined_data.csv') \n",
    "                        and not f.name.endswith('_left_turns.csv') \n",
    "                        and not f.name.endswith('_right_turns.csv')]\n",
    "            \n",
    "            if not csv_files:\n",
    "                print(f\"  ⚠️  No original aligned CSV files found in {aligned_folder}\")\n",
    "                print(f\"  Available files: {[f.name for f in all_csv_files]}\")\n",
    "                results['errors'].append({\n",
    "                    'folder': str(aligned_folder),\n",
    "                    'error': 'No original aligned CSV files found',\n",
    "                    'status': 'skipped'\n",
    "                })\n",
    "                continue\n",
    "            print(f\"  Found {len(csv_files)} aligned CSV files to process\")\n",
    "            \n",
    "            for csv_file in csv_files:\n",
    "                try:\n",
    "                    # Check if the CSV file name matches the event name\n",
    "                    if event_name not in csv_file.name:\n",
    "                        print(f\"    ⚠️ Skipping {csv_file.name} as it does not match the event name '{event_name}'\")\n",
    "                        continue\n",
    "                    \n",
    "                    print(f\"    📊 Processing: {csv_file.name}\")\n",
    "                    \n",
    "                    # Load the data\n",
    "                    aligned_df = pd.read_csv(csv_file)\n",
    "                    \n",
    "                    # Create aligned DataFrames for left and right turns\n",
    "                    # Replace '_aligned' with '_left_turns' and '_right_turns'\n",
    "                    left_turns_csv = csv_file.with_name(csv_file.stem.replace('_aligned', '_left_turns') + \".csv\")\n",
    "                    right_turns_csv = csv_file.with_name(csv_file.stem.replace('_aligned', '_right_turns') + \".csv\")\n",
    "                    \n",
    "                    left_turns_df = None\n",
    "                    right_turns_df = None\n",
    "                    \n",
    "                    if left_turns_csv.exists():\n",
    "                        print(f\"    📂 Found left turns CSV: {left_turns_csv.name}\")\n",
    "                        left_turns_df = pd.read_csv(left_turns_csv)\n",
    "                    else:\n",
    "                        print(f\"    ⚠️ Left turns CSV not found: {left_turns_csv.name}\")\n",
    "                    \n",
    "                    if right_turns_csv.exists():\n",
    "                        print(f\"    📂 Found right turns CSV: {right_turns_csv.name}\")\n",
    "                        right_turns_df = pd.read_csv(right_turns_csv)\n",
    "                    else:\n",
    "                        print(f\"    ⚠️ Right turns CSV not found: {right_turns_csv.name}\")\n",
    "                    \n",
    "                    # Clean up the mouse name (remove extra suffixes)\n",
    "                    mouse_name = csv_file.stem.replace('_aligned', '').replace('_downsampled_data_Apply halt: 2s', '').split('_')[0]\n",
    "                    # Get session name from the folder structure\n",
    "                    session_name = aligned_folder.parent.name\n",
    "                    \n",
    "                    # Check if required columns exist\n",
    "                    required_columns = [\"Time (s)\", \"Halt Time\", \"z_470\", \"z_560\", \"Motor_Velocity\", \n",
    "                                      \"Velocity_0X\", \"Velocity_0Y\", \"Photodiode_int\"]\n",
    "                    missing_columns = [col for col in required_columns if col not in aligned_df.columns]\n",
    "                    \n",
    "                    if missing_columns:\n",
    "                        print(f\"    ⚠️  Missing columns: {missing_columns}\")\n",
    "                        print(f\"    Available columns: {list(aligned_df.columns)}\")\n",
    "                        raise ValueError(f\"Missing required columns: {missing_columns}\")\n",
    "                    \n",
    "                    # Process the data and create plot\n",
    "                    fig = baseline_aligned_data_simple(\n",
    "                        aligned_df=aligned_df,\n",
    "                        left_turns_df=left_turns_df,\n",
    "                        right_turns_df=right_turns_df,\n",
    "                        baseline_window=baseline_window,\n",
    "                        mouse_name=mouse_name,\n",
    "                        session_name=session_name,\n",
    "                        event_name=event_name,\n",
    "                        output_folder=aligned_folder,\n",
    "                        csv_file=csv_file,\n",
    "                        plot_width=plot_width,\n",
    "                        create_plots=create_plots\n",
    "                    )\n",
    "                    \n",
    "                    results['processed'].append({\n",
    "                        'file': str(csv_file),\n",
    "                        'mouse_name': mouse_name,\n",
    "                        'session_name': session_name,\n",
    "                        'folder': str(aligned_folder),\n",
    "                        'status': 'success'\n",
    "                    })\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    error_info = {\n",
    "                        'file': str(csv_file),\n",
    "                        'error': str(e),\n",
    "                        'status': 'failed'\n",
    "                    }\n",
    "                    results['errors'].append(error_info)\n",
    "                    print(f\"    ❌ Error processing {csv_file.name}: {str(e)}\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            error_info = {\n",
    "                'folder': str(aligned_folder),\n",
    "                'error': str(e),\n",
    "                'status': 'failed'\n",
    "            }\n",
    "            results['errors'].append(error_info)\n",
    "            print(f\"❌ Error accessing {aligned_folder}: {str(e)}\")\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"PROCESSING SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Total aligned_data folders: {results['total_folders']}\")\n",
    "    print(f\"Successfully processed files: {len(results['processed'])}\")\n",
    "    print(f\"Errors encountered: {len(results['errors'])}\")\n",
    "    \n",
    "    if results['errors']:\n",
    "        print(f\"\\nErrors:\")\n",
    "        for error in results['errors']:\n",
    "            if 'file' in error:\n",
    "                print(f\"  - File {Path(error['file']).name}: {error['error']}\")\n",
    "            else:\n",
    "                print(f\"  - Folder {Path(error['folder']).name}: {error['error']}\")\n",
    "    \n",
    "    if results['processed']:\n",
    "        print(f\"\\nSuccessfully processed:\")\n",
    "        for proc in results['processed']:\n",
    "            print(f\"  - {proc['mouse_name']} in {Path(proc['folder']).parent.name}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def baseline_aligned_data_simple(aligned_df, left_turns_df, right_turns_df, baseline_window, mouse_name, session_name, event_name, output_folder, csv_file, plot_width=12, create_plots=True):\n",
    "    \"\"\"\n",
    "    Simple baseline correction and plotting function.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    aligned_df : pd.DataFrame\n",
    "        Main aligned data\n",
    "    left_turns_df : pd.DataFrame or None\n",
    "        Left turns data\n",
    "    right_turns_df : pd.DataFrame or None\n",
    "        Right turns data\n",
    "    baseline_window : tuple\n",
    "        Tuple of (start_time, end_time) for baseline window\n",
    "    mouse_name : str\n",
    "        Mouse name for file naming\n",
    "    session_name : str\n",
    "        Session name for file naming\n",
    "    event_name : str\n",
    "        Event name for file naming\n",
    "    output_folder : Path\n",
    "        Output folder path\n",
    "    csv_file : Path\n",
    "        Original CSV file path\n",
    "    plot_width : int\n",
    "        Width of the plot in inches\n",
    "    create_plots : bool\n",
    "        Whether to create and save plots (default: True)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"      🔄 Performing baseline correction...\")\n",
    "\n",
    "    def baseline_dataframe(df, baseline_window, mouse_name, event_name, output_folder, suffix=\"\"):\n",
    "        \"\"\"Helper function to baseline a single dataframe\"\"\"\n",
    "        # Make a copy to avoid modifying the original data\n",
    "        df_copy = df.copy()\n",
    "        \n",
    "        # Calculate baseline values\n",
    "        baseline_df = df_copy[\n",
    "            (df_copy[\"Time (s)\"] >= baseline_window[0]) & \n",
    "            (df_copy[\"Time (s)\"] <= baseline_window[1])\n",
    "        ].groupby(\"Halt Time\").mean(numeric_only=True)\n",
    "        \n",
    "        # Create baseline-corrected columns\n",
    "        for signal_name in [\"z_470\", \"z_560\", \"Motor_Velocity\", \"Velocity_0X\", \"Velocity_0Y\"]:\n",
    "            if signal_name in df_copy.columns:\n",
    "                df_copy[f\"{signal_name}_Baseline\"] = df_copy[signal_name] - df_copy[\"Halt Time\"].map(baseline_df[signal_name])\n",
    "            else:\n",
    "                print(f\"      ⚠️  Column {signal_name} not found in {suffix} data, skipping...\")\n",
    "        \n",
    "        # Define the baseline data file path\n",
    "        if suffix:\n",
    "            baseline_data_file = output_folder / f\"{mouse_name}_{event_name}_{suffix}_baselined_data.csv\"\n",
    "        else:\n",
    "            baseline_data_file = output_folder / f\"{mouse_name}_{event_name}_baselined_data.csv\"\n",
    "        \n",
    "        # Save the baseline-corrected data\n",
    "        df_copy.to_csv(baseline_data_file, index=False)\n",
    "        print(f\"      💾 Saved {suffix} baseline data to: {baseline_data_file.name}\")\n",
    "        \n",
    "        return df_copy\n",
    "\n",
    "    # ---------------- Baseline Correction ----------------\n",
    "    # Process main aligned data\n",
    "    aligned_df_baselined = baseline_dataframe(aligned_df, baseline_window, mouse_name, event_name, output_folder)\n",
    "    \n",
    "    # Process left turns data if available\n",
    "    left_turns_df_baselined = None\n",
    "    if left_turns_df is not None:\n",
    "        print(f\"      🔄 Processing left turns data...\")\n",
    "        left_turns_df_baselined = baseline_dataframe(left_turns_df, baseline_window, mouse_name, event_name, output_folder, \"left_turns\")\n",
    "    \n",
    "    # Process right turns data if available\n",
    "    right_turns_df_baselined = None\n",
    "    if right_turns_df is not None:\n",
    "        print(f\"      🔄 Processing right turns data...\")\n",
    "        right_turns_df_baselined = baseline_dataframe(right_turns_df, baseline_window, mouse_name, event_name, output_folder, \"right_turns\")\n",
    "\n",
    "    # ---------------- Mean and SEM for plotting (using main aligned data) ----------------\n",
    "    # Select only numeric columns for aggregation\n",
    "    numeric_columns = aligned_df_baselined.select_dtypes(include=['number']).columns\n",
    "    mean_baseline_df = aligned_df_baselined.groupby(\"Time (s)\")[numeric_columns].mean()\n",
    "    sem_baseline_df = aligned_df_baselined.groupby(\"Time (s)\")[numeric_columns].sem()\n",
    "\n",
    "    def get_symmetric_ylim(mean_data, sem_data):\n",
    "        max_abs_value = max(\n",
    "            abs(mean_data).max() + sem_data.max(),\n",
    "            abs(mean_data).min() - sem_data.min()\n",
    "        )\n",
    "        return (-max_abs_value, max_abs_value)\n",
    "\n",
    "    if create_plots:\n",
    "        print(f\"      📊 Creating plot...\")\n",
    "\n",
    "        # ---------------- Plotting ----------------\n",
    "        fig, ax = plt.subplots(figsize=(plot_width, 6))\n",
    "\n",
    "        # Photodiode\n",
    "        ax.plot(mean_baseline_df.index, mean_baseline_df[\"Photodiode_int\"], color='grey', alpha=0.8, linewidth=2)\n",
    "        ax.fill_between(mean_baseline_df.index,\n",
    "                        mean_baseline_df[\"Photodiode_int\"] - sem_baseline_df[\"Photodiode_int\"],\n",
    "                        mean_baseline_df[\"Photodiode_int\"] + sem_baseline_df[\"Photodiode_int\"],\n",
    "                        color='grey', alpha=0.2)\n",
    "\n",
    "        ax.set_xlabel('Time (s) relative to halt')\n",
    "        ax.set_ylabel('Photodiode', color='grey')\n",
    "        ax.set_title(f'Baselined Signals - {mouse_name} ({session_name})')\n",
    "\n",
    "        # z_470 and z_560 (Fluorescence)\n",
    "        ax2 = ax.twinx()\n",
    "        ax2.plot(mean_baseline_df.index, mean_baseline_df[\"z_470_Baseline\"], color='green', alpha=0.8, linewidth=2, label='470nm')\n",
    "        ax2.fill_between(mean_baseline_df.index,\n",
    "                         mean_baseline_df[\"z_470_Baseline\"] - sem_baseline_df[\"z_470_Baseline\"],\n",
    "                         mean_baseline_df[\"z_470_Baseline\"] + sem_baseline_df[\"z_470_Baseline\"],\n",
    "                         color='green', alpha=0.2)\n",
    "        ax2.plot(mean_baseline_df.index, mean_baseline_df[\"z_560_Baseline\"], color='red', alpha=0.8, linewidth=2, label='560nm')\n",
    "        ax2.fill_between(mean_baseline_df.index,\n",
    "                         mean_baseline_df[\"z_560_Baseline\"] - sem_baseline_df[\"z_560_Baseline\"],\n",
    "                         mean_baseline_df[\"z_560_Baseline\"] + sem_baseline_df[\"z_560_Baseline\"],\n",
    "                         color='red', alpha=0.2)\n",
    "        ax2.set_ylabel('Fluorescence (z-score)', color='green')\n",
    "        ax2.set_ylim(get_symmetric_ylim(\n",
    "            pd.concat([mean_baseline_df[\"z_470_Baseline\"], mean_baseline_df[\"z_560_Baseline\"]]),\n",
    "            pd.concat([sem_baseline_df[\"z_470_Baseline\"], sem_baseline_df[\"z_560_Baseline\"]])\n",
    "        ))\n",
    "        ax2.yaxis.label.set_color('green')\n",
    "\n",
    "        # Motor velocity\n",
    "        ax3 = ax.twinx()\n",
    "        ax3.spines['right'].set_position(('outward', 50))\n",
    "        ax3.plot(mean_baseline_df.index, mean_baseline_df[\"Motor_Velocity_Baseline\"], color='#00008B', alpha=0.8, linewidth=2)\n",
    "        ax3.fill_between(mean_baseline_df.index,\n",
    "                         mean_baseline_df[\"Motor_Velocity_Baseline\"] - sem_baseline_df[\"Motor_Velocity_Baseline\"],\n",
    "                         mean_baseline_df[\"Motor_Velocity_Baseline\"] + sem_baseline_df[\"Motor_Velocity_Baseline\"],\n",
    "                         color='#00008B', alpha=0.2)\n",
    "        ax3.set_ylabel('Motor Velocity (deg/s²)', color='#00008B')\n",
    "        ax3.set_ylim(get_symmetric_ylim(mean_baseline_df[\"Motor_Velocity_Baseline\"], sem_baseline_df[\"Motor_Velocity_Baseline\"]))\n",
    "        ax3.yaxis.label.set_color('#00008B')\n",
    "\n",
    "        # Running velocity (Velocity_0X)\n",
    "        ax4 = ax.twinx()\n",
    "        ax4.spines['right'].set_position(('outward', 100))\n",
    "        ax4.plot(mean_baseline_df.index, mean_baseline_df[\"Velocity_0X_Baseline\"] * 1000, color='orange', alpha=0.8, linewidth=2)\n",
    "        ax4.fill_between(mean_baseline_df.index,\n",
    "                         (mean_baseline_df[\"Velocity_0X_Baseline\"] - sem_baseline_df[\"Velocity_0X_Baseline\"]) * 1000,\n",
    "                         (mean_baseline_df[\"Velocity_0X_Baseline\"] + sem_baseline_df[\"Velocity_0X_Baseline\"]) * 1000,\n",
    "                         color='orange', alpha=0.2)\n",
    "        ax4.set_ylabel('Running velocity (mm/s²)', color='orange')\n",
    "        ax4.set_ylim(get_symmetric_ylim(mean_baseline_df[\"Velocity_0X_Baseline\"] * 1000, sem_baseline_df[\"Velocity_0X_Baseline\"] * 1000))\n",
    "        ax4.yaxis.label.set_color('orange')\n",
    "\n",
    "        # Turning velocity (Velocity_0Y)\n",
    "        ax5 = ax.twinx()\n",
    "        ax5.spines['right'].set_position(('outward', 150))\n",
    "        ax5.plot(mean_baseline_df.index, mean_baseline_df[\"Velocity_0Y_Baseline\"], color='#4682B4', alpha=0.8, linewidth=2)\n",
    "        ax5.fill_between(mean_baseline_df.index,\n",
    "                         mean_baseline_df[\"Velocity_0Y_Baseline\"] - sem_baseline_df[\"Velocity_0Y_Baseline\"],\n",
    "                         mean_baseline_df[\"Velocity_0Y_Baseline\"] + sem_baseline_df[\"Velocity_0Y_Baseline\"],\n",
    "                         color='#4682B4', alpha=0.2)\n",
    "        ax5.set_ylabel('Turning velocity (deg/s²)', color='#4682B4')\n",
    "        ax5.set_ylim(get_symmetric_ylim(mean_baseline_df[\"Velocity_0Y_Baseline\"], sem_baseline_df[\"Velocity_0Y_Baseline\"]))\n",
    "        ax5.yaxis.label.set_color('#4682B4')\n",
    "\n",
    "        # Add vertical line at event time (t=0)\n",
    "        ax.axvline(x=0, color='black', linestyle='--', alpha=0.5, linewidth=1)\n",
    "\n",
    "        fig.tight_layout()\n",
    "\n",
    "        figure_file = output_folder / f\"{session_name}_{event_name}_baselined.pdf\"\n",
    "\n",
    "        # Save the figure\n",
    "        fig.savefig(figure_file, format='pdf', bbox_inches='tight')\n",
    "        print(f\"      💾 Saved plot to: {figure_file.name}\")\n",
    "        plt.close(fig)\n",
    "        return fig\n",
    "    else:\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c205cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Process all aligned_data folders\n",
    "    results = process_aligned_data_folders(\n",
    "        data_dirs=data_dirs,\n",
    "        baseline_window=baseline_window,\n",
    "        event_name=event_name,\n",
    "        plot_width=plot_width,\n",
    "        create_plots=True  # Ensure this is set to True to save plots\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n🎉 Processing complete!\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "aeon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
